[
  {
    "objectID": "Hobbies.html",
    "href": "Hobbies.html",
    "title": "Addisu Amare",
    "section": "",
    "text": "Hobby\nCategory\nFrequency\nEquipment Needed\n\n\n\n\nCooking\nCreative\nDaily\nKitchen utensils, ingredients\n\n\nReading Novels\nIntellectual\nDaily\nBooks/e-reader\n\n\nFootball\nOutdoor Sport\nWeekly\nCleats, ball, sports gear\n\n\nVolleyball\nTeam Sport\nBi-weekly\nKnee pads, athletic shoes\n\n\n\n\n\n\nCooking - Creative expression and practical life skill\nReading - Mental escape and knowledge expansion\n\nFootball - Physical fitness and teamwork\nVolleyball - Social interaction and coordination"
  },
  {
    "objectID": "Hobbies.html#my-hobbies-at-a-glance",
    "href": "Hobbies.html#my-hobbies-at-a-glance",
    "title": "Addisu Amare",
    "section": "",
    "text": "Hobby\nCategory\nFrequency\nEquipment Needed\n\n\n\n\nCooking\nCreative\nDaily\nKitchen utensils, ingredients\n\n\nReading Novels\nIntellectual\nDaily\nBooks/e-reader\n\n\nFootball\nOutdoor Sport\nWeekly\nCleats, ball, sports gear\n\n\nVolleyball\nTeam Sport\nBi-weekly\nKnee pads, athletic shoes\n\n\n\n\n\n\nCooking - Creative expression and practical life skill\nReading - Mental escape and knowledge expansion\n\nFootball - Physical fitness and teamwork\nVolleyball - Social interaction and coordination"
  },
  {
    "objectID": "documents.html",
    "href": "documents.html",
    "title": "Articles & Projects",
    "section": "",
    "text": "Artificial Intelligence applications in medical imaging analysis.\n\n\nView PDF Download PDF\n\n\n\n\n\n\nFederated learning approaches with explainable AI for medical imaging.\n\n\nView PDF Download PDF\n\n\n\n\n\n\n\n\nAcademic projects and research from undergraduate studies.\n\n\nView PDF Download PDF\n\n\n\n\n\n\nComputer vision and AI-based fire detection system.\n\n\nView PDF Download PDF\n\n\n\n\n\n\n\nOption 1: View in Browser - Click ‚ÄúView PDF‚Äù to open directly in your browser\n\n\nOption 2: Download - Click ‚ÄúDownload PDF‚Äù to save to your device\n\n\nOption 3: GitHub View - All files are accessible via GitHub repository\n\n\n\nNote: All PDFs are hosted on GitHub. For best viewing experience, use Chrome, Firefox, or Edge browsers."
  },
  {
    "objectID": "documents.html#articles",
    "href": "documents.html#articles",
    "title": "Articles & Projects",
    "section": "",
    "text": "Artificial Intelligence applications in medical imaging analysis.\n\n\nView PDF Download PDF\n\n\n\n\n\n\nFederated learning approaches with explainable AI for medical imaging.\n\n\nView PDF Download PDF\n\n\n\n\n\n\n\n\nAcademic projects and research from undergraduate studies.\n\n\nView PDF Download PDF\n\n\n\n\n\n\nComputer vision and AI-based fire detection system.\n\n\nView PDF Download PDF\n\n\n\n\n\n\n\nOption 1: View in Browser - Click ‚ÄúView PDF‚Äù to open directly in your browser\n\n\nOption 2: Download - Click ‚ÄúDownload PDF‚Äù to save to your device\n\n\nOption 3: GitHub View - All files are accessible via GitHub repository\n\n\n\nNote: All PDFs are hosted on GitHub. For best viewing experience, use Chrome, Firefox, or Edge browsers."
  },
  {
    "objectID": "documents.html#projects",
    "href": "documents.html#projects",
    "title": "Articles & Projects",
    "section": "",
    "text": "Academic projects and research from undergraduate studies.\n\n\nView PDF Download PDF\n\n\n\n\n\n\nComputer vision and AI-based fire detection system.\n\n\nView PDF Download PDF\n\n\n\n\n\n\n\nOption 1: View in Browser - Click ‚ÄúView PDF‚Äù to open directly in your browser\n\n\nOption 2: Download - Click ‚ÄúDownload PDF‚Äù to save to your device\n\n\nOption 3: GitHub View - All files are accessible via GitHub repository\n\n\n\nNote: All PDFs are hosted on GitHub. For best viewing experience, use Chrome, Firefox, or Edge browsers."
  },
  {
    "objectID": "documents.html#how-to-view",
    "href": "documents.html#how-to-view",
    "title": "Articles & Projects",
    "section": "",
    "text": "Option 1: View in Browser - Click ‚ÄúView PDF‚Äù to open directly in your browser\n\n\nOption 2: Download - Click ‚ÄúDownload PDF‚Äù to save to your device\n\n\nOption 3: GitHub View - All files are accessible via GitHub repository\n\n\n\nNote: All PDFs are hosted on GitHub. For best viewing experience, use Chrome, Firefox, or Edge browsers."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Introduction: The AI Revolution in Healthcare",
    "section": "",
    "text": "The AI Revolution in Healthcare\n\n\n\nThe Future of Medical Imaging\nMedical imaging is evolving. Artificial intelligence is no longer a simple tool; in fact, it has become part and parcel of diagnostic workflows. There is a growing need for intelligent processing of medical images, which represent approximately 90% of all healthcare data.\n\n\nThe Problem\nRadiologists: Are experiencing a 30-40% increase in workload each year\nImaging technology: High-resolution, 3D and 4D images are becoming more common\nDiagnostic Accuracy: Error rates due to human interpretation can be as high as 30% in some instances\n\n\nHow AI solves these problems\nTwo significant advances have been made with respect to using AI in medical imaging - Convolutional Neural Networks (CNN) have become the standard when it comes to the majority of medical imaging techniques and algorithms.\n\n\n\nüéØ 90%\n\n\nHealthcare data is medical images\n\n\n\n\n‚è±Ô∏è 70%\n\n\nReduction in screening time\n\n\n\n\nüí∞ $4.5B\n\n\nMarket size by 2028\n\n\n\n\nüìà 34%\n\n\nAnnual growth rate\n\n\n\n\n\nWhat You‚Äôll Learn in This Series\n-This comprehensive series explores:\n-CNN applications across various medical specialties\nViT breakthroughs in complex imaging tasks\nReal-world implementations and case studies\nPerformance comparisons and benchmarks\nFuture trends and ethical considerations\nNavigation Guide Next: CNN Overview\nOr jump to: Cancer Detection | Neurology | Ophthalmology"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Below is the course I have taught at the University of Skoltech,Russia during independent period.\n\n\n\nI30202437 Python for Data Science Pseudo-code. Program design and structure. Flow control. Iteration. Lists (arrays). Functions. File I/O. Classes, objects, methods, and libraries."
  },
  {
    "objectID": "teaching.html#courses-taught",
    "href": "teaching.html#courses-taught",
    "title": "Teaching",
    "section": "",
    "text": "Below is the course I have taught at the University of Skoltech,Russia during independent period.\n\n\n\nI30202437 Python for Data Science Pseudo-code. Program design and structure. Flow control. Iteration. Lists (arrays). Functions. File I/O. Classes, objects, methods, and libraries."
  },
  {
    "objectID": "teaching.html#teaching-philosophy",
    "href": "teaching.html#teaching-philosophy",
    "title": "Teaching",
    "section": "Teaching Philosophy",
    "text": "Teaching Philosophy\nI‚Äôve learned that great teaching isn‚Äôt just about delivering content‚Äîit‚Äôs about creating the right conditions for learning to happen. My approach rests on three core pillars: sparking genuine motivation, building a supportive community, and staying flexibly responsive to my students‚Äô needs. When I get these right, I see curiosity catch fire, collaboration flourish, and obstacles to learning start to fall away"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Addisu A. Zena, Btech",
    "section": "",
    "text": "Bio\nAddisu is a technology and biomedical enthusiast who holds a Bachelor‚Äôs degree in Electronics & Communication Engineering (ECE) and Biomedical Engineering from Vellore Institute of Technology (Class of 2022).\nWith a deep interest in programming, data science, and human anatomy and physiology, he continuously works on projects at the intersection of engineering, machine learning, and biomedical research. He is always expanding his skill set through hands-on learning and new technologies.\n###Skills: - Python | MATLAB | LabVIEW\n- Data Science | Machine Learning\n\n\nContact\n\nemail: 0941813057estifanos@gmail.com\nLinkedIn: profile -tiktok:@tech_guy47"
  },
  {
    "objectID": "projects/Varietional_Auto_encoder.html",
    "href": "projects/Varietional_Auto_encoder.html",
    "title": "Addisu Amare",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n%env CUDA_VISIBLE_DEVICES=0\n\nenv: CUDA_VISIBLE_DEVICES=0\n\n\n\nfrom PIL import Image\nfrom pathlib import Path\nfrom matplotlib import pyplot as plt\n\nimport wandb\nimport numpy as np\nimport torch as t\nimport torchvision\nfrom tqdm.auto import tqdm, trange\n\n\nclass MNIST(t.utils.data.Dataset):\n    def __init__(self, data_root: Path):\n        imgs = []\n        for img, _ in tqdm(torchvision.datasets.MNIST(data_root, download=True)):\n            img = img.resize((32, 32), Image.Resampling.BILINEAR)\n            img = np.array(img)[None, :, :]\n            img = img / 255\n            imgs.append(img)\n            \n        self.imgs = t.tensor(np.stack(imgs), dtype=t.float)\n        \n    def __len__(self):\n        return self.imgs.shape[0]\n    \n    def __getitem__(self, index):\n        return self.imgs[index]\n\n    def shuffled_batch_iterate(self, batch_size: int):\n        while True:\n            inds = t.randint(0, self.imgs.shape[0], [batch_size])\n            yield self.imgs[inds]\n\n\nZ_DIM = 2\n\n\ndef conv_block(in_features: int, out_features: int, kernel: int = 3, stride: int = 1, padding: int = 1):\n    return t.nn.Sequential(\n        t.nn.Conv2d(in_features, out_features, kernel, stride, padding),\n        t.nn.BatchNorm2d(out_features),\n        t.nn.ReLU(),\n    )\n\n\nclass Upscale(t.nn.Module):\n    def __init__(self, in_features: int, out_features: int):\n        super().__init__()\n        self.conv = conv_block(in_features, 4 * out_features)\n        \n    def forward(self, x: t.Tensor) -&gt; t.Tensor:\n        x = self.conv(x)\n        b, c, h, w = x.shape\n        x = x.reshape(b, c // 4, 2, 2, h, w)\n        x = t.moveaxis(x, (0, 1, 4, 2, 5, 3), (0, 1, 2, 3, 4, 5))\n        x = x.reshape(b, c // 4, h * 2, w * 2)\n        \n        return x\n\n    \nclass Squeeze(t.nn.Module):\n    def forward(self, x):\n        return x.squeeze(2).squeeze(2)\n    \n    \nclass Unsqueeze(t.nn.Module):\n    def forward(self, x):\n        return x[:, :, None, None]\n    \n    \ndef downscale_block(features: int, out_features: int):\n    return t.nn.Sequential(\n        conv_block(features, features),\n        conv_block(features, features),\n        conv_block(features, out_features, 2, 2, 0)\n    )\n\n    \ndef upscale_block(in_features: int, features: int):\n    return t.nn.Sequential(\n        Upscale(in_features, features),\n        conv_block(features, features),\n        conv_block(features, features),\n    )\n\n\ndef get_encoder():\n    return t.nn.Sequential(\n        conv_block(1, 8),\n        downscale_block(8, 16),\n        downscale_block(16, 32),\n        downscale_block(32, 64),\n        downscale_block(64, 64),\n        downscale_block(64, 128),\n        Squeeze(),\n        t.nn.Linear(128, Z_DIM * 2),\n    )\n\n\ndef get_decoder():\n    return t.nn.Sequential(\n        Unsqueeze(),\n        upscale_block(Z_DIM, 128),\n        upscale_block(128, 64),\n        upscale_block(64, 32),\n        upscale_block(32, 16),\n        upscale_block(16, 8),\n        t.nn.Conv2d(8, 1, 3, padding=1),\n    )\n\n\ndef reparameterize(mean, log_std):\n    return mean + t.randn_like(mean) * t.exp(log_std)\n\n\ndef kl(mean, log_std):\n    return ((t.exp(2 * log_std) + mean * mean) / 2 - 0.5 - log_std).sum(axis=1)\n\n\nclass VAE(t.nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        \n        self.encoder = encoder\n        self.decoder = decoder\n        \n    def forward(self, imgs):\n        z_params = self.encoder(imgs)\n        z_mean, z_log_std = t.split(z_params, [Z_DIM, Z_DIM], dim=1)\n\n        z = reparameterize(z_mean, z_log_std)\n        z_kl = kl(z_mean, z_log_std)\n\n        decoded_imgs = self.decoder(z)\n        log_p_x_given_z = -((imgs - decoded_imgs)**2).sum([1, 2, 3])\n\n        loss = -log_p_x_given_z + z_kl \n\n        return {'loss': loss.mean(), 'log p(x|z)': log_p_x_given_z.mean(), 'kl': z_kl.mean()}\n\n\nBATCH_SIZE = 128\n\n# def run():\n#     with wandb.init(project='autoencoder'):\n\nmodel = VAE(get_encoder(), get_decoder()).cuda()\nprint('model crated')\n\noptimizer = t.optim.AdamW(model.parameters())\nprint('optimzer created')\n\ndata = MNIST('./')\nprint('model created')\n\nmodel.eval()\nprint(data[0].shape)\nmodel(data[0][None].cuda())\nmodel.train()\n\nwandb.init(project='autoencoder')\nfor step, imgs in enumerate(tqdm(data.shuffled_batch_iterate(BATCH_SIZE))):\n    imgs = imgs.cuda()\n    values = model(imgs)\n\n    wandb.log(values, step=step)\n\n    values['loss'].backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n    if step % 50 == 0:\n        model.eval()\n\n        with t.no_grad():\n            z = t.randn(16, Z_DIM, device='cuda')\n            img = model.decoder(z)\n            img = img.detach().cpu().numpy()\n            img = img.transpose(0, 2, 3, 1)\n            img = img.reshape(4, 4, 32, 32, 1)\n            img = img.transpose(0, 2, 1, 3, 4)\n            img = img.reshape(4 * 32, 4 * 32, 1)\n            plt.imshow(img)\n            plt.show()\n\n        model.train()\n\n    if step &gt; 3000:\n        break\n\n# run()\n\nmodel crated\noptimzer created\n\n\n\n\n\nmodel created\ntorch.Size([1, 32, 32])\n\n\n\nwandb: Currently logged in as: pg_lolo. Use `wandb login --relogin` to force relogin\n\n\n\n\nwandb version 0.15.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade\n\n\nTracking run with wandb version 0.13.8\n\n\nRun data is saved locally in /data/code/autoencoder/wandb/run-20230428_085829-jnwtvj8l\n\n\nSyncing run pious-salad-37 to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/pg_lolo/autoencoder\n\n\n View run at https://wandb.ai/pg_lolo/autoencoder/runs/jnwtvj8l\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel.eval()\n\nstart = -2 * t.ones(2)\nend = 2 * t.ones(2)\n\npoints = t.linspace(0, 1, 100)[:, None]\npoints = end[None, :] * points + start[None, :] * (1 - points)\n\nimgs = model.decoder(points.cuda()).detach().cpu().numpy()\n\nfor img in imgs:\n    plt.imshow(img[0])\n    plt.show()"
  },
  {
    "objectID": "projects/project-example.html",
    "href": "projects/project-example.html",
    "title": "Project Example",
    "section": "",
    "text": "This project focuses on [briefly describe the project, its goals, and significance].\n\n\n\nObjective 1: [Describe the first objective]\nObjective 2: [Describe the second objective]\n\n\n\n\n[Provide a brief overview of the methods used in the project, including any specific techniques or tools.]\n\n\n\n[Summarize the outcomes of the project, including any findings or results.]\n\n\n\n[Discuss any potential future work or next steps related to the project.]\n\n\n\n[Include any references or citations relevant to the project, if applicable.]"
  },
  {
    "objectID": "projects/project-example.html#objectives",
    "href": "projects/project-example.html#objectives",
    "title": "Project Example",
    "section": "",
    "text": "Objective 1: [Describe the first objective]\nObjective 2: [Describe the second objective]"
  },
  {
    "objectID": "projects/project-example.html#methodology",
    "href": "projects/project-example.html#methodology",
    "title": "Project Example",
    "section": "",
    "text": "[Provide a brief overview of the methods used in the project, including any specific techniques or tools.]"
  },
  {
    "objectID": "projects/project-example.html#outcomes",
    "href": "projects/project-example.html#outcomes",
    "title": "Project Example",
    "section": "",
    "text": "[Summarize the outcomes of the project, including any findings or results.]"
  },
  {
    "objectID": "projects/project-example.html#future-work",
    "href": "projects/project-example.html#future-work",
    "title": "Project Example",
    "section": "",
    "text": "[Discuss any potential future work or next steps related to the project.]"
  },
  {
    "objectID": "projects/project-example.html#references",
    "href": "projects/project-example.html#references",
    "title": "Project Example",
    "section": "",
    "text": "[Include any references or citations relevant to the project, if applicable.]"
  },
  {
    "objectID": "projects/unet_with_wiener_filter/unet_weiner_filter.html",
    "href": "projects/unet_with_wiener_filter/unet_weiner_filter.html",
    "title": "Wiener Filter + UNet",
    "section": "",
    "text": "# Import some libraries\n\nimport numpy as np\nfrom skimage import color, data, restoration\nimport matplotlib.pyplot as plt\nimport torch\nimport utils\nimport torch.nn as nn\nfrom networks import UNet\nimport math\nimport os\nfrom skimage import io\nimport skimage\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\ndef show_images(im1, im1_title, im2, im2_title, im3, im3_title, font):\n    fig, (image1, image2, image3) = plt.subplots(1, 3, figsize=(15, 50))\n    image1.imshow(im1, cmap='gray')\n    image1.set_title(im1_title, fontsize=font)\n    image1.set_axis_off()\n    image2.imshow(im2, cmap='gray')\n    image2.set_title(im2_title, fontsize=font)\n    image2.set_axis_off()\n    image3.imshow(im3, cmap='gray')\n    image3.set_title(im3_title, fontsize=font)\n    image3.set_axis_off()\n    fig.subplots_adjust(wspace=0.02, hspace=0.2,\n                                top=0.9, bottom=0.05, left=0, right=1)\n    fig.show()\n\n\nLoad the data\n\n#Load the target image\nimage = io.imread('./image.tif')\n\n#Load the blurred and distorted images\nblurred = io.imread('./blurred.tif')\ndistorted = io.imread('./distorted.tif')\n\n#Load the kernel\npsf = io.imread('./PSF.tif')\n\n\nshow_images(image, 'Original image', blurred, 'Blurred image',\\\n           distorted, 'Blurred and noisy image', font=18)\n\n\n\n\n\n\n\n\nWe know, that the solution is described as follows:\n\\(\\hat{\\mathbf{x}} = \\arg\\min_\\mathbf{x}\\underbrace{\\frac{1}{2}\\|\\mathbf{y}-\\mathbf{K} \\mathbf{x}\\|_{2}^{2}+\\lambda r(\\mathbf{x})}_{\\mathbf{J}(\\mathbf{x})}\\), where \\(\\mathbf{J}\\) is the objective function.\nAccording to the gradient descent iterative scheme,\n\\(\\hat{\\mathbf{x}}_{k+1}=\\hat{\\mathbf{x}}_{k}-\\beta \\nabla \\mathbf{J}(\\mathbf{x})\\).\nSolution is described with the iterative gradient descent equation:\n\\(\\hat{\\mathbf{x}}_{k+1} = \\hat{\\mathbf{x}}_{k} - \\beta\\left[\\mathbf{K}^\\top(\\mathbf{K}\\hat{\\mathbf{x}}_{k} - \\mathbf{y}) + e^\\alpha f^{CNN}(\\hat{\\mathbf{x}}_{k})\\right]\\), and here \\(\\lambda = e^\\alpha\\) and \\(r(\\mathbf{x}) = f^{CNN}(\\hat{\\mathbf{x}})\\).\n\n# Anscombe transform to transform Poissonian data into Gaussian\n#https://en.wikipedia.org/wiki/Anscombe_transform\n\ndef anscombe(x):\n    '''\n    Compute the anscombe variance stabilizing transform.\n      the input   x   is noisy Poisson-distributed data\n      the output  fx  has variance approximately equal to 1.\n    Reference: Anscombe, F. J. (1948), \"The transformation of Poisson,\n    binomial and negative-binomial data\", Biometrika 35 (3-4): 246-254\n    '''\n    return 2.0*torch.sqrt(x + 3.0/8.0)\n\n# Exact unbiased Anscombe transform to transform Gaussian data back into Poissonian\ndef exact_unbiased(z):\n    return (1.0 / 4.0 * z.pow(2) +\n            (1.0/4.0) * math.sqrt(3.0/2.0) * z.pow(-1) -\n            (11.0/8.0) * z.pow(-2) +\n            (5.0/8.0) * math.sqrt(3.0/2.0) * z.pow(-3) - (1.0 / 8.0))\n\nclass WienerUNet(torch.nn.Module):\n\n    def __init__(self):\n        '''\n        Deconvolution function for a batch of images. Although the regularization\n        term does not have a shape of Tikhonov regularizer, with a slight abuse of notations\n        the function is called WienerUNet.\n\n        The function is built upon the iterative gradient descent scheme:\n\n        x_k+1 = x_k - lamb[K^T(Kx_k - y) + exp(alpha)*reg(x_k)]\n\n        Initial parameters are:\n        regularizer: a neural network to parametrize the prior on each iteration x_k.\n        alpha: power of the trade-off coefficient.\n        lamb: step of the gradient descent algorithm.\n        '''\n        super(WienerUNet, self).__init__()\n        self.regularizer = UNet(mode='instance')\n        self.alpha = nn.Parameter(torch.FloatTensor([0.0]))\n        self.lamb = nn.Parameter(torch.FloatTensor([0.3]))\n\n    def forward(self, x, y, ker):\n        '''\n        Function that performs one iteration of the gradient descent scheme of the deconvolution algorithm.\n\n        :param x: (torch.(cuda.)Tensor) Image, restored with the previous iteration of the gradient descent scheme, B x C x H x W\n        :param y: (torch.(cuda.)Tensor) Input blurred and noisy image, B x C x H x W\n        :param ker: (torch.(cuda.)Tensor) Blurring kernel, B x C x H_k x W_k\n        :return: (torch.(cuda.)Tensor) Restored image, B x C x H x W\n        '''\n        \n        #Calculate Kx_k\n        x_filtered = utils.imfilter2D_SpatialDomain(x, ker, padType='symmetric', mode=\"conv\")\n        Kx_y = x_filtered - y\n\n        #Calculate K^T(Kx_k - y)\n        y_filtered = utils.imfilter_transpose2D_SpatialDomain(Kx_y, ker,\n                                                              padType='symmetric', mode=\"conv\")\n        \n        #Calculate exp(alpha)*reg(x_k)\n        regul = torch.exp(self.alpha) * self.regularizer(x)\n\n        brackets = y_filtered + regul\n        out = x - self.lamb * brackets\n\n        return out\n    \nclass WienerFilter_UNet(nn.Module):\n    '''\n    Module that uses UNet to predict individual gradient of a regularizer for each input image and then\n    applies gradient descent scheme with predicted gradient of a regularizers per-image.\n    '''\n    def __init__(self):\n\n        super(WienerFilter_UNet, self).__init__()\n        self.function = WienerUNet()\n    \n    #Perform gradient descent iterations\n    def forward(self, y, ker, n_iter):\n        output = y.clone()\n\n        for i in range(n_iter):\n            output = self.function(output, y, ker)\n\n        return output\n\n\n#Let's transform our numpy data into pytorch data\nx = torch.Tensor(distorted[None, None])\nker = torch.Tensor(psf[None, None])\n\n#Define the model\nmodel = WienerFilter_UNet()\n\n#Load the pretrained weights\nstate_dict = torch.load(os.path.join('./', 'WF_UNet_poisson'))\nstate_dict = state_dict['model_state_dict']\nfrom collections import OrderedDict\nnew_state_dict = OrderedDict()\nfor k, v in state_dict.items():\n    name = k[7:]  # remove `module.`\n    new_state_dict[name] = v\n# load params\nmodel.load_state_dict(new_state_dict)\nmodel.eval()\n\nWienerFilter_UNet(\n  (function): WienerUNet(\n    (regularizer): UNet(\n      (inc): inconv(\n        (conv): double_conv(\n          (conv): Sequential(\n            (0): Conv2d(1, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (1): InstanceNorm2d(12, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n            (2): ReLU(inplace=True)\n            (3): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (4): InstanceNorm2d(12, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n            (5): ReLU(inplace=True)\n          )\n        )\n      )\n      (down1): down(\n        (mpconv): Sequential(\n          (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n          (1): double_conv(\n            (conv): Sequential(\n              (0): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (1): InstanceNorm2d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n              (2): ReLU(inplace=True)\n              (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (4): InstanceNorm2d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n              (5): ReLU(inplace=True)\n            )\n          )\n        )\n      )\n      (down2): down(\n        (mpconv): Sequential(\n          (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n          (1): double_conv(\n            (conv): Sequential(\n              (0): Conv2d(24, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (1): InstanceNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n              (2): ReLU(inplace=True)\n              (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (4): InstanceNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n              (5): ReLU(inplace=True)\n            )\n          )\n        )\n      )\n      (down3): down(\n        (mpconv): Sequential(\n          (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n          (1): double_conv(\n            (conv): Sequential(\n              (0): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (1): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n              (2): ReLU(inplace=True)\n              (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (4): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n              (5): ReLU(inplace=True)\n            )\n          )\n        )\n      )\n      (down4): down(\n        (mpconv): Sequential(\n          (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n          (1): double_conv(\n            (conv): Sequential(\n              (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (1): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n              (2): ReLU(inplace=True)\n              (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (4): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n              (5): ReLU(inplace=True)\n            )\n          )\n        )\n      )\n      (up1): up(\n        (up): Upsample(scale_factor=2.0, mode=bilinear)\n        (conv): double_conv(\n          (conv): Sequential(\n            (0): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (1): InstanceNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n            (2): ReLU(inplace=True)\n            (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (4): InstanceNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n            (5): ReLU(inplace=True)\n          )\n        )\n      )\n      (up2): up(\n        (up): Upsample(scale_factor=2.0, mode=bilinear)\n        (conv): double_conv(\n          (conv): Sequential(\n            (0): Conv2d(96, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (1): InstanceNorm2d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n            (2): ReLU(inplace=True)\n            (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (4): InstanceNorm2d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n            (5): ReLU(inplace=True)\n          )\n        )\n      )\n      (up3): up(\n        (up): Upsample(scale_factor=2.0, mode=bilinear)\n        (conv): double_conv(\n          (conv): Sequential(\n            (0): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (1): InstanceNorm2d(12, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n            (2): ReLU(inplace=True)\n            (3): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (4): InstanceNorm2d(12, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n            (5): ReLU(inplace=True)\n          )\n        )\n      )\n      (up4): up(\n        (up): Upsample(scale_factor=2.0, mode=bilinear)\n        (conv): double_conv(\n          (conv): Sequential(\n            (0): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (1): InstanceNorm2d(12, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n            (2): ReLU(inplace=True)\n            (3): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (4): InstanceNorm2d(12, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n            (5): ReLU(inplace=True)\n          )\n        )\n      )\n      (outc): outconv(\n        (conv): Conv2d(12, 1, kernel_size=(1, 1), stride=(1, 1))\n      )\n    )\n  )\n)\n\n\n\n#Perform Anscombe transform\nx = anscombe(x)\n\n#Calculate output\nout = model(x, ker, 10)\n\n#Perform inverse Anscombe transform\nout = exact_unbiased(out)\n\n#Some post-processing of data\nout = out/image.max()\nimage = image/image.max()\n\n\nshow_images(image, 'Original image', distorted, 'Blurred image',\\\n           out[0][0].detach().cpu().numpy().clip(0,1), 'Restored with WF-UNet', font=18)"
  },
  {
    "objectID": "projects/langevien.html",
    "href": "projects/langevien.html",
    "title": "Score Estimations, Langevin Generative models and NCSN",
    "section": "",
    "text": "By Addisu Amare\n\\(\\textbf{Date}:\\) 08.11.24\nimport torch\nfrom torch.distributions import MultivariateNormal, Normal\nimport numpy as np\nimport tqdm\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\n%matplotlib inline"
  },
  {
    "objectID": "projects/langevien.html#teachers---ground-truth-data",
    "href": "projects/langevien.html#teachers---ground-truth-data",
    "title": "Score Estimations, Langevin Generative models and NCSN",
    "section": "Teachers - Ground-Truth data",
    "text": "Teachers - Ground-Truth data\nThe \\(\\textbf{Teacher}\\) is the class of the density distribution, which we want to approximate by neural-network-based approaches. This class has the following methods:\n\nsample(sampling from the density distribution)\nlog_prob(calculation of the logarithm of the distribution)\n\nUndoubtedly, in order to calculate the ground-truth logarithm of the distribution we define the distribution by such way to analytically compute the gradient of the logartihm. Thus, one can take the Teacher, which is the ground-truth distribution thrrough the notebook, as follows:\n\\[\\mathbb{P} = \\frac{1}{5}\\mathcal{N}((-5,-5),I) + \\frac{4}{5}\\mathcal{N}((5,5),I)\\]\n\nclass GMMDist(object):\n    def __init__(self, dim):\n        self.mix_probs = torch.tensor([0.8, 0.2])\n        self.means = torch.stack([5 * torch.ones(dim), -torch.ones(dim) * 5], dim=0)\n        self.sigma = 1\n        self.std = torch.stack([torch.ones(dim) * self.sigma for i in range(len(self.mix_probs))], dim=0)\n\n    def sample(self, n):\n        \"\"\"\n        n - int\n        \"\"\"\n        n = torch.Size([n])[0]\n        mix_idx = torch.multinomial(self.mix_probs, n, replacement=True)\n        means = self.means[mix_idx]\n        stds = self.std[mix_idx]\n        return torch.randn_like(means) * stds + means\n\n    def log_prob(self, samples):\n        \"\"\"\n        samples - torch.Size([B,N])\n        \"\"\"\n        logps = []\n        for i in range(len(self.mix_probs)):\n            logps.append((-((samples - self.means[i]) ** 2).sum(dim=-1) / (2 * self.sigma ** 2) - 0.5 * np.log(\n                2 * np.pi * self.sigma ** 2)) + self.mix_probs[i].log())\n        logp = torch.logsumexp(torch.stack(logps, dim=0), dim=0)\n        return logp\n\n\ndef plot_teachers(teacher, num_samples):\n    \"\"\"\n    num_samples - int\n\n    \"\"\"\n    plt.figure(figsize=(4,4),dpi=150 )\n    samples = teacher.sample(num_samples)\n    plt.scatter(samples[:,0],samples[:,1],s=10,edgecolor='black')\n    plt.grid()\n\n\nDIM = 2\nNUM_SAMPLES_PLOT = 2000\nteacher =  GMMDist(DIM)\nplot_teachers(teacher,  NUM_SAMPLES_PLOT)\n\n\n\n\n\n\n\n\n\ndef data_score(x):\n    x = x.detach()\n    x.requires_grad_(True)\n    y = teacher.log_prob(x).sum()\n    return torch.autograd.grad(y, x)[0]\n\n\ndef toy_net(hiddens):\n    model = []\n    for inp,outp in zip(hiddens[:-1],hiddens[1:]):\n        model.append(torch.nn.Linear(inp,outp,bias=True))\n        model.append(torch.nn.ReLU())\n    model.pop()\n    return torch.nn.Sequential(*model)"
  },
  {
    "objectID": "projects/langevien.html#score-based-genereative-modeling",
    "href": "projects/langevien.html#score-based-genereative-modeling",
    "title": "Score Estimations, Langevin Generative models and NCSN",
    "section": "1. Score-based genereative modeling",
    "text": "1. Score-based genereative modeling\n\ndef plot_loss_quiever(model,teacher, label = None):\n\n    grid_size = 20\n    left_bound=-3\n    right_bound=3\n    mesh = []\n    x = np.linspace(left_bound, right_bound, grid_size)\n    y = np.linspace(left_bound, right_bound, grid_size)\n    for i in x:\n        for j in y:\n            mesh.append(np.asarray([i, j]))\n\n    mesh = np.stack(mesh, axis=0)\n    mesh = torch.from_numpy(mesh).float()\n\n    if label == \"energy\":\n        mesh.requires_grad = True\n        scores = model(mesh)\n        scores = torch.autograd.grad(scores.sum(),mesh,create_graph=True)[0]\n    else:\n        scores = model(mesh.detach())\n\n    mesh = mesh.detach().numpy()\n    scores = scores.detach().numpy()\n    fig,ax = plt.subplots(1,2,figsize=(16,8),sharex=True,sharey=True)\n\n    ax[0].grid(False)\n    ax[0].quiver(mesh[:, 0], mesh[:, 1], scores[:, 0], scores[:, 1], width=0.005)\n    ax[0].set_title('Estimated scores', fontsize=16)\n    ax[0].axis('square')\n\n    scores = data_score(torch.from_numpy(mesh))\n    scores = scores.detach().numpy()\n\n    ax[1].grid(False)\n    ax[1].quiver(mesh[:, 0], mesh[:, 1], scores[:, 0], scores[:, 1], width=0.005)\n    ax[1].set_title('Data scores', fontsize=16)\n    ax[1].axis('square')\n\n\n1.1 Fischer divergence\n\\(\\textbf{Notation}:\\)\n\n\\(x \\in \\mathbb{R}^{D}, \\quad x \\sim \\mathbb{P}(\\cdot)\\)\nThe model for approximation of \\(\\mathbb{P}\\) is \\(\\mathbb{\\hat{P}}(x, \\theta) = \\frac{1}{Z(\\theta)}\\mathbb{Q}(x,\\theta)\\), unnormalized density\nDesires: One would like to find out such parameters to apprroximate the ground-truth distribution better.\n\\(\\mathbb{\\hat{P}}(x, \\theta)\\) is reffered to as Energy-based models.\n\nConsidering the gradient of the approximation:\n\\[ \\nabla_{x} \\log \\mathbb{\\hat{P}}(x,\\theta) = \\nabla_{x} \\log \\mathbb{Q}(x,\\theta) - \\nabla_{x} \\log Z(\\theta) = \\nabla_{x} \\log \\mathbb{Q}(x,\\theta) - 0 \\]\nWe see, that in order to estimate the gradient of logarithm of approximate distribution, we need in the gradient of unnormalized density \\(\\mathbb{Q}(\\cdot, \\theta)\\). Such gradient of unnormalized density is called \\(\\textbf{Score function}\\) and is denoted as \\(\\psi(x,\\theta)\\).\n\\(\\textbf{The main goal of notebook}:\\)\nWe would like to estimate the gradient of logarithm of the ground-truth density \\(\\mathbb{P}\\) by the score function.\nUndoubtedly, when we have the access to the ground-truth grradient of logarithm of density, we can consider easy regression problem between \\(\\mathbb{Q}\\) and \\(\\mathbb{P}\\). This easy regression prroblem is reffered to as \\(\\textbf{Fischer divergence}\\).\nLet \\(p(x)\\) and \\(q(x)\\) are ground-truth distribution of data and unnormalized approximate distribution. The both functions are scalar functios. \\[ F(q||p)=\\frac{1}{2}\\int || - \\nabla_{x}\\log q(x) + \\nabla_{x} \\log p(x) ||_{2}^{2}dp(x)  \\]\n\n\n\nChessUrl\n\n\n\\(\\textbf{Code}\\) for the Fischer divergence.\n\ndef fischer_divergence(energy_net, data, teacher):\n\n    \"\"\"\n    energy_net - torch.nn.Module\n    teacher    - object\n    data.      - torch.Size([B,N])\n    \"\"\"\n\n    data.requires_grad = True\n    logq = -energy_net(data) # torch.Size([B,1])\n    logp = teacher.log_prob(data)# torch.Size([B,1])\n    q_score = torch.autograd.grad(logq.sum(), data,\n                                   create_graph=True,retain_graph=True)[0]\n    p_score = torch.autograd.grad(logp.sum(), data,\n                                 create_graph=True,retain_graph=True)[0]\n    return 0.5*torch.mean(torch.norm((q_score + p_score)**2,dim=-1))\n\n\nHIDDENS = [DIM,64,128,256,128,64,1]\nmodel = toy_net(HIDDENS)\nopt = torch.optim.Adam(model.parameters(),lr=1e-5)\nbatch_size=2048\niterations=5000\nlosses = []\nfor _ in tqdm.tqdm(range(iterations)):\n    opt.zero_grad()\n    samples = teacher.sample(batch_size)\n    loss = fischer_divergence(model,samples,teacher)\n    loss.backward()\n    opt.step()\n    losses.append(loss.item())\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [03:37&lt;00:00, 22.95it/s]\n\n\n\nplt.plot(losses);\n\n\n\n\n\n\n\n\n\nplot_loss_quiever(model, teacher, label='energy')\n\n\n\n\n\n\n\n\n\n\n1.2 Score Matching and Estimation.\n\\(\\textbf{Importantly}:\\)\nWhen we talk \\(\\textbf{Score Estimation}\\), it means, that we solve the regression problem between \\(\\psi(x,\\theta)\\), where \\(\\psi(x,\\theta)\\) is a neural network. When we talk \\(\\textbf{Score Matching}\\), it means, that we solve the regression problem between \\(\\psi(x,\\theta)\\), where \\(\\psi(x,\\theta)\\) is \\(\\nabla_{x} \\log q(x,\\theta)\\) and \\(q(x,\\theta)\\) is a neural-network.\n\n\n\nalt text\n\n\n\\(\\textbf{Theorem 1}:\\) Let \\(\\psi(x,\\theta): \\mathbb{R}^{D} \\to \\mathbb{R}^{D}\\) is a regular differetiable function. Having defined the score function as follows :\n\\[\\psi_{i}(x,\\theta) = \\frac{\\partial \\log q(x,\\theta)}{\\partial x_{i}}, \\quad \\frac{\\partial \\psi(x,\\theta)}{\\partial x_{i}} = \\frac{\\partial^{2} \\log q(x,\\theta)}{\\partial x_{i}^{2}} \\]\nThen:\n\\[\\mathcal{J}(\\theta) = \\int_{\\mathbb{R}^{D}} \\sum_{i=1}^{D} \\{ \\frac{\\partial \\psi(x,\\theta)}{\\partial x_{i}}+ \\frac{1}{2}\\psi(x,\\theta)^{2}\\}d\\mathbb{P}(x) + Const.\\]\n\\(\\textbf{proof}:\\) See the Seminar\n\ndef score_matching(energy_net, data):\n    \"\"\"\n    energy_net - torch.nn.Module\n    data       - torch.Size([B,N])\n    \"\"\"\n    data.requires_grad = True\n    logq = -energy_net(data) # torch.Size([B,1])\n    score = torch.autograd.grad(logq.sum(), data,\n                                create_graph=True, retain_graph=True)[0]# torch.Size([B,N])\n\n    loss1 = 0.5*torch.norm(score,dim=-1)**2 # torch.Size([B])\n\n    grad_score = torch.autograd.grad(score.sum(dim=-1).sum() , data,\n                                     create_graph=True, retain_graph=True)[0] #torch.Size([B,N])\n\n    loss2 = grad_score.sum(dim=-1)#torch.Size([B])\n\n    return torch.mean(loss1 + loss2 ,dim = 0)\n\n\ndef score_estimation(score_net, data):\n    \"\"\"\n    energy_net - torch.nn.Module\n    data       - torch.Size([B,N])\n    \"\"\"\n    data.requires_grad = True\n    score = score_net(data)#torch.Size([B,N])\n    loss1 =  0.5*torch.norm(score,dim=-1)**2 # torch.Size([B])\n    grad_score = torch.autograd.grad(score.sum(),data,\n                                    create_graph=True,retain_graph=True)[0]\n    loss2 = grad_score.sum(dim=-1)# torch.Size([B])\n    return torch.mean(loss1 + loss2 ,dim = 0)\n\n\nHIDDENS = [DIM,64,128,256,128,64,2]\nmodel = toy_net(HIDDENS)\nopt = torch.optim.Adam(model.parameters(),lr=1e-5)\nbatch_size=2048\niterations=5000\nlosses = []\nfor _ in tqdm.tqdm(range(iterations)):\n    opt.zero_grad()\n    samples = teacher.sample(batch_size)\n    loss =  score_estimation(model,samples)\n    loss.backward()\n    opt.step()\n    losses.append(loss.item())\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [03:28&lt;00:00, 24.04it/s]\n\n\n\nplt.plot(losses);\n\n\n\n\n\n\n\n\n\nplot_loss_quiever(model,teacher)\n\n\n\n\n\n\n\n\n\n\n1.3 Denoising Score matching and estimation\nLet‚Äôs consider \\(\\mathbb{P}_{\\sigma}\\) is as a perturbed data distribution of \\(\\mathbb{P}_{\\sigma}\\). First of all, we pick out the noise scale \\(\\sigma\\) manually.\n\n\\(\\mathbb{q}_{\\sigma}(\\tilde{x}|x) = \\mathcal{N}(\\tilde{x}|x,\\sigma^{2}I)\\) - conditional perturbed distribution.\n\\(\\mathbb{P}_{\\sigma}(\\tilde{x},x) =\\mathbb{q}_{\\sigma}(\\tilde{x}|x)\\mathbb{P}(x)\\) - joint distribution.\n\\(\\mathbb{P}_{\\sigma}^{m}(\\tilde{x}) = \\int \\mathbb{P}_{\\sigma}(\\tilde{x},x) dx\\) - marginal distribution.\n\n\\(\\textbf{Theorem 2:}\\) Let \\(\\mathbb{P}(x)\\) is the gound-truth distribution, while \\(\\psi(x,\\theta): \\mathbb{R}^{D} \\to \\mathbb{R}^{D}\\) is learnable scorre function and tries to approximate the gradient of logarothm of \\(\\mathbb{P}\\). Then: \\[\\mathbb{E}_{\\mathbb{P}(x)} \\{ \\frac{1}{2}||\\psi(x,\\theta) - \\frac{\\partial}{\\partial x} \\log \\mathbb{P}(x) ||_{2}^{2}\\} \\sim \\mathbb{E}_{\\mathbb{P}_{\\sigma}(x,\\tilde{x})}\\{\\frac{1}{2}||\\psi(\\tilde{x},\\theta) - \\frac{\\partial}{\\partial \\tilde{x}} \\log \\mathbb{P}_{\\sigma}(\\tilde{x}|x) ||_{2}^{2}\\}\\]\n\\(\\textbf{Proof}:\\) On seminar\n\ndef denoising_score_matching( score_net, samples, sigma):\n\n    \"\"\"\n    score_net - torch.nn.module\n    samples   - torch.Size([B,N])\n    sigma     - int\n    \"\"\"\n\n    samples.requires_grad = True\n    vector = torch.randn_like(samples, device = samples.device)*sigma\n    perturbed_samples = samples + vector\n    logp = - score_net(perturbed_samples)\n    dlogp = sigma**2*torch.autograd.grad(logp.sum(), perturbed_samples,\n                                         create_graph=True, retain_graph=True)[0]\n    kernel = vector\n    return 0.5*torch.mean(torch.norm(dlogp + kernel, dim=-1)**2)\n\n\ndef denoising_score_estimation( score_net, samples, sigma):\n\n    \"\"\"\n    score_net - torch.nn.module\n    samples   - torch.Size([B,N])\n    sigma     - int\n    \"\"\"\n\n    perturbed_samples = samples + torch.randn_like(samples, device = samples.device)*sigma\n    score = score_net(perturbed_samples)\n    dlogq =  1/sigma**2*(samples - perturbed_samples )\n    return 0.5*torch.mean(torch.norm((score - dlogq)**2,dim=-1))\n\n\nmodel = toy_net(HIDDENS)\nopt = torch.optim.Adam(model.parameters(),lr=1e-5)\nbatch_size=2048\niterations=5000\nlosses = []\nfor _ in tqdm.tqdm(range(iterations)):\n    opt.zero_grad()\n    samples = teacher.sample(batch_size)\n    loss =  denoising_score_estimation(model,samples,0.01)\n    loss.backward()\n    opt.step()\n    losses.append(loss.item())\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:30&lt;00:00, 166.51it/s]\n\n\n\nplot_loss_quiever(model,teacher)\n\n\n\n\n\n\n\n\n\n\n1.4 Sliced Score matching and estimation\nTo understand Sliced Score Estimation, we recall \\(S_{d}(x)\\) and \\(\\psi(x,\\theta)\\), where the firrst is the ground-truth score function. If we solve any regression problem between \\(S_{d}(x)\\) and \\(\\psi(x,\\theta)\\), then we have the problem between two high-dimensional vectors. One would like to reduce the dimensionality of the problem. The random projection is one of the possible solutions.\n\\[ F(\\mathbb{Q},\\mathbb{P}) = \\frac{1}{2}\\int_{\\mathbb{R}^{D}}||\\psi(x,\\theta) - S_{d}(x) ||_{2}^{2} \\]\nThen, we rewrite the aforementioned expression via projections on random Gaussian vectors:\n\\[ \\mathcal{L}(\\theta) = \\frac{1}{2}\\mathbb{E}_{p_{v}}\\mathbb{E}_{\\mathbb{P}(x)}||v^{T}\\psi(x,\\theta) - v^{T}S_{d}(x)||_{2}^{2},\\]\nwhere \\(p_{v}(v) = \\mathcal{N}(v|0,I)\\), and we imply, that \\(\\mathbb{E}_{p_{v}}||v||^{2}_{2} &lt; \\infty\\)\n\\(\\textbf{Theorem 3}\\): Let \\(\\psi(x,\\theta)\\) is a score function, \\(S_{d}(x)\\) is the ground-truth scorree function, then:\n\\[\\frac{1}{2}\\mathbb{E}_{p_{v}}\\mathbb{E}_{p_{d}}||v^{T}\\psi(x,\\theta) - v^{T}S_{d}(x)||_{2}^{2} = \\frac{1}{2}\\mathbb{E}_{p_{v}}\\mathbb{E}_{p_{d}}(v^{T}\\psi(x,\\theta))^{2} + \\sum_{i=1}^{D}\\mathbb{E}_{p_{d}} v_{i}v^{T} \\times \\frac{\\partial \\psi(x,\\theta)}{\\partial x_{i}}\\]\n\\(\\textbf{Proof}:\\) On seminar\n\ndef sliced_score_estimation(score_net, samples, n_particles=1 ):\n\n    \"\"\"\n\n\n    \"\"\"\n    samples = samples.unsqueeze(0).expand(n_particles,*samples.shape).contiguous().view(-1,*samples.shape[1:])\n    samples.requires_grad = True\n    vectors = torch.randn_like(samples)\n    vectors = vectors / torch.norm(vectors, dim=-1, keepdim=True)\n    score = score_net(samples)\n    loss1 = 0.5*torch.sum(score*vectors, dim=-1)**2 #torch.Size([M*B])\n    loss1 = loss1.view(n_particles, -1).mean(dim=0)\n\n    return loss1.mean()\n\n\ndef sliced_score_estimation_vr(score_net, samples, n_particles=1):\n    \"\"\"\n    Be careful if the shape of samples is not B x x_dim!!!!\n    \"\"\"\n    dup_samples = samples.unsqueeze(0).expand(n_particles, *samples.shape).contiguous().view(-1, *samples.shape[1:])\n    dup_samples.requires_grad_(True)\n    vectors = torch.randn_like(dup_samples)\n\n    grad1 = score_net(dup_samples)\n    gradv = torch.sum(grad1 * vectors)\n    grad2 = torch.autograd.grad(gradv, dup_samples, create_graph=True)[0]\n\n    grad1 = grad1.view(dup_samples.shape[0], -1)\n    loss1 = torch.sum(grad1 * grad1, dim=-1) / 2.\n\n    loss2 = torch.sum((vectors * grad2).view(dup_samples.shape[0], -1), dim=-1)\n\n    loss1 = loss1.view(n_particles, -1).mean(dim=0)\n    loss2 = loss2.view(n_particles, -1).mean(dim=0)\n\n    loss = loss1 + loss2\n    return loss.mean(), loss1.mean(), loss2.mean()\n\n\ndef sliced_score_estimation_own(score_net, samples, n_particles=1):\n\n    samples.requires_grad = True\n    vectors = torch.randn(n_particles, *samples.shape)\n    vectors = vectors / torch.norm(vectors, dim=-1, keepdim=True)\n    score = score_net(samples)\n    loss1 = 0.5*torch.matmul(score.unsqueeze(0),vectors.permute(0,2,1))#torch.Size([M,B,B])\n    loss1 = torch.sum(loss1,dim=-1)**2 # torch.Size([M,B])\n    loss1 = torch.mean(loss1,dim=-1).mean(dim=0)\n\n    # run the code\n    #loss2 =\n\n    return loss1 + loss2\n\n\nmodel = toy_net(HIDDENS)\nopt = torch.optim.Adam(model.parameters(),lr=1e-5)\nbatch_size=2048\niterations=5000\nlosses = []\nfor _ in tqdm.tqdm(range(iterations)):\n    opt.zero_grad()\n    samples = teacher.sample(batch_size)\n    loss,*_ =  sliced_score_estimation_vr(model,samples)\n    loss.backward()\n    opt.step()\n    losses.append(loss.item())\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [03:35&lt;00:00, 23.23it/s]\n\n\n\nplt.plot(losses);\n\n\n\n\n\n\n\n\n\nplot_loss_quiever(model,teacher)\n\n\n\n\n\n\n\n\n\n\nProblem of score estimation\n\n\n\nAlt Text"
  },
  {
    "objectID": "projects/langevien.html#noise-conditional-score-network-ncsn-on-toy-examples",
    "href": "projects/langevien.html#noise-conditional-score-network-ncsn-on-toy-examples",
    "title": "Score Estimations, Langevin Generative models and NCSN",
    "section": "2. Noise Conditional Score Network (NCSN) on toy Examples",
    "text": "2. Noise Conditional Score Network (NCSN) on toy Examples\nOnce we have trained a score-based model \\(S_{\\theta}(x) = \\nabla_{x} \\log \\mathbb{P}(x)\\) we can use an iterative procedure called Langevin dynamics to draw samples from it.\nLangevin dynamics provides an MCMC procedure to sample from a distribution \\(\\mathbb{P}(x)\\) using only its score function the score function. Specifically, it initializes the chain from an arbitrary prior distribution \\(x_{0} \\sim \\pi(x)\\) and then iterates the following\n\\[x_{k+1} = x_{k} + \\epsilon \\nabla_{x} \\log \\mathbb{P}(x) + \\sqrt{2\\epsilon}z, \\quad z \\sim \\mathcal{N}(z|0,I)\\]\nWhen \\(\\epsilon \\to 0\\) as well as \\(K \\to \\infty\\) , \\(x_{k}\\) obtained from the procedure in Langevin Dynamics algorithm converges to a sample from \\(\\mathbb{P}(x)\\) under some regularity conditions. In practice, the error is negligible when \\(\\epsilon\\) is sufficiently small and \\(K\\) is sufficiently large.\n\n\n\nAlt Text\n\n\nNext, we estimate the score function of each noise-perturbed distribution \\(\\nabla \\log \\mathbb{P}{\\sigma_{i}}(x)\\), by training a Noise Conditional Score-Based Model \\(S(x_{i},\\sigma_{i})\\),with score matching, such that:\n\\[S(x_{i},\\sigma_{i}) = \\nabla_{x} \\log \\mathbb{P}_{\\sigma}(x_{i})\\]\n\n\n\nAlt Text\n\n\nThe training objective for \\(S_{\\theta}(x_{i},\\sigma_{i})\\) is a weighted sum of Fisher divergences for all noise scales. In particular, we use the objective below.\n\\[ \\sum_{i=1}^{L} \\lambda(i) \\mathbb{E}_{\\mathbb{P}_{\\sigma_{i}}}||s_{\\theta}(x,\\sigma_{i}) - \\nabla_{x} \\log \\mathbb{P}_{\\sigma_{i}}(x)||_{2}^{2}\\]\nAfter training our noise-conditional score-based model \\(S_{\\theta}(x,\\sigma_{i})\\), we can produce samples from it by running Langevin dynamics for \\(i = L,L-1,...,1\\) in sequence. This method is called annealed Langevin dynamics since the noise scale \\(\\sigma_{i}\\) decreases (anneals) gradually over time.\n\n\n\nAlt Text\n\n\n\ndef visualize(teacher, model, left_bound=-1., right_bound=1., savefig=None, step=None, device=None):\n\n        #---------------------------------------------------#\n        fig,ax = plt.subplots(2,3, figsize=(27,18),sharex=True, sharey=True,dpi=150 )\n\n        mesh = []\n        grid_size = 100\n        x = np.linspace(left_bound, right_bound, grid_size)\n        y = np.linspace(left_bound, right_bound, grid_size)\n        for i in x:\n            for j in y:\n                mesh.append(np.asarray([i, j]))\n\n        mesh = np.stack(mesh, axis=0)\n        mesh = torch.from_numpy(mesh).float()\n        if device is not None:\n            mesh = mesh.to(device)\n\n        logp_true = teacher.log_prob(mesh)\n        logp_true = logp_true.view(grid_size, grid_size).exp()\n\n        ax[0,0].grid(False)\n        ax[0,0].axis('off')\n        ax[0,0].set_title('Data density', fontsize=16)\n        ax[0,0].imshow(np.flipud(logp_true.cpu().numpy()), cmap='inferno')\n\n\n\n        #---------------------------------------------------------#\n\n        grid_size = 20\n        mesh = []\n        x = np.linspace(left_bound, right_bound, grid_size)\n        y = np.linspace(left_bound, right_bound, grid_size)\n        for i in x:\n            for j in y:\n                mesh.append(np.asarray([i, j]))\n\n        mesh = np.stack(mesh, axis=0)\n        mesh = torch.from_numpy(mesh).float()\n        if device is not None:\n            mesh = mesh.to(device)\n\n\n        scores = model(  mesh.detach().to(DEVICE) )\n        mesh = mesh.detach().cpu().numpy()\n        scores = scores.detach().cpu().numpy()\n\n        ax[0,1].grid(False)\n        ax[0,1].axis('off')\n        ax[0,1].axis('square')\n        ax[0,1].quiver(mesh[:, 0], mesh[:, 1], scores[:, 0], scores[:, 1], width=0.005)\n        ax[0,1].set_title('Estimated scores', fontsize=16)\n\n\n\n        #-------------------------------------------------------------#\n\n        samples = teacher.sample(1280)\n        samples = samples.detach().cpu().numpy()\n        ax[0,2].scatter(samples[:, 0], samples[:, 1], s=0.5)\n        ax[0,2].axis('square')\n        ax[0,2].set_title('data samples',fontsize=16)\n        ax[0,2].set_xlim([left_bound, right_bound])\n        ax[0,2].set_ylim([left_bound, right_bound])\n\n\n        #------------------------------------------------------------#\n\n\n        samples_ = torch.rand(1280, 2) * (right_bound - left_bound) + left_bound\n\n        samples = ncsn.langevin_dynamics(model, samples_).detach().numpy()\n        ax[1,0].scatter(samples[:, 0], samples[:, 1], s=0.5)\n        ax[1,0].axis('square')\n        ax[1,0].axis('square')\n        ax[1,0].set_title('Model Langevin dynamics',fontsize=16)\n        ax[1,0].set_xlim([left_bound, right_bound])\n        ax[1,0].set_ylim([left_bound, right_bound])\n\n\n        #-----------------------------------------------------------#\n\n\n        scores = data_score(torch.from_numpy(mesh) )\n        scores = scores.detach().numpy()\n\n        ax[1,1].axis('off')\n        ax[1,1].quiver(mesh[:, 0], mesh[:, 1], scores[:, 0], scores[:, 1], width=0.005)\n        ax[1,1].set_title('True Data scores', fontsize=16)\n        ax[1,1].axis('square')\n\n\n\n        samples = torch.rand(1280, 2) * (right_bound - left_bound) + left_bound\n\n        samples = ncsn.langevin_dynamics(data_score, samples).detach().numpy()\n        ax[1,2].scatter(samples[:, 0], samples[:, 1], s=0.1)\n        ax[1,2].axis('square')\n        ax[1,2].set_title('True Langevin dynamics data',fontsize=16)\n        ax[1,2].set_xlim([left_bound, right_bound])\n        ax[1,2].set_ylim([left_bound, right_bound])\n\n\n        \"\"\"\n        samples = torch.rand(1280, 2) * (right_bound - left_bound) + left_bound\n        sigmas = torch.exp(torch.linspace(np.log(20), 0., 10)).to(DEVICE)\n        labels = torch.linspace(1,10,10).to(DEVICE)\n        samples = ncsn.anneal_langevin_dynamics(ncsn.score, samples.to(DEVICE), sigmas\n                                             , labels).detach().cpu().numpy()\n\n        plt.scatter(samples[:, 0], samples[:, 1], s=0.2)\n        plt.axis('square')\n        plt.title('Right Annealed Langevin dynamics samples')\n        plt.xlim([left_bound, right_bound])\n        plt.ylim([left_bound, right_bound])\n        plt.show()\n        \"\"\"\n        fig.tight_layout()\n        plt.show()\n\n\nclass NCSN(torch.nn.Module):\n\n    def __init__(self, score, teacher, train_steps, lr, batch_size):\n        super().__init__()\n        self.train_steps = train_steps\n        self.score = score\n        self.teacher = teacher\n        self.lr = lr\n        self.batch_size = batch_size\n\n    def langevin_dynamics(self, score, init, lr=0.1, step=1000):\n\n        \"\"\"\n        score - torch.nn.Module\n        init  - torch.Size([B,N])\n        \"\"\"\n        for step in range(step):\n            init = init + score(init)*lr + torch.randn_like(init,device=init.device)*np.sqrt(2*lr)\n        return init\n\n    def anneal_langevin_dynamics(self, score, init, sigmas, lr=0.1, n_steps_each=100):\n\n        \"\"\"\n        score   - space-time torch.nn.Module\n        init    - torch.Size([B,N])\n        sigmas  - List\n        \"\"\"\n        #with torch.no_grad\n        for sigma in sigmas:\n            current_lr = lr*sigma**2/sigmas[-1]**2\n            for step in range(n_steps_each):\n                init = init + 0.5*current_lr*score(init, sigma).detach()\n                init = init + torch.randn_like(init, device=init.device)*np.sqrt(current_lr)\n\n        return init\n\n    def anneal_dsm_score_estimation(self,scorenet, samples, labels, sigmas, anneal_power=2.):\n\n        batch_size = samples.shape[0]\n        samples = samples.repeat(len(sigmas),1).reshape(len(sigmas),-1,samples.shape[-1])\n        perturbed_samples = samples + torch.randn_like(samples)*sigmas.reshape(-1,1,1)\n\n        scores = scorenet( perturbed_samples.reshape(-1,samples.shape[-1]),\n                           labels.view(-1,1).expand( len(sigmas), batch_size).flatten().view(-1) )\n\n        target = -1.*(perturbed_samples.reshape(-1,samples.shape[-1]) - samples.reshape(-1, samples.shape[-1]) )*\\\n                 (sigmas.view(-1,1).expand(len(sigmas),  batch_size).flatten().view(-1,1))**2\n\n        loss = 1/2.*((scores - target)**2).sum(dim = -1)\n\n        loss =  loss*\\\n              (sigmas.view(-1,1).expand(len(sigmas), batch_size).flatten().view(-1) )\n\n        return loss.mean(dim=0)\n\n    def train_(self, iterations = 10000, batch_size = 128):\n\n        \"\"\"\n        hidden_units = 128\n        score = torch.nn.Sequential(\n            torch.nn.Linear(3, hidden_units),\n            torch.nn.Softplus(),\n            torch.nn.Linear(hidden_units, hidden_units),\n            torch.nn.Softplus(),\n            torch.nn.Linear(hidden_units, 2),\n        )\n        \"\"\"\n        losses = []\n        optimizer = torch.optim.Adam(self.score.parameters(), lr=0.001)\n        teacher = GMMDist(dim=2)\n\n        for step in  tqdm(range(iterations)):\n            samples = teacher.sample((batch_size,)).to(DEVICE)\n\n            #loss, *_ = sliced_score_estimation_vr(score, samples, n_particles=1)\n\n            loss = self.anneal_dsm_score_estimation(self.score, samples, labels = torch.linspace(1,10,10).to(DEVICE) ,\n                                                    sigmas=torch.exp(torch.linspace(np.log(20), 0., 10)).to(DEVICE))\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            losses.append(loss.item())\n\n        return self.score, teacher, losses\n\n    def train(self):\n        opt_score = torch.optim.Adam(self.score.parameters(),lr=self.lr)\n        for step in tqdm.tqdm(range(self.train_steps)):\n            samples = self.teacher.sample(self.batch_size)\n            opt_score.zero_grad()\n            loss,*_ = sliced_score_estimation_vr(self.score, samples, n_particles = 1)\n            loss.backward()\n            opt_score.step()\n        visualize(self.teacher, self.score, -8, 8)\n\n\nhidden_units = 128\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(2, hidden_units),\n    torch.nn.Softplus(),\n    torch.nn.Linear(hidden_units, hidden_units),\n    torch.nn.Softplus(),\n    torch.nn.Linear(hidden_units, 2),\n)\nncsn = NCSN(model, teachers[0], train_steps = 1000,lr=1e-3,batch_size=128)\n\n\nDEVICE=\"cpu\"\n\n\nncsn.train()\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:02&lt;00:00, 482.13it/s]\n\n\n\n\n\n\n\n\n\n\n\n\nAlt Text"
  },
  {
    "objectID": "projects/langevien.html#questions",
    "href": "projects/langevien.html#questions",
    "title": "Score Estimations, Langevin Generative models and NCSN",
    "section": "Questions",
    "text": "Questions"
  },
  {
    "objectID": "projects/RealNVP.html",
    "href": "projects/RealNVP.html",
    "title": "Addisu Amare",
    "section": "",
    "text": "REAL-NVP Modified\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_moons\nfrom collections import defaultdict\nfrom tqdm.notebook import tqdm\nfrom typing import Tuple\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as data\n\nUSE_CUDA = torch.cuda.is_available()\nWe have already familiarized ourselves with normalizing flows and REAL-nvp method. For best understanding ,please, write the paper RealNVP."
  },
  {
    "objectID": "projects/RealNVP.html#data",
    "href": "projects/RealNVP.html#data",
    "title": "Addisu Amare",
    "section": "1. Data",
    "text": "1. Data\n\nTICKS_FONT_SIZE = 12\nLEGEND_FONT_SIZE = 12\nLABEL_FONT_SIZE = 14\nTITLE_FONT_SIZE = 16\n\n\ndef visualize_2d_data(train_data, test_data, train_labels=None, test_labels=None):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n    ax1.set_title('train', fontsize=TITLE_FONT_SIZE)\n    ax1.scatter(train_data[:, 0], train_data[:, 1], s=1, c=train_labels)\n    ax1.tick_params(labelsize=LABEL_FONT_SIZE)\n    ax2.set_title('test', fontsize=TITLE_FONT_SIZE)\n    ax2.scatter(test_data[:, 0], test_data[:, 1], s=1, c=test_labels)\n    ax2.tick_params(labelsize=LABEL_FONT_SIZE)\n    plt.show()\n\n\ndef generate_moons_data(count: int) -&gt; tuple:\n    data, labels = make_moons(n_samples=count, noise=0.1)\n    data = data.astype(\"float32\")\n    split = int(0.8 * count)\n    train_data, test_data = data[:split], data[split:]\n    train_labels, test_labels = labels[:split], labels[split:]\n    return train_data, train_labels, test_data, test_labels\n\n\nCOUNT = 5000\n\ntrain_data, train_labels, test_data, test_labels = generate_moons_data(COUNT)\nvisualize_2d_data(train_data, test_data, train_labels, test_labels)"
  },
  {
    "objectID": "projects/RealNVP.html#transformation",
    "href": "projects/RealNVP.html#transformation",
    "title": "Addisu Amare",
    "section": "2. Transformation",
    "text": "2. Transformation\nREAL-NVP model is a sequence of the affine coupling layers.\nForward transform: \\[\n    \\begin{cases}\n        \\mathbf{y}_1 &= \\mathbf{x}_1; \\\\\n        \\mathbf{y}_2 &= \\mathbf{x}_2 \\odot \\exp (s(\\mathbf{x}_1)) + t(\\mathbf{x}_1).\n    \\end{cases}\n\\]\nInverse transform: \\[\n    \\begin{cases}\n        \\mathbf{x}_1 &= \\mathbf{y}_1; \\\\\n        \\mathbf{x}_2 &= (\\mathbf{y}_2 - t(\\mathbf{y}_1)) \\odot \\exp ( - s(\\mathbf{y}_1)).\n    \\end{cases}\n\\]\nHere \\(s(\\cdot)\\) and \\(t(\\cdot)\\) are outputs of neural network. In this task our networks will be fully connected MLP.\n\nclass FullyConnectedMLP(nn.Module):\n    def __init__(self, input_shape: int, hiddens: list, output_shape: int) -&gt; None:\n        assert isinstance(hiddens, list)\n        super().__init__()\n        self.input_shape = (input_shape,)\n        self.output_shape = (output_shape,)\n        self.hiddens = hiddens\n\n        model = []\n\n        # ====\n        # your code\n        # Stack Dense layers with ReLU activation.\n        # Note that you do not have to add relu after the last dense layer\n        prev_h = input_shape\n        for h in hiddens:\n            model.append(nn.Linear(prev_h, h))\n            model.append(nn.ReLU())\n            prev_h = h\n        model.append(nn.Linear(hiddens[-1], output_shape))\n        # ====\n        self.net = nn.Sequential(*model)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # ====\n        # your code\n        # apply network that was defined in __init__ and return the output\n        batch_size = x.shape[0]\n        x = x.view(batch_size, -1)\n        return self.net(x).view(batch_size, *self.output_shape)\n        # ====\n\n\nclass AffineCouplingLayer(nn.Module):\n    def __init__(self, parity_type: bool, n_hiddens: list) -&gt; None:\n        assert isinstance(parity_type, bool)\n        assert isinstance(n_hiddens, list)\n        super().__init__()\n        self.mask = self.build_mask(parity_type=parity_type)\n        self.mlp = FullyConnectedMLP(input_shape=2, hiddens=n_hiddens, output_shape=2)\n\n    def build_mask(self, parity_type: bool) -&gt; torch.Tensor:\n        # ====\n        # your code\n        # the mask is extremely simple\n        # it is a float tensor of two scalars (1.0 and 0.0)\n        # the partition_type defines the order of these two scalars\n        if parity_type:\n            return torch.FloatTensor([1.0, 0.0])\n        else:\n            return torch.FloatTensor([0.0, 1.0])\n        # ====\n\n    def forward(self, x: torch.Tensor, invert: bool = False) -&gt; tuple:\n        # ====\n        # your code\n        # 1) mask our input x, using self.mask\n        # 2) apply mlp to masked input to get s and t\n        batch_size = x.shape[0]\n        mask = self.mask.repeat(batch_size, 1).cuda()\n        x_masked = x * mask\n\n        s, t = self.mlp(x_masked).split(1, dim=1)\n        # ====\n\n        # we invert mask here\n        t = t * (1.0 - mask)\n        s = s * (1.0 - mask)\n\n        # ====\n        # your code\n        # apply forward (invert=False) or inverse (invert=True) transform (invert=False)\n        if invert:\n            x = (x - t) * torch.exp(-s)\n        else:\n            x = x * torch.exp(s) + t\n        # ====\n\n        # the output is transformed input\n        # and logarithm of jacobian (which equals to s)\n        return x, s"
  },
  {
    "objectID": "projects/RealNVP.html#realnvp",
    "href": "projects/RealNVP.html#realnvp",
    "title": "Addisu Amare",
    "section": "3. RealNVP",
    "text": "3. RealNVP\nWe are ready to define RealNVP model. The model objective is the negative value of log-likelihood. Log-likelihood is given by the change of variables (CoV) theorem: \\[\n    \\log p(\\mathbf{x}| \\boldsymbol{\\theta}) = \\log p(\\mathbf{z}) + \\log \\left|\\det \\left(  \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}} \\right) \\right| = \\log p(f(\\mathbf{x}, \\boldsymbol{\\theta})) + \\log \\left|\\det \\left( \\frac{\\partial f(\\mathbf{x}, \\boldsymbol{\\theta})}{\\partial \\mathbf{x}} \\right) \\right|.\n\\]\n\nclass RealNVP(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n\n        # base distribution p(z) is normal\n        self.prior = torch.distributions.Normal(torch.tensor(0.0), torch.tensor(1.0))\n        # ====\n        # your code\n        # apply sequence of AffineCouplingLayer with alternating parity_type\n        # 6 layers is sufficient (with 2 hidden layers in each affine layer)\n        self.transforms = nn.ModuleList(\n            [\n                AffineCouplingLayer(True, n_hiddens=[64, 64]),\n                AffineCouplingLayer(False, n_hiddens=[64, 64]),\n                AffineCouplingLayer(True, n_hiddens=[64, 64]),\n                AffineCouplingLayer(False, n_hiddens=[64, 64]),\n                AffineCouplingLayer(True, n_hiddens=[64, 64]),\n                AffineCouplingLayer(False, n_hiddens=[64, 64]),\n            ]\n        )\n        # ====\n\n    def forward(self, x: torch.Tensor, invert: bool = False) -&gt; tuple:\n        z = x\n        log_det = 0.0\n\n        # ====\n        # your code\n        # apply sequence of transforms and sum all of log_dets\n        # if invert == True, you have to apply transforms in reversed order (from last to first!) with invert=True flag\n        transforms = reversed(self.transforms) if invert else self.transforms\n        for transform in transforms:\n            z, delta_log_det = transform(z, invert=invert)\n            log_det += delta_log_det\n\n        # ====\n        return z, log_det\n\n    def log_prob(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # ====\n        # your code\n        # 1) make forward pass with right inverse flag\n        # 2) sum log_det with log of base distribution (log p(z)) - see the formula above\n        # 3) we will get tensor of shape [batch_size, 2] - sum it over the the last dimension\n        z, log_det = self.forward(x, invert=False)\n\n        return torch.sum(log_det, dim=1) + torch.sum(self.prior.log_prob(z), dim=1)\n        # ====\n\n    def loss(self, x: torch.Tensor) -&gt; dict:\n        log_prob = self.log_prob(x)\n        # log_prob should be a vector of batch_size\n        assert len(log_prob.shape) == 1\n        return {\"nll_loss\": -log_prob.mean()}\n\n    def sample(self, n: int) -&gt; torch.Tensor:\n        # ====\n        # your code\n        # 1) sample from the prior\n        # 2) apply the forward pass with the right inverse flag\n        # 3) return only the first output of the forward pass (the second is the log of determinant - we don't need it in sampling)\n        z = self.prior.sample([n, 2]).cuda()\n        return self.forward(z, invert=True)[0]\n        # ===="
  },
  {
    "objectID": "projects/RealNVP.html#training",
    "href": "projects/RealNVP.html#training",
    "title": "Addisu Amare",
    "section": "4. Training",
    "text": "4. Training\n\ndef train_epoch(\n    model: object,\n    train_loader: object,\n    optimizer: object,\n    use_cuda: bool,\n    loss_key: str = \"total\",\n) -&gt; defaultdict:\n    model.train()\n\n    stats = defaultdict(list)\n    for x in train_loader:\n        if use_cuda:\n            x = x.cuda()\n        losses = model.loss(x)\n        optimizer.zero_grad()\n        losses[loss_key].backward()\n        optimizer.step()\n\n        for k, v in losses.items():\n            stats[k].append(v.item())\n\n    return stats\n\n\ndef eval_model(model: object, data_loader: object, use_cuda: bool) -&gt; defaultdict:\n    model.eval()\n    stats = defaultdict(float)\n    with torch.no_grad():\n        for x in data_loader:\n            if use_cuda:\n                x = x.cuda()\n            losses = model.loss(x)\n            for k, v in losses.items():\n                stats[k] += v.item() * x.shape[0]\n\n        for k in stats.keys():\n            stats[k] /= len(data_loader.dataset)\n    return stats\n\n\ndef train_model(\n    model: object,\n    train_loader: object,\n    test_loader: object,\n    epochs: int,\n    lr: float,\n    use_tqdm: bool = False,\n    use_cuda: bool = False,\n    loss_key: str = \"total_loss\",\n) -&gt; Tuple[dict, dict]:\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    train_losses = defaultdict(list)\n    test_losses = defaultdict(list)\n    forrange = tqdm(range(epochs)) if use_tqdm else range(epochs)\n    if use_cuda:\n        model = model.cuda()\n\n    for epoch in forrange:\n        model.train()\n        train_loss = train_epoch(model, train_loader, optimizer, use_cuda, loss_key)\n        test_loss = eval_model(model, test_loader, use_cuda)\n\n        for k in train_loss.keys():\n            train_losses[k].extend(train_loss[k])\n            test_losses[k].append(test_loss[k])\n    return dict(train_losses), dict(test_losses)\n\n\n# ====\n# your code\n# choose these parameters\n\nBATCH_SIZE = 128  # any adequate value\nEPOCHS = 50  # &lt; 100\nLR = 0.001  # &lt; 1e-2\n# ====\n\nCOUNT = 5000\n\ntrain_data, train_labels, test_data, test_labels = generate_moons_data(COUNT)\n\nloader_args = dict(batch_size=BATCH_SIZE, shuffle=True)\ntrain_loader = torch.utils.data.DataLoader(train_data, **loader_args)\ntest_loader = torch.utils.data.DataLoader(test_data, **loader_args)\n\n# model\nmodel = RealNVP()\n\n \n\n\n# train\ntrain_losses, test_losses = train_model(\n    model,\n    train_loader,\n    test_loader,\n    epochs=EPOCHS,\n    lr=LR,\n    loss_key=\"nll_loss\",\n    use_cuda=USE_CUDA,\n    use_tqdm=True,\n)"
  },
  {
    "objectID": "projects/RealNVP.html#inference",
    "href": "projects/RealNVP.html#inference",
    "title": "Addisu Amare",
    "section": "5.inference",
    "text": "5.inference\n\ndef plot_training_curves(train_losses, test_losses, logscale_y=False, logscale_x=False):\n    n_train = len(train_losses[list(train_losses.keys())[0]])\n    n_test = len(test_losses[list(train_losses.keys())[0]])\n    x_train = np.linspace(0, n_test - 1, n_train)\n    x_test = np.arange(n_test)\n\n    plt.figure()\n    for key, value in train_losses.items():\n        plt.plot(x_train, value, label=key + '_train')\n\n    for key, value in test_losses.items():\n        plt.plot(x_test, value, label=key + '_test')\n\n    if logscale_y:\n        plt.semilogy()\n    \n    if logscale_x:\n        plt.semilogx()\n\n    plt.legend(fontsize=LEGEND_FONT_SIZE)\n    plt.xlabel('Epoch', fontsize=LABEL_FONT_SIZE)\n    plt.ylabel('Loss', fontsize=LABEL_FONT_SIZE)\n    plt.xticks(fontsize=TICKS_FONT_SIZE)\n    plt.yticks(fontsize=TICKS_FONT_SIZE)\n    plt.grid()\n    plt.show()\n\n\nplot_training_curves(train_losses, test_losses)\n\n\n\n\n\n\n\n\n\ndef visualize_2d_densities(x_grid, y_grid, densities, title, xlabel=None, ylabel=None):\n    densities = densities.reshape([y_grid.shape[0], y_grid.shape[1]])\n    plt.figure(figsize=(5, 5))\n    plt.pcolor(x_grid, y_grid, densities)\n    plt.pcolor(x_grid, y_grid, densities)\n\n    plt.title(title, fontsize=TITLE_FONT_SIZE)\n    plt.xticks(fontsize=TICKS_FONT_SIZE)\n    plt.yticks(fontsize=TICKS_FONT_SIZE)\n    if xlabel is not None:\n        plt.xlabel(xlabel, fontsize=LABEL_FONT_SIZE)\n    if ylabel is not None:\n        plt.ylabel(ylabel, fontsize=LABEL_FONT_SIZE)\n    plt.show()\n    \n\n\ndef visualize_2d_samples(data, title, labels=None, xlabel=None, ylabel=None):\n    plt.figure(figsize=(5, 5))\n    plt.scatter(data[:, 0], data[:, 1], s=1, c=labels)\n    plt.title(title, fontsize=TITLE_FONT_SIZE)\n    plt.xticks(fontsize=TICKS_FONT_SIZE)\n    plt.yticks(fontsize=TICKS_FONT_SIZE)\n    if xlabel is not None:\n        plt.xlabel(xlabel, fontsize=LABEL_FONT_SIZE)\n    if ylabel is not None:\n        plt.ylabel(ylabel, fontsize=LABEL_FONT_SIZE)\n    plt.show()\n\n\n# Samples\n\nx_samples = model.sample(4000).cpu().detach().numpy()\nvisualize_2d_samples(x_samples, title=\"Data space\", xlabel=\"x1\", ylabel=\"x2\")\n\n# Density\ndx, dy = 0.025, 0.025\nx_lim = (-1.5, 2.5)\ny_lim = (-1, 1.5)\ny, x = np.mgrid[slice(y_lim[0], y_lim[1] + dy, dy), slice(x_lim[0], x_lim[1] + dx, dx)]\nmesh_xs = torch.FloatTensor(np.stack([x, y], axis=2).reshape(-1, 2))\n\nif USE_CUDA:\n    mesh_xs = mesh_xs.cuda()\n\ndensities = np.exp(model.log_prob(mesh_xs).cpu().detach().numpy())\n\n# Latents\ntrain_tensor = torch.FloatTensor(train_data)\nif USE_CUDA:\n    train_tensor = train_tensor.cuda()\nz = model(train_tensor)[0]\nlatents = z.cpu().detach().numpy()\n \nvisualize_2d_densities(x, y, densities, title=\"Densities\", xlabel=\"x1\", ylabel=\"x2\")\nvisualize_2d_samples(\n    latents, title=\"Latent space\", labels=train_labels, xlabel=\"z1\", ylabel=\"z2\"\n)\n\n\n\n\n\n\n\n\nMatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3.  Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading'].  This will become an error two minor releases later.\n  plt.pcolor(x_grid, y_grid, densities)\n&lt;ipython-input-24-2953d9ef525c&gt;:5: MatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3.  Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading'].  This will become an error two minor releases later.\n  plt.pcolor(x_grid, y_grid, densities)"
  },
  {
    "objectID": "projects/Efficient_model_training.html",
    "href": "projects/Efficient_model_training.html",
    "title": "",
    "section": "",
    "text": "Efficient Deep learning training"
  },
  {
    "objectID": "projects/Efficient_model_training.html#section-1",
    "href": "projects/Efficient_model_training.html#section-1",
    "title": "",
    "section": "",
    "text": "These days models and datasets are becoming larger and larger and it is becoming increasingly important to increase the efficiency of the training procedure by optimizing memory utilization, speeding up the training, or both.\nIn this work shop we will cover several techniques that could speed up the training of the model and even sometimes make the training feasible: * Batch size optimization * Automated mixed precision * Gradient accumulation * Gradient checkpointing * Model compilation (for torch&gt;=2.0)"
  },
  {
    "objectID": "projects/Efficient_model_training.html#preparatory-stuff",
    "href": "projects/Efficient_model_training.html#preparatory-stuff",
    "title": "",
    "section": "Preparatory stuff",
    "text": "Preparatory stuff\nLoading stuff and defining helper functions\n\nimport time\nfrom tqdm import trange\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.cuda.amp import GradScaler\nfrom torchvision.models import get_model\n\n\ndef print_memory_stats():\n    print(f\"CUDA max memory allocated: {torch.cuda.max_memory_allocated() / 2 ** 30:.2f} GB.\")\n    print(f\"CUDA max memory reserved: {torch.cuda.max_memory_reserved() / 2 ** 30:.2f} GB.\")\n\nMake sure that you are using CUDA Runtime!\n\ndevice = \"cuda\"\n\nSince we have not started using GPU the cell below should output 0.\n\nprint_memory_stats()\n\nCUDA max memory allocated: 0.00 GB.\nCUDA max memory reserved: 0.00 GB.\n\n\nIn the following next sections we‚Äôll illustrate the techiques on a model from the torchvision library. We will adopt SwinTransformer-Base for the demonstrations below.\n\nmodel = get_model(\"swin_b\").to(device) # it is randomly initialized, but we do not care\n\n\nprint(f\"Number of paramerers: {sum(param.numel() for param in model.parameters()) / 10 ** 6 :.2f} M\")\n\nNumber of paramerers: 87.77 M\n\n\nMemory footprint should be roughly 4 * num_model_parameters since by default we are working in single precision float32.\n\nprint_memory_stats()\n\nCUDA max memory allocated: 0.33 GB.\nCUDA max memory reserved: 0.36 GB.\n\n\nDefine a function that yields dummy inputs and targets.\n\ndef make_dummy_dataloader(batch_size: int, img_size: int = 224):\n  while True:\n    yield torch.randn(batch_size, 3, img_size, img_size), torch.randint(low=0, high=1000, size=(batch_size,))\n\nBaseline training loop.\n\ndef train(model, loader, optimizer, loss_fn, num_steps):\n  samples_processed = 0\n  start = time.perf_counter()\n  for i in trange(num_steps, total=num_steps, desc=\"Train\"):\n    inputs, targets = next(loader)\n    inputs, targets = inputs.to(device), targets.to(device)\n    outputs = model(inputs)\n    loss = loss_fn(outputs, targets)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    samples_processed += len(inputs)\n  end = time.perf_counter()\n  samples_per_sec = samples_processed / (end - start)\n  print(\"\\n\")\n  print_memory_stats()\n  print(f\"samples/sec: {samples_per_sec:.2f}\")\n  results = {\n      \"samples/sec\": samples_per_sec,\n      \"max_memory_allocated\": torch.cuda.max_memory_allocated() / 2 ** 30\n  }\n  return results"
  },
  {
    "objectID": "projects/Efficient_model_training.html#batch-size-optimization",
    "href": "projects/Efficient_model_training.html#batch-size-optimization",
    "title": "",
    "section": "Batch size optimization",
    "text": "Batch size optimization\nMaximizing the throughput (samples/second) leads to lower training cost. This is generally achieved by utilizing the GPU as much as possible and thus filling GPU memory to its limit.\nBelow we will show the dependence of the model throughput on the batch size.\n\ntraining_steps = 100\n\n\nresults_for_batch_size = {}\nfor batch_size in (1, 2, 4, 8, 16, 32, 64, 128):\n  print(\"-\" * 10)\n  print(f\"Batch size: {batch_size}\")\n  # prepare dataloader\n  dataloader = make_dummy_dataloader(batch_size)\n  # prepare optimizer\n  optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n  # train\n  try:\n    results_for_batch_size[batch_size] = train(model, dataloader, optimizer, F.cross_entropy, training_steps)\n  except:\n    print(\"\\nCUDA out of memory!\")\n    print_memory_stats()\n    del optimizer\n    torch.cuda.empty_cache()\n    torch.cuda.reset_peak_memory_stats()\n    break\n  del optimizer\n  torch.cuda.empty_cache()\n  torch.cuda.reset_peak_memory_stats()\n\n----------\nBatch size: 1\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:12&lt;00:00,  7.71it/s]\n\n\n\n\nCUDA max memory allocated: 1.67 GB.\nCUDA max memory reserved: 1.78 GB.\nsamples/sec: 7.71\n----------\nBatch size: 2\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10&lt;00:00,  9.18it/s]\n\n\n\n\nCUDA max memory allocated: 1.68 GB.\nCUDA max memory reserved: 1.86 GB.\nsamples/sec: 18.34\n----------\nBatch size: 4\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:16&lt;00:00,  6.20it/s]\n\n\n\n\nCUDA max memory allocated: 2.34 GB.\nCUDA max memory reserved: 2.59 GB.\nsamples/sec: 24.78\n----------\nBatch size: 8\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:28&lt;00:00,  3.46it/s]\n\n\n\n\nCUDA max memory allocated: 2.78 GB.\nCUDA max memory reserved: 3.11 GB.\nsamples/sec: 27.67\n----------\nBatch size: 16\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:56&lt;00:00,  1.78it/s]\n\n\n\n\nCUDA max memory allocated: 4.43 GB.\nCUDA max memory reserved: 4.89 GB.\nsamples/sec: 28.54\n----------\nBatch size: 32\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:46&lt;00:00,  1.06s/it]\n\n\n\n\nCUDA max memory allocated: 7.88 GB.\nCUDA max memory reserved: 8.65 GB.\nsamples/sec: 30.11\n----------\nBatch size: 64\n\n\nTrain:   1%|          | 1/100 [00:03&lt;05:09,  3.13s/it]\n\n\n\nCUDA out of memory!\nCUDA max memory allocated: 14.00 GB.\nCUDA max memory reserved: 14.58 GB.\n\n\n\n\n\nLet us plot samples/sec and max memory vs batch size.\n\nbatch_sizes = []\nsamples_sec_per_batch_size = []\nmax_memory_per_batch_size = []\nfor k, v in results_for_batch_size.items():\n  batch_sizes.append(k)\n  samples_sec_per_batch_size.append(v['samples/sec'])\n  max_memory_per_batch_size.append(v['max_memory_allocated'])\n\n\nfig, ax = plt.subplots(ncols=2, figsize=(12, 5))\n\nax[0].plot(batch_sizes, samples_sec_per_batch_size, '-v');\nax[0].set_xlabel(\"Batch size\", fontsize=20);\nax[0].set_ylabel(\"Samples/sec\", fontsize=20);\n\nax[1].plot(batch_sizes, max_memory_per_batch_size, '-v');\nax[1].set_xlabel(\"Batch size\", fontsize=20);\nax[1].set_ylabel(\"Max memory (Gb)\", fontsize=20);\n\n\n\n\n\n\n\n\nNote!\nBatch size (together with learning rate) determine the degree of stochasticity of gradient descent.\nOften, a certain magnitude of noise in optimization trajectory is crucial for finding well generalizing solutions.\nTherefore, the batch size has to be chosen wisely ü§î."
  },
  {
    "objectID": "projects/Efficient_model_training.html#automated-mixed-precision",
    "href": "projects/Efficient_model_training.html#automated-mixed-precision",
    "title": "",
    "section": "Automated mixed precision",
    "text": "Automated mixed precision\nMachines work with number in finite precision. Different formats are defined in IEEE754.\nUnlike some other applications, such as physics and engineering neural networks are more robust to numerical errors and up to a certain point one reduce precision without impact on performance.\nLower precision offers faster operations with numbers and reduced memory usage.\nBelow we‚Äôll illustrate some commonly used formats.\nFP32 (default in PyTorch)\nThis format was the workhorse of deep learning for a long time.\n\n1 bit sign\n8 bits exponent\n23 bits fraction (aka mantissa)\n\n\nRange: ~1.18e-38 ‚Ä¶ ~3.40e38 with 6-9 significant decimal digits precision.\nFP16\nArguably, the most popular format for large model training and inference:\n\n1 bit sign\n5 bits exponent\n10 bits fraction\n\n\nRange: ~5.96e‚àí8 (6.10e‚àí5) ‚Ä¶ 65504 with 4 significant decimal digits precision.\nBF16\nBrain Float format developed by Google. Has higher dynamical range compared to FP16, but lower precision.\n\n1 bit sign\n8 bits exponent\n7 bits fraction\n\nRange: ~5.96e‚àí8 (6.10e‚àí5) ‚Ä¶ 65504 with 4 significant decimal digits precision.\n\nRange: ~1.18e-38 ‚Ä¶ ~3.40e38 with 3 significant decimal digits.\nNote\nThis format is supported only for NVIDIA architectures from Ampere and newer (A100, RTX30**, RTX40**). Older GPUs, such as fellow T4, do not have hardware support for it.\nFor more details on numerical formats I recommend this blog.\nAutomated mixed precision runs some operations (nn.Linear, nn.Conv*d) in fp16 , whereas other operations that are more vulnerable to numerical approximations are run in higher precision fp32. Gradients w/r to model parameters and optimizer stats are typically kept in fp32. Parameter updates act on fp32 copy of the model.\n\ndef train(model, loader, optimizer, scaler, loss_fn, num_steps):\n  \"\"\"\n  Args:\n    model - model trained\n    loader - data loader\n    optimizer - optimizer\n    scaler - gradient scaler that shifts the gradient range\n    loss_fn - task loss function\n    num_steps.- number of training steps\n  Returns:\n    dict with performance stats\n  \"\"\"\n  samples_processed = 0\n  start = time.perf_counter()\n  for i in trange(num_steps, total=num_steps, desc=\"Train\"):\n    inputs, targets = next(loader)\n    inputs, targets = inputs.to(device), targets.to(device)\n    with torch.autocast(device_type='cuda', dtype=torch.float16):\n      outputs = model(inputs)\n      loss = loss_fn(outputs, targets)\n    assert not loss.isnan(), \"Loss is NaN. Terminating training.\"\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n    optimizer.zero_grad()\n    samples_processed += len(inputs)\n  end = time.perf_counter()\n  samples_per_sec = samples_processed / (end - start)\n  print(\"\\n\")\n  print_memory_stats()\n  print(f\"samples/sec: {samples_per_sec:.2f}\")\n  results = {\n      \"samples/sec\": samples_per_sec,\n      \"max_memory_allocated\": torch.cuda.max_memory_allocated() / 2 ** 30\n  }\n  return results\n\n\nresults_for_batch_size_amp = {}\nfor batch_size in (1, 2, 4, 8, 16, 32, 64, 128):\n  print(\"-\" * 10)\n  print(f\"Batch size: {batch_size}\")\n  # prepare dataloader\n  dataloader = make_dummy_dataloader(batch_size)\n  # prepare optimizer\n  optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n  # prepare scaler\n  scaler = GradScaler()\n  # train\n  try:\n    results_for_batch_size_amp[batch_size] = train(model, dataloader, optimizer, scaler, F.cross_entropy, training_steps)\n  except:\n    print(\"\\nCUDA out of memory!\")\n    print_memory_stats()\n    del optimizer\n    torch.cuda.empty_cache()\n    torch.cuda.reset_peak_memory_stats()\n    break\n  del optimizer\n  torch.cuda.empty_cache()\n  torch.cuda.reset_peak_memory_stats()\n\n----------\nBatch size: 1\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:08&lt;00:00, 11.23it/s]\n\n\n\n\nCUDA max memory allocated: 1.67 GB.\nCUDA max memory reserved: 1.82 GB.\nsamples/sec: 11.22\n----------\nBatch size: 2\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09&lt;00:00, 10.05it/s]\n\n\n\n\nCUDA max memory allocated: 1.68 GB.\nCUDA max memory reserved: 1.84 GB.\nsamples/sec: 20.10\n----------\nBatch size: 4\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09&lt;00:00, 10.48it/s]\n\n\n\n\nCUDA max memory allocated: 1.73 GB.\nCUDA max memory reserved: 1.91 GB.\nsamples/sec: 41.89\n----------\nBatch size: 8\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:11&lt;00:00,  8.94it/s]\n\n\n\n\nCUDA max memory allocated: 2.31 GB.\nCUDA max memory reserved: 2.52 GB.\nsamples/sec: 71.42\n----------\nBatch size: 16\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:18&lt;00:00,  5.44it/s]\n\n\n\n\nCUDA max memory allocated: 3.42 GB.\nCUDA max memory reserved: 3.88 GB.\nsamples/sec: 86.95\n----------\nBatch size: 32\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:34&lt;00:00,  2.94it/s]\n\n\n\n\nCUDA max memory allocated: 5.82 GB.\nCUDA max memory reserved: 6.44 GB.\nsamples/sec: 94.02\n----------\nBatch size: 64\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:06&lt;00:00,  1.50it/s]\n\n\n\n\nCUDA max memory allocated: 10.66 GB.\nCUDA max memory reserved: 11.57 GB.\nsamples/sec: 95.90\n----------\nBatch size: 128\n\n\nTrain:   0%|          | 0/100 [00:00&lt;?, ?it/s]\n\n\n\nCUDA out of memory!\nCUDA max memory allocated: 1.61 GB.\n\n\n\n\n\n\nCUDA max memory reserved: 2.44 GB.\n\n\n\nbatch_sizes_amp = []\nsamples_sec_per_batch_size_amp = []\nmax_memory_per_batch_size_amp = []\nfor k, v in results_for_batch_size_amp.items():\n  batch_sizes_amp.append(k)\n  samples_sec_per_batch_size_amp.append(v['samples/sec'])\n  max_memory_per_batch_size_amp.append(v['max_memory_allocated'])\n\n\nfig, ax = plt.subplots(ncols=2, figsize=(12, 5))\n\nax[0].plot(batch_sizes, samples_sec_per_batch_size, '-v', label='fp32');\nax[0].plot(batch_sizes_amp, samples_sec_per_batch_size_amp, '-v', label='amp');\nax[0].set_xlabel(\"Batch size\", fontsize=20);\nax[0].set_ylabel(\"Samples/sec\", fontsize=20);\nax[0].legend();\n\nax[1].plot(batch_sizes, max_memory_per_batch_size, '-v', label='fp32');\nax[1].plot(batch_sizes_amp, max_memory_per_batch_size_amp, '-v', label='amp');\nax[1].set_xlabel(\"Batch size\", fontsize=20);\nax[1].set_ylabel(\"Max memory (Gb)\", fontsize=20);\nax[1].legend();\n\n\n\n\n\n\n\n\n\nprint(f\"Speedup: {max(samples_sec_per_batch_size_amp) / max(samples_sec_per_batch_size):.2f}x\")\n\nSpeedup: 2.32x\n\n\nWith amp we have accelerated training by more than 2x!\nLargest batch size that fits onto the memory of device has almost doubled as well.\n\nGrad scaler\nIn the example above we‚Äôve observed some mysterious entity called GradScaler. Why do we need it?\nNote, that fp16 has much narrower range of representable values compared to fp32.\n\nIf the values inside a tensor are too small or to large they may not fit inside fp16, that may lead to divergence, especially if training large models for a long time.\n\nGradScaler tracks the magnitude of the loss, builds historgram of gradients and shifts it towards the range that is well represented by the fp16 numerical format.\nLet us illustrate the problem on a toy example below.\n\nW = torch.randn(8192, 8192, device=device, dtype=torch.float16, requires_grad=True)\nX = torch.randn(8, 8192, device=device, dtype=torch.float16)\nY = torch.randn(8, 8192, device=device, dtype=torch.float16)\n\n\nloss = F.mse_loss(X @ W.T, Y)\n\n\nloss\n\ntensor(inf, device='cuda:0', dtype=torch.float16, grad_fn=&lt;MseLossBackward0&gt;)\n\n\nIndeed this example is arfiticial, as one would init weights \\(w_{ij} \\in \\mathcal{N}(0, \\frac{1}{d})\\) but shows the point.\n\ndel W, X, Y, loss\ntorch.cuda.empty_cache()\n\nMaterials for further study.\n\nPyTorch amp examples\nGreat blog about amp by NVIDIA\nA page about fp8 format in Transformer engine"
  },
  {
    "objectID": "projects/Efficient_model_training.html#gradient-accumulation",
    "href": "projects/Efficient_model_training.html#gradient-accumulation",
    "title": "",
    "section": "Gradient accumulation",
    "text": "Gradient accumulation\nCommonly, a researcher or a practioner follows standard training recipes from papers. However, it is often the case that hardware doesn‚Äôt allow to train on the large batches that are affordable for Google and OpenAI guys.\nIn principle, one can fit ‚Äòalmost‚Äô arbitrary large batch size via gradient checkpointing illustrated below.\nThe idea behind gradient accumulation is to instead of calculating the gradients for the whole batch at once to do it in smaller steps. The way we do that is to calculate the gradients iteratively in smaller batches by doing a forward and backward pass through the model and accumulating the gradients in the process. When enough gradients are accumulated we run the model‚Äôs optimization step. This way we can easily increase the overall batch size to numbers that would never fit into the GPU‚Äôs memory. In turn, however, the added forward and backward passes can slow down the training a bit.\n\ndef train(model, loader, optimizer, scaler, loss_fn, num_steps, grad_accum_steps=1):\n  \"\"\"\n  Args:\n    model - model trained\n    loader - data loader\n    optimizer - optimizer\n    scaler - gradient scaler that shifts the gradient range\n    loss_fn - task loss function\n    num_steps - number of training steps\n    grad_accum_steps - number of gradient accumulation steps\n  Returns:\n    dict with performance stats\n  \"\"\"\n  samples_processed = 0\n  start = time.perf_counter()\n  for i in trange(num_steps, total=num_steps, desc=\"Train\"):\n    inputs, targets = next(loader)\n    inputs, targets = inputs.to(device), targets.to(device)\n    with torch.autocast(device_type='cuda', dtype=torch.float16):\n      outputs = model(inputs)\n      loss = loss_fn(outputs, targets)\n    assert not loss.isnan(), \"Loss is NaN. Terminating training.\"\n    scaler.scale(loss / grad_accum_steps).backward()\n    if (i + 1) % grad_accum_steps == 0:\n        scaler.step(optimizer)\n        scaler.update()\n        optimizer.zero_grad()\n    samples_processed += len(inputs)\n  end = time.perf_counter()\n  samples_per_sec = samples_processed / (end - start)\n  print(\"\\n\")\n  print_memory_stats()\n  print(f\"samples/sec: {samples_per_sec:.2f}\")\n  results = {\n      \"samples/sec\": samples_per_sec,\n      \"max_memory_allocated\": torch.cuda.max_memory_allocated() / 2 ** 30\n  }\n  return results\n\nIn the example below largest batch size, that fit into our memory was 64. Let us use it as a microbatch_size.\nTotal batch size is microbatch_size * grad_accum_steps.\n\nresults_for_batch_size_grad_accum = {}\n\nmicrobatch_size = 64\ntraining_steps = 100\n\nfor batch_size in (512, 1024, 2048):\n  print(\"-\" * 10)\n  print(f\"Batch size: {batch_size}\")\n  grad_accum_steps = batch_size // microbatch_size\n  # prepare dataloader\n  dataloader = make_dummy_dataloader(microbatch_size)\n  # prepare optimizer\n  optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n  # prepare scaler\n  scaler = GradScaler()\n  # train\n  results_for_batch_size_grad_accum[batch_size] = train(\n    model, dataloader, optimizer, scaler, F.cross_entropy, training_steps, grad_accum_steps\n  )\n  del optimizer\n  torch.cuda.empty_cache()\n  torch.cuda.reset_peak_memory_stats()\n\n----------\nBatch size: 512\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:23&lt;00:00,  1.20it/s]\n\n\n\n\nCUDA max memory allocated: 10.56 GB.\nCUDA max memory reserved: 10.74 GB.\nsamples/sec: 76.79\n----------\nBatch size: 1024\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:22&lt;00:00,  1.21it/s]\n\n\n\n\nCUDA max memory allocated: 10.56 GB.\nCUDA max memory reserved: 10.84 GB.\nsamples/sec: 77.72\n----------\nBatch size: 2048\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:22&lt;00:00,  1.22it/s]\n\n\n\n\nCUDA max memory allocated: 10.56 GB.\nCUDA max memory reserved: 10.86 GB.\nsamples/sec: 78.00\n\n\n\nbatch_sizes_grad_accum = []\nsamples_sec_per_batch_size_grad_accum = []\nmax_memory_per_batch_size_grad_accum = []\nfor k, v in results_for_batch_size_grad_accum.items():\n  batch_sizes_grad_accum.append(k)\n  samples_sec_per_batch_size_grad_accum.append(v['samples/sec'])\n  max_memory_per_batch_size_grad_accum.append(v['max_memory_allocated'])\n\n\nfig, ax = plt.subplots(ncols=2, figsize=(12, 5))\n\nax[0].plot(batch_sizes_grad_accum, samples_sec_per_batch_size_grad_accum, '-v');\nax[0].set_xlabel(\"Batch size\", fontsize=20);\nax[0].set_ylabel(\"Samples/sec\", fontsize=20);\n\nax[1].plot(batch_sizes_grad_accum, max_memory_per_batch_size_grad_accum, '-v');\nax[1].set_xlabel(\"Batch size\", fontsize=20);\nax[1].set_ylabel(\"Max memory (Gb)\", fontsize=20);\nax[1].set_yticks([10.5, 10.6]);\n\n\n\n\n\n\n\n\nOne can fit large batch size with gradient checkpointing.\nNote, however, that it doesn‚Äôt make the training faster in termps of samples/sec.\n## Gradient checkpointing\nEven when we set the batch size to 1 and use gradient accumulation we can still run out of memory when working with large models. In order to compute the gradients during the backward pass all activations from the forward pass are normally saved. This can create a big memory overhead. Alternatively, one could forget all activations during the forward pass and recompute them on demand during the backward pass.\nThis allows one reduce memory footprint at the cost of additional computations.\n\nfrom types import MethodType\nfrom torch.utils.checkpoint import checkpoint_sequential\n\nBelow we‚Äôll adopt the training loop from AMP section\n\ndef train(model, loader, optimizer, scaler, loss_fn, num_steps):\n  \"\"\"\n  Args:\n    model - model trained\n    loader - data loader\n    optimizer - optimizer\n    scaler - gradient scaler that shifts the gradient range\n    loss_fn - task loss function\n    num_steps.- number of training steps\n  Returns:\n    dict with performance stats\n  \"\"\"\n  samples_processed = 0\n  start = time.perf_counter()\n  for i in trange(num_steps, total=num_steps, desc=\"Train\"):\n    inputs, targets = next(loader)\n    inputs, targets = inputs.to(device), targets.to(device)\n    with torch.autocast(device_type='cuda', dtype=torch.float16):\n      outputs = model(inputs)\n      loss = loss_fn(outputs, targets)\n    assert not loss.isnan(), \"Loss is NaN. Terminating training.\"\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n    optimizer.zero_grad()\n    samples_processed += len(inputs)\n  end = time.perf_counter()\n  samples_per_sec = samples_processed / (end - start)\n  print(\"\\n\")\n  print_memory_stats()\n  print(f\"samples/sec: {samples_per_sec:.2f}\")\n  results = {\n      \"samples/sec\": samples_per_sec,\n      \"max_memory_allocated\": torch.cuda.max_memory_allocated() / 2 ** 30\n  }\n  return results\n\nLet us create the model again for the sake of safety\n\nmodel = get_model(\"swin_b\").to(device)\n\n\nnum_segments = len(model.features) # we set checkpoint after every SwinTransformer block\n\n# a hack to change forward pass of a model\ndef custom_forward(self, x):\n    x.requires_grad = True\n    x = checkpoint_sequential(model.features, num_segments, x)\n    x = self.norm(x)\n    x = self.permute(x)\n    x = self.avgpool(x)\n    x = self.flatten(x)\n    x = self.head(x)\n    return x\n\nmodel.forward = MethodType(custom_forward, model)\n\n\nresults_for_batch_size_grad_ckpt = {}\nfor batch_size in (1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024):\n  print(\"-\" * 10)\n  print(f\"Batch size: {batch_size}\")\n  # prepare dataloader\n  dataloader = make_dummy_dataloader(batch_size)\n  # prepare optimizer\n  optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n  # prepare scaler\n  scaler = GradScaler()\n  # train\n  try:\n    training_steps = min(4096 // batch_size, 100)\n    results_for_batch_size_grad_ckpt[batch_size] = train(model, dataloader, optimizer, scaler, F.cross_entropy, training_steps)\n  except:\n    print(\"\\nCUDA out of memory!\")\n    print_memory_stats()\n    del optimizer\n    torch.cuda.empty_cache()\n    torch.cuda.reset_peak_memory_stats()\n    break\n  del optimizer\n  torch.cuda.empty_cache()\n  torch.cuda.reset_peak_memory_stats()\n\n----------\nBatch size: 1\n\n\nTrain:   0%|          | 0/100 [00:00&lt;?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:542: UserWarning: torch.utils.checkpoint.checkpoint_sequential: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:16&lt;00:00,  6.14it/s]\n\n\n\n\nCUDA max memory allocated: 2.14 GB.\nCUDA max memory reserved: 2.23 GB.\nsamples/sec: 6.14\n----------\nBatch size: 2\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:15&lt;00:00,  6.30it/s]\n\n\n\n\nCUDA max memory allocated: 2.14 GB.\nCUDA max memory reserved: 2.24 GB.\nsamples/sec: 12.59\n----------\nBatch size: 4\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:15&lt;00:00,  6.32it/s]\n\n\n\n\nCUDA max memory allocated: 2.14 GB.\nCUDA max memory reserved: 2.30 GB.\nsamples/sec: 25.27\n----------\nBatch size: 8\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:20&lt;00:00,  4.91it/s]\n\n\n\n\nCUDA max memory allocated: 2.38 GB.\nCUDA max memory reserved: 2.60 GB.\nsamples/sec: 39.26\n----------\nBatch size: 16\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:33&lt;00:00,  3.01it/s]\n\n\n\n\nCUDA max memory allocated: 3.03 GB.\nCUDA max memory reserved: 3.28 GB.\nsamples/sec: 48.19\n----------\nBatch size: 32\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:02&lt;00:00,  1.60it/s]\n\n\n\n\nCUDA max memory allocated: 4.34 GB.\nCUDA max memory reserved: 4.69 GB.\nsamples/sec: 51.15\n----------\nBatch size: 64\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [01:18&lt;00:00,  1.23s/it]\n\n\n\n\nCUDA max memory allocated: 6.98 GB.\nCUDA max memory reserved: 8.12 GB.\nsamples/sec: 52.18\n----------\nBatch size: 128\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [01:18&lt;00:00,  2.44s/it]\n\n\n\n\nCUDA max memory allocated: 12.23 GB.\nCUDA max memory reserved: 14.29 GB.\nsamples/sec: 52.44\n----------\nBatch size: 256\n\n\nTrain:   0%|          | 0/16 [00:02&lt;?, ?it/s]\n\n\n\nCUDA out of memory!\nCUDA max memory allocated: 14.10 GB.\nCUDA max memory reserved: 14.51 GB.\n\n\n\n\n\n\nbatch_sizes_grad_ckpt = []\nsamples_sec_per_batch_size_grad_ckpt = []\nmax_memory_per_batch_size_grad_ckpt = []\nfor k, v in results_for_batch_size_grad_ckpt.items():\n  batch_sizes_grad_ckpt.append(k)\n  samples_sec_per_batch_size_grad_ckpt.append(v['samples/sec'])\n  max_memory_per_batch_size_grad_ckpt.append(v['max_memory_allocated'])\n\n\nfig, ax = plt.subplots(ncols=2, figsize=(12, 5))\n\nax[0].plot(batch_sizes_amp, samples_sec_per_batch_size_amp, '-v', label='amp');\nax[0].plot(batch_sizes_grad_ckpt, samples_sec_per_batch_size_grad_ckpt, '-v', label='grad_ckpt');\nax[0].set_xlabel(\"Batch size\", fontsize=20);\nax[0].set_ylabel(\"Samples/sec\", fontsize=20);\nax[0].legend();\n\nax[1].plot(batch_sizes_amp, max_memory_per_batch_size_amp, '-v', label='amp');\nax[1].plot(batch_sizes_grad_ckpt, max_memory_per_batch_size_grad_ckpt, '-v', label='grad_ckpt');\nax[1].set_xlabel(\"Batch size\", fontsize=20);\nax[1].set_ylabel(\"Max memory (Gb)\", fontsize=20);\nax[1].legend();\n\n\n\n\n\n\n\n\nNice! üôÄ\nNow we can fit even larger batch size 128 onto a single T4 GPU!\nNote, that this tecnique doesn‚Äôt typically accelerate training.\nHowever, in case the model is pretty large (such as modern transformer), and inputs are pretty large (large images or long sequences), gradient_checkpointing is very reasonable option to try.\nSome frameworks and libraries support gradient_checkpointing out of the box: * timm - set_grad_checkpointing method in some models * transformers ü§ó - model.gradient_checkpointing_enable()\nAdditional resources: * Article on gradient checkpointing"
  },
  {
    "objectID": "projects/Efficient_model_training.html#model-compilation",
    "href": "projects/Efficient_model_training.html#model-compilation",
    "title": "",
    "section": "Model compilation",
    "text": "Model compilation\nSince torch &gt; 2.0 users can compile their model prior to running. Compilation can make PyTorch code run faster by JIT-compiling PyTorch code into optimized kernels, all while requiring minimal code changes.\nHowever, note that the compilation takes some time, as the users familiar with compiled languages may remember.\nIf the run is very short it may be not the best option.\nFor long enough runtime torch.compile offers nontrivial speed-up of training and inference.\nBelow we will illustrate the benefit of torch.compile on RMSNorm, a commonly used normalization layer in modern LLMs.\n\nclass RMSNorm(nn.Module):\n\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        RMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        input_dtype = hidden_states.dtype\n        hidden_states = hidden_states.to(torch.float32)\n        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * hidden_states.to(input_dtype)\n\n\n# Llama-2-7b sizes\nbatch_size = 1\nsequence_length = 4096\nhidden_size = 4096\n# benchmark settings\nwarmup_iters = 1000\nbenchmark_iters = 1000\n\n\n# generate random sequence\ninputs = torch.randn(batch_size, sequence_length, hidden_size, device=device, dtype=torch.float16)\n\nDefine RMSNorm layer\n\nrms_norm = RMSNorm(hidden_size).to(device=device, dtype=torch.float16)\n\n\nfor _ in range(warmup_iters):\n  rms_norm(inputs)\n\ntimes = []\nfor _ in range(benchmark_iters):\n  start = torch.cuda.Event(enable_timing=True)\n  end = torch.cuda.Event(enable_timing=True)\n  start.record()\n  rms_norm(inputs)\n  end.record()\n  torch.cuda.synchronize()\n  times.append(start.elapsed_time(end))\n\n\nprint(f\"Average latency: {np.mean(times):.3f} ms\")\n\nAverage latency: 2.627 ms\n\n\nLet us compile the layer.\n\nrms_norm = torch.compile(rms_norm)\n\n\nfor _ in range(warmup_iters):\n  rms_norm(inputs)\n\ntimes = []\nfor _ in range(benchmark_iters):\n  start = torch.cuda.Event(enable_timing=True)\n  end = torch.cuda.Event(enable_timing=True)\n  start.record()\n  rms_norm(inputs)\n  end.record()\n  torch.cuda.synchronize()\n  times.append(start.elapsed_time(end))\n\n\nprint(f\"Average latency: {np.mean(times):.3f} ms\")\n\nAverage latency: 0.449 ms\n\n\n4-5x speedup!\n\n\ndel rms_norm, inputs\ntorch.cuda.empty_cache()\n\nNow let us apply torch compile to training the model from previous examples.\n\ndef train(model, loader, optimizer, scaler, loss_fn, num_steps, warmup_steps=10):\n  \"\"\"\n  Args:\n    model - model trained\n    loader - data loader\n    optimizer - optimizer\n    scaler - gradient scaler that shifts the gradient range\n    loss_fn - task loss function\n    num_steps - number of training steps\n    num_steps - number of warmup steps\n  Returns:\n    dict with performance stats\n  \"\"\"\n  samples_processed = 0\n  # warmup\n  start = time.perf_counter()\n  for i in range(warmup_steps):\n    inputs, targets = next(loader)\n    inputs, targets = inputs.to(device), targets.to(device)\n    with torch.autocast(device_type='cuda', dtype=torch.float16):\n      outputs = model(inputs)\n      loss = loss_fn(outputs, targets)\n    scaler.scale(loss).backward()\n    optimizer.zero_grad()\n  end = time.perf_counter()\n  compile_time = end - start\n  print(f\"Compilation took {(end - start):.2f}s.\")\n  # start of training loop\n  start = time.perf_counter()\n  for i in trange(num_steps , total=num_steps, desc=\"Train\"):\n    inputs, targets = next(loader)\n    inputs, targets = inputs.to(device), targets.to(device)\n    with torch.autocast(device_type='cuda', dtype=torch.float16):\n      outputs = model(inputs)\n      loss = loss_fn(outputs, targets)\n    assert not loss.isnan(), \"Loss is NaN. Terminating training.\"\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n    optimizer.zero_grad()\n    samples_processed += len(inputs)\n  end = time.perf_counter()\n  samples_per_sec = samples_processed / (end - start)\n  print(\"\\n\")\n  print_memory_stats()\n  print(f\"samples/sec: {samples_per_sec:.2f}\")\n  results = {\n      \"samples/sec\": samples_per_sec,\n      \"max_memory_allocated\": torch.cuda.max_memory_allocated() / 2 ** 30,\n      \"compile_time\": compile_time\n  }\n  return results\n\n\nmodel = get_model(\"swin_b\").to(device)\n\n\nmodel = torch.compile(model)\n\n/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n\n\n\nresults_for_batch_size_compile = {}\ntraining_steps = 100\nwarmup_steps = 10\n\nfor batch_size in (1, 2, 4, 8, 16, 32, 64):\n  print(\"-\" * 10)\n  print(f\"Batch size: {batch_size}\")\n  # prepare dataloader\n  dataloader = make_dummy_dataloader(batch_size)\n  # prepare optimizer\n  optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n  # prepare scaler\n  scaler = GradScaler()\n  # train\n  try:\n    results_for_batch_size_compile[batch_size] = train(model, dataloader, optimizer, scaler, F.cross_entropy, training_steps, warmup_steps)\n  except:\n    print(\"\\nCUDA out of memory!\")\n    print_memory_stats()\n    del optimizer\n    torch.cuda.empty_cache()\n    torch.cuda.reset_peak_memory_stats()\n    break\n  del optimizer\n  torch.cuda.empty_cache()\n  torch.cuda.reset_peak_memory_stats()\n\n----------\nBatch size: 1\nCompilation took 110.28s.\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:08&lt;00:00, 11.23it/s]\n\n\n\n\nCUDA max memory allocated: 1.67 GB.\nCUDA max memory reserved: 1.80 GB.\nsamples/sec: 11.22\n----------\nBatch size: 2\nCompilation took 228.16s.\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10&lt;00:00,  9.47it/s]\n\n\n\n\nCUDA max memory allocated: 1.68 GB.\nCUDA max memory reserved: 1.80 GB.\nsamples/sec: 18.93\n----------\nBatch size: 4\nCompilation took 0.65s.\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09&lt;00:00, 10.61it/s]\n\n\n\n\nCUDA max memory allocated: 1.73 GB.\nCUDA max memory reserved: 1.91 GB.\nsamples/sec: 42.42\n----------\nBatch size: 8\nCompilation took 0.79s.\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:11&lt;00:00,  8.85it/s]\n\n\n\n\nCUDA max memory allocated: 2.30 GB.\nCUDA max memory reserved: 2.50 GB.\nsamples/sec: 70.78\n----------\nBatch size: 16\nCompilation took 1.45s.\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:18&lt;00:00,  5.40it/s]\n\n\n\n\nCUDA max memory allocated: 3.42 GB.\nCUDA max memory reserved: 3.86 GB.\nsamples/sec: 86.31\n----------\nBatch size: 32\nCompilation took 2.82s.\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:34&lt;00:00,  2.92it/s]\n\n\n\n\nCUDA max memory allocated: 5.83 GB.\nCUDA max memory reserved: 6.46 GB.\nsamples/sec: 93.34\n----------\nBatch size: 64\nCompilation took 5.45s.\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:08&lt;00:00,  1.47it/s]\n\n\n\n\nCUDA max memory allocated: 10.66 GB.\nCUDA max memory reserved: 11.63 GB.\nsamples/sec: 93.77\n\n\nHow long does the compilation take? ‚è≤\n\nbatch_sizes_compile = []\nsamples_sec_per_batch_size_compile = []\nmax_memory_per_batch_size_compile = []\nfor k, v in results_for_batch_size_compile.items():\n  batch_sizes_compile.append(k)\n  samples_sec_per_batch_size_compile.append(v['samples/sec'])\n  max_memory_per_batch_size_compile.append(v['max_memory_allocated'])\n\n\nfig, ax = plt.subplots(ncols=2, figsize=(12, 5))\n\nax[0].plot(batch_sizes_amp, samples_sec_per_batch_size_amp, '-v', label='amp');\nax[0].plot(batch_sizes_compile, samples_sec_per_batch_size_compile, '-v', label='compile');\nax[0].set_xlabel(\"Batch size\", fontsize=20);\nax[0].set_ylabel(\"Samples/sec\", fontsize=20);\nax[0].legend();\n\nax[1].plot(batch_sizes_amp, max_memory_per_batch_size_amp, '-v', label='amp');\nax[1].plot(batch_sizes_compile, max_memory_per_batch_size_compile, '-v', label='compile');\nax[1].set_xlabel(\"Batch size\", fontsize=20);\nax[1].set_ylabel(\"Max memory (Gb)\", fontsize=20);\nax[1].legend();\n\n\n\n\n\n\n\n\nDo you observe benefits from compilation?\nAdditional resources * PyTorch tutorial"
  },
  {
    "objectID": "projects/Efficient_model_training.html#summary-and-concluding-remarks",
    "href": "projects/Efficient_model_training.html#summary-and-concluding-remarks",
    "title": "",
    "section": "Summary and concluding remarks",
    "text": "Summary and concluding remarks\nIn the table below we summarize the benefits proposed by each method covered in the seminar.\n\n\n\nMethod\nSpeed\nMemory\n\n\n\n\nBatch size\nYes\nYes\n\n\nMixed precision training\nYes\nYes*\n\n\nGradient accumulation\nNo\nYes\n\n\nGradient checkpointing\nNo\nYes\n\n\nCompilation\nYes\nNo\n\n\n\nSome notes are worth being mentioned. * AMP is a great choice for small model, when the model takes small fraction of the system VRAM. However, it requires to store a version of model both in fp16 and fp32 precision and may even increase memory overhead when working with large models.\nAdditional materials * Great overview with examples on Huggingface on efficient training ü§ó"
  },
  {
    "objectID": "projects/DDPM_eng.html",
    "href": "projects/DDPM_eng.html",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "",
    "text": "Author: Addisu Amare"
  },
  {
    "objectID": "projects/DDPM_eng.html#another-view-to-vae-models",
    "href": "projects/DDPM_eng.html#another-view-to-vae-models",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "1. Another view to VAE models",
    "text": "1. Another view to VAE models\nQuestions for the audience\n\nWhat are discriminative models?\nWhat are generative models?\nWhat task do generative models solve?\n\nHaving defined the main task of generative models, it is logical to ask yourself the question - ‚ÄúHow do we actually evaluate the probability of objects from the training sample?‚Äù\n\n\n\ntitle\n\n\n\nThe principle of maximum likelihood:\n\nWhen we want to train a generative model \\(p(x|\\theta)\\), then the first attempt to do this is the basis for solving the so-called \\(\\textbf{MLE-problem}\\) or the problem of maximizing the likelihood of the model by selecting its parameters:\n\\[\\theta = \\arg\\max_{\\theta} \\log p(X|\\theta) = \\{ X = \\{ x_{i}\\}_{i=1}^{n}\\}= \\arg\\max_{\\theta} \\sum_{i=1}^{n} \\log p(x_{i}|\\theta)\\]\nQuestions for the audience Why are we not satisfied with this approach?\n\nLatent space:\n\nBefore we move on to another way of estimating the likelihood of data, we will recall the essence of latent representations. The main intuition of which is shown in the picture below:\n\n\n\ntitle\n\n\n\nModels of the variation encoder:\n\nAnother attempt to assess the likelihood is to introduce latent variables and consider the VAE model. When we are dealing with a variational autoencoder, we do not have access to an honest value of the logarithm of likelihood, so we optimize the corresponding lower bound, which we call as \\(\\textbf{ELBO}\\)\n\\[ \\log p(x|\\theta) \\geq \\mathcal{L}(\\theta,q) = \\int_{Z}q(z|x, \\phi)\\log\\frac{p(x,z|\\theta)}{q(z|x,\\phi)} dz\\]\n\nExpansion of the latent space: more is better than one\n\nWe know that the variational autoencoder has exactly one latent space. That is, we entered the latent space with an encoder and \\(\\bf{immediately}\\) leave it with the help of a decoder, while we do not explore the latent space in any way.\nHowever, let‚Äôs try to expand the number of latent spaces by introducing \\(T\\) consecutive latent spaces with corresponding decompositions of \\(f_{i}\\). That is, we use the encoder to move into the latent space and walk through the latent space for a certain number of steps\n\n\n\ntitle"
  },
  {
    "objectID": "projects/DDPM_eng.html#hierarchical-vae-models",
    "href": "projects/DDPM_eng.html#hierarchical-vae-models",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "2. Hierarchical VAE models",
    "text": "2. Hierarchical VAE models\nFor the simplicity, let me denote the follwoing:\n\nx = \\(x_{0}\\)\n\\(z_{1} = x_{1}\\)\n\\(z_{2} = x_{2}\\)\n\\(z_{T} = x_{T}\\)\n\nAfter introduction more convinient notation for \\(T\\) dimensional vectors, we should define transfromation functions between \\(f_{i}\\) between latent statements. Undoubtedly, one can consider neural networks for this purpose , however, do not worth to complicate our life\n\\(\\textbf{Assumption :}\\) Let these functions \\(f_{i}\\) are \\(\\textbf{not-learnable}\\) certain transfromations. Since we would like to add some stochasticity to the framework, one can consider fixed distributions as such transformations:\n\\[ q(x_{t}|x_{t_1}) = \\mathcal{N}(x_{t}| x_{t-1}, \\beta I) \\]\nNow, our method looks like \\(\\textbf{Brownian motion}\\) or \\(\\textbf{Random movements}\\).\n\n\n\ntitle\n\n\nHowever, if we will learn means of such transformations (normal distributions) , then we obtain method that is referred to as \\(\\textbf{Hierarchical VAE}\\). Please, see this \\(\\href{https://jmtomczak.github.io/blog/9/9_hierarchical_lvm_p1.html}{blog}\\) for best understanding of this concept\nThus, thanks to this copncept, we realize that one can whole latent spaces \\(Z = \\{ x_{1},...,x_{T}\\}\\) and then the corresponding \\(\\textbf{ELBO}\\) formula is:\n\\[ \\log p(x|\\theta) \\geq \\mathcal{L}(\\theta,q) = \\int_{Z}q(x_{1},...,x_{T}|x_{0}, \\phi)\\log\\frac{p(x_{0},x_{1},..,x_{T}|\\theta)}{q(x_{1},...,x_{T}|x_{0},\\phi)} dx_{1:T}\\]"
  },
  {
    "objectID": "projects/DDPM_eng.html#gaussian-diffusion-processes",
    "href": "projects/DDPM_eng.html#gaussian-diffusion-processes",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "3. Gaussian diffusion processes",
    "text": "3. Gaussian diffusion processes"
  },
  {
    "objectID": "projects/DDPM_eng.html#what-is-the-main-motivation-to-consider-many-steps-in-latent-space",
    "href": "projects/DDPM_eng.html#what-is-the-main-motivation-to-consider-many-steps-in-latent-space",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "1. What is the main motivation to consider many steps in latent space?",
    "text": "1. What is the main motivation to consider many steps in latent space?\n - answer is here"
  },
  {
    "objectID": "projects/DDPM_eng.html#what-components-do-lie-at-the-heart-of-the-loss-function-of-vae",
    "href": "projects/DDPM_eng.html#what-components-do-lie-at-the-heart-of-the-loss-function-of-vae",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "2. What components do lie at the heart of the loss function of VAE?",
    "text": "2. What components do lie at the heart of the loss function of VAE?\n - answer is here\n\n\n\ntitle\n\n\nBecause the quality of generation of the hierarchical model of the variation encoder is better than the usual one.\nA question to the audience\n\nthen what conclusion does the number of latent representations suggest?\n\nThen we realize that the more latent spaces we consider, the better and better the quality of the model.\nThen the question arises - ‚ÄúBut if you take a lot of such latent representations, what will happen?‚Äù\nAnd answering this question, let‚Äôs slightly correct our transitional density of the hierarchical model of the variation encoder:\n\\[ q(x_t | x_{t-1}) = \\mathcal{N}(x_t | x_{t-1}, \\beta I) \\to q(x_t | x_{t-1}) = \\mathcal{N}(x_t | \\sqrt{1-\\beta}x_{t-1}, \\beta I)\\]\nThen it turns out that such a transient density determines the Markov process (where the present \\(x_{t}\\) depends only on the past \\(x_{t-1}\\)):\n\\[ x_t = \\sqrt{1 - \\beta} x_{t-1} + \\sqrt{\\beta} \\epsilon \\]\nwhere $ $ is usually a standard normal random variable\nThus, our transition process between latent spaces is a Markov process, and here‚Äôs what‚Äôs interesting to say about it:\n\n\n\ntitle\n\n\n\nTheorem 1:\n\nGiven:\n\n$ x_0 (x) $\n$(0,1) $\n\n\nThen applying the Markov chain to an arbitrary distribution of \\(\\pi(x)\\)infinitely many times, we get \\(\\mathcal{N}(0,I)\\). Thus, \\(\\mathcal{N}(0,I)\\) is a stationary distribution of the chain. That is, the following condition will be fulfilled $ p_(x) = (0, I) = q(x |x‚Äô) p_(x‚Äô) dx‚Äô $\nIf we denote $ t ={s=1}^{t} (1 - _s) $. Then we can express the sample of the process at any point in time using:\n\n\\[ x_t = \\sqrt{\\overline{\\alpha}_t} x_0 + \\sqrt{1 - \\overline{\\alpha}_t} \\epsilon \\]\n\\[ q(x_t | x_0) = \\mathcal{N}(x_t | \\sqrt{\\overline{\\alpha}_t} x_0, (1 - \\overline{\\alpha}_t) I) \\]\nThis means that we can select any \\(x_t\\) using only \\(x_0\\).\n\n\n\nChessUrl\n\n\n\\(\\textbf{The essence of theorem 1}\\)\n\nThe Markov process that we are considering will transform any data distribution into a normal standard distribution in an infinitely long time\nThe process is the so-called \\(\\textbf{free simulation}\\), that is, taking any point \\(x_{0}\\) and any point in time \\(t\\), you can instantly find \\(x_{t}\\)\n\nWe have proved the existence of a stochastic transformation from data to noise.\nRemember that the diffusion process does not depend on the initial density of \\(\\pi(x)\\)(complex) and the only requirement is access to a sample from it. The main idea of diffusion models is to use any data distribution of our choice as a complex initial density and gradually noise them. Thus, we understand the direct diffusion process as:\n\\[ x_{0} \\sim p_{data}(x) \\implies \\mathcal{F}(x_{0}) = x_{T} : x_{T} \\sim \\mathcal{N}(0,I) \\]\nThe idea: We have an equation for the direct noise reduction process that looks like this:\n\\[ dx_t = \\sqrt{1 - \\beta_t} x_{t-1} + \\sqrt{\\beta} \\epsilon \\]\nThanks to this equation, you can construct \\(\\color{red}{\\textbf{!! Untrained !!}}\\) trajectory from data to noise.\nIf we are dealing with an ordinary differential equation (ODE), then we can run this ODE in reverse time and get trajectories from noise to data. However, our process is not defined by an ODE, but by some kind of complex Stochastic diff equation. And this means that I would like to learn how to unfold such random equations in time.\nMotivation for learning diffusion models\n\nThere is a process from data to noise\nI want to expand the process\nThe detailed process goes from noise to data\n\nAlso, one can compare architectures and concepts of another generative models:\n\nVAE\nFLOW-based models\nDiffusion models\nGAN\n\n\n\n\nChessUrl"
  },
  {
    "objectID": "projects/DDPM_eng.html#reverse-process",
    "href": "projects/DDPM_eng.html#reverse-process",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "4. Reverse process",
    "text": "4. Reverse process\n\\(\\textbf{Theoretical statetment:}\\) If forward process is represented as set of Gaussian condtional transfromtions \\(q(x_{t}|x_{t-1})\\), then the reverse process will be the same, but with unknown parameters \\(p{x_{t-1}|x_{t}}\\)\nThus, we have 2 joint distributions of latent codes:\nBy the property of Markovian chains, one can represent forward process as:\n\n\\(q(x_{1},...,x_{T}|x_{0}) = q( x_{T} | x_{T-1},x_{0}) q(x_{T-1} | x_{T-2},x_{0})...q(x_{2} | x_{1},x_{0})\\)\n\nBy the property of Markovian chains, one can represent reversed process as:\n\n\\(p(x_{1},...,x_{T}) = p(x_{T-1}|x_{T})p(x_{T-1}|x_{T})....p(x_{1}|x_{2})\\)\n\nNow, we pay our attention to the loss function:\n\\[\\int_{x_{1:T}}q(x_{1},...,x_{T}|x_{0})\\log\\frac{p(x_{0},x_{1},....,x_{T})}{q(x_{1},....,x_{T}|x_{0})}dx_{1: T}\\]\nThus, one can represent this loss as corresponding KL-divergenges:\n\\[ \\int_{x_{1:T}} q(x_{1},...,x_{T}|x_{0})\\log p(x_{0}|x_{1})dx_{1:T} + \\int_{x_{1:T}}q(x_{1},...,x_{T}|x_{0})\\log\\frac{p(x_{1},....,x_{T})}{q(x_{1},....,x_{T}|x_{0})}dx_{1: T} \\]\nNow , we should take into account the second term, that it is similar to minimization of KL-divergences, however, we chains in opposite directions. Then, one can represent forward Markov chains transformation probabilities in opposite direction via \\(\\textbf{Bayes Theorem}\\)\n\\(\\textbf{My desire:}\\) i would like to represent the forward markov chain as:\n\\(q(x_{1},...,x_{T}|x_{0}) = q(x_{T}|x_{0})q(x_{T-1}|x_{T},x_{0})q(x_{T-2}|x_{T-1},x_{0})...q(x_{1}|x_{2},x_{0})\\)\n\\(\\textbf{Bayes theorem:}\\)\n\\[ q(x_{t-1}|x_{t},x_{0}) = \\frac{q(x_{t}|x_{t-1},x_{0})q(x_{t-1}|x_{0})}{q(x_{t}|x_{0})}\\]\nClarifications:\n\nI have Markovian process,hence, there is no dependences on $x_{0} q(x_{t}|x_{t-1},x_{0}) q(x_{t}|x_{t-1}) $\nAll distributions are gaussian \\(\\implies\\) one can perform accurate bayesiam inference\n\nAt home, you can substitute corresponding probabilities and get formula for posterior:\n\\[q(x_{t-1}|x_{t},x_{0}) = \\mathcal{N}(x_{t-1}| \\hat{\\mu}_{t}(x_{t},x_{0}),\\hat{\\beta_{t}}I)\\]\n\nMean:\n\n\\[\\hat{\\mu}_{t}(x_{t},x_{0})= \\frac{\\sqrt{\\overline{\\alpha}_{t-1}}}{1 - \\overline{\\alpha_{t}}}\\beta x_{0} +\\frac{\\sqrt{1 - \\beta}(1 - \\overline{\\alpha_{t}} )}{1 - \\overline{\\alpha_{t}}}x_{t}  \\]\n\nVariance:\n\n\\[\\hat{\\beta_{t}} = \\beta_{t}(1 - \\overline{\\alpha_{t-1}}) \\frac{1}{1 - \\overline{\\alpha}_{t}}\\]"
  },
  {
    "objectID": "projects/DDPM_eng.html#the-loss-deriavation",
    "href": "projects/DDPM_eng.html#the-loss-deriavation",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "5. The loss deriavation",
    "text": "5. The loss deriavation\nNow, we return to ELBO:\n\\[ \\int q(x_{1}|x_{0}) \\log p(x_{0}|x_{1})dx_{1} + \\int q(x_{T}|x_{0})...q(x_{1}|x_{2},x_{0})\\log \\frac{p(x_{1}|x_{2})}{q(x_{1}|x_{2},x_{0})}\n\\frac{p(x_{T-1}|x_{T})}{q(x_{T-1}|x_{T},x_{0})}\\frac{....}{....} \\frac{p(x_{T})}{q(x_{T}|x_{0})}\\]\nThen:\n\\[ - \\sum_{t=1}^{T} \\mathbb{E}_{x_{1},..,x_{T}} KL(q(x_{t-1}|x_{t},x_{0}) || p(x_{t-1}|x_{t}))) - KL(q(x_{T}|x_{0}||p(x_{T})) + \\int q(x_{1}|x_{0}) \\log p(x_{0}|x_{1})dx_{1}\\]\n\nIf \\(x_{1} \\approx x_{0}\\), then const\nAs for certain KL in terminate state ?\n\n\\(\\textbf{Idea}\\): Minimization of KL divergences\n\nfor simplicity: \\(p_{\\theta}(x_{t-1}|x_{t}) = \\mathcal{N}(\\mu_{\\theta}(x_{t},t),\\hat{\\beta_{t}}I)\\)\nMean matching:\n\n\\[ KL(q(x_{t-1}|x_{t},x_{0})|| p_{\\theta}(x_{t-1}|x_{t})) = \\mathbb{E}_{x_{0},x_{1},...,x_{T},t}\\frac{1}{2\\hat{\\beta_{t}}}|| \\hat{\\mu}_{t}(x_{t},x_{0}) - \\mu_{\\theta}(x_{t},t)||_{2}^{2}\\]\n\n\n\nChessUrl"
  },
  {
    "objectID": "projects/DDPM_eng.html#re-parameterization-like-simplification",
    "href": "projects/DDPM_eng.html#re-parameterization-like-simplification",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "6. Re-parameterization like simplification",
    "text": "6. Re-parameterization like simplification\nWe would like to approximate \\(\\mu_{\\theta}\\) by \\(\\hat{\\mu_{t}}\\), but there is the problem! \\(\\hat{\\mu_{t}}(x_{0})\\), while trainable mean does not depend on \\(x_{0}\\). As a consequence of that, we cannot converge to 0 this problem:\n\\(\\textbf{Re-parametrization:}\\)\n\\[\\hat{\\mu}_{t}(x_{t},x_{0})= \\frac{\\sqrt{\\overline{\\alpha}_{t-1}}}{1 - \\overline{\\alpha_{t}}}\\beta x_{0} +\\frac{\\sqrt{1 - \\beta}(1 - \\overline{\\alpha_{t}} )}{1 - \\overline{\\alpha_{t}}}x_{t}  \\]\n\\[ \\mu_{theta}(x_{t},t) = \\frac{\\sqrt{\\overline{\\alpha}_{t-1}}}{1 - \\overline{\\alpha_{t}}}\\beta \\color{red}{x_{\\theta}(x_{t},t)} +\\frac{\\sqrt{1 - \\beta}(1 - \\overline{\\alpha_{t}} )}{1 - \\overline{\\alpha_{t}}}x_{t} \\]\n\nRed term is as estimatiomn for \\(x_{0}\\)\n\nThen:\n\\[\\frac{1}{2\\hat{\\beta}_{t}}||\\hat{\\mu_{t}}(x_{t},x_{0}) - \\mu_{\\theta}(x_{t},t)||_{2}^{2}\n= \\frac{1}{2\\hat{\\beta}_{t}} || \\frac{\\sqrt{\\overline{\\alpha}_{t-1}}}{1-\\overline{\\alpha_{t}}}\\beta (x_{0} - x_{\\theta}(x_{t},t))||^{2}_{2}\\]\n\\(\\textbf{Final reparamterization:}\\)\n\\[ x_{t} = \\sqrt{\\overline{\\alpha}_{t}}x_{0} + \\sqrt{1 - \\overline{\\alpha}_{t}}\\hat{\\epsilon} \\]\nThen, one can express:\n\\[x_{0} = \\frac{1}{\\sqrt{\\overline{\\alpha_{t}}}}(x_{t} - \\sqrt{1 - \\overline{\\alpha}_{t}}\\hat{\\epsilon} ) \\]\nThe, one can reparametrize predicted value as:\n\\[x_{\\theta} = \\frac{1}{\\sqrt{\\overline{\\alpha_{t}}}}(x_{t} - \\sqrt{1 - \\overline{\\alpha}_{t}} \\epsilon_{\\theta}(x_{t},t) \\]\nThus, the final loss function:\n\\[ \\mathcal{L}(\\theta) = \\sum_{i=1}^{n}\\sum_{t=2}^{T} \\frac{\\beta^{2}}{2\\hat{\\beta}_{t}(1-\\beta)(1- \\overline{\\alpha}_{t})} ||  \\frac{x^{i}_{t} - \\sqrt{\\overline{\\alpha}}_{t}x_{0}^{i}}{\\sqrt{1-\\overline{\\alpha}}_{t}} - \\epsilon_{\\theta}(x^{i}_{t},t) ||\\]"
  },
  {
    "objectID": "projects/DDPM_eng.html#coding-part",
    "href": "projects/DDPM_eng.html#coding-part",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "7. Coding part",
    "text": "7. Coding part\n\nimport torch\nfrom torch.nn import init\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import TensorDataset\nfrom sklearn.datasets import make_moons\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n7.1 models\nIt is importantly to point out that our neural network will predict \\(\\textbf{noise}\\).\nOur model is neural network, that has 2 inputs:\n\n\\(x_{t}\\) data\n\\(t\\) time like condition for more accurate prediction noise of \\(x_{t}\\)\n\nIt is worth to notice that we have the same model for each step , we do not train new model for each new step. \\(\\color{red}{One\\quad model\\quad for\\quad all\\quad steps!}\\)\nAlso, You should understand that \\(t\\) is like a value condition and in order to estimate his influence to corresponding noise as a condition , it would be great to create embeddings of the time by corresponding network below.\n\nclass SinusoidalEmbedding(nn.Module):\n    def __init__(self, size: int, scale: float = 1.0):\n        super().__init__()\n        self.size = size\n        self.scale = scale\n\n    def forward(self, x: torch.Tensor):\n        x = x * self.scale\n        half_size = self.size // 2\n        emb = torch.log(torch.Tensor([10000.0])) / (half_size - 1)\n        emb = torch.exp(-emb * torch.arange(half_size))\n        emb = x.unsqueeze(-1) * emb.unsqueeze(0)\n        emb = torch.cat((torch.sin(emb), torch.cos(emb)), dim=-1)\n        return emb\n\n    def __len__(self):\n        return self.size\n\n\nclass PositionalEmbedding(nn.Module):\n    def __init__(self, size: int, type: str, **kwargs):\n        super().__init__()\n\n        self.layer = SinusoidalEmbedding(size, **kwargs)\n\n    def forward(self, x: torch.Tensor):\n        return self.layer(x)\n\n\nclass Block(nn.Module):\n    def __init__(self, size: int):\n        super().__init__()\n\n        self.ff = nn.Linear(size, size)\n        self.act = nn.GELU()\n\n    def forward(self, x: torch.Tensor):\n        return x + self.act(self.ff(x))\n\n\nclass MLP(nn.Module):\n    def __init__(self, hidden_size: int = 128, hidden_layers: int = 3, emb_size: int = 128,\n                 time_emb: str = \"sinusoidal\", input_emb: str = \"sinusoidal\"):\n        super().__init__()\n\n        self.time_mlp = PositionalEmbedding(emb_size, time_emb)\n        self.input_mlp1 = PositionalEmbedding(emb_size, input_emb, scale=25.0)\n        self.input_mlp2 = PositionalEmbedding(emb_size, input_emb, scale=25.0)\n\n        concat_size = len(self.time_mlp.layer) + \\\n            len(self.input_mlp1.layer) + len(self.input_mlp2.layer)\n        layers = [nn.Linear(concat_size, hidden_size), nn.GELU()]\n\n        for _ in range(hidden_layers):\n            layers.append(Block(hidden_size))\n\n        layers.append(nn.Linear(hidden_size, 2))\n        self.joint_mlp = nn.Sequential(*layers)\n\n    def forward(self, x, t):\n        x1_emb = self.input_mlp1(x[:, 0])\n        x2_emb = self.input_mlp2(x[:, 1])\n        t_emb = self.time_mlp(t)\n        x = torch.cat((x1_emb, x2_emb, t_emb), dim=-1)\n        x = self.joint_mlp(x)\n        return x\n\n\n\n7.2 Data\nWe use simple two moons dataset as data.\n\ndef moons_dataset(n=8000):\n    X, _ = make_moons(n_samples=n, random_state=42, noise=0.03)\n    X[:, 0] = (X[:, 0] + 0.3) * 2 - 1\n    X[:, 1] = (X[:, 1] + 0.3) * 3 - 1\n    return TensorDataset(torch.from_numpy(X.astype(np.float32)))\n\n\n\n7.3 Gaussian Diffusion\nThis calss is composed of whole formulas from senimar , please familiari`e you with these formulas accurately.\n\nreconstruct x0:\n\n\\[x_{0} = \\frac{1}{\\sqrt{\\overline{\\alpha_{t}}}}(x_{t} - \\sqrt{1 - \\overline{\\alpha}_{t}}\\hat{\\epsilon} ) \\]\n\nq posterior:\n\n\\[\\hat{\\mu}_{t}(x_{t},x_{0})= \\frac{\\sqrt{\\overline{\\alpha}_{t-1}}}{1 - \\overline{\\alpha_{t}}}\\beta x_{0} +\\frac{\\sqrt{1 - \\beta}(1 - \\overline{\\alpha_{t}} )}{1 - \\overline{\\alpha_{t}}}x_{t}  \\]\n\nget_variance:\n\n\\[\\hat{\\beta_{t}} = \\beta_{t}(1 - \\overline{\\alpha_{t-1}}) \\frac{1}{1 - \\overline{\\alpha}_{t}}\\]\n\nstep:\n\n\nThe sampling moments of time to get \\(x_{t}\\)\nMake the prediction \\(x_{0}\\)\nMake the prediction for \\(x_{t-1}\\)\nCalculate noise\nMake one step of the backward process\n\n\nadd noise:\n\n\nThe expression noise through \\(x_{0}\\) and \\(x_{t}\\)\n\\(\\hat{\\epsilon} =  \\sqrt{\\overline{\\alpha_{t}}}\\frac{x_{0}}{\\sqrt{1 -\\overline{\\alpha_{t}}  }} - x_{t}\\frac{1}{\\sqrt{1- \\overline{\\alpha_{t}}}}\\)\n\n\nclass NoiseScheduler():\n    def __init__(self,\n                 num_timesteps=1000,\n                 beta_start=0.0001,\n                 beta_end=0.02,\n                 beta_schedule=\"linear\"):\n\n        self.num_timesteps = num_timesteps\n        if beta_schedule == \"linear\":\n            self.betas = torch.linspace(\n                beta_start, beta_end, num_timesteps, dtype=torch.float32)\n        elif beta_schedule == \"quadratic\":\n            self.betas = torch.linspace(\n                beta_start ** 0.5, beta_end ** 0.5, num_timesteps, dtype=torch.float32) ** 2\n\n        self.alphas = 1.0 - self.betas\n        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n        self.alphas_cumprod_prev = F.pad(\n            self.alphas_cumprod[:-1], (1, 0), value=1.)\n\n        # required for self.add_noise\n        self.sqrt_alphas_cumprod = self.alphas_cumprod ** 0.5\n        self.sqrt_one_minus_alphas_cumprod = (1 - self.alphas_cumprod) ** 0.5\n\n        # required for reconstruct_x0\n        self.sqrt_inv_alphas_cumprod = torch.sqrt(1 / self.alphas_cumprod)\n        self.sqrt_inv_alphas_cumprod_minus_one = torch.sqrt(\n            1 / self.alphas_cumprod - 1)\n\n        # required for q_posterior\n        self.posterior_mean_coef1 = self.betas * torch.sqrt(self.alphas_cumprod_prev) / (1. - self.alphas_cumprod)\n        self.posterior_mean_coef2 = (1. - self.alphas_cumprod_prev) * torch.sqrt(self.alphas) / (1. - self.alphas_cumprod)\n\n\n    def reconstruct_x0(self, x_t, t, noise):\n        s1 = self.sqrt_inv_alphas_cumprod[t]\n        s2 = self.sqrt_inv_alphas_cumprod_minus_one[t]\n        s1 = s1.reshape(-1, 1)\n        s2 = s2.reshape(-1, 1)\n        return s1 * x_t - s2 * noise\n\n    def q_posterior(self, x_0, x_t, t):\n        s1 = self.posterior_mean_coef1[t]\n        s2 = self.posterior_mean_coef2[t]\n        s1 = s1.reshape(-1, 1)\n        s2 = s2.reshape(-1, 1)\n        mu = s1 * x_0 + s2 * x_t\n        return mu\n\n    def get_variance(self, t):\n        if t == 0:\n            return 0\n\n        variance = self.betas[t] * (1. - self.alphas_cumprod_prev[t]) / (1. - self.alphas_cumprod[t])\n        variance = variance.clip(1e-20)\n        return variance\n\n    def step(self, model_output, timestep, sample):\n        t = timestep\n        pred_original_sample = self.reconstruct_x0(sample, t, model_output)\n        pred_prev_sample = self.q_posterior(pred_original_sample, sample, t)\n\n        variance = 0\n        if t &gt; 0:\n            noise = torch.randn_like(model_output)\n            variance = (self.get_variance(t) ** 0.5) * noise\n\n        pred_prev_sample = pred_prev_sample + variance\n\n        return pred_prev_sample\n\n    def add_noise(self, x_start, x_noise, timesteps):\n        s1 = self.sqrt_alphas_cumprod[timesteps]\n        s2 = self.sqrt_one_minus_alphas_cumprod[timesteps]\n\n        s1 = s1.reshape(-1, 1)\n        s2 = s2.reshape(-1, 1)\n\n        return s1 * x_start + s2 * x_noise\n\n    def __len__(self):\n        return self.num_timesteps\n\n\n\n7.4 Training\nWe choose hyper parameters for method and run it\n\nNUM_SAMPLES_DATA = 10_000\nBATCH_SIZE=128\n\nHIDDEN_SIZE = 128\nHIDDEN_LAYERS=3\nEMBEDDING_SIZE = 128\nTIME_EMBEDDING=\"sinusoidal\"\nINPUT_EMEDDING=\"sinusoidal\"\n\nNUM_TIMESTEPS = 50\nBETA_SCHEDULE = 'linear'\nLR = 5e-4\n\nNUM_EPOCHS=200\n\n\ndataset = moons_dataset(NUM_SAMPLES_DATA)\ndataloader = torch.utils.data.DataLoader(\n        dataset, batch_size=BATCH_SIZE , shuffle=True, drop_last=True)\n\nmodel = MLP(\n        hidden_size=HIDDEN_SIZE,\n        hidden_layers=HIDDEN_LAYERS,\n        emb_size=EMBEDDING_SIZE,\n        time_emb=TIME_EMBEDDING,\n        input_emb=INPUT_EMEDDING)\n\nnoise_scheduler = NoiseScheduler(\n        num_timesteps=NUM_TIMESTEPS,\n        beta_schedule=BETA_SCHEDULE)\n\noptimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr=LR,\n    )\n\n\nglobal_step = 0\nframes = []\nlosses = []\n\nfor epoch in tqdm(range(NUM_EPOCHS)):\n\n    model.train()\n\n\n    for step, batch in enumerate(dataloader):\n        batch = batch[0]\n        noise = torch.randn(batch.shape)\n        timesteps = torch.randint(a\n            0, noise_scheduler.num_timesteps, (batch.shape[0],)\n        ).long()\n\n        noisy = noise_scheduler.add_noise(batch, noise, timesteps)\n        noise_pred = model(noisy, timesteps)\n        loss = F.mse_loss(noise_pred, noise)\n        loss.backward(loss)\n\n        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        optimizer.zero_grad()\n        losses.append(loss.detach().item())\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:07&lt;00:00,  1.56it/s]\n\n\n\nmodel.eval()\nsample = torch.randn(1024, 2) # sampling from noise\ntimesteps = list(range(len(noise_scheduler)))[::-1]\nfor i, t in enumerate(tqdm(timesteps)):\n    t = torch.from_numpy(np.repeat(t,  1024)).long()\n    with torch.no_grad():\n        residual = model(sample, t)\n    sample = noise_scheduler.step(residual, t[0], sample)\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00&lt;00:00, 104.94it/s]\n\n\n\nplt.scatter(sample[:,0],sample[:,1], edgecolor='black', label=\"generated data\")\nplt.grid();\nplt.legend();\n\n\n\n\n\n\n\n\n\nmodel.eval()\ntraj = []\nsample = torch.randn(1024, 2) # sampling from noise\ntimesteps = list(range(len(noise_scheduler)))[::-1]\nfor i, t in enumerate(tqdm(timesteps)):\n    t = torch.from_numpy(np.repeat(t,  1024)).long()\n    with torch.no_grad():\n        residual = model(sample, t)\n    sample = noise_scheduler.step(residual, t[0], sample)\n    if t[0].item() % 5 == 0:\n        traj.append(sample.cpu())\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00&lt;00:00, 118.63it/s]\n\n\n\nfig,ax = plt.subplots(1,10,figsize=(40,4),dpi=200)\nfor idx in range(10):\n    ax[idx].scatter(traj[9-idx][:,0],traj[9-idx][:,1],edgecolor='black')\n    ax[idx].set_xticks([]);ax[idx].set_yticks([])\n    ax[idx].grid();\nfig.tight_layout(pad=0.01)"
  },
  {
    "objectID": "projects/GANs1.html",
    "href": "projects/GANs1.html",
    "title": "GANs",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nIn this seminar, you will learn how to\nRelated works and repositories to consider: * The seminal Generative Adversarial Networks paper by Ian Goodfellow et. al.; * NIPS 2016 tutorial on Generative Adversarial Networks by Ian Goodfellow (saturating and non-saturating GANs covered);  * Deep Generative Models course at MIPT, github. This seminar is partially based on the materials of this course."
  },
  {
    "objectID": "projects/GANs1.html#generative-modelling-problem",
    "href": "projects/GANs1.html#generative-modelling-problem",
    "title": "GANs",
    "section": "0. Generative modelling problem",
    "text": "0. Generative modelling problem\nWe have samples \\(\\{x_1, x_2, \\dots, x_n\\} = X \\sim \\mathbb{P}(x)\\). The distribution \\(\\mathbb{P} \\in \\mathcal{P}(\\mathbb{R}^{D_x})\\) is unknown.\nWe look for parametric approximation \\(p(x|\\theta), \\theta \\in \\Theta\\) of \\(\\mathbb{P}(x)\\), and pick up \\(\\theta^*\\) such that \\(\\mathbb{P}(x) \\approx p(x|\\theta^*)\\).\nWe want to estimate \\(\\hat{\\theta^*} = \\hat{\\theta^*}(X)\\) in order to\n\nsample from \\(p(x|\\hat{\\theta^*})\\)  (Primary goal) \nestimate the corresponding pdf.  (Less relevant) \n\n\n0.1 Latent Variable Model\n\n\n\nLikelihood\n\\(p(x |\\theta) = \\int\\limits_{\\mathbb{R}^{D_z}} p(x | z, \\theta) p_z(z| \\theta) dz\\)\n\nTypically, latent distribution \\(p_z(z| \\theta)\\) is simple and does not depend on \\(\\theta\\): \\(p_z(z| \\theta) = p_z(z)\\)\n\nSampling procedure\n\nSample \\(z^* \\sim p_z(z | \\theta)\\)\nSample \\(x \\sim p(x | z^*, \\theta)\\)"
  },
  {
    "objectID": "projects/GANs1.html#toy-2d-problem-setup",
    "href": "projects/GANs1.html#toy-2d-problem-setup",
    "title": "GANs",
    "section": "1. Toy \\(2D\\) problem setup",
    "text": "1. Toy \\(2D\\) problem setup\nSetting up reference distribution \\(\\mathbb{P}\\)\n\ndef generate_2d_data(size : int, var : float = 0.02) -&gt; np.ndarray:\n    scale = 2\n    centers = [\n        (1, 0),\n        (-1, 0),\n        (0, 1),\n        (0, -1),\n        (1. / np.sqrt(2), 1. / np.sqrt(2)),\n        (1. / np.sqrt(2), -1. / np.sqrt(2)),\n        (-1. / np.sqrt(2), 1. / np.sqrt(2)),\n        (-1. / np.sqrt(2), -1. / np.sqrt(2))\n    ]\n\n    centers = [(scale * x, scale * y) for x, y in centers]\n    dataset = []\n\n    for i in range(size):\n        point = np.random.randn(2) * var\n        center = centers[np.random.choice(np.arange(len(centers)))]\n        point[0] += center[0]\n        point[1] += center[1]\n        dataset.append(point)\n\n    dataset = np.array(dataset, dtype='float32')\n    dataset /= 1.414  # stdev\n\n    return dataset\n\nGenerate and visualize training samples \\(X \\sim \\mathbb{P}\\)\n\nCOUNT = 20000\n\ntrain_data = generate_2d_data(COUNT, var=0.1)\nvisualize_2d_samples(train_data[:1000], \"Train data\", colors='tomato')"
  },
  {
    "objectID": "projects/GANs1.html#gan-losses",
    "href": "projects/GANs1.html#gan-losses",
    "title": "GANs",
    "section": "2. GAN losses",
    "text": "2. GAN losses\nIn this part of the seminar you will get into details and implement Vanillan GAN and Non-saturating Vanilla GAN models. Then you will apply these models on the toy \\(2D\\) problem from above.\n\n2.0. GAN preliminaries\nGenerative Adversarial Networks parameterizes the distributions \\(p(x \\vert \\theta)\\) by means of\n\n\nSimple\n\n, pre-defined, typically  low-dimensional  latent distribution \\(p_z(z)\\), e.g., Gaussian or Uniform\nA parametric generator model \\(G_{\\theta} : \\mathbb{R}^{D_z} \\rightarrow \\mathbb{R}^{D_x}\\) which maps latent space to data space\n\nFormal probability model. Generator \\(G_{\\theta}\\) coupled with latent distribution \\(p_z(z)\\) forms the parametric distribution \\(p(x \\vert \\theta)\\) which is the distribution of points \\(x = G_{\\theta}(z)\\), where \\(z \\sim p_z(z)\\). I.e.,\n\\[\np(x \\vert \\theta) = p_z\\big(G_{\\theta}^{-1}(x)\\big).\n\\]\nüîé Remark. In the literature, \\(p(x \\vert \\theta)\\) typically called as pushforward of \\(p_z(z)\\) under \\(G_{\\theta}\\):\n\\[\np(\\cdot \\vert \\theta) = G_{\\theta}\\sharp p_z\n\\]\nSampling from \\(p(x \\vert \\theta)\\) procedure\n\nSample \\(z^* \\sim p_z(z)\\)\nObtain \\(x = G_{\\theta}(z^*)\\)\n\n\nTraining GANs\nWe need to adjust the optimal NN parameters \\(\\hat{\\theta^*}\\) based on samples \\(X \\sim \\mathbb{P}\\) such that \\(\\mathbb{P}(x) \\approx p(x|\\hat{\\theta^*})\\)\nThis can be done by choosing a discrepancy\n\\[\\mathfrak{D} : \\mathcal{P}(\\mathbb{R}^{D_x}) \\times \\mathcal{P}(\\mathbb{R}^{D_x}) \\rightarrow \\mathbb{R}\\]\nbetween probability distributions and minimizing \\(\\mathfrak{D}(p(x|\\theta), \\mathbb{P})\\) based on samples \\(X\\).\n‚ùî What discrepancies \\(\\mathfrak{D}\\) do you know? Which of them are used in generative modelling?\n\nIn GAN setup, the discrepancy is typically follow the variational manner, i.e., it is defined as an auxiliary optimization problem w.r.t. a discriminator (or critic) \\(D: \\mathbb{R}^{D_x} \\rightarrow \\mathbb{R}\\): \\[\\mathfrak{D}(p(x|\\theta), \\mathbb{P}) = \\sup_{D} V(G_{\\theta}, D)\\]\n\n\nDiscriminator \\(D\\) is parameterized as a NN: \\(D = D_{\\phi}, \\phi \\in \\Phi\\)\nTypically, the ‚Äúphysical‚Äù role of discriminator \\(D\\) is to distinguis real samples \\(X\\) from generated samples \\(X_{\\text{gen}} \\sim G_{\\theta}\\sharp p_z\\)\nThe resulting Generative Modelling problem is solved by \\(\\min - \\max\\) adversarial optimization problem:\n\n\\[\\min_{G_{\\theta}} \\max_{D_{\\phi}} V(G_{\\theta}, D_{\\phi}) = \\min_{\\theta \\in \\Theta} \\max_{\\phi \\in \\Phi} V(G_{\\theta}, D_{\\phi}) \\rightarrow \\hat{\\theta^*}, \\hat{\\phi^*}\\]\n\n\nParameterizing with Neural Networks\nNow let‚Äôs gradually move to practice. Define a class for MultiLayerPerceptron. It will be used for modelling generators \\(G_{\\theta}\\) and discriminators \\(D_{\\phi}\\) of our GAN models on the toy 2D problem.\n\nclass FullyConnectedMLP(nn.Module):\n\n    def __init__(\n        self,\n        input_dim : int,\n        hiddens : List[int], # hidden layer's dimensions\n        output_dim : int\n    ) -&gt; None:\n        super().__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.hiddens = hiddens\n\n        model = []\n        prev_h = input_dim\n        for h in hiddens:\n            model.append(nn.Linear(prev_h, h))\n            model.append(nn.ReLU())\n            prev_h = h\n        model.append(nn.Linear(hiddens[-1], output_dim))\n        self.net = nn.Sequential(*model)\n\n    def forward(self, x : torch.Tensor) -&gt; torch.Tensor:\n        batch_size = x.shape[0]\n        x = x.view(batch_size, -1)\n        return self.net(x).view(batch_size, self.output_dim)\n\n\n\n\n2.1 Vanilla GAN\nVanilla GAN objective\n\\[\\begin{align}\n\\min_G \\max_D V(G, D) &= \\min_G \\max_D \\big[\\mathbb{E}_{x \\sim \\mathbb{P}} \\log D(x) + \\mathbb{E}_{x_{\\text{gen}} \\sim p(x \\vert \\theta)} \\log(1 - D(x_{\\text{gen}}))\\big] \\\\\n&= \\min_G \\max_D \\big[\\mathbb{E}_{x \\sim \\mathbb{P}} \\log D(x) + \\mathbb{E}_{z \\sim p_z(z)} \\log(1 - D(G(z)))\\big]\n\\end{align}\\]\n\nGenerator: generative model \\(x = G(z)\\), which try to generate samples indistiguishable from real samples \\(X \\sim \\mathbb{P}\\)\nDiscriminator: a classifier \\(D(x) \\in [0, 1]\\), which distiguishes reals samples from generated samples\n\n‚ùî Which discrepancy \\(\\mathfrak{D}\\) is optimized in the internal maximization problem? [üí° Hint: Recall the lecture]\n\nSample estimate of Vanilla GAN objective:\n\nThe objective \\(V\\) is stochastically estimated by deriving a random batch \\(X_B \\subset X\\) and random batch sample \\(Z_B \\sim p_z\\):\n\n\\[\n\\hat{V}(G, D) = \\bigg[\\frac{1}{\\vert X_B \\vert} \\sum_{x \\in X_B} \\log D(x) + \\frac{1}{\\vert Z_B \\vert} \\sum_{z \\in Z_B} \\log(1 - D(G(z)))\\bigg].\n\\]\n\nThe problem is solved by alternating stochastic gradient descent-ascent steps w.r.t. parameters \\(\\theta\\) and \\(\\phi\\).\n\n\nImplementing Vanilla GAN for toy 2D problem\n\nAs latent distribution \\(p(z)\\) we will use \\(D_z\\)-dimensional Standard gaussian distribution. Choose \\(D_z\\) on your own. The author utilizes \\(D_z = 16\\).\n\n‚ùî What are the input and output dimensions of generator \\(G_\\theta\\) and discriminator \\(D_\\theta\\) models?\n\n\nDefine the Vanilla GAN generator \\(G :\\mathbb{R}^{D_z}\\rightarrow \\mathbb{R}^{2}\\). * It sufficies to use medium-size multi-layer perceptron with ReLU activations. * Take the advantage of FullyConnectedMLP class. * Implement sample method.\n\nclass VanillaGenerator(FullyConnectedMLP):\n\n    def sample(self, n : int) -&gt; torch.Tensor:\n        z = torch.randn(size=(n, self.input_dim)).to(\n            next(iter(self.parameters())))\n        return self.forward(z)\n\nGEN_HIDDENS = [128, 128, 128]\n\nG = VanillaGenerator(2, GEN_HIDDENS, 2).to(DEVICE)\n\nDefine the Vanilla GAN discriminator \\(D :\\mathbb{R}^{2}\\rightarrow \\mathbb{R}\\). * It sufficies to use medium-size multi-layer perceptron with ReLU activations. * Take the advantage of FullyConnectedMLP class. * Note that the output of \\(D\\) should be restricted to \\([0, 1]\\) segment.\n\nclass VanillaDiscriminator(FullyConnectedMLP):\n\n    def forward(self, z : torch.Tensor) -&gt; torch.Tensor:\n        x = super().forward(z)\n        return torch.sigmoid(x)\n\nDISCR_HIDDENS = [128, 128, 128]\n\nD = VanillaDiscriminator(2, DISCR_HIDDENS, 1).to(DEVICE)\n\n\n\nImplementing Vanilla GAN Loss\n\nNote, you will use the same function on images data problem\nIt is recommended to use torch.nn.functional.binary_cross_entropy (link to docs) (to eliminate possible inifinity or large values)\n\nImplement Vanilla GAN generator training step.\n\nThe function below should compute the stocastic estimate \\(\\hat{V}\\) of Vanilla GAN loss and perform a single gradient descent step w.r.t the parameter \\(\\theta\\) of the generator.\n\n\ndef vanilla_gen_step(\n    X : torch.Tensor, # random batch from the dataset\n    G : nn.Module, # generator model\n    D : nn.Module, # discriminator model\n    G_optim : torch.optim.Optimizer, # optimizer for generator (Adam/RMSprop/SGD etc.)\n) -&gt; torch.Tensor: # returns the value of loss to track the training statistics\n    G.train()\n    D.eval()\n    batch_size = X.size(0)\n    X_gen = G.sample(batch_size)\n    scores_gen = D(X_gen)\n    loss = - F.binary_cross_entropy(scores_gen, torch.zeros_like(scores_gen))\n    G_optim.zero_grad()\n    loss.backward()\n    G_optim.step()\n    return loss.item()\n\nImplement Vanilla GAN discriminator training step.\n\nThe function below should compute the stocastic estimate \\(\\hat{V}\\) of Vanilla GAN loss and perform a single gradient ascent step w.r.t the parameter \\(\\phi\\) of the discriminator.\n\n\ndef vanilla_discr_step(\n    X : torch.Tensor, # random batch from the dataset\n    G : nn.Module, # generator model\n    D : nn.Module, # discriminator model\n    D_optim : torch.optim.Optimizer, # optimizer for the discriminator (Adam/RMSprop/SGD etc.)\n) -&gt; torch.Tensor: # returns the value of loss to track the training statistics\n    G.eval()\n    D.train()\n    batch_size = X.size(0)\n    with torch.no_grad():\n        X_gen = G.sample(batch_size)\n    scores_gen = D(X_gen)\n    scores_real = D(X)\n    loss_gen = F.binary_cross_entropy(scores_gen, torch.zeros_like(scores_gen))\n    loss_real = F.binary_cross_entropy(scores_real, torch.ones_like(scores_real))\n    loss = loss_gen + loss_real\n\n    D_optim.zero_grad()\n    loss.backward()\n    D_optim.step()\n    return loss.item()\n\nImplement Vanilla GAN training loop\n\nIt is not possible (and, actually, is a bad strategy from the optimization perspectives) to solve the internal maximization problem \\(\\max_{\\phi} V(G_\\theta, D_\\phi)\\) for each particular generator \\(G_\\theta\\).\nInstead, we will perform a fixed number of discriminator_steps gradient ascent updates of the discriminator parameter \\(\\phi\\) per each gradient descent update of the generator parameter \\(\\theta\\)\nAfterwards we will see that setting discriminator_steps to large numbers will spoil the optimization and convergence.\n\n\ndef train_vanilla(\n    train_loader : DataLoader, # dataloader of training dataset\n    G : nn.Module, # generator model\n    D : nn.Module, # discriminator model\n    G_optim : torch.optim.Optimizer, # optimizer for the generator\n    D_optim : torch.optim.Optimizer, # optimizer for the discriminator\n    discriminator_steps : int, # number of discriminators steps per each generator step\n    n_epochs : int, # number of training epochs\n    diagnostic : GANDiagnosticCompanion, # tracking statistics & visualization\n    visualize_steps : int = 10, # for visualization purposes\n) -&gt; None:\n\n    G.train()\n    D.train()\n    step_i = 0\n    for epoch_i in tqdm(range(n_epochs)):\n        for batch_i, X in enumerate(train_loader):\n            X = X.to(DEVICE)\n\n            # DISCRIMINATOR UPDATE\n            d_loss = vanilla_discr_step(X, G, D, D_optim)\n            diagnostic.upd_d_loss(d_loss)\n\n            # GENERATOR UPDATE\n            if step_i % discriminator_steps == 0:\n                g_loss = vanilla_gen_step(X, G, D, G_optim)\n                diagnostic.upd_g_loss(g_loss)\n            step_i += 1\n\n        if visualize_steps and epoch_i % visualize_steps == 0:\n            print('Epoch {}'.format(epoch_i))\n            diagnostic.visualize()\n\n\n\nVanilla GAN training\nüîé Remark. It is recommended to use RMSprop optimizer when training a GAN model. Empirically, it was observed that this optimizer works more stable compared to Adam.\n\nBATCH_SIZE = 1024\nGEN_HIDDENS = [128, 128, 128]\nDISCR_HIDDENS = [128, 128, 128]\nDISCRIMINATOR_STEPS = 5 # 5- good, 15-mode not covered\nLR = 2e-4 # &lt; 1e-2\nN_EPOCHS = 1000 # change it if you want\n\ntrain_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n\nG = VanillaGenerator(16, GEN_HIDDENS, 2).to(DEVICE)\nD = VanillaDiscriminator(2, DISCR_HIDDENS, 1).to(DEVICE)\nG_optim = torch.optim.RMSprop(G.parameters(), lr=LR)\nD_optim = torch.optim.RMSprop(D.parameters(), lr=LR)\ndiagnostic = GANDiagnosticCompanion2D(G, D, train_data)\n\ntrain_losses = train_vanilla(\n    train_loader,\n    G,\n    D,\n    G_optim,\n    D_optim,\n    discriminator_steps=DISCRIMINATOR_STEPS,\n    n_epochs=N_EPOCHS,\n    diagnostic=diagnostic,\n    visualize_steps=100\n)\n\n  0%|                                                                                                                                              | 0/1000 [00:00&lt;?, ?it/s]\n\n\nEpoch 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                                                                      | 100/1000 [00:22&lt;02:53,  5.17it/s]\n\n\nEpoch 100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                                                         | 200/1000 [00:42&lt;02:39,  5.01it/s]\n\n\nEpoch 200\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                            | 300/1000 [01:05&lt;02:16,  5.12it/s]\n\n\nEpoch 300\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                               | 400/1000 [01:27&lt;01:58,  5.06it/s]\n\n\nEpoch 400\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                  | 500/1000 [01:49&lt;01:40,  4.98it/s]\n\n\nEpoch 500\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                    | 600/1000 [02:12&lt;01:17,  5.17it/s]\n\n\nEpoch 600\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                       | 700/1000 [02:33&lt;00:59,  5.04it/s]\n\n\nEpoch 700\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 800/1000 [02:55&lt;00:39,  5.04it/s]\n\n\nEpoch 800\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 900/1000 [03:17&lt;00:18,  5.44it/s]\n\n\nEpoch 900\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [03:38&lt;00:00,  4.57it/s]\n\n\n\ndiagnostic.visualize()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVanishing gradient problem [ saturation of \\(\\log(1 - D(G(z))\\) ]\nSee Towards Principled Methods for Training Generative Adversarial Networks paper by Martin Arjovsky for more details.\nProblem 1. When generated distribution \\(p(x \\vert \\theta)\\) and reference distribution \\(\\mathbb{P}\\) have disjoint compact supports (there are even softer conditions), then the perfect discriminator \\(D^* : \\mathbb{R}^{D_x} \\rightarrow [0, 1]\\) (which maximizes \\(V(G_\\theta, D)\\)) will perfectly distinguish generated and real samples and \\(\\nabla_x D^*(x) = 0\\) for \\(x \\sim \\mathbb{P}\\) or \\(x\\sim p(x \\vert \\theta)\\)\n\n\\(\\Rightarrow\\) any learning will stop!\nAt the beginning of GAN training it is typical that \\(p(x \\vert \\theta)\\) is not aligned with \\(\\mathbb{P}\\). \\(\\Rightarrow\\) learning may stuck if the discriminator is sub-optimal.\n\n\n\n\n\nWe will fix this problem later.\n\nProblem 2. (particular conclusion of Problem 1.). In the conditions of Problem 1.:\n\\[ \\lim\\limits_{\\Vert D - D^*\\Vert \\rightarrow 0} \\nabla_{\\theta} \\big\\{\\mathbb{E}_{z \\sim p_z(z)} \\log(1 - D(G_{\\theta}(z)))\\big\\} = 0,\\] i.e., if \\(p(x \\vert \\theta)\\) is not aligned with \\(\\mathbb{P}\\) then the closer discriminator \\(D\\) to the optimal one \\(D^*\\) the weaker the gradient of the objective \\(V(G_\\theta, D)\\) w.r.t. \\(\\theta\\) and the slower the optimization of the generator \\(G_\\theta\\). This effect is known as saturation of \\(\\log(1 - D(G(z))\\).\n\n\n\n\n\n\n2.2 Non-saturating GAN (‚Äúfixes‚Äù Problem 2)\n\nNS-GAN discriminator optimization\nThe same as Vanilla GAN disciminator optimization\n\n\n\nNS-GAN generator optimization\nVanilla GAN generator \\(G\\) gradient step update (for current distriminator \\(D\\)):\n\\[\\theta_{\\text{new}} = \\theta_{\\text{old}} - \\gamma \\nabla_{\\theta} \\big[\\mathbb{E}_{z \\sim p_z(z)} \\log(1 - D(G_{\\theta}(z)))\\big]\\Big|_{\\theta = \\theta_{\\text{old}}}\\]\n(this corresponds to minimization of \\(\\mathbb{E}_{z \\sim p_z(z)} \\log(1 - D(G_{\\theta}(z)))\\) w.r.t. \\(\\theta\\))\nNon-saturating GAN generator \\(G\\) gradient step update (for current distriminator \\(D\\)):\n\\[\\theta_{\\text{new}} = \\theta_{\\text{old}} + \\gamma \\nabla_{\\theta} \\big[\\mathbb{E}_{z \\sim p_z(z)} \\log(D(G_{\\theta}(z)))\\big]\\Big|_{\\theta = \\theta_{\\text{old}}}\\]\n(this corresponds to maximization of \\(\\mathbb{E}_{z \\sim p_z(z)} \\log(D(G_{\\theta}(z)))\\) w.r.t. \\(\\theta\\))\n\nGradients became much stronger\n\n\n\n\n\nLess stable training\n\n\n\nPractical aspects of NS GAN\n\nWhen implementing Vanilla GAN discriminator \\(D\\) we apply Sigmoid after the final layer:\n\n\\[\n\\sigma(x) = \\frac{1}{1 + e^{-x}}\n\\]\n\nLet us instead parameterize NS GAN discriminator \\(D\\) as arbitrary function \\(D : \\mathbb{R}^{D_x} \\rightarrow \\mathbb{R}\\) by explicitly introducing the Sigmoid in the loss function.\n\n‚ùî Exercise: Prove that optimization steps for the discriminator and the generator of NS GAN are as follows:\nüí° Hint: Recall that \\(\\text{Softplus}(x) = \\log(1 + \\exp(x))\\). Use the fact: \\(\\sigma(-x) = 1 - \\sigma(x)\\).\nNote: In what follows, discriminator \\(D_{\\phi}\\) is an arbitrary MLP (not restricted to produce numbers in \\([0, 1]\\) interval)!\n\nD update:\n\n\\[\n\\text{loss}_{D_{\\phi}} = \\frac{1}{\\vert X_{B} \\vert} \\sum\\limits_{x \\in X_B} \\text{Softplus}(-D_{\\phi}(x)) +  \\frac{1}{\\vert Z_B \\vert} \\sum\\limits_{z \\in Z_B} \\text{Softplus}(D_{\\phi}(G_{\\theta}(z)))  \\rightarrow \\min_{\\phi}\n\\]\n\nG update:\n\n\\[\n\\text{loss}_{G_{\\theta}} =  \\frac{1}{\\vert Z_B \\vert} \\sum\\limits_{z \\in Z_B} \\text{Softplus}(-D_{\\phi}(G_{\\theta}(z)))  \\rightarrow \\min_{\\theta}\n\\]\n\n\nImplementing NS GAN\nDefine the NS GAN generator \\(G :\\mathbb{R}^{D_z}\\rightarrow \\mathbb{R}^{2}\\) and discriminator \\(D : \\mathbb{R}^{2}\\rightarrow \\mathbb{R}\\)\n\nYou do not need to restrict the discriminator‚Äôs output to be in range \\([0, 1]\\).\n\n\nclass NSGenerator(VanillaGenerator):\n    pass\n\nGEN_HIDDENS = [128, 128, 128]\n\nG = NSGenerator(2, GEN_HIDDENS, 2).to(DEVICE)\n\nclass NSDiscriminator(FullyConnectedMLP):\n    pass\n\nDISCR_HIDDENS = [128, 128, 128]\n\nD = NSDiscriminator(2, DISCR_HIDDENS, 1).to(DEVICE)\n\n\n\nImplementing NS GAN Loss\nImplement NS GAN generator training step.\n\nThe function below should compute the stocastic estimate of Non-saturating GAN generator loss \\(\\frac{1}{\\vert Z_B \\vert} \\sum\\limits_{z \\in Z_B} \\text{Softplus}(-D_{\\phi}(G_{\\theta}(z)))\\), \\(Z_B \\sim p_z\\) and perform a single gradient descent step w.r.t the parameter \\(\\theta\\) of the generator.\n\n\ndef ns_gen_step(\n    X : torch.Tensor, # random batch from the dataset\n    G : nn.Module, # generator model\n    D : nn.Module, # discriminator model\n    G_optim : torch.optim.Optimizer, # optimizer for generator (Adam/RMSprop/SGD etc.)\n) -&gt; torch.Tensor: # returns the value of loss to track the training statistics\n    G.train()\n    D.eval()\n    batch_size = X.size(0)\n    X_gen = G.sample(batch_size)\n    scores_gen = D(X_gen)\n    loss = F.softplus(-scores_gen).mean()\n    G_optim.zero_grad()\n    loss.backward()\n    G_optim.step()\n    return loss.item()\n\n\n\n\\(R_1\\) regularizer (for discriminator)\nSee Mescheder et. al. for more details.\n\\[R_1(D_\\phi) = \\mathbb{E}_{x \\sim \\mathbb{P}} \\big[\\Vert \\nabla_x D_\\phi(x) \\Vert^2\\big]\\]\n\nThe stochastic estimate of \\(R_1\\) value is added with factor \\(\\lambda\\) to the discriminator loss:\n\n\\[\n\\text{loss}_{D_{\\phi}} = \\frac{1}{\\vert X_{B} \\vert} \\sum\\limits_{x \\in X_B} \\text{Softplus}(-D_{\\phi}(x)) +  \\frac{1}{\\vert Z_B \\vert} \\sum\\limits_{z \\in Z_B} \\text{Softplus}(D_{\\phi}(G_{\\theta}(z))) + \\lambda \\frac{1}{\\vert X_{B} \\vert} \\sum\\limits_{x \\in X_B} \\big[\\Vert \\nabla_x D_\\phi(x) \\Vert^2\\big] \\rightarrow \\min_{\\phi}\n\\]\n\nIt stabilizes the training. Large and Powerful GAN models, e.g., StyleGAN paper, utilizes Non-saturating GAN loss with \\(R_1\\) regularizer.\n\n\nImplement NS GAN discriminator training step with additional \\(R_1\\) regularizer.\n\nDo not forget to set the requries_grad flag of the input data batch (Why?)\nDo not forget to zero_grad the discriminator parameters after computing the gradient penalty (Why?)\nüí° Hint: Utilize torch.Tensor.backward(retain_graph=True, create_graph=True) function.\n\n‚ùî What do retain_graph and create_graph parameters mean? Why do we need to set the to True?\n\ndef ns_discr_step(\n    X : torch.Tensor, # random batch from the dataset\n    G : nn.Module, # generator model\n    D : nn.Module, # discriminator model\n    D_optim : torch.optim.Optimizer, # optimizer for generator (Adam/RMSprop/SGD etc.)\n    r1_regularizer : float = 1., # the factor of R_1 regularization\n) -&gt; torch.Tensor: # returns the value of loss to track the training statistics\n    G.eval()\n    D.train()\n    D_optim.zero_grad()\n    batch_size = X.size(0)\n    with torch.no_grad():\n        X_gen = G.sample(batch_size)\n    X.requires_grad_()\n    scores_gen = D(X_gen)\n    scores_real = D(X)\n    loss_gen = F.softplus(scores_gen).mean()\n    loss_real = F.softplus(-scores_real).mean()\n    scores_real.sum().backward(retain_graph=True, create_graph=True)\n    gradients = X.grad # the same shape as X\n    grad_penalty = (\n        gradients.view(gradients.size(0), -1).norm(2, dim=1) ** 2\n    ).mean()\n    D_optim.zero_grad()\n    loss = loss_gen + loss_real + r1_regularizer * grad_penalty\n\n    loss.backward()\n    D_optim.step()\n    gradients.detach_() # to avoid memory leak!\n    return loss.item()\n\nImplement NS GAN training loop\n\ndef train_ns(\n    train_loader : DataLoader, # dataloader of training dataset\n    G : nn.Module, # generator model\n    D : nn.Module, # discriminator model\n    G_optim : torch.optim.Optimizer, # optimizer for the generator\n    D_optim : torch.optim.Optimizer, # optimizer for the discriminator\n    discriminator_steps : int, # number of discriminators steps per each generator step\n    n_epochs : int, # number of training epochs\n    diagnostic : GANDiagnosticCompanion, # tracking statistics & visualization\n    r1_regularizer : float = 1., # the factor of R_1 regularization for the discriminator training\n    visualize_steps : int = 10, # for visualization purposes\n) -&gt; None:\n\n    G.train()\n    D.train()\n    step_i = 0\n    for epoch_i in tqdm(range(n_epochs)):\n        for batch_i, X in enumerate(train_loader):\n            X = X.to(DEVICE)\n\n            # DISCRIMINATOR UPDATE\n            d_loss = ns_discr_step(X, G, D, D_optim, r1_regularizer)\n            diagnostic.upd_d_loss(d_loss)\n\n            # GENERATOR UPDATE\n            if step_i % discriminator_steps == 0:\n                g_loss = ns_gen_step(X, G, D, G_optim)\n                diagnostic.upd_g_loss(g_loss)\n            step_i += 1\n\n        if visualize_steps and epoch_i % visualize_steps == 0:\n            print('Epoch {}'.format(epoch_i))\n            diagnostic.visualize()\n\n\n\nNS GAN training\n\nBATCH_SIZE = 1024\nGEN_HIDDENS = [128, 128, 128]\nDISCR_HIDDENS = [128, 128, 128]\nDISCRIMINATOR_STEPS = 5 # 2 - more or less, 5 - lose modes\nLR = 2e-4 # &lt; 1e-2\nR1_REGULARIZER = 0.1\nN_EPOCHS = 1000 # change it if you want\n\ntrain_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n\nG = NSGenerator(16, GEN_HIDDENS, 2).to(DEVICE)\nD = NSDiscriminator(2, DISCR_HIDDENS, 1).to(DEVICE)\nG_optim = torch.optim.RMSprop(G.parameters(), lr=LR)\nD_optim = torch.optim.RMSprop(D.parameters(), lr=LR)\ndiagnostic = GANDiagnosticCompanion2D(G, D, train_data)\n\ntrain_losses = train_ns(\n    train_loader,\n    G,\n    D,\n    G_optim,\n    D_optim,\n    discriminator_steps=DISCRIMINATOR_STEPS,\n    n_epochs=N_EPOCHS,\n    diagnostic=diagnostic,\n    r1_regularizer=R1_REGULARIZER,\n    visualize_steps=100\n)\n\n  0%|                                                                                                                                              | 0/1000 [00:00&lt;?, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at ../torch/csrc/autograd/engine.cpp:1151.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\n\nEpoch 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                                                                      | 100/1000 [00:38&lt;05:40,  2.64it/s]\n\n\nEpoch 100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                                                         | 200/1000 [01:18&lt;05:17,  2.52it/s]\n\n\nEpoch 200\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                            | 300/1000 [01:57&lt;04:17,  2.72it/s]\n\n\nEpoch 300\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                               | 400/1000 [02:34&lt;04:06,  2.43it/s]\n\n\nEpoch 400\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                  | 500/1000 [03:16&lt;03:18,  2.52it/s]\n\n\nEpoch 500\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                    | 600/1000 [03:55&lt;02:32,  2.62it/s]\n\n\nEpoch 600\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                       | 700/1000 [04:33&lt;01:45,  2.84it/s]\n\n\nEpoch 700\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 800/1000 [05:12&lt;01:15,  2.63it/s]\n\n\nEpoch 800\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 900/1000 [05:54&lt;00:38,  2.57it/s]\n\n\nEpoch 900\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [06:33&lt;00:00,  2.54it/s]\n\n\n\ndiagnostic.visualize()"
  },
  {
    "objectID": "projects/GANs1.html#gans-on-3times-16times-16-image-data",
    "href": "projects/GANs1.html#gans-on-3times-16times-16-image-data",
    "title": "GANs",
    "section": "3. GANs on \\(3\\times 16\\times 16\\) image data",
    "text": "3. GANs on \\(3\\times 16\\times 16\\) image data\nNow we apply our implemented Vanilla GAN and NS GAN R_1 approaches to solve the Generative Modelling problem on the \\(3\\times 16\\times 16\\) image data\n\n3.1 Image data problem setup\nAs the distribution \\(\\mathbb{P}\\) we aim to model, we consider Colored MNIST dataset downscaled to \\(16\\times 16\\) spatial resolutions. You will meet this distribution in the further seminars again üôÇ.\n\nLook into the code below carefully\nNote that Colored MNIST dataset produces images with the pixel range \\([-1, 1]\\).\n\n\nimport torchvision.datasets as TVdatasets\nfrom torchvision import transforms as TVtransforms\n\n# draw grayscaled image with random color\ndef random_color(im : torch.Tensor) -&gt; torch.Tensor:\n    hue = 360*np.random.rand()\n    d = (im *(hue%60)/60)\n    im_min, im_inc, im_dec = torch.zeros_like(im), d, im - d\n    H = round(hue/60) % 6\n    cmap = [[0, 3, 2], [2, 0, 3], [1, 0, 3], [1, 2, 0], [3, 1, 0], [0, 1, 2]]\n    return torch.cat((im, im_min, im_dec, im_inc), dim=0)[cmap[H]]\n\nclass CMNISTDataset(torch.utils.data.Dataset):\n\n    def __init__(\n        self,\n        train : bool =True,\n        spat_dim : Tuple[int, int] = (16, 16),\n        download : bool = False,\n        pix_range : Tuple[float, float] = (-1., 1.)\n    ) -&gt; None:\n        _m, _std = pix_range[0]/float(pix_range[0] - pix_range[1]), 1./float(pix_range[1] - pix_range[0])\n        TRANSFORM = TVtransforms.Compose([\n            TVtransforms.Resize(spat_dim),\n            TVtransforms.ToTensor(),\n            random_color,\n            TVtransforms.Normalize([_m],[_std])\n        ])\n        self.mnist = TVdatasets.MNIST(root='./data', train=train, download=download, transform=TRANSFORM)\n\n    def __len__(self):\n        return len(self.mnist)\n\n    def __getitem__(self, idx : int) -&gt; torch.Tensor:\n        return self.mnist[idx][0]\n\ncmnist_train = CMNISTDataset(train=True, download=True)\ncmnist_test = CMNISTDataset(train=False)\n\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9912422/9912422 [00:04&lt;00:00, 2364788.66it/s]\n\n\nExtracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28881/28881 [00:00&lt;00:00, 218773.77it/s]\n\n\nExtracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1648877/1648877 [00:01&lt;00:00, 1532206.17it/s]\n\n\nExtracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4542/4542 [00:00&lt;00:00, 4599355.09it/s]\n\n\nExtracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\n\n\n\n\n\nLet us take a look at the images from the datasets.\n\nREFERENCE_IMAGES = next(iter(DataLoader(cmnist_test, batch_size=10)))\nplot_images(REFERENCE_IMAGES)\n\n\n\n\n\n\n\n\n\n\n3.2 Implementing Vanilla GAN and NS GAN on image data\nWhen people deal with images, they typically use Convolutional Neural Networks. Implement the generic classes for ConvGenerator \\(G : \\mathbb{R}^{D_z} \\rightarrow \\mathbb{R}^{3 \\times 16 \\times 16}\\) and ConvDiscriminator \\(D : \\mathbb{R}^{3 \\times 16 \\times 16} \\rightarrow \\mathbb{R}\\) below. For now, do not restrict somehow the output of the ConvDiscriminator class\n\nConvGenerator\n\nTakes a latent sample (vector) of size (batch_size, input_size) as input, produces a tensor of shape (batch_size, 3, 16, 16) as output.\nThe output tensor values are in range \\([-1, 1]\\).\nImplements sample method. Choose standard Normal distribution as \\(p_z\\).\n\nüí° Hint: Start with linear block, then combine several nn.ConvTranspose2d (docs) blocks with BatchNorm2d and ReLU nonlinearity.\n\n\nConvDiscriminator\n\nFor now, do not restrict somehow the output of the ConvDiscriminator class.\nTakes an image tensor (batch_size, 3, 16, 16) as the input, produces scalar values of size (batch_size, 1) as output\n\nüí° Hint: Just combine several nn.Conv2d layers with LeakyReLU nonlinearity. Apply linear layer at the end.\n\nclass ConvGenerator(nn.Module):\n\n    def __init__(\n        self, input_size: int = 128,\n    ) -&gt; None:\n        super().__init__()\n        self.n_channels = 64\n        self.input_size = input_size\n        self.linear_block = nn.Sequential(\n            nn.Linear(input_size, 4 * 4 * 4 * self.n_channels),\n            nn.ReLU(True),\n        )\n        self.conv_block = nn.Sequential(\n            nn.ConvTranspose2d(4 * self.n_channels, 2 * self.n_channels, 2, stride=2),\n            nn.BatchNorm2d(2 * self.n_channels),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(2 * self.n_channels, self.n_channels, 2, stride=2),\n            nn.BatchNorm2d(self.n_channels),\n            nn.ReLU(True),\n            nn.Conv2d(self.n_channels, 3, 3, padding=1)\n        )\n        self.noise = torch.distributions.Normal(torch.tensor(0.0), torch.tensor(1.0))\n\n    def forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n        assert input.size(1) == self.input_size\n        output = self.linear_block(input)\n        output = output.view(-1, 4 * self.n_channels, 4, 4)\n        output = self.conv_block(output)\n        output = torch.tanh(output)\n        return output.view(-1, 3, 16, 16)\n\n    def sample(self, n_samples: int) -&gt; torch.Tensor:\n        z = self.noise.sample([n_samples, self.input_size]).to(next(iter(self.parameters())))\n        return self.forward(z)\n\nclass ConvDiscriminator(nn.Module):\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.n_channels = 64\n\n        self.net = nn.Sequential(\n            nn.Conv2d(3, self.n_channels, 3, stride=2, padding=1),\n            nn.LeakyReLU(),\n            nn.Conv2d(self.n_channels, 2 * self.n_channels, 3, stride=2, padding=1),\n            nn.LeakyReLU(),\n            nn.Conv2d(2 * self.n_channels, 4 * self.n_channels, 3, stride=2, padding=1),\n            nn.LeakyReLU(),\n        )\n        self.linear = nn.Linear(4 * 2 * 2 * self.n_channels, 1)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        output = self.net(x)\n        output = output.view(-1, 4 * 2 * 2 * self.n_channels)\n        output = self.linear(output)\n        return output\n\nCG = ConvGenerator()\nCD = ConvDiscriminator()\n\nConvDiscriminator for Vanilla GAN\nYou know what to do.\n\nclass VanillaConvDiscriminator(ConvDiscriminator):\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        output = super().forward(x)\n        return torch.sigmoid(output)\n\n\n\n\n3.3 Training on image data\n\nTraining Vanilla GAN on image data\n\nuse train_vanilla function you implement in the previous sections\ndo not forget that the discriminator‚Äôs outpus should be restricted\n\n\nBATCH_SIZE = 256\nDISCRIMINATOR_STEPS = 1 # &gt;=2 - no learning, saturation!, 1 - something manages to learn\nLR = 2e-4 # &lt; 1e-2\nN_EPOCHS = 10 # change it if you want\n\ntrain_mnist_loader = DataLoader(cmnist_train, batch_size=BATCH_SIZE, shuffle=True)\nG = ConvGenerator().to(DEVICE)\nD = VanillaConvDiscriminator().to(DEVICE)\nG_optim = torch.optim.RMSprop(G.parameters(), lr=LR)\nD_optim = torch.optim.RMSprop(D.parameters(), lr=LR)\ndiagnostic = GANDiagnosticCompanionImages(G, D, REFERENCE_IMAGES.numpy())\n\ntrain_losses = train_vanilla(\n    train_mnist_loader,\n    G,\n    D,\n    G_optim,\n    D_optim,\n    discriminator_steps=DISCRIMINATOR_STEPS,\n    n_epochs=N_EPOCHS,\n    diagnostic=diagnostic,\n    visualize_steps=1\n)\n\n  0%|                                                                                                                                                | 0/10 [00:00&lt;?, ?it/s]\n\n\nEpoch 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                                                          | 1/10 [00:30&lt;04:32, 30.32s/it]\n\n\nEpoch 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                                                            | 2/10 [01:00&lt;04:00, 30.05s/it]\n\n\nEpoch 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                                               | 3/10 [01:30&lt;03:29, 29.99s/it]\n\n\nEpoch 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                                 | 4/10 [02:00&lt;02:59, 29.97s/it]\n\n\nEpoch 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                    | 5/10 [02:29&lt;02:29, 29.93s/it]\n\n\nEpoch 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                      | 6/10 [02:59&lt;01:59, 29.94s/it]\n\n\nEpoch 6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                        | 7/10 [03:29&lt;01:29, 29.90s/it]\n\n\nEpoch 7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                           | 8/10 [03:59&lt;00:59, 29.83s/it]\n\n\nEpoch 8\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 9/10 [04:28&lt;00:29, 29.67s/it]\n\n\nEpoch 9\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [04:57&lt;00:00, 29.78s/it]\n\n\nDiagnostic after 10 epochs\n\ndiagnostic.visualize()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSamples after 70 epochs of a successful launch.\n\nGENERATED_SAMPLES = G.sample(10).detach().cpu()\nplot_images(GENERATED_SAMPLES, 'Final generated samples')\n\n\n\n\n\n\n\n\n\n\nTraining NS GAN on image data\n\nuse train_ns function you implement in the previous sections\n\n\nBATCH_SIZE = 256\nDISCRIMINATOR_STEPS = 5 # 5 is ok\nLR = 2e-3 # &lt; 1e-2\nR1_REGULARIZER = 0.1\nN_EPOCHS = 10 # change it if you want\n\ntrain_mnist_loader = DataLoader(cmnist_train, batch_size=BATCH_SIZE, shuffle=True)\nG = ConvGenerator().to(DEVICE)\nD = ConvDiscriminator().to(DEVICE)\nG_optim = torch.optim.RMSprop(G.parameters(), lr=LR)\nD_optim = torch.optim.RMSprop(D.parameters(), lr=LR)\ndiagnostic = GANDiagnosticCompanionImages(G, D, REFERENCE_IMAGES.numpy())\n\ntrain_losses = train_ns(\n    train_mnist_loader,\n    G,\n    D,\n    G_optim,\n    D_optim,\n    discriminator_steps=DISCRIMINATOR_STEPS,\n    n_epochs=N_EPOCHS,\n    diagnostic=diagnostic,\n    r1_regularizer=R1_REGULARIZER,\n    visualize_steps=1\n)\n\n  0%|                                                                                                                                                | 0/10 [00:00&lt;?, ?it/s]\n\n\nEpoch 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                                                          | 1/10 [00:29&lt;04:21, 29.01s/it]\n\n\nEpoch 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                                                            | 2/10 [00:58&lt;03:52, 29.09s/it]\n\n\nEpoch 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                                               | 3/10 [01:27&lt;03:25, 29.36s/it]\n\n\nEpoch 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                                 | 4/10 [01:57&lt;02:57, 29.51s/it]\n\n\nEpoch 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                    | 5/10 [02:26&lt;02:27, 29.47s/it]\n\n\nEpoch 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                      | 6/10 [02:56&lt;01:57, 29.43s/it]\n\n\nEpoch 6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                        | 7/10 [03:25&lt;01:28, 29.45s/it]\n\n\nEpoch 7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                           | 8/10 [03:55&lt;00:58, 29.45s/it]\n\n\nEpoch 8\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 9/10 [04:24&lt;00:29, 29.45s/it]\n\n\nEpoch 9\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [04:53&lt;00:00, 29.40s/it]\n\n\nDiagnostic after 10 epochs\n\ndiagnostic.visualize()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSamples after 100 epochs\n\nGENERATED_SAMPLES = G.sample(10).detach().cpu()\nplot_images(GENERATED_SAMPLES, 'Final generated samples')\n\n\n\n\n\n\n\n\nConclusion\nWhen applying GANs on complex data (e.g., images) one need to properly regularize your models.\nüîé Remark. Except NS GAN with \\(R_1\\) regularization, a popular choise of well-behaving regularized GAN loss is Wasserstein GAN with Gradient Penalty. For more information, see original Wasserstein GAN paper and Improved Training of Wasserstein GANs paper (the latter introduces WGAN with GP)."
  },
  {
    "objectID": "projects/GANs1.html#i-know-everything-from-above-let-me-do-something-interesting.",
    "href": "projects/GANs1.html#i-know-everything-from-above-let-me-do-something-interesting.",
    "title": "GANs",
    "section": "I know everything from above, let me do something interesting.",
    "text": "I know everything from above, let me do something interesting.\n‚ùîAdditional exercise. Learn NS GAN \\(R_1\\) on Cifar10 dataset. Come up with your own architectures for the generator and the discriminator, download the dataset, tune hyperparameters and launch train_ns function. Demonstrate the samples."
  },
  {
    "objectID": "projects/Seminar_VAE.html",
    "href": "projects/Seminar_VAE.html",
    "title": "Addisu Amare",
    "section": "",
    "text": "#Variational Autoencoders\nAuthor: Addisu Amare\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nimport torch.utils.data as data\n\nfrom typing import Tuple\nfrom collections import defaultdict\nassert torch.cuda.is_available()\nDEVICE = 'cuda'\nUSE_CUDA = True"
  },
  {
    "objectID": "projects/Seminar_VAE.html#variational-inference",
    "href": "projects/Seminar_VAE.html#variational-inference",
    "title": "Addisu Amare",
    "section": "1. Variational inference",
    "text": "1. Variational inference\n\n1.1 Problem statement\n\\(\\textbf{Data}\\): $X = {x_{1},‚Ä¶,x_{n}} $ are independent samples \\(D\\)-dimensional samples \\(\\textbf{Task}\\): We solve the task of generative modeling \\(p(X|\\Theta)\\), where \\(\\Theta\\) are parameters of the model.\nWhile we use this model, one can set optimization problem like \\(\\textbf{MLE-problem}\\):\n\\[ \\Theta^{*} = \\arg \\max_{\\Theta}  p(X|\\Theta)\\]\nHowever, one can consider our model like model with \\(d\\)-dimensional latent (hidden) variables \\(Z\\). Thus, having assumed existence of latent codes of our model, we can represent of likelihood as follows:\n\\[ p(X|\\Theta)  = p(X|Z,\\Theta) p(Z|\\Theta) \\]\n\n\\(\\textbf{Importantly}\\), we do this step because we sure, that introduced models might be easily parameterized. For example, like normal distributions:\n\n\\(p(X|Z,\\Theta) = \\mathcal{N}(X| \\mu(Z,\\Theta), \\Sigma^{-1}(Z, \\Theta))\\)\n\\(p(Z|\\Theta) = \\mathcal{N}(Z| \\mu(\\Theta), \\Sigma^{-1}(\\Theta))\\)\n\nThen, our \\(\\textbf{MLE}\\)-problem is the following:\n\\[ \\Theta^{*} = \\arg\\max_{\\Theta}\\prod_{i=1}^{n} p(x_{i}|\\Theta)=  \\arg\\max_{\\Theta}\\prod_{i=1}^{n} \\int_{\\mathbb{R}^{d}} p(x_{i}|Z, \\Theta)p(Z|\\Theta)dZ \\]\n\n\n1.2 Naive approach\nUndoubtedly, this problem might be solved through Monte-Carlo simulation:\n\\[ \\int p(x_{i}|Z,\\Theta)p(Z|\\Theta) dZ = \\mathbb{E}_{\\hat{Z} \\sim p(Z|\\Theta)} p(x_{i}|\\hat{Z},\\Theta) = \\frac{1}{K}\\sum_{k=1}^{K} p(x_{i}|\\hat{Z}_{k},\\Theta) \\]\n\n\\(\\textbf{Challenge}\\): The curse of dimensionality. Namely, we cannot properly cover whole space of \\(p(Z|\\Theta)\\) due to high dimension of the latent code. One can avoid this problem with sampleing more samples, however this amount will grow with increasing of dimensionality of latent code. Thus, to cover the space properly., the number of samples grows exponentially with respect to dimensionality of \\(Z\\). Finally, we cannot make accurate estimation for the integral.\n\\(\\textbf{Another explanation}\\) ‚Ä¶\n\n\n1.3 EM-algorithm\nSince:\n\\[ p(x_{i}|\\Theta) = \\int_{\\mathbb{R}^{d}}p(x_{i}|Z,\\Theta)p(Z|\\Theta)dZ \\]\nThen, we can consider this problem like problem for \\(\\textbf{EM}\\)-algorithm. As far as we know^ \\(\\textbf{E}\\)-step might be solved by two ways:\n\nAccurate Bayesian inference\nMean-field approximation\n\nIn \\(\\textbf{E}\\)-step, we have the following tractable integral that we cannot calculate and as a consequence of that we cannot perform Accurate Bayesian inference.\n\\[ q(Z|x_{i},\\Theta) = \\frac{p(x_{i}|Z,\\Theta) p(Z|\\Theta)}{\\int_{\\mathbb{R}^{d}}p(x_{i}|Z,\\Theta) p(Z|\\Theta)dZ}\\]\nMean-field is sufficnetly difficult for high dimensional latent codes.\n\n\n1.4 Variational Inference\nThen, we move on the Variational Inference:\n\\[ \\sum_{i=1}^{n} \\log p(x_{i}|\\Theta) = \\sum_{i=1}^{n} \\log p(x_{i}|\\Theta) \\int_{\\mathbb{R}^{d}}q(Z)dZ =  \n\\sum_{i=1}^{n} \\int_{\\mathbb{R}^{d}}  \\log p(x_{i}|\\Theta) q(Z)dZ\\]\n\\[\\sum_{i=1}^{n} \\int_{\\mathbb{R}^{d}}  \\log p(x_{i}|\\Theta) q(Z)dZ  =  \\sum_{i=1}^{n} \\int_{\\mathbb{R}^{d}} q(Z)\\log\\frac{p(x_{i},Z|\\Theta)q(Z)}{p(Z|\\Theta,x_{i})q(Z)}dZ = \\]\n\\[ = \\sum_{i=1}^{n} \\int q(Z)\\log\\frac{p(x_{i},Z|\\Theta)}{q(Z)}dZ + \\sum_{i=1}^{n}\\int q(Z)\\log \\frac{q(Z)}{p(Z|x_{i},\\Theta)}dZ\\]\nThe last term is \\(\\textbf{KL}\\)-divergence between our prior knowledge and observed posterior distribution. The main properties of this disctance are:\n\n\\(KL \\geq 0\\)\n\\(KL( \\mathbb{P}|| \\mathbb{Q}) = 0\\) , if \\(\\mathbb{P}=\\mathbb{Q}\\)\n\nFinally, we get \\(\\textbf{ELBO}\\):\n\\[ \\sum_{i=1}^{n} \\log p(x_{i}|\\Theta) \\geq \\sum_{i=1}^{n} \\int q(Z)\\log\\frac{p(x_{i},Z|\\Theta)}{q(Z)}dZ\\]\nIt is worth noticing, that \\(\\textbf{ELBO}\\) has the same formula for optimization like for VAE. We will held maximization for parameters \\(\\Theta\\) and latent distribution \\(q(Z)\\) like in EM-algorithm. Nonetheless, the main distinguish is that likelihood and prior will be constrained by normal distribution during the \\(\\textbf{E}\\)-step.\nThus, we have two models:\n\n\\(p(x_{i},Z|\\Theta)\\) is normal distribution parametrized by NN with parameters \\(\\Theta\\).\n\\(q(Z|x_{i},\\phi)\\) is normal distribution parameterized by NN with parameters \\(\\phi\\).\n\nUndoubtedly, we make more constraints , than we had with acurrate and mean-field solution of EM.\n\\(\\textbf{Our problem with Variational inference}\\):\n\\[ \\sum_{i=1}^{n} \\int_{\\mathbb{R}^{d}} q(Z|x_{i},\\phi) \\log\\frac{p(x_{i},Z|\\Theta)}{q(Z|x_{i},\\phi)}dZ \\to \\max_{\\Theta,\\phi}\\]"
  },
  {
    "objectID": "projects/Seminar_VAE.html#variational-auto-encoders-vae",
    "href": "projects/Seminar_VAE.html#variational-auto-encoders-vae",
    "title": "Addisu Amare",
    "section": "2. Variational Auto Encoders (VAE)",
    "text": "2. Variational Auto Encoders (VAE)\nNow, we make assumption about \\(\\textbf{non-linear}\\) dependence between \\(X\\) and \\(Z\\). Then:\n\n2.1 like M-step:\nour task is maximization during M-step:\n\\[\\sum_{i=1}^{n} \\mathbb{E}_{q(Z|x_{i},\\phi)} \\log p(x_{i},Z|\\Theta) \\to \\max_{\\Theta} \\]\nThen, to calculate this, we need in:\n\nmodel \\(q(Z|x_{i},\\phi)\\) should give us samples\nlogarithm of $ p(x_{i},Z|)$\n\nThen we introduce model $ p(x_{i},Z|)$ as:\n\\[ p(x_{i},Z|\\Theta) = \\mathcal{N}(x_{i}| \\mu(z_{i},\\Theta) , \\sigma^{2}(z_{i},\\Theta)I)*\\mathcal{N}(z_{i}|0,I) \\] \\[p(x_{i},Z|\\Theta) = \\prod_{j=1}^{D}\\mathcal{N}(x_{ij}| \\mu_{j}(z_{i},\\Theta) , \\sigma_{j}^{2}(z_{i},\\Theta))*\\mathcal{N}(z_{i}|0,I) \\]\nThus, the first multiplier is the \\(\\textbf{decoder}\\) :\n\ntakes \\(d\\)-dimensional latent code\noutputs 2\\(D\\)-dimensional vector, where the \\(D\\)-first are means for the corresponding pixel of image, while the second are variances.\n\nAlso, we introduce the following model with ability of sampling latent codes from data sample.\n\\[ q(z|x_{i},\\phi) = \\prod_{j=1}^{d} \\mathcal{N}(z_{j}|m_{j}(x_{i},\\phi),s_{j}^{2}(x_{i},\\phi))\\]\nThis model is the \\(\\textbf{encoder}\\)\n\ntakes \\(D\\)-dimensional sample from data\noutputs 2\\(d\\)-dimensional vector, where the \\(d\\)-first are means for the corresponding latent code, while the second are variances.\n\nIt is worth noticing, that the posterior distribution \\(q(z|x_{i},\\phi)\\). is multiplication of 1-dimensional normal distributions.\n\n\n2.2 like E-step:\nDuring the \\(\\textbf{E}\\)-step, we maximize by parameters of encoder:\nHowever, we know, that the maximization of ELBO corresponds to the minimization of \\(\\textbf{KL}\\)-divergence between prior and current posterior (encoder):\n\\[ KL(q(z_{i}|x_{i},\\phi) || p(z)) \\to \\min_{\\phi}\\]\nYet another reason for choosing normal distribution for \\(q(z_{i}|x_{i},\\phi)\\) is the existence closed form of KL-divergence between gaussian distributions. Thta is why, we pick prior knowledge \\(p(z)\\) as gaussian too."
  },
  {
    "objectID": "projects/Seminar_VAE.html#vae-on-2d-data",
    "href": "projects/Seminar_VAE.html#vae-on-2d-data",
    "title": "Addisu Amare",
    "section": "3. VAE on 2d data",
    "text": "3. VAE on 2d data\nIn this task we will implement simple VAE model for 2d gaussian distribution \\(\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\).\nWe will consider two cases: * 2d univariate distribution (diagonal covariance matrix \\(\\boldsymbol{\\Sigma}\\)); * 2d multivariate distribution (strictly non-diagonal covariance matrix \\(\\boldsymbol{\\Sigma}\\)).\nThe goal is to analyze the difference between these two cases and understand why the trained VAE models will behave differently.\n\n3.1 Data generation\n\nTICKS_FONT_SIZE = 12\nLEGEND_FONT_SIZE = 12\nLABEL_FONT_SIZE = 14\nTITLE_FONT_SIZE = 16\n\n\ndef visualize_2d_data(\n    train_data,test_data,train_labels=None ,\n    test_labels=None ):\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n    ax1.set_title(\"train\", fontsize=TITLE_FONT_SIZE)\n    ax1.scatter(train_data[:, 0], train_data[:, 1], s=1, c=train_labels)\n    ax1.tick_params(labelsize=LABEL_FONT_SIZE)\n    ax2.set_title(\"test\", fontsize=TITLE_FONT_SIZE)\n    ax2.scatter(test_data[:, 0], test_data[:, 1], s=1, c=test_labels)\n    ax2.tick_params(labelsize=LABEL_FONT_SIZE)\n    plt.show()\n\n\ndef visualize_2d_samples(data, title, labels=None, xlabel=None, ylabel=None):\n    plt.figure(figsize=(5, 5))\n    plt.scatter(data[:, 0], data[:, 1], s=1, c=labels)\n    plt.title(title, fontsize=TITLE_FONT_SIZE)\n    plt.xticks(fontsize=TICKS_FONT_SIZE)\n    plt.yticks(fontsize=TICKS_FONT_SIZE)\n    if xlabel is not None:\n        plt.xlabel(xlabel, fontsize=LABEL_FONT_SIZE)\n    if ylabel is not None:\n        plt.ylabel(ylabel, fontsize=LABEL_FONT_SIZE)\n    plt.show()\n\n\ndef generate_2d_data(count, mode='univariate'):\n    assert mode in ['univariate', 'multivariate']\n    np.random.seed(42)\n    mean = [[2.0, 3.0]]\n    sigma = [[3.0, 1.0]]\n    if mode == 'univariate':\n        rotate = [\n            [1.0, 0.0],\n            [0.0, 1.0]\n        ]\n    else:\n        rotate = [\n            [np.sqrt(2) / 2, np.sqrt(2) / 2],\n            [-np.sqrt(2) / 2, np.sqrt(2) / 2]\n        ]\n    data = mean + (np.random.randn(count, 2) * sigma).dot(rotate)\n    data = data.astype('float32')\n    split = int(0.7 * count)\n    train_data, test_data = data[:split], data[split:]\n    return train_data, test_data\n\n\ndef plot_training_curves(train_losses, test_losses, logscale_y=False, logscale_x=False):\n    n_train = len(train_losses[list(train_losses.keys())[0]])\n    n_test = len(test_losses[list(train_losses.keys())[0]])\n    x_train = np.linspace(0, n_test - 1, n_train)\n    x_test = np.arange(n_test)\n\n    plt.figure()\n    for key, value in train_losses.items():\n        plt.plot(x_train, value, label=key + '_train')\n\n    for key, value in test_losses.items():\n        plt.plot(x_test, value, label=key + '_test')\n\n    if logscale_y:\n        plt.semilogy()\n\n    if logscale_x:\n        plt.semilogx()\n\n    plt.legend(fontsize=LEGEND_FONT_SIZE)\n    plt.xlabel('Epoch', fontsize=LABEL_FONT_SIZE)\n    plt.ylabel('Loss', fontsize=LABEL_FONT_SIZE)\n    plt.xticks(fontsize=TICKS_FONT_SIZE)\n    plt.yticks(fontsize=TICKS_FONT_SIZE)\n    plt.grid()\n    plt.show()\n\n\nCOUNT = 15000\n\ntrain_data, test_data = generate_2d_data(COUNT, mode='multivariate')\nvisualize_2d_data(train_data, test_data)\n\ntrain_data, test_data = generate_2d_data(COUNT, mode='univariate')\nvisualize_2d_data(train_data, test_data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe difference of these two cases is the form of covariance matrix \\(\\boldsymbol{\\Sigma}\\).\nIn multivariate case the matrix is non-diagonal, in univariate case it is strictly diagonal. As you will see, our VAE model will have absolutely different results for these datasets.\n\n\n3.2 Kl-divergence and log-likelihood\nNow it is time to define our model. Our model will have the following structure:\n\nThe latent dimensionality is equal to 2, the same as the data dimensionality (\\(\\mathbf{z} \\in \\mathbb{R}^2\\), \\(\\mathbf{x} \\in \\mathbb{R}^2\\)).\nPrior distribution is standard Normal (\\(p(\\mathbf{z}) = \\mathcal{N}(0, I)\\)).\nVariational posterior distribution (or encoder) is \\(q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi}) = \\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\phi}}(\\mathbf{x}), \\boldsymbol{\\Sigma}_{\\boldsymbol{\\phi}}(\\mathbf{x}))\\). Here \\(\\boldsymbol{\\phi}\\) denotes all parameters of the encoder neural network.\nGenerative distribution (or decoder) is \\(p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}(\\mathbf{z}), \\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}(\\mathbf{z}))\\). Here \\(\\boldsymbol{\\theta}\\) denotes all parameters of the decoder neural network. Please note, that here we will use continuous distribution for our variables \\(\\mathbf{x}\\).\nWe will consider only diagonal covariance matrices \\(\\boldsymbol{\\Sigma}_{\\boldsymbol{\\phi}}(\\mathbf{x})\\), \\(\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}(\\mathbf{z})\\).\n\nModel objective is ELBO: \\[\n    \\mathcal{L}(\\boldsymbol{\\phi}, \\boldsymbol{\\theta}) = \\mathbb{E}_{q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi})} \\log p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta}) - KL (q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi}) || p(\\mathbf{z})).\n\\]\nTo make the expectation is independent of parameters \\(\\boldsymbol{\\phi}\\), we will use reparametrization trick.\nTo calculate the loss, we should derive - \\(\\log p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta})\\), note that generative distribution is \\(\\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}(\\mathbf{z}), \\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}(\\mathbf{z}))\\). - KL between \\(\\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\phi}}(\\mathbf{x}), \\boldsymbol{\\Sigma}_{\\boldsymbol{\\phi}}(\\mathbf{x}))\\) and \\(\\mathcal{N}(0, I)\\).\n\ndef get_normal_KL(mean_1, log_std_1, mean_2=None, log_std_2=None):\n    \"\"\"\n        This function should return the value of KL(p1 || p2),\n        where p1 = Normal(mean_1, exp(log_std_1)), p2 = Normal(mean_2, exp(log_std_2) ** 2).\n        If mean_2 and log_std_2 are None values, we will use standard normal distribution.\n        Note that we consider the case of diagonal covariance matrix.\n    \"\"\"\n    if mean_2 is None:\n        mean_2 = torch.zeros_like(mean_1)\n    if log_std_2 is None:\n        log_std_2 = torch.zeros_like(log_std_1)\n\n    std_1 = torch.exp(log_std_1)\n    std_2 = torch.exp(log_std_2)\n\n    mean_1, mean_2 = mean_1.float(), mean_2.float()\n    std_1 , std_2  = std_1 .float(), std_2 .float()\n\n    p  = torch.distributions.Normal(mean_1, std_1)\n    q  = torch.distributions.Normal(mean_2, std_2)\n    kl = torch.distributions.kl_divergence(p, q)\n\n    return kl\n\n\ndef get_normal_nll(x, mean, log_std):\n    \"\"\"\n        This function should return the negative log likelihood log p(x),\n        where p(x) = Normal(x | mean, exp(log_std) ** 2).\n        Note that we consider the case of diagonal covariance matrix.\n    \"\"\"\n    # ====\n    mean = mean              .float()\n    std  = torch.exp(log_std).float()\n\n    #if (mean.dim() == 0) and (std.dim() == 0):\n    prob = torch.distributions.Normal(mean, std)\n    #else:\n    #    scale_tril=torch.diag(std)\n    #    prob = torch.distributions.MultivariateNormal(mean, scale_tril=scale_tril)\n\n    nnl = -prob.log_prob(x)\n    return nnl\n\n\n\n3.3 VAE\nWe will use simple fully connected dense networks for encoder and decoder.\n\nclass FullyConnectedMLP(nn.Module):\n    def __init__(self, input_shape, hiddens, output_shape):\n        assert isinstance(hiddens, list)\n        super().__init__()\n        self.input_shape  = (input_shape,)\n        self.output_shape = (output_shape,)\n        self.hiddens = hiddens\n\n        model = []\n\n        # ====\n        # your code\n        # stack Dense layers with ReLU activation\n        # note: you do not have to add relu after the last dense layer\n        # ====\n        model.append( nn.Linear(input_shape, hiddens[0]) )\n        model.append( nn.ReLU() )\n\n        for i in range( len(hiddens)-1 ):\n            model.append( nn.Linear(hiddens[i+0], hiddens[i+1]) )\n            model.append( nn.ReLU() )\n\n        model.append( nn.Linear(hiddens[-1], output_shape) )\n        self.net = nn.Sequential(*model)\n\n    def forward(self, x):\n        # ====\n        # your code\n        # apply network that was defined in __init__ and return the output\n        # ====\n        return self.net(x)\n\n\nclass VAE2d(nn.Module):\n    def __init__(self, n_in, n_latent, enc_hidden_sizes, dec_hidden_sizes):\n        assert isinstance(enc_hidden_sizes, list)\n        assert isinstance(dec_hidden_sizes, list)\n        super().__init__()\n        self.n_latent = n_latent\n\n        # ====\n        # your code\n        # define encoder and decoder networks\n        # the encoder takes n_in elements, has enc_hidden_sizes neurons in hidden layers\n        # and outputs 2 * n_latent (n_latent for means, and n_latent for std)\n        # the decoder takes n_latent elements, has dec_hidden_sizes neurons in hidden layers\n        # and outputs 2 * n_in (n_in for means, and n_in for std)\n        # ====\n        self.encoder = FullyConnectedMLP(n_in    , enc_hidden_sizes, 2 * n_latent )\n        self.decoder = FullyConnectedMLP(n_latent, dec_hidden_sizes, 2 * n_in     )\n    def prior(self, n):\n        # ====\n        # your code\n        # return n samples from prior distribution (we use standard normal for prior)\n        # ====\n        loc   = torch.zeros(self.n_latent)\n        scale = torch.ones (self.n_latent)\n        p = torch.distributions.Normal(loc, scale)\n        prior_s = p.sample_n(n)\n\n        if USE_CUDA:\n            prior_s = prior_s.cuda()\n        return prior_s\n\n    def forward(self, x):\n        # ====\n        # your code\n        # now you have to return from the model\n        # - mu_z - means for variational distribution\n        # - mu_x - means for generative distribution\n        # - log_std_z - logarithm of std for variational distribution\n        # - log_std_x - logarithm of std for generative distribution\n        # we use logarithm, since the std is always positive\n        # to get std we will exponentiate it to get rid of this constraint\n\n        # 1) mu_z, log_std_z are outputs from the encoder\n        # 2) apply reparametrization trick to get z (input of decoder)\n        # (do not forget to use self.prior())\n        # 3) mu_x, log_std_x are outputs from the decoder\n        #    Note: [mu, log_std = decoder(input).chunk(2, dim=1)]\n\n        # ====\n        mu_z, log_std_z = self.encoder(x).chunk(2, dim=1)\n        z = torch.exp(log_std_z.to('cuda'))*self.prior( x.size(0) ) + mu_z.to('cuda')\n        mu_x, log_std_x = self.decoder(z).chunk(2, dim=1)\n\n        return mu_z, log_std_z, mu_x, log_std_x\n\n    def loss(self, x):\n        mu_z, log_std_z, mu_x, log_std_x = self(x)\n        # ====\n        # your code\n        # 1) apply model to get mu_z, log_std_z, mu_x, log_std_x\n        # 2) compute reconstruction loss using get_normal_nll (it is the first term in ELBO)\n        # 3) compute KL loss using get_normal_KL (it is the second term in ELBO)\n        # ====\n        recon_loss = torch.sum(get_normal_nll( x, mu_x, log_std_x ))\n        #kl_loss    = torch.sum(get_normal_KL ( mu_z, log_std_z, mu_x, log_std_x ))\n        kl_loss    = torch.sum(get_normal_KL ( mu_z, log_std_z, torch.zeros_like(mu_z), torch.zeros_like(log_std_z) ))\n\n\n        return {\n            'elbo_loss': recon_loss + kl_loss,\n            'recon_loss': recon_loss,\n            'kl_loss': kl_loss\n        }\n\n    def sample(self, n, sample_from_decoder=True):\n        z = None\n        with torch.no_grad():\n            # ====\n            # your code\n            # to sample from VAE model you have to sample from prior\n            # and then apply decoder to prior samples.\n            # parameter noise indicates whether to sample from decoder\n            # or just use means of generative distribution as samples\n            # 1) generate prior samples\n            # 2) apply decoder\n            # 3) sample from the decoder distribution if sample_from_decoder=True\n            # ====\n            prior_s = self.prior(n)\n            mu_x, log_std_x = self.decoder(prior_s).chunk(2, dim=1)\n            if sample_from_decoder:\n                z = torch.exp(log_std_x)*prior_s + mu_x\n            else:\n                z = mu_x\n        return z.cpu().numpy()\n\n\ndef solve_task(train_data, test_data, model, batch_size, epochs, lr, use_cuda=True, use_tqdm=False):\n    train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n    test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE)\n\n    train_losses, test_losses = train_model(\n        model, train_loader, test_loader, epochs=EPOCHS, lr=LR, use_cuda=use_cuda, use_tqdm=use_tqdm, loss_key='elbo_loss'\n    )\n    samples_noise = model.sample(3000, sample_from_decoder=True)\n    samples_nonoise = model.sample(3000, sample_from_decoder=False)\n\n    for key, value in test_losses.items():\n        print('{}: {:.4f}'.format(key, value[-1]))\n\n    plot_training_curves(train_losses, test_losses)\n    visualize_2d_samples(samples_noise, title='Samples with Decoder Noise')\n    visualize_2d_samples(samples_nonoise, title='Samples without Decoder Noise')\n\n\n# ====\n# your code\n# choose these parameters (2 hidden layers could be enough for encoder and decoder)\nENC_HIDDEN_SIZES = [20, 20]\nDEC_HIDDEN_SIZES = [20, 20]\nBATCH_SIZE = 32    # any adequate value\nEPOCHS = 20         # &lt; 10\nLR = 0.001        # &lt; 1e-2\n# ====\n\nCOUNT = 10000\n\n\nfrom tqdm import tqdm\n\n\ndef train_epoch(\n    model: object,\n    train_loader: object,\n    optimizer: object,\n    use_cuda: bool,\n    loss_key: str = \"total\",\n) -&gt; defaultdict:\n    model.train()\n\n    stats = defaultdict(list)\n    for x in tqdm(train_loader):\n        if use_cuda:\n            x = x.cuda()\n        losses = model.loss(x)\n        optimizer.zero_grad()\n        losses[loss_key].backward()\n        optimizer.step()\n\n        for k, v in losses.items():\n            stats[k].append(v.item())\n\n    return stats\n\n\ndef eval_model(model: object, data_loader: object, use_cuda: bool) -&gt; defaultdict:\n    model.eval()\n    stats = defaultdict(float)\n    with torch.no_grad():\n        for x in data_loader:\n            if use_cuda:\n                x = x.cuda()\n            losses = model.loss(x)\n            for k, v in losses.items():\n                stats[k] += v.item() * x.shape[0]\n\n        for k in stats.keys():\n            stats[k] /= len(data_loader.dataset)\n    return stats\n\n\ndef train_model(\n    model: object,\n    train_loader: object,\n    test_loader: object,\n    epochs: int,\n    lr: float,\n    use_tqdm: bool = False,\n    use_cuda: bool = False,\n    loss_key: str = \"total_loss\",\n) -&gt; Tuple[dict, dict]:\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    train_losses = defaultdict(list)\n    test_losses = defaultdict(list)\n    forrange = tqdm(range(epochs)) if use_tqdm else range(epochs)\n    if use_cuda:\n        model = model.cuda()\n\n    for epoch in forrange:\n        model.train()\n        train_loss = train_epoch(model, train_loader, optimizer, use_cuda, loss_key)\n        test_loss = eval_model(model, test_loader, use_cuda)\n\n        for k in train_loss.keys():\n            train_losses[k].extend(train_loss[k])\n            test_losses[k].append(test_loss[k])\n    return dict(train_losses), dict(test_losses)\n\n\ntrain_data, test_data = generate_2d_data(COUNT, mode='multivariate')\nvisualize_2d_data(train_data, test_data)\n\nmodel = VAE2d(2, 2, ENC_HIDDEN_SIZES, DEC_HIDDEN_SIZES).to('cuda')\nsolve_task(train_data, test_data, model, BATCH_SIZE, EPOCHS, LR, use_cuda=True)\n\n\n\n\n\n\n\n\n  0%|          | 0/219 [00:00&lt;?, ?it/s]/tmp/ipython-input-117888137.py:26: FutureWarning: `sample_n(n)` will be deprecated. Use `sample((n,))` instead.\n  prior_s = p.sample_n(n)\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:01&lt;00:00, 129.83it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 235.93it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 243.09it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:01&lt;00:00, 190.84it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:01&lt;00:00, 203.71it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 240.85it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 236.11it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 233.17it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 240.47it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 242.56it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 233.15it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 240.58it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 231.52it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:01&lt;00:00, 208.00it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:01&lt;00:00, 189.54it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 234.39it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 230.90it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 240.08it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 239.26it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 243.36it/s]\n\n\nelbo_loss: 125.9687\nrecon_loss: 90.7217\nkl_loss: 35.2470\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo analyze our models we will use the following function. Look carefully, do not change.\nThis function calculates the mean \\(\\boldsymbol{\\mu}_{\\boldsymbol{\\phi}}(\\mathbf{x})\\), and covariances \\(\\boldsymbol{\\Sigma}_{\\boldsymbol{\\phi}}(\\mathbf{x})\\) of the variational posterior distribution \\(q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi})\\).\n\ndef get_latent_stats(model, test_data, use_cuda=True, batch_size=3000):\n    batch = next(iter(data.DataLoader(test_data, batch_size=batch_size, shuffle=True)))\n    if use_cuda:\n        batch = batch.cuda()\n\n    with torch.no_grad():\n        mu_z, log_std_z = model(batch)[:2]\n\n    mu_z = mu_z.cpu().numpy()\n    std_z = log_std_z.exp().cpu().numpy()\n\n    return mu_z, std_z\n\n\n# just look at these numbers and read the comments after this task\nmu_z, std_z = get_latent_stats(model, test_data, use_cuda=USE_CUDA)\n\nprint('mu_z = ', mu_z.mean(axis=0), '+-', mu_z.std(axis=0))\nprint('std_z = ', std_z.mean(axis=0), '+-', std_z.std(axis=0))\n\nmu_z =  [-0.00333838  0.00629993] +- [0.9503864  0.02816329]\nstd_z =  [0.33444908 0.98905873] +- [0.00776618 0.01851054]\n\n\n/tmp/ipython-input-117888137.py:26: FutureWarning: `sample_n(n)` will be deprecated. Use `sample((n,))` instead.\n  prior_s = p.sample_n(n)\n\n\nSecondly, we will train the VAE model for univariate gaussian distribution.\n\ntrain_data, test_data = generate_2d_data(COUNT, mode='univariate')\nvisualize_2d_data(train_data, test_data)\n\nmodel = VAE2d(2, 2, ENC_HIDDEN_SIZES, DEC_HIDDEN_SIZES).cuda()\nsolve_task(train_data, test_data, model, BATCH_SIZE, EPOCHS, LR, use_cuda=USE_CUDA)\n\n\n\n\n\n\n\n\n  0%|          | 0/219 [00:00&lt;?, ?it/s]/tmp/ipython-input-117888137.py:26: FutureWarning: `sample_n(n)` will be deprecated. Use `sample((n,))` instead.\n  prior_s = p.sample_n(n)\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 230.18it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 235.42it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 235.94it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:01&lt;00:00, 204.08it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:01&lt;00:00, 184.54it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 237.33it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 239.81it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 235.39it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 238.16it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 233.77it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 243.84it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 240.69it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 239.44it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 233.66it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:01&lt;00:00, 187.62it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:01&lt;00:00, 184.49it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 235.14it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 236.41it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 242.78it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 240.52it/s]\n\n\nelbo_loss: 126.0842\nrecon_loss: 125.7401\nkl_loss: 0.3441\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmu_z, std_z = get_latent_stats(model, test_data, use_cuda=USE_CUDA)\n\nprint('mu_z = ', mu_z.mean(axis=0), '+-', mu_z.std(axis=0))\nprint('std_z = ', std_z.mean(axis=0), '+-', std_z.std(axis=0))\n\nmu_z =  [-0.01729548  0.00055771] +- [0.06285747 0.021273  ]\nstd_z =  [1.007619   0.99548346] +- [0.06453445 0.06496055]\n\n\n/tmp/ipython-input-117888137.py:26: FutureWarning: `sample_n(n)` will be deprecated. Use `sample((n,))` instead.\n  prior_s = p.sample_n(n)\n\n\nAfter training the VAE model on these 2 datasets, have a look at ‚ÄúSamples without Decoder Noise‚Äù figures. These figures show the means \\(\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}(\\mathbf{z})\\) of the generative distribution \\(p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta})\\). In the case of multivariate gaussian, the means are perfectly aligned with the data distribution. Otherwise, you have to see the strange figure in the univariate gaussian case . This happens due to so called posterior collapse (we will discuss it at the one of our lectures).\nTo be brief, the reason is the following. Our posterior distribution \\(p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}(\\mathbf{z}), \\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}(\\mathbf{z}))\\) is a univariate (covariance matrix is diagonal). Thus, the model does not need latent variable since the data distribution is also univariate. In this case VAE ignores latent variable, cause the model fits the distribution without any information from latent space.\nIf the decoder ignores latent variable, the second term in ELBO (KL) could be low (variational posterior distribution, which is given by encoder model, is close to prior distribution for each datapoint). In the training curves you have to see that KL loss behaves differently in these two cases.\nThe mean and std of variational posterior distribution also proves this concept. For the second case you have to see that mean is almost zero and std is almost one.\nIt is a real problem for generative models and we will discuss later how to overcome it."
  },
  {
    "objectID": "projects/Reconstruction/image_colorization.html",
    "href": "projects/Reconstruction/image_colorization.html",
    "title": "Addisu Amare",
    "section": "",
    "text": "Here we assume one has been trained, and demonstrate its use. You can train one very similar to how you trained the 2D models above.\n\n# imports\nimport os, sys\n\n# third party imports\nimport numpy as np\nimport tensorflow as tf\nimport voxelmorph as vxm\nimport neurite as ne\nassert tf.__version__.startswith('2.'), 'This tutorial assumes Tensorflow 2.0+'\n\n\n\n\n\n# our data will be of shape 160 x 192 x 224\nvol_shape = (160, 192, 224)\nnb_features = [\n    [16, 32, 32, 32],\n    [32, 32, 32, 32, 32, 16, 16]\n]\n\n\n# build vxm network\nvxm_model = vxm.networks.VxmDense(vol_shape, nb_features, int_steps=0);\n\n\n\n\n\nval_volume_1 = np.load('subj1.npz')['vol']\nseg_volume_1 = np.load('subj1.npz')['seg']\nval_volume_2 = np.load('subj2.npz')['vol']\nseg_volume_2 = np.load('subj2.npz')['seg']\n\nval_input = [\n    val_volume_1[np.newaxis, ..., np.newaxis],\n    val_volume_2[np.newaxis, ..., np.newaxis]\n]\n\nLoad a trained 3D model.\n\nvxm_model.load_weights('brain_3d.h5')\n\nNow let‚Äôs register.\n\nval_pred = vxm_model.predict(val_input);\n\n1/1 [==============================] - 8s 8s/step\n\n\n\nmoved_pred = val_pred[0].squeeze()\npred_warp = val_pred[1]\n\n\nmid_slices_fixed = [np.take(val_volume_2, vol_shape[d]//2, axis=d) for d in range(3)]\nmid_slices_fixed[1] = np.rot90(mid_slices_fixed[1], 1)\nmid_slices_fixed[2] = np.rot90(mid_slices_fixed[2], -1)\n\nmid_slices_pred = [np.take(moved_pred, vol_shape[d]//2, axis=d) for d in range(3)]\nmid_slices_pred[1] = np.rot90(mid_slices_pred[1], 1)\nmid_slices_pred[2] = np.rot90(mid_slices_pred[2], -1)\nne.plot.slices(mid_slices_fixed + mid_slices_pred, cmaps=['gray'], do_colorbars=True, grid=[2,3]);\n\n\n\n\n\n\n\n\nLet‚Äôs look at the segmentations! To do this, we‚Äôll need to warp segmentations.\n\nwarp_model = vxm.networks.Transform(vol_shape, interp_method='nearest')\n\n\nwarped_seg = warp_model.predict([seg_volume_1[np.newaxis,...,np.newaxis], pred_warp])\n\n1/1 [==============================] - 0s 319ms/step\n\n\nWe‚Äôre first going to prepare a colormap.\n\nfrom pystrum.pytools.plot import jitter\nimport matplotlib\n\n[ccmap, scrambled_cmap] = jitter(255, nargout=2)\nscrambled_cmap[0, :] = np.array([0, 0, 0, 1])\nccmap = matplotlib.colors.ListedColormap(scrambled_cmap)\n\nLet‚Äôs visualize the segmentations .\n\nmid_slices_fixed = [np.take(seg_volume_1, vol_shape[d]//1.8, axis=d) for d in range(3)]\nmid_slices_fixed[1] = np.rot90(mid_slices_fixed[1], 1)\nmid_slices_fixed[2] = np.rot90(mid_slices_fixed[2], -1)\n\nmid_slices_pred = [np.take(warped_seg.squeeze(), vol_shape[d]//1.8, axis=d) for d in range(3)]\nmid_slices_pred[1] = np.rot90(mid_slices_pred[1], 1)\nmid_slices_pred[2] = np.rot90(mid_slices_pred[2], -1)\n\nslices = mid_slices_fixed + mid_slices_pred\nfor si, slc  in enumerate(slices):\n    slices[si][0] = 255\nne.plot.slices(slices, cmaps = [ccmap], grid=[2,3]);"
  },
  {
    "objectID": "projects/Reconstruction/image_colorization.html#image-reconstruction-advanced",
    "href": "projects/Reconstruction/image_colorization.html#image-reconstruction-advanced",
    "title": "Addisu Amare",
    "section": "",
    "text": "Here we assume one has been trained, and demonstrate its use. You can train one very similar to how you trained the 2D models above.\n\n# imports\nimport os, sys\n\n# third party imports\nimport numpy as np\nimport tensorflow as tf\nimport voxelmorph as vxm\nimport neurite as ne\nassert tf.__version__.startswith('2.'), 'This tutorial assumes Tensorflow 2.0+'\n\n\n\n\n\n# our data will be of shape 160 x 192 x 224\nvol_shape = (160, 192, 224)\nnb_features = [\n    [16, 32, 32, 32],\n    [32, 32, 32, 32, 32, 16, 16]\n]\n\n\n# build vxm network\nvxm_model = vxm.networks.VxmDense(vol_shape, nb_features, int_steps=0);\n\n\n\n\n\nval_volume_1 = np.load('subj1.npz')['vol']\nseg_volume_1 = np.load('subj1.npz')['seg']\nval_volume_2 = np.load('subj2.npz')['vol']\nseg_volume_2 = np.load('subj2.npz')['seg']\n\nval_input = [\n    val_volume_1[np.newaxis, ..., np.newaxis],\n    val_volume_2[np.newaxis, ..., np.newaxis]\n]\n\nLoad a trained 3D model.\n\nvxm_model.load_weights('brain_3d.h5')\n\nNow let‚Äôs register.\n\nval_pred = vxm_model.predict(val_input);\n\n1/1 [==============================] - 8s 8s/step\n\n\n\nmoved_pred = val_pred[0].squeeze()\npred_warp = val_pred[1]\n\n\nmid_slices_fixed = [np.take(val_volume_2, vol_shape[d]//2, axis=d) for d in range(3)]\nmid_slices_fixed[1] = np.rot90(mid_slices_fixed[1], 1)\nmid_slices_fixed[2] = np.rot90(mid_slices_fixed[2], -1)\n\nmid_slices_pred = [np.take(moved_pred, vol_shape[d]//2, axis=d) for d in range(3)]\nmid_slices_pred[1] = np.rot90(mid_slices_pred[1], 1)\nmid_slices_pred[2] = np.rot90(mid_slices_pred[2], -1)\nne.plot.slices(mid_slices_fixed + mid_slices_pred, cmaps=['gray'], do_colorbars=True, grid=[2,3]);\n\n\n\n\n\n\n\n\nLet‚Äôs look at the segmentations! To do this, we‚Äôll need to warp segmentations.\n\nwarp_model = vxm.networks.Transform(vol_shape, interp_method='nearest')\n\n\nwarped_seg = warp_model.predict([seg_volume_1[np.newaxis,...,np.newaxis], pred_warp])\n\n1/1 [==============================] - 0s 319ms/step\n\n\nWe‚Äôre first going to prepare a colormap.\n\nfrom pystrum.pytools.plot import jitter\nimport matplotlib\n\n[ccmap, scrambled_cmap] = jitter(255, nargout=2)\nscrambled_cmap[0, :] = np.array([0, 0, 0, 1])\nccmap = matplotlib.colors.ListedColormap(scrambled_cmap)\n\nLet‚Äôs visualize the segmentations .\n\nmid_slices_fixed = [np.take(seg_volume_1, vol_shape[d]//1.8, axis=d) for d in range(3)]\nmid_slices_fixed[1] = np.rot90(mid_slices_fixed[1], 1)\nmid_slices_fixed[2] = np.rot90(mid_slices_fixed[2], -1)\n\nmid_slices_pred = [np.take(warped_seg.squeeze(), vol_shape[d]//1.8, axis=d) for d in range(3)]\nmid_slices_pred[1] = np.rot90(mid_slices_pred[1], 1)\nmid_slices_pred[2] = np.rot90(mid_slices_pred[2], -1)\n\nslices = mid_slices_fixed + mid_slices_pred\nfor si, slc  in enumerate(slices):\n    slices[si][0] = 255\nne.plot.slices(slices, cmaps = [ccmap], grid=[2,3]);"
  },
  {
    "objectID": "projects/Reconstruction/image_colorization.html#runtime",
    "href": "projects/Reconstruction/image_colorization.html#runtime",
    "title": "Addisu Amare",
    "section": "Runtime",
    "text": "Runtime\nAn important advantage of learning-based methods is the dramatically lowered runtime.\n\n%timeit vxm_model.predict(val_input)\n\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 1s 962ms/step\n1/1 [==============================] - 0s 262ms/step\n1/1 [==============================] - 0s 195ms/step\n1/1 [==============================] - 0s 200ms/step\n1/1 [==============================] - 0s 213ms/step\n1/1 [==============================] - 0s 198ms/step\n1/1 [==============================] - 0s 198ms/step\n564 ms ¬± 259 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)"
  },
  {
    "objectID": "projects/bayesian_linear_regression.html",
    "href": "projects/bayesian_linear_regression.html",
    "title": "Bayesian Linear Regression",
    "section": "",
    "text": "import numpy as np\nfrom scipy.stats import multivariate_normal as mvn\nfrom numpy.polynomial.polynomial import polyval\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\\(\\textbf{Notation:}\\)"
  },
  {
    "objectID": "projects/bayesian_linear_regression.html#classical-ml-against-bayesian-ml",
    "href": "projects/bayesian_linear_regression.html#classical-ml-against-bayesian-ml",
    "title": "Bayesian Linear Regression",
    "section": "1. Classical ML against Bayesian ML",
    "text": "1. Classical ML against Bayesian ML\n\nWhile we talk about classical ML approach it means, that we want to choose approprate weight matrix \\(w \\in \\mathbb{R}^{d\\times1}\\) in order to set equality in the equation below:\n\\[ Y = Xw\\]\nIt is worth noticing that \\(w = [ w_{1},..,w_{d}]^{T}\\) is column, and \\(w^{T}\\) is a raw.\n\\(\\textbf{Motivation:}\\) Learn these weight from the data to set \\(\\textbf{equality}\\)\nWhile we solve classic linear regression, it means that we maximize likelihood of the model, \\(\\textbf{why???}\\) let‚Äôs go:\n\\[\\max_{w}p(Y|X,w) = \\max_{w} \\sum_{i=1}^{n} \\log p(y_{i}|x_{i},w)\\]\nLet \\(p(y_{i}|x_{i},w) = \\mathcal{N}(y_{i}| w^{T}x_{i},\\beta^{-1})\\), where \\(\\beta = (\\frac{1}{n}\\sum_{i=1}^{n} (y_{i} - w^{T}x_{i})^{2})^{-1}\\)then:\n\\[ \\max_{w} \\sum_{i=1}^{n} \\log \\frac{1}{\\sqrt{2\\pi}\\beta^{-0.5}}exp(- \\frac{(y_{i} - w^{T}x_{i})^{2}}{2\\beta^{-1}})\\]\n\\[ \\max_{w} [ -n\\frac{1}{2}\\log(2\\pi) +. \\frac{n}{2}\\log\\beta - \\beta\\sum_{i=1}^{n} \\frac{(y_{i} - w^{T}x_{i})^{2}}{2}]\\]\nWhile maximize negative term, one can minimize the same positive term:\n\\[ \\min_{w}  \\sum_{i=1}^{n}(y_{i}  - w^{T}x_{i})^{2} = \\min_{w} || Y- Xw||^{2}_{2}\\]\nTo find the optimal value of weights , we make differentiaon over these weights:\n\\[ \\frac{\\partial|| Y- Xw||^{2}_{2} }{\\partial w} = 0  \\implies Y - Xw= 0\\]\nA little bit of manipulations from Linear Algebra:\n\\[  Y = Xw \\implies X^{T}Y = X^{T}Xw  \\implies  \\boxed{w_{ml} = (X^{T}X)^{-1} X^{T}Y}\\]\n\n1.2 Bayesian ML\n\nFirst of all, we should understand that the Bayesian Linear Regression grows up from Bayes formula:\n\\[ p(w| X,y ) = \\frac{p(y|X,w)p(w)}{\\int_{W}p(y|X,w)p(w)dw}\\]\nWe don‚Äôt pay our attention th the integral is a constant, thus:\n\\[ p(w| X,y ) \\approx p(y|X,w)p(w)\\]\n\\(\\textbf{Important:}\\) \\(p(w| X,y )\\) is the distribution, however we do not a law that describes this distribution. To define which law describes the posterior, we should pay our attention to the final representation of the posterior\n\n\\(p(w)\\) is prior distribtuion for parameters. We define oureselves that. Sincce the weights of learnable net lie near 0. Hence, one can define the prior as follows:\n\n\\[ p(w) = p(w|A) = \\mathcal{N}(w|0, A^{-1}) = \\prod_{j=1}^{d} \\mathcal{N}(w_{j}|0,\\alpha_{j}^{-1}): \\quad A =\\alpha I\\]\n\n\\(p(y|X,w)\\) is likelihood. As we have already sais, that:\n\n\\[ p(y|X, w) = \\prod_{i=1}^{d} p(y_{i}|x_{i},w) = \\mathcal{N}(y_{i}|w^{T}x_{i}, \\beta^{-1})\\]\nSince likelihood and prior are normnal, then it means that posterior is normla too. Thus:\n\\[ p(w|X,y) = \\mathcal{N}(w|\\mu, \\Sigma) \\]\n\n\n1.3 Find Mean of posterior\nDerivation of mean for the posterior distribution. One can recall dimensionalities of data:\n$ X ^{nd}, Y ^{n}, w ^{d} $\n\\(\\textbf{Idea:}\\) Since the prior is one-mode Gaussian distribution and likelihood is too. Then, the posterior is one-mode normal too. The mean of normal distribution correpsond to the point where the propbability is highest. By other words, it is extremum point of the probability and mean is such point where derivative of the distribution \\(p(w|X,y)\\) equals to zero.\n\\[\\mu = w^{*}: \\frac{\\partial p(w|X,y)}{\\partial w}|_{w^{*}} = 0  \\]\nSince the logarithmoc case is easier, let me move on the logarithmic expression:\n\\[ \\log p(w|X,y) \\approx \\log p(y|X,w) + \\log p(w|A)\\]\n\\[ \\log p(w|X,y) \\approx \\log(\\frac{1}{(2\\pi)^{d/2}det(B)^{1/2}})exp(...) + \\log(\\frac{1}{(2\\pi)^{d/2} \\sqrt{det(A^{-1})}}exp(\\frac{-w^{T}Aw}{2}))\\]\n\\[ \\log p(w|X,y) \\approx C_{1} - \\frac{\\beta}{2}\\sum_{i=1}^{n}(y_{i} - w^{T}x_{i})^{2} +  C_{2} - \\frac{1}{2}w^{T}Aw \\]\n\\[ \\log p(w|X,y) \\approx  -\\frac{\\beta}{2}(Y - Xw)^{T}(Y- Xw) - \\frac{w^{T}Aw}{2}\\]\n\\[ \\log p(w|X,y) \\approx \\frac{-\\beta}{2}(Y^{T}Y - Y^{T}Xw - (Xw)^{T}Y + (Xw)^{T}Xw) - \\frac{w^{T}Aw}{2}\\]\n\\[ \\log p(w|X,y) \\approx \\frac{-\\beta}{2}(-2Y^{T}Xw + w^{T}X^{T}Xw) - \\frac{w^{T}Aw}{2}\\]\n\\[ \\frac{ \\partial \\log p(w|X,y)}{\\partial w}  = 0 = \\beta Y^{T}X - \\beta w^{T}X^{T}X- w^{T}A  \\]\n\\[ w^{*} = ((\\beta X^{T}X + A)^{T})^{-1}\\beta X^{T}Y  \\]\nSince \\((A+B)^{T} = A^{T} + B^{T}\\), hence: $(X^{T}X + A)^{T} = X^{T}X + _{diag} = X^{T}X + A $\nThen: \\[ \\boxed{w^{*} = (\\beta X^{T}X + A)^{-1} \\beta X^{T}Y}\\]\n\\(\\textbf{Notes:}\\) Different prior covariance setups:\n\n\\(\\lim_{\\alpha \\to \\infty} w^{*} = 0\\)\n\\(\\lim_{\\alpha \\to 0} w^{*} = w_{ml}\\)\n\n\\(\\textbf{Notes:}\\)\nBayesian ML = Classical ML + regularization , in other words:\n\\[ || Y-  Xw ||^{2}_{2} + \\alpha||w||^{2} \\]\n\n\n1.4 Find covariance of posterior\n\\[\\frac{\\partial }{\\partial w}(\\beta Y^{T}X - \\beta w^{T}X^{T}X - w^{T}A) = 0 \\]\n\\[ -\\beta X^{T}X - A = 0 \\implies \\Sigma_{w} = \\beta X^{T}X + A  \\implies \\Sigma^{-1}_{w} = (\\beta X^{T}X + A)^{-1} \\]\n\nquestions:\n\n\nHow to choose \\(\\alpha\\)?\nSmall \\(\\alpha\\), what does it mean?\nBig \\(\\alpha\\)?"
  },
  {
    "objectID": "projects/bayesian_linear_regression.html#relevance-vector-machine-sequential-updates",
    "href": "projects/bayesian_linear_regression.html#relevance-vector-machine-sequential-updates",
    "title": "Bayesian Linear Regression",
    "section": "2. Relevance Vector Machine: Sequential updates",
    "text": "2. Relevance Vector Machine: Sequential updates\n\\(\\textbf{Idea:}\\)\nLet our data is composed of 2 data points \\((x_{1},y_{1}), (x_{2},y_{2})\\) and are independent.\n\\[ p(w|X,y) = p(w| (x_{1},y_{1}), (x_{2},y_{2}) )p(w) \\approx p((x_{2},y_{2})|w) \\underbrace{p((x_{1},y_{1})|w)p(w)}_{\\text{posterior after 1st step}} =  p((x_{2},y_{2})|w)\\underbrace{p(w|(x_{1},y_{1}))}_{\\text{new prior}}\\]\nWe write the formula of posterior when \\(n = 2\\)\n\\[ \\log p(w | (x_{1},y_{1}), (x_{2},y_{2})) \\approx -\\frac{\\beta}{2}(Y_{(1,2)} - X_{(1,2)}w)^{T}(Y_{(1,2)} - X_{(1,2)}w) - \\frac{1}{2}(w- \\mu_{1})^{T}\\Sigma^{-1}_{w} (w - \\mu_{1}) \\]\nWe make the same operations to find \\(\\textbf{mean}\\) and \\(\\textbf{covariance}\\)\n\\[ \\log p(w|(x_{1},y_{1}), (x_{2},y_{2})) \\approx - \\frac{\\beta}{2}(Y^{T}Y - 2Y^{T}Xw  + (Xw)^{T}Xw) - \\frac{1}{2}(w - \\mu_{1})^{T}\\Sigma^{-1}_{1}(w - \\mu_{1})\\]\nWe perform differentiation over \\(w\\) and it equals to zero to find mode:\n\\[ \\frac{\\partial \\log p(w|(x_{1},y_{1}), (x_{2},y_{2})) }{\\partial w} \\approx \\beta Y^{T}X - \\beta w^{T}X^{T}X - w^{T}\\Sigma^{-1}_{1} + \\mu_{1}^{T}\\Sigma^{-1}_{1} = 0 \\]\n\\[ (\\beta X^{T}X + \\Sigma^{-1}_{1})^{T} w = \\beta Y^{T}X + \\mu_{1}^{T} \\Sigma^{-1}_{1}\\]\nSince \\(\\Sigma^{-1}_{1}\\) is diaginal matrix:\n\\[ w = (\\beta X_{(1,2)}^{T}X_{(1,2)} + \\Sigma^{-1}_{1})^{-1} ( \\beta X_{(1,2)}^{T}Y_{(1,2)} +  \\Sigma^{-1}_{1}\\mu_{1}) \\]\n\nfind variance\n\\[ \\Sigma_{(1,2)} = (\\beta X^{T}_{(1,2)}X_{(1,2)} +\\Sigma_{1}^{-1})^{-1} \\]\n\n\nAlgorithm\nWe consider the following model:\n\nLikelihood model for one item from data : \\(p(y_n|x_n, w;\\beta) = \\mathcal{N}(y_n| \\textbf{w}^Tx_n, \\beta^{-1})\\)\nLikelihood model for whole data: \\(p(\\textbf{y}|X,\\textbf{w};\\beta) = \\prod\\limits_{n=1}^{N}p(y_n|x_n,\\textbf{w};\\beta) = \\mathcal{N}(\\textbf{y}|X\\textbf{w}, \\beta^{-1})\\)\nPrior: \\(p(\\textbf{w};\\alpha) = \\prod\\limits_{d=1}^{D}\\mathcal{N}(w_d|0, \\alpha_d^{-1})=\\mathcal{N}(\\textbf{w}|0,A^{-1})\\)\n\nAnd for sequential updates we come up with the following equations:\n\nObserve \\((X,\\textbf{t})^{1}\\) and obtain:\n\\[\\Sigma_{w}^{-1} = [\\beta X_{(1)}^TX_{(1)} + A]\\] \\[\\mu_{w} = \\Sigma_{w}\\beta X^T_{(1)}\\textbf{t}_{(1)}\\]\nObserve \\((X,\\textbf{t})^{2}\\) and then for joint data:\n\\[\\Sigma_{w,(1,2)}^{-1} = [\\beta X_{(2)}^T X_{(2)} + \\Sigma_{w}^{-1}]\\]\n\\[\\mu_{w,(1,2)} = \\Sigma_{w,(1,2)}(\\beta X_{(2)}^T\\textbf{t}_{(2)}+\\Sigma^{-1}_w\\mu_w)\\]\n\\(\\dots\\)\n\n\n\nTask 1\nHere, we define our model:\n\n\\(\\textit{init}\\) takes \\(\\beta\\) for likelihood model and W_pr constitutes parameters of current prior\n\\(\\textit{update_w}\\) is calculation of parameters of current posterior(future prior)\n\\(\\textit{plot_w_space}\\) makes plotting of current posterior distribution\n\n\nclass BLR:\n    def __init__(self, A, beta, x_dim=2):\n        self.w_Pr = {'mu': np.zeros(x_dim), 'cov': np.linalg.inv(A), 'inv_cov': A}\n        self.beta = beta\n    \n    \n    def update_w(self, X, y, verbose=False):\n        inv_cov = self.beta * X.T @ X + self.w_Pr['inv_cov']\n        cov = np.linalg.inv(inv_cov)\n        mu = cov @ (self.beta * X.T @ y[:,None] + self.w_Pr['inv_cov'] @ self.w_Pr['mu'][:,None])\n        \n        self.w_Pr = {'mu': mu.flatten(), 'cov': cov, 'inv_cov': inv_cov}\n        return {'mu': mu.flatten(), 'cov': cov, 'inv_cov': inv_cov} if verbose else 0.0\n    \n    \n    def plot_w_space(self, ax, b=15.):\n        xx, yy = np.mgrid[-b:b:0.01, -b:b:0.01]\n        grid = np.c_[xx.ravel(), yy.ravel()]\n        probs = mvn.logpdf(grid, mean=self.w_Pr['mu'], cov=self.w_Pr['cov'])\n        contour = ax.contourf(xx,yy,probs.reshape(xx.shape))\n    \n    def plot_data_space(self, x, y, ax):\n        a, b = (-1.1, 1.1) \n        for i in range(50): \n            w = np.random.multivariate_normal(self.w_Pr['mu'], self.w_Pr['cov'])\n            ax.plot(np.array([a,b]), np.array([w[0]+w[1]*a, w[0]+w[1]*b]), 'r-', alpha=0.15)\n            \n        ax.scatter(x[:,:,1].flatten(), y.flatten(), edgecolor=\"white\", alpha=0.75, color='blue')\n        ax.scatter(x[-1,:,1].flatten(), y[-1].flatten(), edgecolor=\"white\", alpha=1., color='green')\n        \n        \n        \n    def plot_poly_data_space(self, x, y, ax, k=2):\n        k_points = np.linspace(-1.1, 1.1, num=k+1)\n        for i in range(50):\n            w = np.random.multivariate_normal(self.w_Pr['mu'], self.w_Pr['cov'])\n            poly_values = np.zeros_like(k_points)\n            poly_values = polyval(k_points, w)\n            ax.plot(k_points, poly_values, 'r-', alpha=0.15)\n        \n        ax.scatter(x[:,:,1].flatten(), y.flatten(), edgecolor=\"white\", alpha=0.75, color='blue')\n        ax.scatter(x[-1,:,1].flatten(), y[-1].flatten(), edgecolor=\"white\", alpha=1., color='green')\n        \n     \n\n\nreg = BLR(np.diag([0.1, 0.1]), 1.)\n\n\n# make matrice X\nx1 = np.linspace(-1, 1, 10)\nnp.random.shuffle(x1)\nx1 = x1.reshape(5, 2)\nx0 = np.ones_like(x1)\nX = np.stack([x0, x1], axis=-1)\n\n# make targets y \ny = 1.5 + 12. * x1\ny += np.random.randn(*y.shape) * 3.5\n\n\nchunks = 5 # divide dataset to 5 parts (M=5)\n\n#plotting utils\nfig, ax = plt.subplots(chunks,2, figsize=(8,16))\nymin, ymax = y[:,1].min(), y[:,1].max()\nyrange = ymax - ymin\nymin, ymax = ymin - 0.1 * yrange, ymax + 0.1 * yrange\n\nfor i in range(chunks):\n    reg.update_w(X[i], y[i])\n    reg.plot_data_space(X[:i+1], y[:i+1], ax[i,0])\n    ax[i, 0].set_ylim(ymin, ymax)\n    reg.plot_w_space(ax[i,1])\n    ax[i, 0].grid(), ax[i, 1].grid() \n    ax[0,0].set_title('regression results')\n    ax[0,1].set_title('Posterior distribtuions')"
  },
  {
    "objectID": "projects/bayesian_linear_regression.html#relevance-vector-machine",
    "href": "projects/bayesian_linear_regression.html#relevance-vector-machine",
    "title": "Bayesian Linear Regression",
    "section": "3. Relevance Vector Machine",
    "text": "3. Relevance Vector Machine\n\n# see the seminar 3 (on whiteboard)"
  },
  {
    "objectID": "Jupyter_note_book.html",
    "href": "Jupyter_note_book.html",
    "title": "Breast Cancer classification Using Machine Learning Classifier",
    "section": "",
    "text": "# import libraries\nimport pandas as pd # for data manupulation or analysis\nimport numpy as np # for numeric calculation\nimport matplotlib.pyplot as plt # for data visualization\nimport seaborn as sns # for data visualization\nimport pickle #for dumping the model or we can use joblib library\n\n\n# Suppressing Warnings\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "Jupyter_note_book.html#create-dataframe",
    "href": "Jupyter_note_book.html#create-dataframe",
    "title": "Breast Cancer classification Using Machine Learning Classifier",
    "section": "Create DataFrame",
    "text": "Create DataFrame\n\n# create datafrmae\ncancer_df = pd.DataFrame(np.c_[cancer_dataset['data'],cancer_dataset['target']],\n             columns = np.append(cancer_dataset['feature_names'], ['target']))\n\n\n# DataFrame to CSV file\ncancer_df.to_csv('breast_cancer_dataframe.csv')\n\n\n# Head of cancer DataFrame\ncancer_df.head(6) \n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\ntarget\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n0.07871\n...\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n0.0\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n0.05667\n...\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n0.0\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n0.05999\n...\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n0.0\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n0.2597\n0.09744\n...\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n0.0\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n0.1809\n0.05883\n...\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n0.0\n\n\n5\n12.45\n15.70\n82.57\n477.1\n0.12780\n0.17000\n0.1578\n0.08089\n0.2087\n0.07613\n...\n23.75\n103.40\n741.6\n0.1791\n0.5249\n0.5355\n0.1741\n0.3985\n0.12440\n0.0\n\n\n\n\n6 rows √ó 31 columns\n\n\n\n\n# Tail of cancer DataFrame\ncancer_df.tail(6) \n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\ntarget\n\n\n\n\n563\n20.92\n25.09\n143.00\n1347.0\n0.10990\n0.22360\n0.31740\n0.14740\n0.2149\n0.06879\n...\n29.41\n179.10\n1819.0\n0.14070\n0.41860\n0.6599\n0.2542\n0.2929\n0.09873\n0.0\n\n\n564\n21.56\n22.39\n142.00\n1479.0\n0.11100\n0.11590\n0.24390\n0.13890\n0.1726\n0.05623\n...\n26.40\n166.10\n2027.0\n0.14100\n0.21130\n0.4107\n0.2216\n0.2060\n0.07115\n0.0\n\n\n565\n20.13\n28.25\n131.20\n1261.0\n0.09780\n0.10340\n0.14400\n0.09791\n0.1752\n0.05533\n...\n38.25\n155.00\n1731.0\n0.11660\n0.19220\n0.3215\n0.1628\n0.2572\n0.06637\n0.0\n\n\n566\n16.60\n28.08\n108.30\n858.1\n0.08455\n0.10230\n0.09251\n0.05302\n0.1590\n0.05648\n...\n34.12\n126.70\n1124.0\n0.11390\n0.30940\n0.3403\n0.1418\n0.2218\n0.07820\n0.0\n\n\n567\n20.60\n29.33\n140.10\n1265.0\n0.11780\n0.27700\n0.35140\n0.15200\n0.2397\n0.07016\n...\n39.42\n184.60\n1821.0\n0.16500\n0.86810\n0.9387\n0.2650\n0.4087\n0.12400\n0.0\n\n\n568\n7.76\n24.54\n47.92\n181.0\n0.05263\n0.04362\n0.00000\n0.00000\n0.1587\n0.05884\n...\n30.37\n59.16\n268.6\n0.08996\n0.06444\n0.0000\n0.0000\n0.2871\n0.07039\n1.0\n\n\n\n\n6 rows √ó 31 columns\n\n\n\n\n# Information of cancer Dataframe\ncancer_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 569 entries, 0 to 568\nData columns (total 31 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   mean radius              569 non-null    float64\n 1   mean texture             569 non-null    float64\n 2   mean perimeter           569 non-null    float64\n 3   mean area                569 non-null    float64\n 4   mean smoothness          569 non-null    float64\n 5   mean compactness         569 non-null    float64\n 6   mean concavity           569 non-null    float64\n 7   mean concave points      569 non-null    float64\n 8   mean symmetry            569 non-null    float64\n 9   mean fractal dimension   569 non-null    float64\n 10  radius error             569 non-null    float64\n 11  texture error            569 non-null    float64\n 12  perimeter error          569 non-null    float64\n 13  area error               569 non-null    float64\n 14  smoothness error         569 non-null    float64\n 15  compactness error        569 non-null    float64\n 16  concavity error          569 non-null    float64\n 17  concave points error     569 non-null    float64\n 18  symmetry error           569 non-null    float64\n 19  fractal dimension error  569 non-null    float64\n 20  worst radius             569 non-null    float64\n 21  worst texture            569 non-null    float64\n 22  worst perimeter          569 non-null    float64\n 23  worst area               569 non-null    float64\n 24  worst smoothness         569 non-null    float64\n 25  worst compactness        569 non-null    float64\n 26  worst concavity          569 non-null    float64\n 27  worst concave points     569 non-null    float64\n 28  worst symmetry           569 non-null    float64\n 29  worst fractal dimension  569 non-null    float64\n 30  target                   569 non-null    float64\ndtypes: float64(31)\nmemory usage: 137.9 KB\n\n\n\n# Numerical distribution of data\ncancer_df.describe() \n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\ntarget\n\n\n\n\ncount\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n...\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n\n\nmean\n14.127292\n19.289649\n91.969033\n654.889104\n0.096360\n0.104341\n0.088799\n0.048919\n0.181162\n0.062798\n...\n25.677223\n107.261213\n880.583128\n0.132369\n0.254265\n0.272188\n0.114606\n0.290076\n0.083946\n0.627417\n\n\nstd\n3.524049\n4.301036\n24.298981\n351.914129\n0.014064\n0.052813\n0.079720\n0.038803\n0.027414\n0.007060\n...\n6.146258\n33.602542\n569.356993\n0.022832\n0.157336\n0.208624\n0.065732\n0.061867\n0.018061\n0.483918\n\n\nmin\n6.981000\n9.710000\n43.790000\n143.500000\n0.052630\n0.019380\n0.000000\n0.000000\n0.106000\n0.049960\n...\n12.020000\n50.410000\n185.200000\n0.071170\n0.027290\n0.000000\n0.000000\n0.156500\n0.055040\n0.000000\n\n\n25%\n11.700000\n16.170000\n75.170000\n420.300000\n0.086370\n0.064920\n0.029560\n0.020310\n0.161900\n0.057700\n...\n21.080000\n84.110000\n515.300000\n0.116600\n0.147200\n0.114500\n0.064930\n0.250400\n0.071460\n0.000000\n\n\n50%\n13.370000\n18.840000\n86.240000\n551.100000\n0.095870\n0.092630\n0.061540\n0.033500\n0.179200\n0.061540\n...\n25.410000\n97.660000\n686.500000\n0.131300\n0.211900\n0.226700\n0.099930\n0.282200\n0.080040\n1.000000\n\n\n75%\n15.780000\n21.800000\n104.100000\n782.700000\n0.105300\n0.130400\n0.130700\n0.074000\n0.195700\n0.066120\n...\n29.720000\n125.400000\n1084.000000\n0.146000\n0.339100\n0.382900\n0.161400\n0.317900\n0.092080\n1.000000\n\n\nmax\n28.110000\n39.280000\n188.500000\n2501.000000\n0.163400\n0.345400\n0.426800\n0.201200\n0.304000\n0.097440\n...\n49.540000\n251.200000\n4254.000000\n0.222600\n1.058000\n1.252000\n0.291000\n0.663800\n0.207500\n1.000000\n\n\n\n\n8 rows √ó 31 columns\n\n\n\n\ncancer_df.isnull().sum()\n\nmean radius                0\nmean texture               0\nmean perimeter             0\nmean area                  0\nmean smoothness            0\nmean compactness           0\nmean concavity             0\nmean concave points        0\nmean symmetry              0\nmean fractal dimension     0\nradius error               0\ntexture error              0\nperimeter error            0\narea error                 0\nsmoothness error           0\ncompactness error          0\nconcavity error            0\nconcave points error       0\nsymmetry error             0\nfractal dimension error    0\nworst radius               0\nworst texture              0\nworst perimeter            0\nworst area                 0\nworst smoothness           0\nworst compactness          0\nworst concavity            0\nworst concave points       0\nworst symmetry             0\nworst fractal dimension    0\ntarget                     0\ndtype: int64"
  },
  {
    "objectID": "Jupyter_note_book.html#heatmap-of-a-correlation-matrix",
    "href": "Jupyter_note_book.html#heatmap-of-a-correlation-matrix",
    "title": "Breast Cancer classification Using Machine Learning Classifier",
    "section": "Heatmap of a correlation matrix",
    "text": "Heatmap of a correlation matrix\n\ncancer_df.corr()#gives the correlation between them\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\ntarget\n\n\n\n\nmean radius\n1.000000\n0.323782\n0.997855\n0.987357\n0.170581\n0.506124\n0.676764\n0.822529\n0.147741\n-0.311631\n...\n0.297008\n0.965137\n0.941082\n0.119616\n0.413463\n0.526911\n0.744214\n0.163953\n0.007066\n-0.730029\n\n\nmean texture\n0.323782\n1.000000\n0.329533\n0.321086\n-0.023389\n0.236702\n0.302418\n0.293464\n0.071401\n-0.076437\n...\n0.912045\n0.358040\n0.343546\n0.077503\n0.277830\n0.301025\n0.295316\n0.105008\n0.119205\n-0.415185\n\n\nmean perimeter\n0.997855\n0.329533\n1.000000\n0.986507\n0.207278\n0.556936\n0.716136\n0.850977\n0.183027\n-0.261477\n...\n0.303038\n0.970387\n0.941550\n0.150549\n0.455774\n0.563879\n0.771241\n0.189115\n0.051019\n-0.742636\n\n\nmean area\n0.987357\n0.321086\n0.986507\n1.000000\n0.177028\n0.498502\n0.685983\n0.823269\n0.151293\n-0.283110\n...\n0.287489\n0.959120\n0.959213\n0.123523\n0.390410\n0.512606\n0.722017\n0.143570\n0.003738\n-0.708984\n\n\nmean smoothness\n0.170581\n-0.023389\n0.207278\n0.177028\n1.000000\n0.659123\n0.521984\n0.553695\n0.557775\n0.584792\n...\n0.036072\n0.238853\n0.206718\n0.805324\n0.472468\n0.434926\n0.503053\n0.394309\n0.499316\n-0.358560\n\n\nmean compactness\n0.506124\n0.236702\n0.556936\n0.498502\n0.659123\n1.000000\n0.883121\n0.831135\n0.602641\n0.565369\n...\n0.248133\n0.590210\n0.509604\n0.565541\n0.865809\n0.816275\n0.815573\n0.510223\n0.687382\n-0.596534\n\n\nmean concavity\n0.676764\n0.302418\n0.716136\n0.685983\n0.521984\n0.883121\n1.000000\n0.921391\n0.500667\n0.336783\n...\n0.299879\n0.729565\n0.675987\n0.448822\n0.754968\n0.884103\n0.861323\n0.409464\n0.514930\n-0.696360\n\n\nmean concave points\n0.822529\n0.293464\n0.850977\n0.823269\n0.553695\n0.831135\n0.921391\n1.000000\n0.462497\n0.166917\n...\n0.292752\n0.855923\n0.809630\n0.452753\n0.667454\n0.752399\n0.910155\n0.375744\n0.368661\n-0.776614\n\n\nmean symmetry\n0.147741\n0.071401\n0.183027\n0.151293\n0.557775\n0.602641\n0.500667\n0.462497\n1.000000\n0.479921\n...\n0.090651\n0.219169\n0.177193\n0.426675\n0.473200\n0.433721\n0.430297\n0.699826\n0.438413\n-0.330499\n\n\nmean fractal dimension\n-0.311631\n-0.076437\n-0.261477\n-0.283110\n0.584792\n0.565369\n0.336783\n0.166917\n0.479921\n1.000000\n...\n-0.051269\n-0.205151\n-0.231854\n0.504942\n0.458798\n0.346234\n0.175325\n0.334019\n0.767297\n0.012838\n\n\nradius error\n0.679090\n0.275869\n0.691765\n0.732562\n0.301467\n0.497473\n0.631925\n0.698050\n0.303379\n0.000111\n...\n0.194799\n0.719684\n0.751548\n0.141919\n0.287103\n0.380585\n0.531062\n0.094543\n0.049559\n-0.567134\n\n\ntexture error\n-0.097317\n0.386358\n-0.086761\n-0.066280\n0.068406\n0.046205\n0.076218\n0.021480\n0.128053\n0.164174\n...\n0.409003\n-0.102242\n-0.083195\n-0.073658\n-0.092439\n-0.068956\n-0.119638\n-0.128215\n-0.045655\n0.008303\n\n\nperimeter error\n0.674172\n0.281673\n0.693135\n0.726628\n0.296092\n0.548905\n0.660391\n0.710650\n0.313893\n0.039830\n...\n0.200371\n0.721031\n0.730713\n0.130054\n0.341919\n0.418899\n0.554897\n0.109930\n0.085433\n-0.556141\n\n\narea error\n0.735864\n0.259845\n0.744983\n0.800086\n0.246552\n0.455653\n0.617427\n0.690299\n0.223970\n-0.090170\n...\n0.196497\n0.761213\n0.811408\n0.125389\n0.283257\n0.385100\n0.538166\n0.074126\n0.017539\n-0.548236\n\n\nsmoothness error\n-0.222600\n0.006614\n-0.202694\n-0.166777\n0.332375\n0.135299\n0.098564\n0.027653\n0.187321\n0.401964\n...\n-0.074743\n-0.217304\n-0.182195\n0.314457\n-0.055558\n-0.058298\n-0.102007\n-0.107342\n0.101480\n0.067016\n\n\ncompactness error\n0.206000\n0.191975\n0.250744\n0.212583\n0.318943\n0.738722\n0.670279\n0.490424\n0.421659\n0.559837\n...\n0.143003\n0.260516\n0.199371\n0.227394\n0.678780\n0.639147\n0.483208\n0.277878\n0.590973\n-0.292999\n\n\nconcavity error\n0.194204\n0.143293\n0.228082\n0.207660\n0.248396\n0.570517\n0.691270\n0.439167\n0.342627\n0.446630\n...\n0.100241\n0.226680\n0.188353\n0.168481\n0.484858\n0.662564\n0.440472\n0.197788\n0.439329\n-0.253730\n\n\nconcave points error\n0.376169\n0.163851\n0.407217\n0.372320\n0.380676\n0.642262\n0.683260\n0.615634\n0.393298\n0.341198\n...\n0.086741\n0.394999\n0.342271\n0.215351\n0.452888\n0.549592\n0.602450\n0.143116\n0.310655\n-0.408042\n\n\nsymmetry error\n-0.104321\n0.009127\n-0.081629\n-0.072497\n0.200774\n0.229977\n0.178009\n0.095351\n0.449137\n0.345007\n...\n-0.077473\n-0.103753\n-0.110343\n-0.012662\n0.060255\n0.037119\n-0.030413\n0.389402\n0.078079\n0.006522\n\n\nfractal dimension error\n-0.042641\n0.054458\n-0.005523\n-0.019887\n0.283607\n0.507318\n0.449301\n0.257584\n0.331786\n0.688132\n...\n-0.003195\n-0.001000\n-0.022736\n0.170568\n0.390159\n0.379975\n0.215204\n0.111094\n0.591328\n-0.077972\n\n\nworst radius\n0.969539\n0.352573\n0.969476\n0.962746\n0.213120\n0.535315\n0.688236\n0.830318\n0.185728\n-0.253691\n...\n0.359921\n0.993708\n0.984015\n0.216574\n0.475820\n0.573975\n0.787424\n0.243529\n0.093492\n-0.776454\n\n\nworst texture\n0.297008\n0.912045\n0.303038\n0.287489\n0.036072\n0.248133\n0.299879\n0.292752\n0.090651\n-0.051269\n...\n1.000000\n0.365098\n0.345842\n0.225429\n0.360832\n0.368366\n0.359755\n0.233027\n0.219122\n-0.456903\n\n\nworst perimeter\n0.965137\n0.358040\n0.970387\n0.959120\n0.238853\n0.590210\n0.729565\n0.855923\n0.219169\n-0.205151\n...\n0.365098\n1.000000\n0.977578\n0.236775\n0.529408\n0.618344\n0.816322\n0.269493\n0.138957\n-0.782914\n\n\nworst area\n0.941082\n0.343546\n0.941550\n0.959213\n0.206718\n0.509604\n0.675987\n0.809630\n0.177193\n-0.231854\n...\n0.345842\n0.977578\n1.000000\n0.209145\n0.438296\n0.543331\n0.747419\n0.209146\n0.079647\n-0.733825\n\n\nworst smoothness\n0.119616\n0.077503\n0.150549\n0.123523\n0.805324\n0.565541\n0.448822\n0.452753\n0.426675\n0.504942\n...\n0.225429\n0.236775\n0.209145\n1.000000\n0.568187\n0.518523\n0.547691\n0.493838\n0.617624\n-0.421465\n\n\nworst compactness\n0.413463\n0.277830\n0.455774\n0.390410\n0.472468\n0.865809\n0.754968\n0.667454\n0.473200\n0.458798\n...\n0.360832\n0.529408\n0.438296\n0.568187\n1.000000\n0.892261\n0.801080\n0.614441\n0.810455\n-0.590998\n\n\nworst concavity\n0.526911\n0.301025\n0.563879\n0.512606\n0.434926\n0.816275\n0.884103\n0.752399\n0.433721\n0.346234\n...\n0.368366\n0.618344\n0.543331\n0.518523\n0.892261\n1.000000\n0.855434\n0.532520\n0.686511\n-0.659610\n\n\nworst concave points\n0.744214\n0.295316\n0.771241\n0.722017\n0.503053\n0.815573\n0.861323\n0.910155\n0.430297\n0.175325\n...\n0.359755\n0.816322\n0.747419\n0.547691\n0.801080\n0.855434\n1.000000\n0.502528\n0.511114\n-0.793566\n\n\nworst symmetry\n0.163953\n0.105008\n0.189115\n0.143570\n0.394309\n0.510223\n0.409464\n0.375744\n0.699826\n0.334019\n...\n0.233027\n0.269493\n0.209146\n0.493838\n0.614441\n0.532520\n0.502528\n1.000000\n0.537848\n-0.416294\n\n\nworst fractal dimension\n0.007066\n0.119205\n0.051019\n0.003738\n0.499316\n0.687382\n0.514930\n0.368661\n0.438413\n0.767297\n...\n0.219122\n0.138957\n0.079647\n0.617624\n0.810455\n0.686511\n0.511114\n0.537848\n1.000000\n-0.323872\n\n\ntarget\n-0.730029\n-0.415185\n-0.742636\n-0.708984\n-0.358560\n-0.596534\n-0.696360\n-0.776614\n-0.330499\n0.012838\n...\n-0.456903\n-0.782914\n-0.733825\n-0.421465\n-0.590998\n-0.659610\n-0.793566\n-0.416294\n-0.323872\n1.000000\n\n\n\n\n31 rows √ó 31 columns"
  },
  {
    "objectID": "Jupyter_note_book.html#suppor-vector-classifier",
    "href": "Jupyter_note_book.html#suppor-vector-classifier",
    "title": "Breast Cancer classification Using Machine Learning Classifier",
    "section": "Suppor vector Classifier",
    "text": "Suppor vector Classifier\n\n# Support vector classifier\nfrom sklearn.svm import SVC\nsvc_classifier = SVC()\nsvc_classifier.fit(X_train, y_train)\ny_pred_scv = svc_classifier.predict(X_test)\naccuracy_score(y_test, y_pred_scv)\n\n0.9385964912280702\n\n\n\nTrain with Standard scaled Data\n\n# Train with Standard scaled Data\nsvc_classifier2 = SVC()\nsvc_classifier2.fit(X_train_sc, y_train)\ny_pred_svc_sc = svc_classifier2.predict(X_test_sc)\naccuracy_score(y_test, y_pred_svc_sc)\n\n0.9649122807017544"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Scholarship interests\nMy scholarship interests lie at the intersection of data science and biomedical engineering."
  }
]