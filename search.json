[
  {
    "objectID": "Hobbies.html",
    "href": "Hobbies.html",
    "title": "Addisu Amare",
    "section": "",
    "text": "Hobby\nCategory\nFrequency\nEquipment Needed\n\n\n\n\nCooking\nCreative\nDaily\nKitchen utensils, ingredients\n\n\nReading Novels\nIntellectual\nDaily\nBooks/e-reader\n\n\nFootball\nOutdoor Sport\nWeekly\nCleats, ball, sports gear\n\n\nVolleyball\nTeam Sport\nBi-weekly\nKnee pads, athletic shoes\n\n\n\n\n\n\nCooking - Creative expression and practical life skill\nReading - Mental escape and knowledge expansion\n\nFootball - Physical fitness and teamwork\nVolleyball - Social interaction and coordination"
  },
  {
    "objectID": "Hobbies.html#my-hobbies-at-a-glance",
    "href": "Hobbies.html#my-hobbies-at-a-glance",
    "title": "Addisu Amare",
    "section": "",
    "text": "Hobby\nCategory\nFrequency\nEquipment Needed\n\n\n\n\nCooking\nCreative\nDaily\nKitchen utensils, ingredients\n\n\nReading Novels\nIntellectual\nDaily\nBooks/e-reader\n\n\nFootball\nOutdoor Sport\nWeekly\nCleats, ball, sports gear\n\n\nVolleyball\nTeam Sport\nBi-weekly\nKnee pads, athletic shoes\n\n\n\n\n\n\nCooking - Creative expression and practical life skill\nReading - Mental escape and knowledge expansion\n\nFootball - Physical fitness and teamwork\nVolleyball - Social interaction and coordination"
  },
  {
    "objectID": "documents.html",
    "href": "documents.html",
    "title": "Articles & Projects",
    "section": "",
    "text": "Artificial Intelligence applications in medical imaging analysis.\n\n\nView PDF Download PDF\n\n\n\n\n\n\nFederated learning approaches with explainable AI for medical imaging.\n\n\nView PDF Download PDF\n\n\n\n\n\n\n\n\nAcademic projects and research from undergraduate studies.\n\n\nView PDF Download PDF\n\n\n\n\n\n\nComputer vision and AI-based fire detection system.\n\n\nView PDF Download PDF\n\n\n\n\n\n\n\nOption 1: View in Browser - Click ‚ÄúView PDF‚Äù to open directly in your browser\n\n\nOption 2: Download - Click ‚ÄúDownload PDF‚Äù to save to your device\n\n\nOption 3: GitHub View - All files are accessible via GitHub repository\n\n\n\nNote: All PDFs are hosted on GitHub. For best viewing experience, use Chrome, Firefox, or Edge browsers."
  },
  {
    "objectID": "documents.html#articles",
    "href": "documents.html#articles",
    "title": "Articles & Projects",
    "section": "",
    "text": "Artificial Intelligence applications in medical imaging analysis.\n\n\nView PDF Download PDF\n\n\n\n\n\n\nFederated learning approaches with explainable AI for medical imaging.\n\n\nView PDF Download PDF\n\n\n\n\n\n\n\n\nAcademic projects and research from undergraduate studies.\n\n\nView PDF Download PDF\n\n\n\n\n\n\nComputer vision and AI-based fire detection system.\n\n\nView PDF Download PDF\n\n\n\n\n\n\n\nOption 1: View in Browser - Click ‚ÄúView PDF‚Äù to open directly in your browser\n\n\nOption 2: Download - Click ‚ÄúDownload PDF‚Äù to save to your device\n\n\nOption 3: GitHub View - All files are accessible via GitHub repository\n\n\n\nNote: All PDFs are hosted on GitHub. For best viewing experience, use Chrome, Firefox, or Edge browsers."
  },
  {
    "objectID": "documents.html#projects",
    "href": "documents.html#projects",
    "title": "Articles & Projects",
    "section": "",
    "text": "Academic projects and research from undergraduate studies.\n\n\nView PDF Download PDF\n\n\n\n\n\n\nComputer vision and AI-based fire detection system.\n\n\nView PDF Download PDF\n\n\n\n\n\n\n\nOption 1: View in Browser - Click ‚ÄúView PDF‚Äù to open directly in your browser\n\n\nOption 2: Download - Click ‚ÄúDownload PDF‚Äù to save to your device\n\n\nOption 3: GitHub View - All files are accessible via GitHub repository\n\n\n\nNote: All PDFs are hosted on GitHub. For best viewing experience, use Chrome, Firefox, or Edge browsers."
  },
  {
    "objectID": "documents.html#how-to-view",
    "href": "documents.html#how-to-view",
    "title": "Articles & Projects",
    "section": "",
    "text": "Option 1: View in Browser - Click ‚ÄúView PDF‚Äù to open directly in your browser\n\n\nOption 2: Download - Click ‚ÄúDownload PDF‚Äù to save to your device\n\n\nOption 3: GitHub View - All files are accessible via GitHub repository\n\n\n\nNote: All PDFs are hosted on GitHub. For best viewing experience, use Chrome, Firefox, or Edge browsers."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Introduction: The AI Revolution in Healthcare",
    "section": "",
    "text": "The AI Revolution in Healthcare\n\n\n\nThe Future of Medical Imaging\nMedical imaging is evolving. Artificial intelligence is no longer a simple tool; in fact, it has become part and parcel of diagnostic workflows. There is a growing need for intelligent processing of medical images, which represent approximately 90% of all healthcare data.\n\n\nThe Problem\nRadiologists: Are experiencing a 30-40% increase in workload each year\nImaging technology: High-resolution, 3D and 4D images are becoming more common\nDiagnostic Accuracy: Error rates due to human interpretation can be as high as 30% in some instances\n\n\nHow AI solves these problems\nTwo significant advances have been made with respect to using AI in medical imaging - Convolutional Neural Networks (CNN) have become the standard when it comes to the majority of medical imaging techniques and algorithms.\n\n\n\nüéØ 90%\n\n\nHealthcare data is medical images\n\n\n\n\n‚è±Ô∏è 70%\n\n\nReduction in screening time\n\n\n\n\nüí∞ $4.5B\n\n\nMarket size by 2028\n\n\n\n\nüìà 34%\n\n\nAnnual growth rate\n\n\n\n\n\nWhat You‚Äôll Learn in This Series\n-This comprehensive series explores:\n-CNN applications across various medical specialties\nViT breakthroughs in complex imaging tasks\nReal-world implementations and case studies\nPerformance comparisons and benchmarks\nFuture trends and ethical considerations\nNavigation Guide Next: CNN Overview\nOr jump to: Cancer Detection | Neurology | Ophthalmology"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Below is the course I have taught at the University of Skoltech,Russia during independent period.\n\n\n\nI30202437 Python for Data Science Pseudo-code. Program design and structure. Flow control. Iteration. Lists (arrays). Functions. File I/O. Classes, objects, methods, and libraries."
  },
  {
    "objectID": "teaching.html#courses-taught",
    "href": "teaching.html#courses-taught",
    "title": "Teaching",
    "section": "",
    "text": "Below is the course I have taught at the University of Skoltech,Russia during independent period.\n\n\n\nI30202437 Python for Data Science Pseudo-code. Program design and structure. Flow control. Iteration. Lists (arrays). Functions. File I/O. Classes, objects, methods, and libraries."
  },
  {
    "objectID": "teaching.html#teaching-philosophy",
    "href": "teaching.html#teaching-philosophy",
    "title": "Teaching",
    "section": "Teaching Philosophy",
    "text": "Teaching Philosophy\nI‚Äôve learned that great teaching isn‚Äôt just about delivering content‚Äîit‚Äôs about creating the right conditions for learning to happen. My approach rests on three core pillars: sparking genuine motivation, building a supportive community, and staying flexibly responsive to my students‚Äô needs. When I get these right, I see curiosity catch fire, collaboration flourish, and obstacles to learning start to fall away"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Addisu A. Zena, Btech",
    "section": "",
    "text": "Bio\nAddisu is a technology and biomedical enthusiast who holds a Bachelor‚Äôs degree in Electronics & Communication Engineering (ECE) and Biomedical Engineering from Vellore Institute of Technology (Class of 2022).\nWith a deep interest in programming, data science, and human anatomy and physiology, he continuously works on projects at the intersection of engineering, machine learning, and biomedical research. He is always expanding his skill set through hands-on learning and new technologies.\n###Skills: - Python | MATLAB | LabVIEW\n- Data Science | Machine Learning\n\n\nContact\n\nemail: 0941813057estifanos@gmail.com\nLinkedIn: profile -tiktok:@tech_guy47"
  },
  {
    "objectID": "projects/SemanticSegmentation.html",
    "href": "projects/SemanticSegmentation.html",
    "title": "Semantic Segmentation",
    "section": "",
    "text": "img"
  },
  {
    "objectID": "projects/SemanticSegmentation.html#unet-architecture",
    "href": "projects/SemanticSegmentation.html#unet-architecture",
    "title": "Semantic Segmentation",
    "section": "Unet architecture",
    "text": "Unet architecture\n\n\n\nimg\n\n\n\nimport torch\nimport torchvision\nfrom torchvision import transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport gdown\n\n\ngdown.download(url=\"https://drive.google.com/file/d/1kiR9pPP3gIqoyT9bWUnA0rgZuRTiUv7q/view?usp=share_link\", output=\"./unet_dict.pt\", quiet=False, fuzzy=True)\n\n\nclass Encoder_Block(torch.nn.Module):\n  def __init__(self,inp_channels,out_channels):\n    super().__init__()\n    self.model = torch.nn.Sequential(\n        #implement encoder block\n    )\n    self.pooling = # implement pooling\n  def forward(self,x):\n    #compute intermediate output (before pooling)\n    return # return intermediate and after pooling outputs\n\nclass Decoder_Block(torch.nn.Module):\n  def __init__(self,inp_channels,out_channels):\n    super().__init__()\n    self.upsample = # implement upsampling using transposed convolution\n    self.model = torch.nn.Sequential(\n        #implement decoder block\n    )\n  def forward(self,x,enc_x):\n    x = #make upsampling\n    x = #concatenate upscaled input with encoder block output (enc_x)\n    return self.model(x)"
  },
  {
    "objectID": "projects/SemanticSegmentation.html#check-that-your-implementation-works-correctly",
    "href": "projects/SemanticSegmentation.html#check-that-your-implementation-works-correctly",
    "title": "Semantic Segmentation",
    "section": "Check that your implementation works correctly",
    "text": "Check that your implementation works correctly\n\nunet = Unet(3,11)\n\nassert unet(torch.randn(1,3,128,128)).shape == (1,11,128,128), \"check your implementation\""
  },
  {
    "objectID": "projects/SemanticSegmentation.html#lets-start-working-with-data",
    "href": "projects/SemanticSegmentation.html#lets-start-working-with-data",
    "title": "Semantic Segmentation",
    "section": "Let‚Äôs start working with data",
    "text": "Let‚Äôs start working with data\n\ntransform = transforms.Compose([\n    transforms.Resize(128),\n    transforms.CenterCrop(128),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=0.5,std=0.5)\n])\ndef class_split(data,n=3):\n  data = np.array(data)\n  res = []\n  for i in range(1,1+n):\n    mask = np.zeros_like(data)\n    mask[data==i] = 1.\n    res.append(mask[None])\n  return torch.from_numpy(np.concatenate(res,axis=0)).to(torch.float)\n\ntarget_transform = transforms.Compose([\n    transforms.Resize(128),\n    transforms.CenterCrop(128),\n    class_split\n])\ndataset = torchvision.datasets.OxfordIIITPet(\"./data\",split=\"trainval\",target_types=\"segmentation\",download=True,transform=transform,target_transform=target_transform)\n\nDownloading https://thor.robots.ox.ac.uk/datasets/pets/images.tar.gz to data/oxford-iiit-pet/images.tar.gz\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 791918971/791918971 [00:37&lt;00:00, 21168795.70it/s]\n\n\nExtracting data/oxford-iiit-pet/images.tar.gz to data/oxford-iiit-pet\nDownloading https://thor.robots.ox.ac.uk/datasets/pets/annotations.tar.gz to data/oxford-iiit-pet/annotations.tar.gz\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19173078/19173078 [00:01&lt;00:00, 10020703.36it/s]\n\n\nExtracting data/oxford-iiit-pet/annotations.tar.gz to data/oxford-iiit-pet\n\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nunet = Unet(3,3).to(device)\ndataloader = torch.utils.data.DataLoader(dataset,batch_size=32,shuffle=True)\noptimizer = torch.optim.Adam(unet.parameters(),lr=0.001)\n\n\ndef train(model,dataloader,optimizer,loss_func=torch.nn.CrossEntropyLoss(),epochs=5):\n  for i in range(epochs):\n    for x,y in dataloader:\n      #implement standard training stuff\n\n\ntrain(unet,dataloader,optimizer)\n\n\niou = []\nfor x,y in tqdm(dataloader):\n  #implement evaluation of iou by directly computing intersection and union between class masks generated by model and ground truth\n\n\nround(np.mean(iou)*100,1)\n\n\n!wget -nv \"https://fikiwiki.com/uploads/posts/2022-02/1644990866_45-fikiwiki-com-p-prikolnie-kartinki-pro-zhivotnikh-47.png\"\n\n2023-04-03 23:15:50 URL:https://fikiwiki.com/uploads/posts/2022-02/1644990866_45-fikiwiki-com-p-prikolnie-kartinki-pro-zhivotnikh-47.png [1205459/1205459] -&gt; \"1644990866_45-fikiwiki-com-p-prikolnie-kartinki-pro-zhivotnikh-47.png\" [1]\n\n\n\nfrom PIL import Image\nimg = Image.open(\"/content/1644990866_45-fikiwiki-com-p-prikolnie-kartinki-pro-zhivotnikh-47.png\")\n\n\nimg = transform(img)[:3][None].to(device)\nmask = unet(img)[0]\n\n\ntransforms.ToPILImage()(torch.nn.Softmax(dim=0)(mask)[0:1].detach().cpu())\n\n\n\n\n\n\n\n\n\ntransforms.ToPILImage()(img[0].detach().cpu()/2+0.5)"
  },
  {
    "objectID": "projects/Regularization.html",
    "href": "projects/Regularization.html",
    "title": "SOME AUXILARY FUNCTIONS",
    "section": "",
    "text": "!nvidia-smi\n\nMon Apr  1 09:07:58 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n| N/A   56C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\nimport numpy as np\nimport os\nimport time\n\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision import transforms\n\nfrom IPython.display import clear_output\nfrom matplotlib import pyplot as plt\nfrom matplotlib.pyplot import figure\nimport plotly.graph_objects as go\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nprint('Your device is: {}'.format(device))\n\ninput_shape = 28*28\nnum_classes = 10\ninput_channels = 1\n\ndef plot_train_process(train_loss, val_loss, val_accuracy):\n    clear_output(True)\n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n\n    axes[0].set_title('Loss')\n    axes[0].plot(train_loss, label='train')\n    axes[0].plot(val_loss, label='validation')\n    axes[0].legend()\n\n    axes[1].set_title('Validation accuracy')\n    axes[1].plot(val_accuracy)\n    plt.show()\n\nYour device is: cuda:0\nout_dict = dict()\n\n# Technical function\ndef mkdir(path):\n    if not os.path.exists(root_path):\n        os.mkdir(root_path)\n        print('Directory', path, 'is created!')\n    else:\n        print('Directory', path, 'already exists!')\n\nroot_path = 'fmnist_data'\nmkdir(root_path)\n\ndownload = True\ntrain_transform = transforms.Compose([\n\n     transforms.ToTensor(),\n ])\n\n\ntest_transform = transforms.Compose([\n                                       transforms.ToTensor()])\n\n\nfmnist_dataset_train = torchvision.datasets.FashionMNIST(root_path,\n                                                        train=True,\n                                                        transform=train_transform,\n                                                        target_transform=None,\n                                                        download=download)\nfmnist_dataset_test = torchvision.datasets.FashionMNIST(root_path,\n                                                       train=False,\n                                                       transform=test_transform,\n                                                       target_transform=None,\n                                                       download=download)\n\n\nDirectory fmnist_data is created!\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to fmnist_data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26421880/26421880 [00:02&lt;00:00, 12409431.38it/s]\n\n\nExtracting fmnist_data/FashionMNIST/raw/train-images-idx3-ubyte.gz to fmnist_data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to fmnist_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29515/29515 [00:00&lt;00:00, 212388.50it/s]\n\n\nExtracting fmnist_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to fmnist_data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to fmnist_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4422102/4422102 [00:01&lt;00:00, 3866219.72it/s]\n\n\nExtracting fmnist_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to fmnist_data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to fmnist_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5148/5148 [00:00&lt;00:00, 4893988.44it/s]\n\n\nExtracting fmnist_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to fmnist_data/FashionMNIST/raw\ndef collect_parameters(model):\n  model.eval()\n  w1 = []\n  w2=  []\n  params = []\n  for param in model.parameters():\n    torch.manual_seed(1)\n    w_ = torch.rand_like (param)\n    w1.append(w_)\n    torch.manual_seed(2)\n    w_2 = torch.rand_like (param)\n    w2.append(w_2)\n    params.append(param.data)\n  return w1, w2, params\ndef draw_loss_landscape(model, params, w1, w2, train_loader):\n  model.eval()\n  losses3d = []\n  for alpha in alphas:\n    loss2d = []\n    for beta in betas:\n      losses_ = []\n\n      for i, param in enumerate(model.parameters()):\n        param.data = params[i] + alpha * w2[i] + beta * w1[i]\n\n      for X_batch, y_batch in train_loader:\n\n\n\n        X_batch = X_batch.to(device)\n        y_batch = y_batch.to(device)\n        predictions = model(X_batch)\n        loss =loss_func(predictions, y_batch)\n        losses_.append(loss.item())\n\n        break\n\n\n      loss2d.append(np.mean(losses_))\n    losses3d.append(loss2d)\n\n  return losses3d"
  },
  {
    "objectID": "projects/Regularization.html#where-to-insert-batch-norm",
    "href": "projects/Regularization.html#where-to-insert-batch-norm",
    "title": "SOME AUXILARY FUNCTIONS",
    "section": "Where to insert Batch Norm?",
    "text": "Where to insert Batch Norm?\n\nAnywhere, but authers offer to insert it before non-linearity\n\n\n\nimage.png\n\n\nBut when we have skip connections it is better to use BN before the layer\n\n\nFill in the model with dropout\n\nmodel_bn = nn.Sequential(\n    nn.Flatten(),\n\n    #insert your code with BatchNorm (BatchNorm1d)\n    nn.Linear(input_shape, ...),\n\n    nn.ReLU(),\n    ...\n\n    nn.Linear( ..., num_classes),\n\n).to(device)\n\nopt_bn = torch.optim.Adam(model_bn.parameters(), lr=3e-4)\nloss_func_bn = nn.CrossEntropyLoss()\n\n\nn_epochs_1 = 20\n\ntrain_loss_bn, val_loss_bn, val_accuracy_bn = train_model(\n    model_bn,\n    train_loader,\n    test_loader,\n    loss_func_bn,\n    opt_bn,\n    n_epochs_1\n)\n\n\n\n\n\n\n\n\n\n\nAs over data and model are simple, we cannot see the expecting effect of BN\n\n\nLETS NOW BUILD THE 3D PROJECTION OF A LOSS LANDSCAPE\n\nw1, w2, params = collect_parameters(model_bn)\n\n\nalphas = torch.linspace(-1, 1, 40)\nbetas  = torch.linspace(-1, 1, 40)\n\n\n\nA ,B  = np.meshgrid(alphas, betas)\n\n\nlosses3d = draw_loss_landscape(model_bn, params, w1, w2,train_loader)\n\nfig = go.Figure(data=[go.Surface(x = A,y=B, z=np.array(losses3d),)])\n\nfig.update_layout(title='Loss Landscape Projection', autosize=False,\n                width=500, height=500,\n                margin=dict(l=65, r=50, b=65, t=90))\nfig.show()\n\n\n\n\n                                \n                                            \n\n\n\n\nAs we can see projection of the loss landscape become a little smoother"
  },
  {
    "objectID": "projects/Regularization.html#dropout",
    "href": "projects/Regularization.html#dropout",
    "title": "SOME AUXILARY FUNCTIONS",
    "section": "DROPOUT",
    "text": "DROPOUT\n\n\n\nScreenshot 2023-10-29 at 18.03.46.png\n\n\n\n\n\nScreenshot 2023-10-29 at 17.12.24.png\n\n\n\np - probability to turn off nueron\n\n\nLarge models are proned to overfitting\nWith unlimited computational resources, the most effective method to ‚Äúregularize‚Äù a fixed-sized model is to average the predictions of all possible parameter settings, weighting each setting based on its posterior probability given the training data.\nTraining numerous architectures is challenging due to the complexity of finding optimal hyperparameters for each one and the substantial computational resources required to train large networks. Additionally, large networks typically necessitate abundant training data, which may not always be available to train various networks on different data subsets. Even if multiple large networks could be trained, using them all at test time is impractical in scenarios where rapid response is crucial.\nDropout, a technique that addresses these challenges, prevents overfitting and offers an efficient way to combine a vast number of neural network architectures.\n\n\nDropout acts differently on training and validation phases.\n\n\n\nimage.png\n\n\n\n\nFill in the model with dropout\n\nmodel_do = nn.Sequential(\n    nn.Flatten(),\n    # insert the dropout regularization into the net\n    nn.Linear(input_shape, ...),\n    ...\n    nn.ReLU(),\n    ....\n\n    nn.Linear( ..., num_classes),\n\n).to(device)\n\nopt_do = torch.optim.Adam(model_do.parameters(), lr=3e-4)\nloss_func_do = nn.CrossEntropyLoss()\n\n\nn_epochs_1 = 20\n\ntrain_loss_do, val_loss_do, val_accuracy_do = train_model(\n    model_do,\n    train_loader,\n    test_loader,\n    loss_func_do,\n    opt_do,\n    n_epochs_1\n)"
  },
  {
    "objectID": "projects/Regularization.html#l1-regularization",
    "href": "projects/Regularization.html#l1-regularization",
    "title": "SOME AUXILARY FUNCTIONS",
    "section": "L1 REGULARIZATION",
    "text": "L1 REGULARIZATION\n\nL1 regularization in NN has the same effect as in ML models (like logistic regression). It is prone to sparsify the tensor\n\n\nFill in the perceptron model\n\nclass MLP_model(nn.Module):\n  '''\n    Multilayer Perceptron.\n  '''\n  def __init__(self, input_shape):\n    super().__init__()\n    self.layers = nn.Sequential(\n      nn.Flatten(),\n      nn.Linear(input_shape, input_shape//2),\n      nn.ReLU(),\n      nn.Linear(input_shape//2, input_shape//4),\n      nn.ReLU(),\n      nn.Linear( input_shape//4, 10)\n    )\n\n\n\n\n\n  def forward(self, x):\n    '''Forward pass'''\n    return self.layers(x)\n\n  def compute_l2_loss(self, w):\n      # insert your code\n      return ...\n\n  def compute_l1_loss(self, w):\n      # insert your code\n      return ...\n\n\n\nFill in the train function\n\n\ndef train_reg(model, train_loader, val_loader, loss_fn, opt, reg,n_epochs: int, build_plot=True):\n    train_loss = []\n    val_loss = []\n    val_accuracy = []\n\n\n    for epoch in range(n_epochs):\n        ep_train_loss = []\n        ep_val_loss = []\n        ep_val_accuracy = []\n        start_time = time.time()\n\n        model.train(True)\n        for X_batch, y_batch in train_loader:\n          #zero grad\n            opt.zero_grad()\n\n            # to device\n            X_batch = X_batch.to(device)\n            y_batch = y_batch.to(device)\n\n            predictions = model(X_batch)\n            loss =loss_func(predictions, y_batch)\n\n            if reg == 'l1':\n              l1_weight = 0.001\n              l1_parameters = []\n              for parameter in model.parameters():\n                  # insert your code\n                  l1_parameters.append(..)\n              l1 = ...\n              loss += l1.to(device)\n\n            if reg == 'l2':\n              l2_weight = 0.001\n              l2_parameters = []\n              for parameter in model.parameters():\n                   # insert your code\n                  l2_parameters.append()\n              l2 = ...\n              loss += l2\n            #back step\n            loss.backward()\n            # optimizer step\n            opt.step()\n\n            ep_train_loss.append(loss.item())\n\n        model.train(False)\n        model.eval()\n        with torch.no_grad():\n            for X_batch, y_batch in val_loader:\n\n                X_batch = X_batch.to(device)\n                y_batch = y_batch.to(device)\n\n                predictions = model(X_batch)\n                loss = loss_func(predictions, y_batch)\n\n                ep_val_loss.append(loss.item())\n                y_pred = predictions.max(1)[1].data\n                ep_val_accuracy.append(np.mean((y_batch.cpu() == y_pred.cpu()).numpy()))\n\n        train_loss.append(np.mean(ep_train_loss))\n        val_loss.append(np.mean(ep_val_loss))\n        val_accuracy.append(np.mean(ep_val_accuracy))\n\n        if build_plot:\n            plot_train_process(train_loss, val_loss, val_accuracy)\n    return train_loss, val_loss, val_accuracy\n# __________end of block__________\n\n\nmodel_l1 = MLP_model(input_shape).to(device)\n\nopt_l1 = torch.optim.Adam(model_l1.parameters(), lr=3e-4)\nloss_func_l1 = nn.CrossEntropyLoss()\n\n\nn_epochs_1 = 20\n\ntrain_loss_l1, val_loss_l1, val_accuracy_l1 = train_reg(\n    model_l1,\n    train_loader,\n    test_loader,\n    loss_func_l1,\n    opt_l1,\n    'l1',\n    n_epochs_1,\n\n)\n\n\n\n\n\n\n\n\nLet‚Äôs draw the first layer weights of a model trained with dropout\n\n\nplt.spy(model_do[1].weight.detach().cpu().numpy(), precision=1e-4)\n\n\n\n\n\n\n\n\nAnd here we draw the first layer weights of a model trained with l1\n\nplt.spy(model_l1.layers[1].weight.detach().cpu().numpy(), precision=1e-4)\n\n\n\n\n\n\n\n\nl1 works!"
  },
  {
    "objectID": "projects/Regularization.html#why-is-it-working-like-that",
    "href": "projects/Regularization.html#why-is-it-working-like-that",
    "title": "SOME AUXILARY FUNCTIONS",
    "section": "Why is it working like that??",
    "text": "Why is it working like that??\nImagine you have vector \\(\\vec{x}=(1, \\epsilon)\\) where \\(\\epsilon &gt;0\\) is small. \\(l1\\) and \\(l2\\) norms are given like that:\n\\[\n\\|\\vec{x}\\|^2_2 = 1+\\epsilon^2\n\\]\n\\[\n\\|\\vec{x}\\|_1 = 1+\\epsilon\n\\]\nSuppose, we update our vector with the vector of \\(v = (\\delta, 0)\\)\nLEt‚Äôs look at norms of vector \\(x-v\\)\n\\[\n\\|x-v\\|^2_2 = 1-2\\delta + \\delta^2 + \\epsilon^2\n\\]\n\\[\n\\|x-v\\|_1 = 1-\\delta + \\epsilon\n\\]\nLet‚Äôs now put \\(d = (0, \\delta)\\)\n\\[\n\\|x-d\\|^2_2 = 1-2\\delta \\epsilon + \\delta^2 + \\epsilon^2\n\\]\n\\[\n\\|x-d\\|_1 = 1-\\delta + \\epsilon\n\\]\nWe can see that the that L2 regularization is prone to reduce the bigger weights, whereas L1 regularization does‚Äônot draw any attantion on the magnitude of the weights"
  },
  {
    "objectID": "projects/Varietional_Auto_encoder.html",
    "href": "projects/Varietional_Auto_encoder.html",
    "title": "Addisu Amare",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n%env CUDA_VISIBLE_DEVICES=0\n\nenv: CUDA_VISIBLE_DEVICES=0\n\n\n\nfrom PIL import Image\nfrom pathlib import Path\nfrom matplotlib import pyplot as plt\n\nimport wandb\nimport numpy as np\nimport torch as t\nimport torchvision\nfrom tqdm.auto import tqdm, trange\n\n\nclass MNIST(t.utils.data.Dataset):\n    def __init__(self, data_root: Path):\n        imgs = []\n        for img, _ in tqdm(torchvision.datasets.MNIST(data_root, download=True)):\n            img = img.resize((32, 32), Image.Resampling.BILINEAR)\n            img = np.array(img)[None, :, :]\n            img = img / 255\n            imgs.append(img)\n            \n        self.imgs = t.tensor(np.stack(imgs), dtype=t.float)\n        \n    def __len__(self):\n        return self.imgs.shape[0]\n    \n    def __getitem__(self, index):\n        return self.imgs[index]\n\n    def shuffled_batch_iterate(self, batch_size: int):\n        while True:\n            inds = t.randint(0, self.imgs.shape[0], [batch_size])\n            yield self.imgs[inds]\n\n\nZ_DIM = 2\n\n\ndef conv_block(in_features: int, out_features: int, kernel: int = 3, stride: int = 1, padding: int = 1):\n    return t.nn.Sequential(\n        t.nn.Conv2d(in_features, out_features, kernel, stride, padding),\n        t.nn.BatchNorm2d(out_features),\n        t.nn.ReLU(),\n    )\n\n\nclass Upscale(t.nn.Module):\n    def __init__(self, in_features: int, out_features: int):\n        super().__init__()\n        self.conv = conv_block(in_features, 4 * out_features)\n        \n    def forward(self, x: t.Tensor) -&gt; t.Tensor:\n        x = self.conv(x)\n        b, c, h, w = x.shape\n        x = x.reshape(b, c // 4, 2, 2, h, w)\n        x = t.moveaxis(x, (0, 1, 4, 2, 5, 3), (0, 1, 2, 3, 4, 5))\n        x = x.reshape(b, c // 4, h * 2, w * 2)\n        \n        return x\n\n    \nclass Squeeze(t.nn.Module):\n    def forward(self, x):\n        return x.squeeze(2).squeeze(2)\n    \n    \nclass Unsqueeze(t.nn.Module):\n    def forward(self, x):\n        return x[:, :, None, None]\n    \n    \ndef downscale_block(features: int, out_features: int):\n    return t.nn.Sequential(\n        conv_block(features, features),\n        conv_block(features, features),\n        conv_block(features, out_features, 2, 2, 0)\n    )\n\n    \ndef upscale_block(in_features: int, features: int):\n    return t.nn.Sequential(\n        Upscale(in_features, features),\n        conv_block(features, features),\n        conv_block(features, features),\n    )\n\n\ndef get_encoder():\n    return t.nn.Sequential(\n        conv_block(1, 8),\n        downscale_block(8, 16),\n        downscale_block(16, 32),\n        downscale_block(32, 64),\n        downscale_block(64, 64),\n        downscale_block(64, 128),\n        Squeeze(),\n        t.nn.Linear(128, Z_DIM * 2),\n    )\n\n\ndef get_decoder():\n    return t.nn.Sequential(\n        Unsqueeze(),\n        upscale_block(Z_DIM, 128),\n        upscale_block(128, 64),\n        upscale_block(64, 32),\n        upscale_block(32, 16),\n        upscale_block(16, 8),\n        t.nn.Conv2d(8, 1, 3, padding=1),\n    )\n\n\ndef reparameterize(mean, log_std):\n    return mean + t.randn_like(mean) * t.exp(log_std)\n\n\ndef kl(mean, log_std):\n    return ((t.exp(2 * log_std) + mean * mean) / 2 - 0.5 - log_std).sum(axis=1)\n\n\nclass VAE(t.nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        \n        self.encoder = encoder\n        self.decoder = decoder\n        \n    def forward(self, imgs):\n        z_params = self.encoder(imgs)\n        z_mean, z_log_std = t.split(z_params, [Z_DIM, Z_DIM], dim=1)\n\n        z = reparameterize(z_mean, z_log_std)\n        z_kl = kl(z_mean, z_log_std)\n\n        decoded_imgs = self.decoder(z)\n        log_p_x_given_z = -((imgs - decoded_imgs)**2).sum([1, 2, 3])\n\n        loss = -log_p_x_given_z + z_kl \n\n        return {'loss': loss.mean(), 'log p(x|z)': log_p_x_given_z.mean(), 'kl': z_kl.mean()}\n\n\nBATCH_SIZE = 128\n\n# def run():\n#     with wandb.init(project='autoencoder'):\n\nmodel = VAE(get_encoder(), get_decoder()).cuda()\nprint('model crated')\n\noptimizer = t.optim.AdamW(model.parameters())\nprint('optimzer created')\n\ndata = MNIST('./')\nprint('model created')\n\nmodel.eval()\nprint(data[0].shape)\nmodel(data[0][None].cuda())\nmodel.train()\n\nwandb.init(project='autoencoder')\nfor step, imgs in enumerate(tqdm(data.shuffled_batch_iterate(BATCH_SIZE))):\n    imgs = imgs.cuda()\n    values = model(imgs)\n\n    wandb.log(values, step=step)\n\n    values['loss'].backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n    if step % 50 == 0:\n        model.eval()\n\n        with t.no_grad():\n            z = t.randn(16, Z_DIM, device='cuda')\n            img = model.decoder(z)\n            img = img.detach().cpu().numpy()\n            img = img.transpose(0, 2, 3, 1)\n            img = img.reshape(4, 4, 32, 32, 1)\n            img = img.transpose(0, 2, 1, 3, 4)\n            img = img.reshape(4 * 32, 4 * 32, 1)\n            plt.imshow(img)\n            plt.show()\n\n        model.train()\n\n    if step &gt; 3000:\n        break\n\n# run()\n\nmodel crated\noptimzer created\n\n\n\n\n\nmodel created\ntorch.Size([1, 32, 32])\n\n\n\nwandb: Currently logged in as: pg_lolo. Use `wandb login --relogin` to force relogin\n\n\n\n\nwandb version 0.15.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade\n\n\nTracking run with wandb version 0.13.8\n\n\nRun data is saved locally in /data/code/autoencoder/wandb/run-20230428_085829-jnwtvj8l\n\n\nSyncing run pious-salad-37 to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/pg_lolo/autoencoder\n\n\n View run at https://wandb.ai/pg_lolo/autoencoder/runs/jnwtvj8l\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel.eval()\n\nstart = -2 * t.ones(2)\nend = 2 * t.ones(2)\n\npoints = t.linspace(0, 1, 100)[:, None]\npoints = end[None, :] * points + start[None, :] * (1 - points)\n\nimgs = model.decoder(points.cuda()).detach().cpu().numpy()\n\nfor img in imgs:\n    plt.imshow(img[0])\n    plt.show()"
  },
  {
    "objectID": "projects/project-example.html",
    "href": "projects/project-example.html",
    "title": "Project Example",
    "section": "",
    "text": "This project focuses on [briefly describe the project, its goals, and significance].\n\n\n\nObjective 1: [Describe the first objective]\nObjective 2: [Describe the second objective]\n\n\n\n\n[Provide a brief overview of the methods used in the project, including any specific techniques or tools.]\n\n\n\n[Summarize the outcomes of the project, including any findings or results.]\n\n\n\n[Discuss any potential future work or next steps related to the project.]\n\n\n\n[Include any references or citations relevant to the project, if applicable.]"
  },
  {
    "objectID": "projects/project-example.html#objectives",
    "href": "projects/project-example.html#objectives",
    "title": "Project Example",
    "section": "",
    "text": "Objective 1: [Describe the first objective]\nObjective 2: [Describe the second objective]"
  },
  {
    "objectID": "projects/project-example.html#methodology",
    "href": "projects/project-example.html#methodology",
    "title": "Project Example",
    "section": "",
    "text": "[Provide a brief overview of the methods used in the project, including any specific techniques or tools.]"
  },
  {
    "objectID": "projects/project-example.html#outcomes",
    "href": "projects/project-example.html#outcomes",
    "title": "Project Example",
    "section": "",
    "text": "[Summarize the outcomes of the project, including any findings or results.]"
  },
  {
    "objectID": "projects/project-example.html#future-work",
    "href": "projects/project-example.html#future-work",
    "title": "Project Example",
    "section": "",
    "text": "[Discuss any potential future work or next steps related to the project.]"
  },
  {
    "objectID": "projects/project-example.html#references",
    "href": "projects/project-example.html#references",
    "title": "Project Example",
    "section": "",
    "text": "[Include any references or citations relevant to the project, if applicable.]"
  },
  {
    "objectID": "projects/backpropagation.html",
    "href": "projects/backpropagation.html",
    "title": "Backpropagation",
    "section": "",
    "text": "Understanding backpropagation is crucial for both researchers and practitioners in the field of deep learning for different reasons:\n\nFor Researchers: The ability to write custom layers and define their differentiation process is essential.\nFor Practitioners: Questions on backpropagation are very popular on deep learning interviews.\n\nPyTorch is very similar to NumPy, but these are key differences: - GPU Support PyTorch tensors can be easily moved to a GPU for accelerated computing. See device argument and method .to(device).\n\nAutomatic Differentiation (and Dynamic Computational Graphs) PyTorch provides a powerful autograd system that automatically computes gradients for tensor operations, making it easy to train neural networks.\n\nThis guide assumes you have a basic understanding of PyTorch. If you‚Äôre not familiar, consider reading the Deep Learning with PyTorch: A 60 Minute Blitz tutorial on the official PyTorch website.\nI will cover the most important functions for a custom backpropagation."
  },
  {
    "objectID": "projects/backpropagation.html#the-importance-of-understanding-backpropagation",
    "href": "projects/backpropagation.html#the-importance-of-understanding-backpropagation",
    "title": "Backpropagation",
    "section": "",
    "text": "Understanding backpropagation is crucial for both researchers and practitioners in the field of deep learning for different reasons:\n\nFor Researchers: The ability to write custom layers and define their differentiation process is essential.\nFor Practitioners: Questions on backpropagation are very popular on deep learning interviews.\n\nPyTorch is very similar to NumPy, but these are key differences: - GPU Support PyTorch tensors can be easily moved to a GPU for accelerated computing. See device argument and method .to(device).\n\nAutomatic Differentiation (and Dynamic Computational Graphs) PyTorch provides a powerful autograd system that automatically computes gradients for tensor operations, making it easy to train neural networks.\n\nThis guide assumes you have a basic understanding of PyTorch. If you‚Äôre not familiar, consider reading the Deep Learning with PyTorch: A 60 Minute Blitz tutorial on the official PyTorch website.\nI will cover the most important functions for a custom backpropagation."
  },
  {
    "objectID": "projects/backpropagation.html#custom-gradients-in-pytorch",
    "href": "projects/backpropagation.html#custom-gradients-in-pytorch",
    "title": "Backpropagation",
    "section": "Custom Gradients in PyTorch",
    "text": "Custom Gradients in PyTorch\n\nAutograd Functions\n\nPurpose: Enable fine-grained control over both the forward and backward computations.\nUsage: Define a class inheriting from torch.autograd.Function and implement the @staticmethods forward and backward.\n\nfrom torch.autograd import Function\n\nclass CustomFunction(Function):\n    @staticmethod\n    def forward(ctx, input):\n        ctx.save_for_backward(input)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, = ctx.saved_tensors\n        grad_input = ...\n        return grad_input\n\n# Custom gradients in PyTorch\n\n::: {#5cc3303e-a758-4d68-80bc-fc9291e97038 .cell execution_count=67}\n``` {.python .cell-code}\nimport torch\nfrom torch.autograd import gradcheck\n\nclass MyMatVec(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, A, x):\n        ctx.save_for_backward(A, x)\n        return torch.mv(A, x)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        A, x = ctx.saved_tensors  # Retrieve saved tensors\n        grad_A = grad_output[:, None] @ x[None, :]\n        grad_x = A.T @ grad_output\n        return grad_A, grad_x\n\n# Alias for applying the operation\nmatvec = MyMatVec.apply\n\n# Example usage\nA = torch.randn(5, 7, dtype=torch.double, requires_grad=True)\nx = torch.randn(7, dtype=torch.double, requires_grad=True)\ninputs = (A, x)\nif gradcheck(matvec, inputs, eps=1e-6, atol=1e-4):\n    print('OK!')\n\nOK!\n\n:::"
  },
  {
    "objectID": "projects/backpropagation.html#dropout",
    "href": "projects/backpropagation.html#dropout",
    "title": "Backpropagation",
    "section": "Dropout",
    "text": "Dropout\nDuring training:\n\\(\\text{dropout}(x) = \\dfrac{1}{1 - p} \\text{Bernoulli}(1 - p) \\odot x\\),\nwhere \\(\\odot\\) is an element-wise product, \\(\\text{Bernoulli}(1 - p)\\) is a random binary mask from Bernoulli distribution.\nDuring inference do nothing.\n\nimport torch\nfrom torch.autograd import gradcheck\n\nclass MyDropOutTraining(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, mask, p=0.1):\n        res = x * mask / (1 - p)\n        ctx.save_for_backward(mask / (1 - p))\n        return res\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        scaled_mask, = ctx.saved_tensors\n        return scaled_mask * grad_output, None, None\n\n# Alias for applying the operation\ndropout = MyDropOutTraining.apply\n\n\n# Example usage\nx = torch.randn(7, dtype=torch.double, requires_grad=True)\np = 0.1\nmask = torch.empty_like(x)\nmask.bernoulli_(1 - p) \ninputs = (x, mask, p)\n# Gradcheck will not work with a mask that is not an input\nif gradcheck(dropout, inputs, eps=1e-6, atol=1e-4):\n    print('OK!')\n\nOK!"
  },
  {
    "objectID": "projects/backpropagation.html#softmax",
    "href": "projects/backpropagation.html#softmax",
    "title": "Backpropagation",
    "section": "SoftMax",
    "text": "SoftMax\nSoftmax converts an arbitrary vector to a vector that sums up to one. It is a typical last layer for multiclass classification problems, where we have to output the probability.\n\\[\n\\text{softmax}(x)_i = \\frac{\\exp{x_i}}{\\sum_{k} \\exp{x_k}}, \\quad x \\in \\mathbb{R}^n\n\\]\nExercise Implement custom gradients for this function.\n\nimport torch\nfrom torch.autograd import gradcheck\n\nclass MySoftMax(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        x = x - x.max()  # for stability\n        exp_vals = torch.exp(x)\n        res = exp_vals / torch.sum(exp_vals)\n        ctx.save_for_backward(x, res)\n        return res\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x, res = ctx.saved_tensors  \n        # Your code\n        return torch.ones_like(x)\n\n# Alias for applying the operation\nsoftmax = MySoftMax.apply\n\n# Example usage\nx = torch.randn(7, dtype=torch.double, requires_grad=True)\ninputs = x\nif gradcheck(softmax, inputs, eps=1e-6, atol=1e-4):\n    print('OK!')\n\n\nf = torch.randn(5)\n\ntorch.einsum('i,j-&gt;ij', f, f)\n\ntensor([[ 0.3448,  0.0757, -0.8317,  0.9454, -0.2449],\n        [ 0.0757,  0.0166, -0.1825,  0.2074, -0.0537],\n        [-0.8317, -0.1825,  2.0063, -2.2806,  0.5908],\n        [ 0.9454,  0.2074, -2.2806,  2.5924, -0.6716],\n        [-0.2449, -0.0537,  0.5908, -0.6716,  0.1740]])\n\n\n\nf[:, None] @ f[None, :]\n\ntensor([[ 0.3448,  0.0757, -0.8317,  0.9454, -0.2449],\n        [ 0.0757,  0.0166, -0.1825,  0.2074, -0.0537],\n        [-0.8317, -0.1825,  2.0063, -2.2806,  0.5908],\n        [ 0.9454,  0.2074, -2.2806,  2.5924, -0.6716],\n        [-0.2449, -0.0537,  0.5908, -0.6716,  0.1740]])\n\n\n\nf.reshape(-1, 1) @ f.reshape(1, -1)\n\ntensor([[ 0.3448,  0.0757, -0.8317,  0.9454, -0.2449],\n        [ 0.0757,  0.0166, -0.1825,  0.2074, -0.0537],\n        [-0.8317, -0.1825,  2.0063, -2.2806,  0.5908],\n        [ 0.9454,  0.2074, -2.2806,  2.5924, -0.6716],\n        [-0.2449, -0.0537,  0.5908, -0.6716,  0.1740]])"
  },
  {
    "objectID": "projects/vision_transformer.html",
    "href": "projects/vision_transformer.html",
    "title": "Vision_Transformer",
    "section": "",
    "text": "Probably, ‚Äûtransformer‚Äú is one of the most frequent word in the lecture\nTransformers are the best idea in AI by Andrej Karpathy\nPaperswithcode for image classification\n‚ÄûHow to achieve success in a machine learning PhD‚Äú by Patrick Kidger: &gt; ‚Ä¶Write your own implementation of multihead attention‚Ä¶\n\nToday we are going to write our own implementation of ViT!"
  },
  {
    "objectID": "projects/vision_transformer.html#bit.lyvit_sem",
    "href": "projects/vision_transformer.html#bit.lyvit_sem",
    "title": "Vision_Transformer",
    "section": "bit.ly/vit_sem",
    "text": "bit.ly/vit_sem\n\nimport torch\nfrom torch import nn\n\nclass Attention(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n        self.scale = self.dim ** -0.5\n        self.qkv = nn.Linear(dim, dim * 3)\n\n    def forward(self, x):\n        '''\n        Args: \n            x: Tensor of shape (batch_size, seq_len, input_dim)\n            \n        Returns:\n            Tensor of shape (batch_size, seq_len, input_dim)\n        '''\n        # Your code is here\n        return x"
  },
  {
    "objectID": "projects/convlution.html",
    "href": "projects/convlution.html",
    "title": "Convolution",
    "section": "",
    "text": "import numpy as np\n\n\ndef convolve(image, kernel):\n    assert image.ndim == kernel.ndim == 2\n    \n    kernel = np.flip(kernel, axis=range(kernel.ndim))\n    shape = np.array(image.shape) - kernel.shape + 1\n    result = np.empty(shape, dtype=float) # we'll keep things simple and always return float\n    h, w = kernel.shape\n        \n    for i, j in np.ndindex(*shape):\n        result[i, j] = (image[i:i + h, j:j + w] * kernel).sum()\n\n    return result\n\n\n# a more general version, but much slower than `convolve`\ndef convolve_nd(image, kernel):\n    assert image.ndim == kernel.ndim\n    \n    kernel = np.flip(kernel, axis=range(kernel.ndim))\n    shape = np.array(image.shape) - kernel.shape + 1\n    result = np.empty(shape, dtype=image.dtype)\n        \n    for start in np.ndindex(*shape):\n        stop = np.array(start) + kernel.shape\n        slices = tuple(\n            slice(x, y) for x, y in zip(start, stop)\n        )\n        result[start] = (image[slices] * kernel).sum()\n\n    return result\n\n    \ndef convolve_same(image, kernel):\n    # pad image before convolution to obtain same resulting shape\n    delta = np.array(kernel.shape) - 1\n    # left and right padding\n    left = delta // 2\n    right = delta - left\n    padding = np.array([left, right]).T\n\n    image = np.pad(image, padding, mode='constant') \n    return convolve(image, kernel)"
  },
  {
    "objectID": "projects/convlution.html#a-simple-edge-detector",
    "href": "projects/convlution.html#a-simple-edge-detector",
    "title": "Convolution",
    "section": "A simple edge detector",
    "text": "A simple edge detector\n\nres_x = convolve(image, sobel_x)\nres_y = convolve(image, sobel_y)\n\n\nshow(res_x)\nshow(res_y)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnorm = np.hypot(res_x, res_y)\n\n\nshow(norm)"
  },
  {
    "objectID": "projects/metrics-auroc.html",
    "href": "projects/metrics-auroc.html",
    "title": "Addisu Amare",
    "section": "",
    "text": "##Linear Models: AUROC\n\n\n\nAn example of ROC Curve\n\n\nAn unbiased estimate of Area Under Receive-Operating Curve (AUROC) is\n\\[\n    AUROC(a) = \\frac{1}{|\\mathcal{D}_0| |\\mathcal{D}_1|} \\sum_{x_0 \\in D_0} \\sum_{x_1 \\in D_1} I[a(x_0) &lt; a(x_1)] .\n\\]\nfor an algorithm \\(a\\), \\(\\mathcal{D}_{0,1}\\) mean a set of negative (0) and positive (1) examples. It is useful since\n\nThreshold Independence\nRobustness to Class Imbalance\nModel Comparison\nInterpretability\n‚Ä¶\n\n\nDataset\nLet‚Äôs generate some synthetic 2d dataset for classification problem.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nrs = np.random.RandomState(42)\n\n\nn_points = 200\nclusters = [\n    rs.normal(loc=(1, 1), size=(n_points // 2, 2)),\n    rs.normal(loc=(-1, -1), size=(n_points // 2, 2)),\n]\ncoords = np.vstack(clusters)\nlabels = np.zeros(n_points, dtype=int)\nlabels[n_points // 2:] += 1\n\n\nfor i, cluster in enumerate(clusters):\n    plt.scatter(clusters[i][:, 0], clusters[i][:, 1], label=f'cluster {i}')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\nfrom sklearn.linear_model import LogisticRegression\n\n\nclf = LogisticRegression()\nclf.fit(coords, labels)\nprobas = clf.predict_proba(coords)\n\n\n\nConfusion Matrix\nConsusion matrix for a binary classifier is\n\\[\n    C = \\begin{bmatrix}\n        TP & FP \\\\\n        FN & TN\n    \\end{bmatrix}.\n\\]\nThen let‚Äôs define false-positive rate (FPR) and true-positive rate (TPR) as follows \\[\nFPR = \\frac{FP}{TP + TN},\n\\] \\[\nTPR = \\frac{TP}{TP + FN}.\n\\]\nReceiver-Operation Curve (ROC) is a relation between TPR and FPR\n\\[\n    TPR= TPR(FPR).\n\\]\n\nfrom sklearn.metrics import confusion_matrix\n\n\nthreshold = 0.5\npreds = (probas[:, 1] &gt;= threshold).astype(int)\n\n\nconfmat = confusion_matrix(labels, preds)\ntn, fp, fn, tp  = confmat.ravel()\n\n\nfpr = fp / (fp + tn)\nfpr\n\n\ntpr = tp / (tp + fn)\ntpr\n\n\n\nROC and Area under ROC\nWe need to change a threshold contiously from 0 to 1, calculate confusion matrix, calculate tpr/fpr, and plot ROC and calculate AUROC.\n\n# thresholds = (0, 0.5, 1)\nthresholds = np.linspace(0, 1, 101)\n\nfprs = np.empty(len(thresholds))\ntprs = np.empty(len(thresholds))\nfor i, threshold in enumerate(thresholds):\n    preds = (probas[:, 0] &lt;= threshold).astype(int)\n    \n    confmat = confusion_matrix(labels, preds)\n    tn, fp, fn, tp  = confmat.ravel()\n\n    fprs[i] = fp / (fp + tn)\n    tprs[i] = tp / (tp + fn)\n\n\nplt.step(fprs, tprs, '-')\nplt.grid(True)\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.show()\n\nIf we intergrate numerical over tprs/fprs then the value of that integral is an area under curve (i.e.¬†AUROC).\n\nnp.trapz(tprs, fprs)\n\nGreat! Let‚Äôs compare with library implementation.\n\nfrom sklearn.metrics import roc_curve, roc_auc_score\n\n\nroc_auc_score(labels, probas[:, 1])\n\n\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 4))\n\nfpr, tpr, th = roc_curve(labels, probas[:, 1])\nax = axs[0]\nax.plot(fpr, tpr, '.-', label='library')\nax.legend()\nax.grid(True)\nax.set_xlabel('FPR')\nax.set_ylabel('TPR')\n\nax = axs[1]\nax.step(fprs, tprs, '.-', label='ours')\nax.legend()\nax.grid(True)\nax.set_xlabel('FPR')\nax.set_ylabel('TPR')\n\nplt.show()"
  },
  {
    "objectID": "projects/XAI_3D_CNN.html",
    "href": "projects/XAI_3D_CNN.html",
    "title": "Interpretation of 3D CNNs for Brain MRI Data Classification",
    "section": "",
    "text": "Introduction In this notebook we will explore how the model makes a decision on the example of simple 3D CNN classificationl model on pytorch. In the current notebook we use Alzheimer‚Äôs Disease Neuroimaging Initiative (ADNI) dataset (ADNI is a global research effort that actively supports the investigation and development of treatments that slow or stop the progression of Alzheimer‚Äôs disease (AD)) , preprocessed by the Clinica software platform.\nOur goal will be to study various techniques for interpreting –°NN models to test the reliability of neural networks in the case of MRI brain classification.\nProceesing with this notebook you confirm, that you have granted access to ADNI 1, 2, 3, GO Study Data. And your agreement on data terms and conditions.\nThe data accesed from the original sourse: https://ida.loni.usc.edu/pages/access/studyData.jsp?categoryId=14&subCategoryId=30\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive\n!pip install nilearn==0.9.2\n!pip install nibabel==4.0.2\n\n\nCollecting nilearn==0.9.2\n\n  Downloading nilearn-0.9.2-py3-none-any.whl.metadata (6.7 kB)\n\nRequirement already satisfied: joblib&gt;=0.15 in /usr/local/lib/python3.10/dist-packages (from nilearn==0.9.2) (1.4.2)\n\nRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nilearn==0.9.2) (4.9.4)\n\nRequirement already satisfied: nibabel&gt;=3.0.0 in /usr/local/lib/python3.10/dist-packages (from nilearn==0.9.2) (5.2.1)\n\nRequirement already satisfied: numpy&gt;=1.18 in /usr/local/lib/python3.10/dist-packages (from nilearn==0.9.2) (1.26.4)\n\nRequirement already satisfied: pandas&gt;=1.0 in /usr/local/lib/python3.10/dist-packages (from nilearn==0.9.2) (2.1.4)\n\nRequirement already satisfied: requests&gt;=2 in /usr/local/lib/python3.10/dist-packages (from nilearn==0.9.2) (2.32.3)\n\nRequirement already satisfied: scikit-learn&gt;=0.22 in /usr/local/lib/python3.10/dist-packages (from nilearn==0.9.2) (1.5.2)\n\nRequirement already satisfied: scipy&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from nilearn==0.9.2) (1.13.1)\n\nRequirement already satisfied: packaging&gt;=17 in /usr/local/lib/python3.10/dist-packages (from nibabel&gt;=3.0.0-&gt;nilearn==0.9.2) (24.1)\n\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas&gt;=1.0-&gt;nilearn==0.9.2) (2.8.2)\n\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas&gt;=1.0-&gt;nilearn==0.9.2) (2024.2)\n\nRequirement already satisfied: tzdata&gt;=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas&gt;=1.0-&gt;nilearn==0.9.2) (2024.1)\n\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2-&gt;nilearn==0.9.2) (3.3.2)\n\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2-&gt;nilearn==0.9.2) (3.10)\n\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2-&gt;nilearn==0.9.2) (2.2.3)\n\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2-&gt;nilearn==0.9.2) (2024.8.30)\n\nRequirement already satisfied: threadpoolctl&gt;=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn&gt;=0.22-&gt;nilearn==0.9.2) (3.5.0)\n\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.8.2-&gt;pandas&gt;=1.0-&gt;nilearn==0.9.2) (1.16.0)\n\nDownloading nilearn-0.9.2-py3-none-any.whl (9.6 MB)\n\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9.6/9.6 MB 24.5 MB/s eta 0:00:00\n\nInstalling collected packages: nilearn\n\nSuccessfully installed nilearn-0.9.2\n\nCollecting nibabel==4.0.2\n\n  Downloading nibabel-4.0.2-py3-none-any.whl.metadata (6.1 kB)\n\nRequirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.10/dist-packages (from nibabel==4.0.2) (1.26.4)\n\nRequirement already satisfied: packaging&gt;=17.0 in /usr/local/lib/python3.10/dist-packages (from nibabel==4.0.2) (24.1)\n\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nibabel==4.0.2) (71.0.4)\n\nDownloading nibabel-4.0.2-py3-none-any.whl (3.3 MB)\n\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3.3/3.3 MB 39.9 MB/s eta 0:00:00\n\nInstalling collected packages: nibabel\n\n  Attempting uninstall: nibabel\n\n    Found existing installation: nibabel 5.2.1\n\n    Uninstalling nibabel-5.2.1:\n\n      Successfully uninstalled nibabel-5.2.1\n\nSuccessfully installed nibabel-4.0.2\n\n\n\n\nUnable to display output for mime type(s): application/vnd.colab-display-data+json\nimport os\nfrom os import path\nimport sys\nimport argparse\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset, sampler\nimport torchvision.transforms as transforms\nimport abc\nimport logging\nimport nibabel as nib\nimport matplotlib.pyplot as plt\nfrom nilearn import plotting\nfrom tqdm import tqdm\nLink to the data - https://drive.google.com/file/d/1FnE-zduDsd7R6EOKxTpecdF0riRgFfy-/view?usp=sharing\n!wget -O 10_sub_for_validation.zip \"https://getfile.dokpub.com/yandex/get/https://disk.yandex.ru/d/88OCJ8NM_91IRg\"\n\n--2024-10-01 08:14:00--  https://getfile.dokpub.com/yandex/get/https://disk.yandex.ru/d/88OCJ8NM_91IRg\nResolving getfile.dokpub.com (getfile.dokpub.com)... 142.132.255.217\nConnecting to getfile.dokpub.com (getfile.dokpub.com)|142.132.255.217|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://downloader.disk.yandex.ru/disk/ed24efdb4875fc5ff820fdc89acb811203b1820743d072e00d3c3cd47cde5d11/66fbe77a/yLVzctR40OkW-hBDuEw6jDCp6ngDgPALQ2SDKu2AA2GKD_uq0viO6FzDzu_kVmphG-Drz7GPkJziyWwWLK84Xg%3D%3D?uid=0&filename=10_sub_for_validation.zip&disposition=attachment&hash=UDeXMWro4Yy9SewW5d4RkyQlRPDoGA3kwHnm%2BnINFlfxnTJjxNXH4To6UrVSrr2Vq/J6bpmRyOJonT3VoXnDag%3D%3D%3A&limit=0&content_type=application%2Fzip&owner_uid=222635455&fsize=818631908&hid=c7ece160fe69d440642d3425a44b73e0&media_type=compressed&tknv=v2 [following]\n--2024-10-01 08:14:01--  https://downloader.disk.yandex.ru/disk/ed24efdb4875fc5ff820fdc89acb811203b1820743d072e00d3c3cd47cde5d11/66fbe77a/yLVzctR40OkW-hBDuEw6jDCp6ngDgPALQ2SDKu2AA2GKD_uq0viO6FzDzu_kVmphG-Drz7GPkJziyWwWLK84Xg%3D%3D?uid=0&filename=10_sub_for_validation.zip&disposition=attachment&hash=UDeXMWro4Yy9SewW5d4RkyQlRPDoGA3kwHnm%2BnINFlfxnTJjxNXH4To6UrVSrr2Vq/J6bpmRyOJonT3VoXnDag%3D%3D%3A&limit=0&content_type=application%2Fzip&owner_uid=222635455&fsize=818631908&hid=c7ece160fe69d440642d3425a44b73e0&media_type=compressed&tknv=v2\nResolving downloader.disk.yandex.ru (downloader.disk.yandex.ru)... 77.88.21.127, 2a02:6b8::2:127\nConnecting to downloader.disk.yandex.ru (downloader.disk.yandex.ru)|77.88.21.127|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://s779sas.storage.yandex.net/rdisk/ed24efdb4875fc5ff820fdc89acb811203b1820743d072e00d3c3cd47cde5d11/66fbe77a/yLVzctR40OkW-hBDuEw6jDCp6ngDgPALQ2SDKu2AA2GKD_uq0viO6FzDzu_kVmphG-Drz7GPkJziyWwWLK84Xg==?uid=0&filename=10_sub_for_validation.zip&disposition=attachment&hash=UDeXMWro4Yy9SewW5d4RkyQlRPDoGA3kwHnm%2BnINFlfxnTJjxNXH4To6UrVSrr2Vq/J6bpmRyOJonT3VoXnDag%3D%3D%3A&limit=0&content_type=application%2Fzip&owner_uid=222635455&fsize=818631908&hid=c7ece160fe69d440642d3425a44b73e0&media_type=compressed&tknv=v2&ts=6236940cd5280&s=3cda0f24ccb0c103f8f226fee695c0d876290a802c4546094128fecea0f904fe&pb=U2FsdGVkX19r6MVLA_LCgdc-ebQwA21usMS7Eyd2c9owIAxMPXSgyMnTJC9pSNTwFnmCAZClDR6_rBnvvP-edDSNtVPcc9CiyuvW4_QNn8g [following]\n--2024-10-01 08:14:02--  https://s779sas.storage.yandex.net/rdisk/ed24efdb4875fc5ff820fdc89acb811203b1820743d072e00d3c3cd47cde5d11/66fbe77a/yLVzctR40OkW-hBDuEw6jDCp6ngDgPALQ2SDKu2AA2GKD_uq0viO6FzDzu_kVmphG-Drz7GPkJziyWwWLK84Xg==?uid=0&filename=10_sub_for_validation.zip&disposition=attachment&hash=UDeXMWro4Yy9SewW5d4RkyQlRPDoGA3kwHnm%2BnINFlfxnTJjxNXH4To6UrVSrr2Vq/J6bpmRyOJonT3VoXnDag%3D%3D%3A&limit=0&content_type=application%2Fzip&owner_uid=222635455&fsize=818631908&hid=c7ece160fe69d440642d3425a44b73e0&media_type=compressed&tknv=v2&ts=6236940cd5280&s=3cda0f24ccb0c103f8f226fee695c0d876290a802c4546094128fecea0f904fe&pb=U2FsdGVkX19r6MVLA_LCgdc-ebQwA21usMS7Eyd2c9owIAxMPXSgyMnTJC9pSNTwFnmCAZClDR6_rBnvvP-edDSNtVPcc9CiyuvW4_QNn8g\nResolving s779sas.storage.yandex.net (s779sas.storage.yandex.net)... 5.255.228.149, 2a02:6b8:c02:127b:0:41af:da2b:113d\nConnecting to s779sas.storage.yandex.net (s779sas.storage.yandex.net)|5.255.228.149|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 818631908 (781M) [application/zip]\nSaving to: ‚Äò10_sub_for_validation.zip‚Äô\n\n10_sub_for_validati 100%[===================&gt;] 780.71M  9.97MB/s    in 83s     \n\n2024-10-01 08:15:26 (9.39 MB/s) - ‚Äò10_sub_for_validation.zip‚Äô saved [818631908/818631908]\n!cd /content/drive/ &&  ls\n\nMyDrive\n!unzip 10_sub_for_validation.zip\n\nArchive:  10_sub_for_validation.zip\n   creating: 10_sub_for_validation/\n   creating: 10_sub_for_validation/10_sub_for_validation/\n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI035S0555/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI035S0555/df_scores.csv  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI035S0555/sub-ADNI035S0555_ses-M00_T1_cn.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI035S0555/interpretation_mean_pertrub.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI035S0555/sub-ADNI035S0555_ses-M108_T1_ad.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI035S0555/interpretation_guided_backprop.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI035S0555/interpretation_gradcam_mask.png  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI035S0555/raw_data/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI035S0555/raw_data/sub-ADNI035S0555_ses-M00_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI035S0555/raw_data/ground_truth_mask_sub-ADNI035S0555.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI035S0555/raw_data/interpretation_guided_backprop_mask_T1_sub-ADNI035S0555_ses-M108.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI035S0555/raw_data/interpretation_mean_pertrub_mask_T1_sub-ADNI035S0555_ses-M108.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI035S0555/raw_data/interpretation_gradcam_mask_mask_T1_sub-ADNI035S0555_ses-M108.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI035S0555/raw_data/sub-ADNI035S0555_ses-M108_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI035S0555/ground_truth/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI035S0555/ground_truth/slices_from_inferior_to_superior.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI035S0555/ground_truth/gt_slices.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI035S0555/ground_truth/slices_from_left_to_right.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI035S0555/ground_truth/slices_from_from_anterior_to_posterior.png  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI137S0972/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI137S0972/sub-ADNI137S0972_ses-M96_T1_ad.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI137S0972/df_scores.csv  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI137S0972/sub-ADNI137S0972_ses-M00_T1_cn.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI137S0972/interpretation_mean_pertrub.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI137S0972/interpretation_guided_backprop.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI137S0972/interpretation_gradcam_mask.png  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI137S0972/raw_data/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI137S0972/raw_data/sub-ADNI137S0972_ses-M00_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI137S0972/raw_data/sub-ADNI137S0972_ses-M96_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI137S0972/raw_data/interpretation_mean_pertrub_mask_T1_sub-ADNI137S0972_ses-M96.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI137S0972/raw_data/interpretation_guided_backprop_mask_T1_sub-ADNI137S0972_ses-M96.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI137S0972/raw_data/ground_truth_mask_sub-ADNI137S0972.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI137S0972/raw_data/interpretation_gradcam_mask_mask_T1_sub-ADNI137S0972_ses-M96.nii.gz  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI137S0972/ground_truth/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI137S0972/ground_truth/gt_slices.png  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI116S1249/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI116S1249/sub-ADNI116S1249_ses-M00_T1_cn.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI116S1249/df_scores.csv  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI116S1249/interpretation_mean_pertrub.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI116S1249/interpretation_guided_backprop.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI116S1249/interpretation_gradcam_mask.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI116S1249/sub-ADNI116S1249_ses-M108_T1_ad.png  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI116S1249/raw_data/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI116S1249/raw_data/sub-ADNI116S1249_ses-M00_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI116S1249/raw_data/ground_truth_mask_sub-ADNI116S1249.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI116S1249/raw_data/interpretation_guided_backprop_mask_T1_sub-ADNI116S1249_ses-M108.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI116S1249/raw_data/sub-ADNI116S1249_ses-M108_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI116S1249/raw_data/interpretation_gradcam_mask_mask_T1_sub-ADNI116S1249_ses-M108.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI116S1249/raw_data/interpretation_mean_pertrub_mask_T1_sub-ADNI116S1249_ses-M108.nii.gz  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI116S1249/ground_truth/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI116S1249/ground_truth/slices_from_inferior_to_superior.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI116S1249/ground_truth/gt_slices.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI116S1249/ground_truth/slices_from_left_to_right.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI116S1249/ground_truth/slices_from_from_anterior_to_posterior.png  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI051S1123/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI051S1123/df_scores.csv  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI051S1123/sub-ADNI051S1123_ses-M96_T1_ad.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI051S1123/interpretation_mean_pertrub.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI051S1123/interpretation_guided_backprop.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI051S1123/interpretation_gradcam_mask.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI051S1123/sub-ADNI051S1123_ses-M00_T1_cn.png  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI051S1123/raw_data/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI051S1123/raw_data/interpretation_mean_pertrub_mask_T1_sub-ADNI051S1123_ses-M96.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI051S1123/raw_data/sub-ADNI051S1123_ses-M00_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI051S1123/raw_data/interpretation_guided_backprop_mask_T1_sub-ADNI051S1123_ses-M96.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI051S1123/raw_data/ground_truth_mask_sub-ADNI051S1123.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI051S1123/raw_data/interpretation_gradcam_mask_mask_T1_sub-ADNI051S1123_ses-M96.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI051S1123/raw_data/sub-ADNI051S1123_ses-M96_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI051S1123/ground_truth/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI051S1123/ground_truth/slices_from_inferior_to_superior.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI051S1123/ground_truth/gt_slices.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI051S1123/ground_truth/slices_from_left_to_right.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI051S1123/ground_truth/slices_from_from_anterior_to_posterior.png  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI114S0166/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI114S0166/df_scores.csv  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI114S0166/sub-ADNI114S0166_ses-M108_T1_ad.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI114S0166/interpretation_mean_pertrub.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI114S0166/interpretation_guided_backprop.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI114S0166/interpretation_gradcam_mask.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI114S0166/sub-ADNI114S0166_ses-M00_T1_cn.png  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI114S0166/raw_data/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI114S0166/raw_data/interpretation_mean_pertrub_mask_T1_sub-ADNI114S0166_ses-M108.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI114S0166/raw_data/interpretation_guided_backprop_mask_T1_sub-ADNI114S0166_ses-M108.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI114S0166/raw_data/sub-ADNI114S0166_ses-M108_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI114S0166/raw_data/sub-ADNI114S0166_ses-M00_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI114S0166/raw_data/ground_truth_mask_sub-ADNI114S0166.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI114S0166/raw_data/interpretation_gradcam_mask_mask_T1_sub-ADNI114S0166_ses-M108.nii.gz  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI114S0166/ground_truth/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI114S0166/ground_truth/slices_from_inferior_to_superior.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI114S0166/ground_truth/gt_slices.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI114S0166/ground_truth/slices_from_left_to_right.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI114S0166/ground_truth/slices_from_from_anterior_to_posterior.png  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI114S0166/ground_truth/.ipynb_checkpoints/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI114S0166/ground_truth/.ipynb_checkpoints/slices_from_left_to_right-checkpoint.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI114S0166/ground_truth/.ipynb_checkpoints/gt_slices-checkpoint.png  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI114S0166/.ipynb_checkpoints/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI114S0166/.ipynb_checkpoints/df_scores-checkpoint.csv  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI114S0166/.ipynb_checkpoints/interpretation_gradcam_mask-checkpoint.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI114S0166/.ipynb_checkpoints/interpretation_guided_backprop-checkpoint.png  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI002S4262/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI002S4262/df_scores.csv  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI002S4262/interpretation_mean_pertrub.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI002S4262/interpretation_guided_backprop.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI002S4262/interpretation_gradcam_mask.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI002S4262/sub-ADNI002S4262_ses-M60_T1_ad.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI002S4262/sub-ADNI002S4262_ses-M00_T1_cn.png  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI002S4262/raw_data/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI002S4262/raw_data/interpretation_mean_pertrub_mask_T1_sub-ADNI002S4262_ses-M60.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI002S4262/raw_data/interpretation_guided_backprop_mask_T1_sub-ADNI002S4262_ses-M60.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI002S4262/raw_data/interpretation_gradcam_mask_mask_T1_sub-ADNI002S4262_ses-M60.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI002S4262/raw_data/sub-ADNI002S4262_ses-M00_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI002S4262/raw_data/ground_truth_mask_sub-ADNI002S4262.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI002S4262/raw_data/sub-ADNI002S4262_ses-M60_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI002S4262/raw_data/.ipynb_checkpoints/\n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI002S4262/ground_truth/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI002S4262/ground_truth/slices_from_inferior_to_superior.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI002S4262/ground_truth/gt_slices.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI002S4262/ground_truth/slices_from_left_to_right.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI002S4262/ground_truth/slices_from_from_anterior_to_posterior.png  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI002S4262/ground_truth/.ipynb_checkpoints/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI002S4262/ground_truth/.ipynb_checkpoints/gt_slices-checkpoint.png  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI002S4262/.ipynb_checkpoints/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI002S4262/.ipynb_checkpoints/interpretation_gradcam_mask-checkpoint.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI002S4262/.ipynb_checkpoints/interpretation_mean_pertrub-checkpoint.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI002S4262/.ipynb_checkpoints/sub-ADNI002S4262_ses-M60_T1_ad-checkpoint.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI002S4262/.ipynb_checkpoints/interpretation_guided_backprop-checkpoint.png  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S4706/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S4706/df_scores.csv  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S4706/sub-ADNI037S4706_ses-M12_T1_ad.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S4706/interpretation_mean_pertrub.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S4706/interpretation_guided_backprop.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S4706/sub-ADNI037S4706_ses-M48_T1_cn.png  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S4706/raw_data/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S4706/raw_data/interpretation_mean_pertrub_mask_T1_sub-ADNI037S4706_ses-M12.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S4706/raw_data/sub-ADNI037S4706_ses-M12_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S4706/raw_data/sub-ADNI037S4706_ses-M48_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S4706/raw_data/interpretation_guided_backprop_mask_T1_sub-ADNI037S4706_ses-M12.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S4706/raw_data/ground_truth_mask_sub-ADNI037S4706.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S4706/raw_data/interpretation_gradcam_mask_mask_T1_sub-ADNI037S4706_ses-M12.nii.gz  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S4706/ground_truth/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S4706/ground_truth/slices_from_inferior_to_superior.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S4706/ground_truth/gt_slices.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S4706/ground_truth/slices_from_left_to_right.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S4706/ground_truth/slices_from_from_anterior_to_posterior.png  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S4706/.ipynb_checkpoints/\n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S0467/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S0467/df_scores.csv  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S0467/sub-ADNI037S0467_ses-M00_T1_cn.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S0467/interpretation_mean_pertrub.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S0467/interpretation_guided_backprop.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S0467/interpretation_gradcam_mask.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S0467/sub-ADNI037S0467_ses-M108_T1_ad.png  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S0467/raw_data/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S0467/raw_data/interpretation_mean_pertrub_mask_T1_sub-ADNI037S0467_ses-M108.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S0467/raw_data/interpretation_gradcam_mask_mask_T1_sub-ADNI037S0467_ses-M108.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S0467/raw_data/ground_truth_mask_sub-ADNI037S0467.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S0467/raw_data/sub-ADNI037S0467_ses-M00_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S0467/raw_data/sub-ADNI037S0467_ses-M108_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S0467/raw_data/interpretation_guided_backprop_mask_T1_sub-ADNI037S0467_ses-M108.nii.gz  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S0467/ground_truth/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S0467/ground_truth/slices_from_inferior_to_superior.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S0467/ground_truth/gt_slices.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S0467/ground_truth/slices_from_left_to_right.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S0467/ground_truth/slices_from_from_anterior_to_posterior.png  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S0467/.ipynb_checkpoints/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI037S0467/.ipynb_checkpoints/interpretation_mean_pertrub-checkpoint.png  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI123S0106/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI123S0106/df_scores.csv  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI123S0106/sub-ADNI123S0106_ses-M36_T1_cn.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI123S0106/interpretation_mean_pertrub.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI123S0106/interpretation_guided_backprop.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI123S0106/interpretation_gradcam_mask.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI123S0106/sub-ADNI123S0106_ses-M108_T1_ad.png  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI123S0106/raw_data/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI123S0106/raw_data/interpretation_gradcam_mask_mask_T1_sub-ADNI123S0106_ses-M108.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI123S0106/raw_data/ground_truth_mask_sub-ADNI123S0106.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI123S0106/raw_data/sub-ADNI123S0106_ses-M36_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI123S0106/raw_data/interpretation_guided_backprop_mask_T1_sub-ADNI123S0106_ses-M108.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI123S0106/raw_data/sub-ADNI123S0106_ses-M108_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI123S0106/raw_data/interpretation_mean_pertrub_mask_T1_sub-ADNI123S0106_ses-M108.nii.gz  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI123S0106/ground_truth/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI123S0106/ground_truth/slices_from_inferior_to_superior.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI123S0106/ground_truth/gt_slices.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI123S0106/ground_truth/slices_from_left_to_right.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI123S0106/ground_truth/slices_from_from_anterior_to_posterior.png  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI023S0061/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI023S0061/df_scores.csv  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI023S0061/sub-ADNI023S0061_ses-M108_T1_ad.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI023S0061/interpretation_mean_pertrub.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI023S0061/interpretation_guided_backprop.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI023S0061/interpretation_gradcam_mask.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI023S0061/sub-ADNI023S0061_ses-M00_T1_cn.png  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI023S0061/raw_data/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI023S0061/raw_data/interpretation_gradcam_mask_mask_T1_sub-ADNI023S0061_ses-M108.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI023S0061/raw_data/interpretation_mean_pertrub_mask_T1_sub-ADNI023S0061_ses-M108.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI023S0061/raw_data/ground_truth_mask_sub-ADNI023S0061.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI023S0061/raw_data/interpretation_guided_backprop_mask_T1_sub-ADNI023S0061_ses-M108.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI023S0061/raw_data/sub-ADNI023S0061_ses-M108_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI023S0061/raw_data/sub-ADNI023S0061_ses-M00_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI023S0061/ground_truth/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI023S0061/ground_truth/slices_from_inferior_to_superior.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI023S0061/ground_truth/gt_slices.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI023S0061/ground_truth/slices_from_left_to_right.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI023S0061/ground_truth/slices_from_from_anterior_to_posterior.png  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI023S0061/ground_truth/.ipynb_checkpoints/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI023S0061/ground_truth/.ipynb_checkpoints/slices_from_from_anterior_to_posterior-checkpoint.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI023S0061/ground_truth/.ipynb_checkpoints/gt_slices-checkpoint.png  \n   creating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI023S0061/.ipynb_checkpoints/\n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI023S0061/.ipynb_checkpoints/interpretation_mean_pertrub-checkpoint.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI023S0061/.ipynb_checkpoints/sub-ADNI023S0061_ses-M00_T1_cn-checkpoint.png  \n  inflating: 10_sub_for_validation/10_sub_for_validation/sub-ADNI023S0061/.ipynb_checkpoints/sub-ADNI023S0061_ses-M108_T1_ad-checkpoint.png  \n   creating: 10_sub_for_validation/labels_list_10/\n  inflating: 10_sub_for_validation/labels_list_10/getlabels.json  \n  inflating: 10_sub_for_validation/labels_list_10/AD.tsv  \n  inflating: 10_sub_for_validation/labels_list_10/CN.tsv  \n  inflating: 10_sub_for_validation/labels_list_10/pMCI.tsv  \n  inflating: 10_sub_for_validation/labels_list_10/sMCI.tsv  \n   creating: 10_sub_for_validation/model/\n  inflating: 10_sub_for_validation/model/model_best.pth.tar"
  },
  {
    "objectID": "projects/XAI_3D_CNN.html#pipeline-with-dataset-and-model",
    "href": "projects/XAI_3D_CNN.html#pipeline-with-dataset-and-model",
    "title": "Interpretation of 3D CNNs for Brain MRI Data Classification",
    "section": "Pipeline with dataset and model",
    "text": "Pipeline with dataset and model\n\nWriting dataset\n\nFILENAME_TYPE = {\n    'full': '_T1w_space-MNI152NLin2009cSym_res-1x1x1_T1w',\n    'cropped': '_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w',\n    'skull_stripped': '_space-Ixi549Space_desc-skullstripped_T1w',\n    'gm_maps': '_T1w_segm-graymatter_space-Ixi549Space_modulated-off_probability',\n    'shepplogan': '_phantom-SheppLogan'\n}\n\n\nclass ToTensor(object):\n    \"\"\"Convert image type to Tensor and diagnosis to diagnosis code\"\"\"\n\n    def __call__(self, image):\n        np.nan_to_num(image, copy=False)\n        image = image.astype(float)\n\n        return torch.from_numpy(image[np.newaxis, :]).float()\n\n\ndef get_nii_path(caps_dict, participant_id, session_id, cohort, preprocessing):\n\n    if cohort not in caps_dict.keys():\n        raise ValueError('Cohort names in labels and CAPS definitions do not match.')\n\n    image_path = path.join(caps_dict[cohort], participant_id, 'raw_data',\n                               participant_id + '_' + session_id +\n                               FILENAME_TYPE['cropped'] + '.nii.gz')\n    return image_path\n\nclass MRIDataset(Dataset):\n    \"\"\"Abstract class for all derived MRIDatasets.\"\"\"\n\n    def __init__(self, caps_directory, data_file,\n                 preprocessing, transformations, labels,\n                 augmentation_transformations=None, multi_cohort=False):\n        self.caps_dict = self.create_caps_dict(caps_directory, multi_cohort)\n        self.transformations = transformations\n        self.augmentation_transformations = augmentation_transformations\n        self.eval_mode = False\n        self.labels = labels\n        self.diagnosis_code = {\n            'CN': 0,\n            'BV': 1,\n            'AD': 1,\n            'sMCI': 0,\n            'pMCI': 1,\n            'MCI': 1,\n            'unlabeled': -1}\n        self.preprocessing = preprocessing\n\n        if not hasattr(self, 'elem_index'):\n            raise ValueError(\n                \"Child class of MRIDataset must set elem_index attribute.\")\n        if not hasattr(self, 'mode'):\n            raise ValueError(\n                \"Child class of MRIDataset must set mode attribute.\")\n\n        # Check the format of the tsv file here\n        if isinstance(data_file, str):\n            self.df = pd.read_csv(data_file, sep='\\t')\n        elif isinstance(data_file, pd.DataFrame):\n            self.df = data_file\n        else:\n            raise Exception('The argument data_file is not of correct type.')\n\n        if not multi_cohort:\n            self.df[\"cohort\"] = \"single\"\n\n        mandatory_col = {\"participant_id\", \"session_id\"}\n        if self.labels:\n            mandatory_col.add(\"diagnosis\")\n        if multi_cohort:\n            mandatory_col.add(\"cohort\")\n        if self.elem_index == \"mixed\":\n            mandatory_col.add(\"%s_id\" % self.mode)\n\n        if not mandatory_col.issubset(set(self.df.columns.values)):\n            raise Exception(\"the data file is not in the correct format.\"\n                            \"Columns should include %s\" % mandatory_col)\n\n        unique_diagnoses = set(self.df.diagnosis)\n        unique_codes = set()\n        for diagnosis in unique_diagnoses:\n            unique_codes.add(self.diagnosis_code[diagnosis])\n        self.elem_per_image = self.num_elem_per_image()\n        self.size = self[0]['image'].size()\n\n    def __len__(self):\n        return len(self.df) * self.elem_per_image\n\n    @staticmethod\n    def create_caps_dict(caps_directory, multi_cohort):\n        caps_dict = {'single': caps_directory}\n        return caps_dict\n\n    def _get_path(self, participant, session, cohort, mode=\"image\"):\n\n        if cohort not in self.caps_dict.keys():\n            raise ValueError('Cohort names in labels and CAPS definitions do not match.')\n\n        image_path = path.join(self.caps_dict[cohort], participant, 'raw_data',\n                                      participant + '_' + session\n                                   + FILENAME_TYPE['cropped'] +'.nii.gz')\n        # image_path = path.join(self.caps_dict[cohort], participant, session,\n        #                            'deeplearning_prepare_data', '%s_based' % mode, 't1_linear',\n        #                            participant + '_' + session\n        #                            + FILENAME_TYPE['cropped'] + '.pt')\n\n        return image_path\n\n    def _get_meta_data(self, idx):\n        image_idx = idx // self.elem_per_image\n        participant = self.df.loc[image_idx, 'participant_id']\n        session = self.df.loc[image_idx, 'session_id']\n        cohort = self.df.loc[image_idx, 'cohort']\n\n        if self.elem_index is None:\n            elem_idx = idx % self.elem_per_image\n        elif self.elem_index == \"mixed\":\n            elem_idx = self.df.loc[image_idx, '%s_id' % self.mode]\n        else:\n            elem_idx = self.elem_index\n\n        if self.labels:\n            diagnosis = self.df.loc[image_idx, 'diagnosis']\n            label = self.diagnosis_code[diagnosis]\n        else:\n            label = self.diagnosis_code['unlabeled']\n\n        return participant, session, cohort, elem_idx, label\n\n    def _get_full_image(self):\n\n        participant_id = self.df.loc[0, 'participant_id']\n        session_id = self.df.loc[0, 'session_id']\n        cohort = self.df.loc[0, 'cohort']\n\n        # try:\n        #     image_path = self._get_path(participant_id, session_id, cohort, mode=\"image\")\n        #     image = torch.load(image_path)\n        # except FileNotFoundError:\n        image_path = get_nii_path(\n                self.caps_dict,\n                participant_id,\n                session_id,\n                cohort=cohort,\n                preprocessing=self.preprocessing)\n        print(image_path)\n        image_nii = nib.load(image_path)\n        image_np = image_nii.get_fdata()\n        image = ToTensor()(image_np)\n\n        return image\n\n    @abc.abstractmethod\n    def __getitem__(self, idx):\n        pass\n\n    @abc.abstractmethod\n    def num_elem_per_image(self):\n        pass\n\n    def eval(self):\n        self.eval_mode = True\n        return self\n\n    def train(self):\n        self.eval_mode = False\n        return self\n\n\nclass MRIDatasetImage(MRIDataset):\n    \"\"\"Dataset of MRI organized in a CAPS folder.\"\"\"\n\n    def __init__(self, caps_directory, data_df,\n                 preprocessing='t1-linear', train_transformations=None,\n                 labels=True, all_transformations=None, multi_cohort=False):\n        \"\"\"\n        Args:\n            caps_directory (string): Directory of all the images.\n            data_file (string or DataFrame): Path to the tsv file or DataFrame containing the subject/session list.\n            preprocessing (string): Defines the path to the data in CAPS.\n            train_transformations (callable, optional): Optional transform to be applied only on training mode.\n            labels (bool): If True the diagnosis will be extracted from the given DataFrame.\n            all_transformations (callable, options): Optional transform to be applied during training and evaluation.\n            multi_cohort (bool): If True caps_directory is the path to a TSV file linking cohort names and paths.\n        \"\"\"\n        self.elem_index = None\n        self.mode = \"image\"\n        super().__init__(caps_directory, data_df, preprocessing,\n                         augmentation_transformations=train_transformations, labels=labels,\n                         transformations=all_transformations, multi_cohort=multi_cohort)\n\n    def __getitem__(self, idx):\n        participant, session, cohort, _, label = self._get_meta_data(idx)\n        image_path =  get_nii_path(self.caps_dict, participant, session, cohort, self.preprocessing)\n        image_nii = nib.load(image_path)\n        image_np = image_nii.get_fdata()\n        image = ToTensor()(image_np)\n        # image_path = self._get_path(participant, session, cohort, \"image\")\n        # image = torch.load(image_path)\n\n        if self.transformations:\n            image = self.transformations(image)\n\n        if self.augmentation_transformations and not self.eval_mode:\n            image = self.augmentation_transformations(image)\n\n        sample = {'image': image, 'label': label, 'participant_id': participant, 'session_id': session,\n                  'image_path': image_path}\n\n        return sample\n\n    def num_elem_per_image(self):\n        return 1\n\nclass MinMaxNormalization(object):\n    \"\"\"Normalizes a tensor between 0 and 1\"\"\"\n\n    def __call__(self, image):\n        return (image - image.min()) / (image.max() - image.min())\n\ndef get_transforms(mode, minmaxnormalization=True, data_augmentation=None):\n    \"\"\"\n    Outputs the transformations that will be applied to the dataset\n    :param mode: (str) input used by the network. Chosen from ['image', 'patch', 'roi', 'slice'].\n    :param minmaxnormalization: (bool) if True will perform MinMaxNormalization\n    :param data_augmentation: (list[str]) list of data augmentation performed on the training set.\n    :return:\n    - container transforms.Compose including transforms to apply in train and evaluation mode.\n    - container transforms.Compose including transforms to apply in evaluation mode only.\n    \"\"\"\n    augmentation_dict = {\"None\": None}\n    if data_augmentation:\n        augmentation_list = [augmentation_dict[augmentation] for augmentation in data_augmentation]\n    else:\n        augmentation_list = []\n\n    if minmaxnormalization:\n        transformations_list = [MinMaxNormalization()]\n    else:\n        transformations_list = []\n\n    all_transformations = transforms.Compose(transformations_list)\n    train_transformations = transforms.Compose(augmentation_list)\n\n    return train_transformations, all_transformations\n\n\n\nDefine model\n\nclass Conv5_FC3(nn.Module):\n    \"\"\"\n    Classifier for a binary classification task\n    Image level architecture used on Minimal preprocessing\n    \"\"\"\n    def __init__(self, dropout=0.5):\n        super(Conv5_FC3, self).__init__()\n\n        self.features = nn.Sequential(\n            nn.Conv3d(1, 8, 3, padding=1),\n            nn.BatchNorm3d(8),\n            nn.ReLU(),\n            PadMaxPool3d(2, 2),\n\n            nn.Conv3d(8, 16, 3, padding=1),\n            nn.BatchNorm3d(16),\n            nn.ReLU(),\n            PadMaxPool3d(2, 2),\n\n            nn.Conv3d(16, 32, 3, padding=1),\n            nn.BatchNorm3d(32),\n            nn.ReLU(),\n            PadMaxPool3d(2, 2),\n\n            nn.Conv3d(32, 64, 3, padding=1),\n            nn.BatchNorm3d(64),\n            nn.ReLU(),\n            PadMaxPool3d(2, 2),\n\n            nn.Conv3d(64, 128, 3, padding=1),\n            nn.BatchNorm3d(128),\n            nn.ReLU(),\n            PadMaxPool3d(2, 2),\n\n        )\n\n        self.classifier = nn.Sequential(\n            Flatten(),\n            nn.Dropout(p=0.5),\n\n            nn.Linear(128 * 6 * 7 * 6, 1300),\n            nn.ReLU(),\n\n            nn.Linear(1300, 50),\n            nn.ReLU(),\n\n            nn.Linear(50, 2)\n\n        )\n        self.gradients = None\n        self.flattened_shape = [-1, 128, 6, 7, 6]\n\n    def activations_hook(self, grad):\n        self.gradients = grad\n\n    def forward(self, x):\n        x = self.features(x)\n        if self.train and x.requires_grad:\n            h = x.register_hook(self.activations_hook)\n        x = self.classifier(x)\n\n        return x\n\n    def get_activations_gradient(self):\n        return self.gradients\n\n    def get_activations(self, x):\n        return self.features(x)\n\nclass Flatten(nn.Module):\n    def forward(self, input):\n        return input.view(input.size(0), -1)\n\nclass PadMaxPool3d(nn.Module):\n    def __init__(self, kernel_size, stride, return_indices=False, return_pad=False):\n        super(PadMaxPool3d, self).__init__()\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.pool = nn.MaxPool3d(\n            kernel_size, stride, return_indices=return_indices)\n        self.pad = nn.ConstantPad3d(padding=0, value=0)\n        self.return_indices = return_indices\n        self.return_pad = return_pad\n\n    def set_new_return(self, return_indices=True, return_pad=True):\n        self.return_indices = return_indices\n        self.return_pad = return_pad\n        self.pool.return_indices = return_indices\n\n    def forward(self, f_maps):\n        coords = [self.stride -\n                  f_maps.size(i + 2) % self.stride for i in range(3)]\n        for i, coord in enumerate(coords):\n            if coord == self.stride:\n                coords[i] = 0\n\n        self.pad.padding = (coords[2], 0, coords[1], 0, coords[0], 0)\n\n        if self.return_indices:\n            output, indices = self.pool(self.pad(f_maps))\n\n            if self.return_pad:\n                return output, indices, (coords[2], 0, coords[1], 0, coords[0], 0)\n            else:\n                return output, indices\n\n        else:\n            output = self.pool(self.pad(f_maps))\n\n            if self.return_pad:\n                return output, (coords[2], 0, coords[1], 0, coords[0], 0)\n            else:\n                return output"
  },
  {
    "objectID": "projects/XAI_3D_CNN.html#interpretation-methods",
    "href": "projects/XAI_3D_CNN.html#interpretation-methods",
    "title": "Interpretation of 3D CNNs for Brain MRI Data Classification",
    "section": "Interpretation methods",
    "text": "Interpretation methods\n\nGrad-cam\nThe Idea: to take the gradients of the target class flowing into the final convolutional layer to produce a heatmap highlighting the important regions in the image to predict the concept.\n\nSelect the class of interest ( target class)\n–°alculate the gradient of the class logit nd the activation maps\nAverage over activations using the global average pooling\nObtain he neuron importance weights coefficients Œ± k c for each map (this weight Œ± c k represents a partial linearization of the deep network downstream from A, and captures the ‚Äòimportance‚Äô of feature map k for a target class c)\nConsider a linear combination\nApply ReLU\nInterpolate the heat-map (increase the dimension)\n\n\nclass GradCam():\n    def __init__(self, model):\n        self.model = model\n\n    def grad_cam(self, input, logit, size):\n        logit[:, logit.data.max(1)[1]].backward() ## get the gradient of the output with respect to the parameters of the model\n            #             logit[:,0].backward()\n        activation = self.model.get_activations(input).detach() # get the activations of the last convolutional layer\n        act_grad = self.model.get_activations_gradient() # pull the gradients out of the model\n        pool_act_grad = torch.mean(act_grad, dim=[2, 3, 4], keepdim=True) # pool the gradients across the channels\n        activation = activation * pool_act_grad # weight the channels by corresponding gradients\n        heatmap = torch.sum(activation, dim=1) #\n        heatmap = F.relu(heatmap) # relu on top of the heatmap\n        heatmap /= torch.max(heatmap) # normalize the heatmap\n        heatmap = F.interpolate(heatmap.unsqueeze(0), size[1:], mode='trilinear', align_corners=False)  # 58 70 58 interpolate the heat-map and project it onto the original image\n        return heatmap\n\n\ndef get_masks(model, loader, mean_mask = True, mask_type='grad_cam', size=(180, 180, 180), save = None, save_binary=None, task='AD_CN'):\n    masks = []\n    labels = []\n    output_dir = '/output'\n    mask_dir = os.path.join(output_dir, 'img_mask')\n    os.makedirs(mask_dir, exist_ok=True)\n    for i, data in tqdm(enumerate(loader, 0)):\n        image = data['image'].cuda()\n        labels.append(data['label'].numpy().item())\n        logit = model(image)\n        if mask_type == 'grad_cam':\n            gc = GradCam(model)\n            heatmap = gc.grad_cam(image, logit, size)\n            masks.append(heatmap.cpu().numpy())\n            name = data['image_path'][0][-80:-53]\n            if save:\n                nib.save(nib.Nifti1Image(heatmap.cpu().numpy(), affine=np.eye(4)),\n                     os.path.join(mask_dir, '{}_gradcam_mask.nii.gz'.format(name)))\n            if save_binary:\n                mask_binary_dir = os.path.join(output_dir, 'img_mask_binary')\n                os.makedirs(mask_binary_dir, exist_ok=True)\n                binary = heatmap.cpu().numpy()[heatmap.cpu().numpy() &lt;= 0.35] = 0\n                nib.save(nib.Nifti1Image(binary, affine=np.eye(4)),\n                         os.path.join(mask_binary_dir, '{}_gradcam_mask.nii.gz'.format(name)))\n            del image, heatmap\n        elif mask_type == 'guided_backprop':\n            gp = GuidedBackprop(model)\n            pred = logit.data.max(1)[1].item()\n            img_grad = gp.guided_backprop(image, pred)\n            masks.append(img_grad)\n            del image, img_grad, pred\n        elif mask_type == 'mean_pertrub':\n            mp = MeanPertrub(rep=9)\n            pred = logit.data.max(1)[1].item()\n            for param in model.parameters():\n                param.requires_grad = False\n            masks_pertrub = mp.get_masks(image, pred, model)\n            masks.append(masks_pertrub)\n            del image, masks_pertrub, pred\n        else:\n            raise NotImplementedType('define mask_type')\n    if mean_mask:\n            name = data['image_path'][0][-80:-53]\n            concat = np.concatenate(masks, axis=0).squeeze(axis=1)\n            labels_cn = np.array(labels) == 0\n            labels_ad = np.array(labels) == 1\n            mean_0 = concat[labels_cn].mean(axis=0)\n            mean_1 = concat[labels_ad].mean(axis=0)\n            nib.save(nib.Nifti1Image(mean_0, affine=np.eye(4)),\n                     os.path.join(output_dir, '{}_{}_mean_0_{}.nii.gz'.format(name, mask_type, task)))\n            nib.save(nib.Nifti1Image(mean_1, affine=np.eye(4)),\n                     os.path.join(output_dir, '{}_{}_mean_1_{}.nii.gz'.format(name, mask_type, task)))\n            return mean_0, mean_1\n\n\ntsv_path = Path('10_sub_for_validation/labels_list_10')\ncaps = Path(\"10_sub_for_validation/10_sub_for_validation\")\ntask_diagnoses = ['AD','CN']\nmask_type = 'grad_cam'\nsource_path = Path('10_sub_for_validation/model')\ntask = 'test'\n\n\n!cd 10_sub_for_validation/10_sub_for_validation/sub-ADNI002S4262/raw_data && ls\n\nground_truth_mask_sub-ADNI002S4262.nii.gz\ninterpretation_gradcam_mask_mask_T1_sub-ADNI002S4262_ses-M60.nii.gz\ninterpretation_guided_backprop_mask_T1_sub-ADNI002S4262_ses-M60.nii.gz\ninterpretation_mean_pertrub_mask_T1_sub-ADNI002S4262_ses-M60.nii.gz\nsub-ADNI002S4262_ses-M00_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz\nsub-ADNI002S4262_ses-M60_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz\n\n\n\nmodel = Conv5_FC3().eval()\nmodel.cuda()\nmodel_path = os.path.join(source_path, \"model_best.pth.tar\")\nresults = torch.load(model_path, map_location='cuda')\nmodel.load_state_dict(results['model'])\ntest_transforms, all_transforms = get_transforms('image',\n                                                      minmaxnormalization=True,\n                                                      data_augmentation=None)\ntest_df = pd.DataFrame()\n\nfor diagnosis in task_diagnoses:\n    test_diagnosis_path = path.join(\n                tsv_path, diagnosis + '.tsv')\n\n    test_diagnosis_df = pd.read_csv(test_diagnosis_path, sep='\\t')\n\n    test_df = pd.concat([test_df, test_diagnosis_df])\n\n    test_df.reset_index(inplace=True, drop=True)\n    test_df[\"cohort\"] = \"single\"\n\n    data_test = MRIDatasetImage(caps, data_df=test_df, preprocessing='t1-linear',\n                                    train_transformations=test_transforms, all_transformations=all_transforms,\n                                    labels=True)\n    test_loader = DataLoader(data_test, batch_size=1, shuffle=False,\n                                 num_workers=0, pin_memory=True)\n    _, mean_1 = get_masks(model, test_loader, mean_mask=True, mask_type=mask_type,\n                  size=data_test.size, task=task, save=True)\n\nFutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  results = torch.load(model_path, map_location='cuda')\n10it [00:19,  1.95s/it]\n&lt;ipython-input-8-e734e7606190&gt;:47: RuntimeWarning: Mean of empty slice.\n  mean_0 = concat[labels_cn].mean(axis=0)\n/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:121: RuntimeWarning: invalid value encountered in divide\n  ret = um.true_divide(\n20it [00:29,  1.47s/it]\n\n\n\n\nDisplay Grad-cam Interpetation\n\ndef display_interpretation(interp_img, data_img, cut_coords=(40, 25, 55), threshold=0.35, name = 'mean'):\n    fig, axes = plt.subplots(figsize=(16, 8))\n    roi_img = nib.Nifti1Image(interp_img, affine=np.eye(4))\n    bim_img = nib.Nifti1Image(np.squeeze(data_img['image']).cpu().detach().numpy(), affine=np.eye(4))\n    if cut_coords is None:\n        plotting.plot_roi(roi_img, bim_img, axes=axes, colorbar=True, cmap='jet',\n                          threshold=threshold)\n    else:\n        plotting.plot_roi(roi_img, bim_img, cut_coords=cut_coords, axes=axes, colorbar=True, cmap='jet', threshold=threshold)\n    plt.show()\n    fig.savefig(\"grad_cam_{}\".format(name), bbox_inches='tight')\n\n\n\ntest_loader.dataset[0]['image_path'], test_loader.dataset[0]['label']\n\n('10_sub_for_validation/10_sub_for_validation/sub-ADNI002S4262/raw_data/sub-ADNI002S4262_ses-M60_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz',\n 1)\n\n\n\ndataset_img = data_test.__getitem__(0)\ndisplay_interpretation(mean_1, dataset_img, threshold=0.30)\n\n/usr/local/lib/python3.10/dist-packages/numpy/ma/core.py:2820: UserWarning: Warning: converting a masked element to nan.\n  _data = np.array(data, dtype=dtype, copy=copy,\n\n\n\n\n\n\n\n\n\n\n\nGuided Backprop\nIdea: using gradient back propagation as it is except at the ReLU stages. Guided Backpropagation basically combines vanilla backpropagation and DeconvNets when handling the ReLU nonlinearity: - Like DeconvNets, in Guided Backpropagation we only backpropagate positive error signals ‚Äì i.e.¬†we set the negative gradients to zero (ref). This is the application of the ReLU to the error signal itself during the backward pass. - Like vanilla backpropagation, we also restrict ourselves to only positive inputs.\nThus, the gradient is ‚Äúguided‚Äù by both the input and the error signal.\n\nclass GuidedBackprop():\n    def __init__(self, model):\n        self.model = model\n\n    def guided_backprop(self, input, label):\n\n        def hookfunc(module, gradInput, gradOutput):\n            return tuple([(None if g is None else g.clamp(min=0)) for g in gradInput])\n\n        input.requires_grad = True\n        h = [0] * len(list(self.model.features) + list(self.model.classifier))\n        for i, module in enumerate(list(self.model.features) + list(self.model.classifier)):\n            if type(module) == nn.ReLU:\n                h[i] = module.register_backward_hook(hookfunc)\n\n        self.model.eval()\n        output = self.model(input)\n        self.model.zero_grad()\n        output[0][label].backward()\n        grad = input.grad.data\n        grad /= grad.max()\n        return np.clip(grad.cpu().numpy(), 0, 1)\n\n\nmean_0_gp, mean_1_gp = get_masks(model, test_loader, mean_mask=True, mask_type='guided_backprop',\n                  size=data_test.size)\n\n20it [00:11,  1.72it/s]\n\n\n\ndataset_img = data_test.__getitem__(0)\ndisplay_interpretation(mean_1_gp, dataset_img, threshold=0.015)\n\n\n\n\n\n\n\n\n\ndataset_img = data_test.__getitem__(10)\ndisplay_interpretation(mean_0_gp, dataset_img, threshold=0.015)\n\n\n\n\n\n\n\n\n\n\nMeanPertrub\n\nfrom scipy.ndimage import gaussian_filter\nfrom torch.optim import Adam\ndef jittering(img, jit, C, D, H, W):\n    return np.pad(img, [(0, 0), (0, jit), (0, jit), (0, jit)], mode='constant')\n\n\ndef upsample(mask, img_size):\n    x = F.interpolate(mask, size=img_size, mode='trilinear', align_corners=False)\n    return x\n\n\ndef np_to_torch(X, img_size, requires_grad=False):\n    output = torch.tensor(X, requires_grad=requires_grad).cuda()\n    return output.reshape(img_size)\n\n\ndef tv_norm(x, beta=1):\n    d1 = torch.mean(torch.abs(x[:, :, :-1, :, :] - x[:, :, 1:, :, :]).pow(beta))\n    d2 = torch.mean(torch.abs(x[:, :, :, :-1, :] - x[:, :, :, 1:, :]).pow(beta))\n    d3 = torch.mean(torch.abs(x[:, :, :, :, :-1] - x[:, :, :, :, 1:]).pow(beta))\n    tv = d1 + d2 + d3\n    return tv\n\nclass GaussianFilter(nn.Module):\n    def __init__(self, k_size, g_filter):\n        super(GaussianFilter, self).__init__()\n        # self.device = device\n        pad = (k_size - 1) // 2\n        self.k_size = k_size\n        self.conv = nn.Conv3d(1, 1, k_size, padding=(pad, pad, pad), bias=None)\n        self.conv.cuda()\n        self.g_filter = g_filter\n\n    def forward(self, x, sigma):\n        n = np.zeros((self.k_size, self.k_size, self.k_size))\n        n[self.k_size // 2 + 1, self.k_size // 2 + 1, self.k_size // 2 + 1] = 1\n        k = self.g_filter(n, sigma=sigma)[None, None, :, :, :]\n        self.conv.weight = torch.nn.Parameter(torch.from_numpy(k).float().cuda())\n        for param in self.conv.parameters():\n            param.requires_grad = False\n        return self.conv(x)\n\n\nclass MeanPertrub():\n    def __init__(self, mask_scale=4, blur_img=10, blur_mask=10, max_iter=300,\n                 l1_coef=3, tv_coef=1, tv_beta=7, rep=10, jit=5, k_size=5, lr=1e-4):\n        # self.device = device\n        self.lr = lr\n        self.mask_scale = 4\n        self.blur_img = blur_img\n        self.blur_mask = blur_mask\n        self.max_iter = max_iter\n        self.l1_coef = l1_coef\n        self.tv_coef = tv_coef\n        self.tv_beta = tv_beta\n        self.rep = rep\n        self.jit = jit\n        self.filter_gaus = GaussianFilter(k_size, gaussian_filter)\n\n    def get_masks(self, img, pred, model):\n        res = []\n        rw_max = self.max_iter // 5\n        i = 0\n        img = img.squeeze(axis=0)\n        C, D, H, W = img.shape\n        model_ans = pred\n        mask = torch.ones((1, C, D // self.mask_scale, H // self.mask_scale, W // self.mask_scale),\n                          requires_grad=True, device='cuda')\n        optimizer = Adam([mask], lr=self.lr, betas=(0.9, 0.99), amsgrad=True, weight_decay=1e-4)\n        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n        best_loss, best_mask = float('inf'), None\n        for epoch in tqdm(range(self.max_iter)):\n            mask_up = upsample(mask, img_size=(D, H, W))\n            mask_up = self.filter_gaus(mask_up, self.blur_mask)\n            total_pred_loss = 0\n            for _ in range(self.rep):\n                img_jit = jittering(img.cpu(), self.jit, C, D, H, W)\n                j0 = np.random.randint(self.jit)\n                j1 = np.random.randint(self.jit)\n                j2 = np.random.randint(self.jit)\n                img_jit = img_jit[:, j0:(D + j0), j1:(H + j1), j2:(W + j2)]\n                img_torch = np_to_torch(img_jit, img_size=(1, C, D, H, W), requires_grad=False)\n                blur = self.filter_gaus(img_torch, self.blur_img)\n                perturbated_input = img_torch.mul(mask_up) + blur.mul(1 - mask_up).cuda()\n                outputs = model(perturbated_input.float())  # problem\n                prob = torch.exp(outputs)\n                total_pred_loss += F.relu(prob[0, model_ans] - 0.05)\n                del outputs, prob, perturbated_input, blur, img_torch, img_jit\n            reg_loss = self.l1_coef * torch.mean(torch.abs(1 - mask)) + self.tv_coef * tv_norm(mask_up,\n                                                                                               self.tv_beta)\n            rw = 1 if epoch &gt; rw_max else epoch / rw_max\n            loss = total_pred_loss / self.rep + rw * reg_loss\n\n            if epoch &gt; 50 and loss.item() &lt;= best_loss:\n                best_loss = loss.item()\n                best_mask = mask.clone().detach()\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            mask.data.clamp_(0, 1)\n\n        res_mask = upsample((1 - best_mask), img_size=(D, H, W))\n        res.append(res_mask.cpu().numpy())\n        i += 1\n        return res_mask.cpu().numpy()\n\n\nmean_0_mp, mean_1_mp = get_masks(model, test_loader, mean_mask=True, mask_type='mean_pertrub',\n                  size=data_test.size)\n\n\nStreaming output truncated to the last 5000 lines.\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 134/300 [03:08&lt;03:53,  1.41s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 135/300 [03:10&lt;03:52,  1.41s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 136/300 [03:11&lt;03:50,  1.41s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 137/300 [03:12&lt;03:49,  1.41s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 138/300 [03:14&lt;03:47,  1.40s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 139/300 [03:15&lt;03:46,  1.41s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 140/300 [03:17&lt;03:45,  1.41s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 141/300 [03:18&lt;03:44,  1.41s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 142/300 [03:19&lt;03:42,  1.41s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 143/300 [03:21&lt;03:40,  1.41s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 144/300 [03:22&lt;03:40,  1.41s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 145/300 [03:24&lt;03:38,  1.41s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 146/300 [03:25&lt;03:36,  1.41s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 147/300 [03:26&lt;03:35,  1.41s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 148/300 [03:28&lt;03:33,  1.41s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 149/300 [03:29&lt;03:33,  1.41s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 150/300 [03:31&lt;03:31,  1.41s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 151/300 [03:32&lt;03:29,  1.41s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 152/300 [03:33&lt;03:28,  1.41s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 153/300 [03:35&lt;03:26,  1.41s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 154/300 [03:36&lt;03:26,  1.41s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 155/300 [03:38&lt;03:24,  1.41s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 156/300 [03:39&lt;03:23,  1.41s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 157/300 [03:41&lt;03:21,  1.41s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 158/300 [03:42&lt;03:22,  1.43s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 159/300 [03:43&lt;03:22,  1.44s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 160/300 [03:45&lt;03:20,  1.43s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 161/300 [03:46&lt;03:18,  1.43s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 162/300 [03:48&lt;03:15,  1.42s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 163/300 [03:49&lt;03:13,  1.41s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 164/300 [03:50&lt;03:12,  1.41s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 165/300 [03:52&lt;03:10,  1.41s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 166/300 [03:53&lt;03:09,  1.41s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 167/300 [03:55&lt;03:07,  1.41s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 168/300 [03:56&lt;03:07,  1.42s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 169/300 [03:58&lt;03:05,  1.41s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 170/300 [03:59&lt;03:03,  1.41s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 171/300 [04:00&lt;03:01,  1.41s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 172/300 [04:02&lt;02:59,  1.40s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 173/300 [04:03&lt;02:57,  1.40s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 174/300 [04:05&lt;02:56,  1.40s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 175/300 [04:06&lt;02:55,  1.40s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 176/300 [04:07&lt;02:54,  1.41s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 177/300 [04:09&lt;02:52,  1.41s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 178/300 [04:10&lt;02:52,  1.41s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 179/300 [04:12&lt;02:51,  1.41s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 180/300 [04:13&lt;02:49,  1.42s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 181/300 [04:14&lt;02:47,  1.41s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 182/300 [04:16&lt;02:46,  1.41s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 183/300 [04:17&lt;02:44,  1.41s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 184/300 [04:19&lt;02:44,  1.42s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 185/300 [04:20&lt;02:43,  1.42s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 186/300 [04:22&lt;02:42,  1.42s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 187/300 [04:23&lt;02:39,  1.42s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 188/300 [04:24&lt;02:38,  1.41s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 189/300 [04:26&lt;02:36,  1.41s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 190/300 [04:27&lt;02:34,  1.41s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 191/300 [04:29&lt;02:33,  1.41s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 192/300 [04:30&lt;02:32,  1.41s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 193/300 [04:31&lt;02:30,  1.41s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 194/300 [04:33&lt;02:29,  1.41s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 195/300 [04:34&lt;02:27,  1.41s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 196/300 [04:36&lt;02:27,  1.42s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 197/300 [04:37&lt;02:25,  1.41s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 198/300 [04:38&lt;02:24,  1.42s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 199/300 [04:40&lt;02:22,  1.41s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 200/300 [04:41&lt;02:20,  1.41s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 201/300 [04:43&lt;02:19,  1.40s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 202/300 [04:44&lt;02:17,  1.40s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 203/300 [04:46&lt;02:18,  1.42s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 204/300 [04:47&lt;02:16,  1.42s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 205/300 [04:48&lt;02:14,  1.42s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 206/300 [04:50&lt;02:12,  1.41s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 207/300 [04:51&lt;02:11,  1.41s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 208/300 [04:53&lt;02:09,  1.41s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 209/300 [04:54&lt;02:07,  1.40s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 210/300 [04:55&lt;02:06,  1.40s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 211/300 [04:57&lt;02:04,  1.40s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 212/300 [04:58&lt;02:04,  1.41s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 213/300 [05:00&lt;02:03,  1.41s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 214/300 [05:01&lt;02:01,  1.42s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 215/300 [05:03&lt;02:04,  1.47s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 216/300 [05:04&lt;02:03,  1.48s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 217/300 [05:06&lt;02:00,  1.45s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 218/300 [05:07&lt;01:57,  1.44s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 219/300 [05:08&lt;01:55,  1.43s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 220/300 [05:10&lt;01:53,  1.42s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 221/300 [05:11&lt;01:51,  1.41s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 222/300 [05:13&lt;01:50,  1.41s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 223/300 [05:14&lt;01:48,  1.41s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 224/300 [05:15&lt;01:46,  1.41s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 225/300 [05:17&lt;01:45,  1.41s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 226/300 [05:18&lt;01:44,  1.41s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 227/300 [05:20&lt;01:42,  1.41s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 228/300 [05:21&lt;01:41,  1.40s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 229/300 [05:22&lt;01:39,  1.40s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 230/300 [05:24&lt;01:38,  1.40s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 231/300 [05:25&lt;01:36,  1.40s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 232/300 [05:27&lt;01:35,  1.40s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 233/300 [05:28&lt;01:33,  1.40s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 234/300 [05:29&lt;01:33,  1.42s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 235/300 [05:31&lt;01:31,  1.42s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 236/300 [05:32&lt;01:30,  1.41s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 237/300 [05:34&lt;01:28,  1.41s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 238/300 [05:35&lt;01:27,  1.41s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 239/300 [05:36&lt;01:26,  1.41s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 240/300 [05:38&lt;01:24,  1.41s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 241/300 [05:39&lt;01:22,  1.40s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 242/300 [05:41&lt;01:21,  1.40s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 243/300 [05:42&lt;01:20,  1.41s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 244/300 [05:43&lt;01:19,  1.42s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 245/300 [05:45&lt;01:17,  1.41s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 246/300 [05:46&lt;01:16,  1.42s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 247/300 [05:48&lt;01:14,  1.41s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 248/300 [05:49&lt;01:13,  1.42s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 249/300 [05:51&lt;01:12,  1.41s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 250/300 [05:52&lt;01:10,  1.41s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 251/300 [05:53&lt;01:08,  1.41s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 252/300 [05:55&lt;01:07,  1.40s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 253/300 [05:56&lt;01:05,  1.40s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 254/300 [05:58&lt;01:04,  1.40s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 255/300 [05:59&lt;01:03,  1.42s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 256/300 [06:00&lt;01:03,  1.44s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 257/300 [06:02&lt;01:01,  1.43s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 258/300 [06:03&lt;00:59,  1.42s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 259/300 [06:05&lt;00:57,  1.41s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 260/300 [06:06&lt;00:56,  1.41s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 261/300 [06:07&lt;00:54,  1.41s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 262/300 [06:09&lt;00:53,  1.41s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 263/300 [06:10&lt;00:52,  1.41s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 264/300 [06:12&lt;00:51,  1.43s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 265/300 [06:13&lt;00:49,  1.42s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 266/300 [06:15&lt;00:48,  1.42s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 267/300 [06:16&lt;00:46,  1.42s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 268/300 [06:17&lt;00:45,  1.41s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 269/300 [06:19&lt;00:43,  1.41s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 270/300 [06:20&lt;00:42,  1.40s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 271/300 [06:22&lt;00:40,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 272/300 [06:23&lt;00:39,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 273/300 [06:24&lt;00:37,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 274/300 [06:26&lt;00:36,  1.40s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 275/300 [06:27&lt;00:35,  1.40s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 276/300 [06:29&lt;00:34,  1.43s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 277/300 [06:30&lt;00:32,  1.42s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 278/300 [06:31&lt;00:31,  1.41s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 279/300 [06:33&lt;00:29,  1.41s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 280/300 [06:34&lt;00:28,  1.41s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 281/300 [06:36&lt;00:26,  1.41s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 282/300 [06:37&lt;00:25,  1.40s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 283/300 [06:38&lt;00:23,  1.40s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 284/300 [06:40&lt;00:22,  1.40s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 285/300 [06:41&lt;00:21,  1.40s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 286/300 [06:43&lt;00:19,  1.42s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 287/300 [06:44&lt;00:18,  1.41s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 288/300 [06:46&lt;00:16,  1.42s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 289/300 [06:47&lt;00:15,  1.41s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 290/300 [06:48&lt;00:14,  1.40s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 291/300 [06:50&lt;00:12,  1.40s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 292/300 [06:51&lt;00:11,  1.40s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 293/300 [06:53&lt;00:09,  1.40s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 294/300 [06:54&lt;00:08,  1.40s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 295/300 [06:55&lt;00:07,  1.42s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 296/300 [06:57&lt;00:05,  1.42s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 297/300 [06:58&lt;00:04,  1.42s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 298/300 [07:00&lt;00:02,  1.41s/it]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 299/300 [07:01&lt;00:01,  1.42s/it]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [07:02&lt;00:00,  1.41s/it]\n\n4it [28:13, 423.39s/it]\n\n  0%|          | 0/300 [00:00&lt;?, ?it/s]\n\n  0%|          | 1/300 [00:00&lt;04:54,  1.01it/s]\n\n  1%|          | 2/300 [00:02&lt;06:08,  1.24s/it]\n\n  1%|          | 3/300 [00:03&lt;06:29,  1.31s/it]\n\n  1%|‚ñè         | 4/300 [00:05&lt;06:58,  1.41s/it]\n\n  2%|‚ñè         | 5/300 [00:06&lt;06:56,  1.41s/it]\n\n  2%|‚ñè         | 6/300 [00:08&lt;06:56,  1.42s/it]\n\n  2%|‚ñè         | 7/300 [00:09&lt;06:53,  1.41s/it]\n\n  3%|‚ñé         | 8/300 [00:11&lt;06:52,  1.41s/it]\n\n  3%|‚ñé         | 9/300 [00:12&lt;06:49,  1.41s/it]\n\n  3%|‚ñé         | 10/300 [00:13&lt;06:48,  1.41s/it]\n\n  4%|‚ñé         | 11/300 [00:15&lt;06:47,  1.41s/it]\n\n  4%|‚ñç         | 12/300 [00:16&lt;06:45,  1.41s/it]\n\n  4%|‚ñç         | 13/300 [00:18&lt;06:43,  1.41s/it]\n\n  5%|‚ñç         | 14/300 [00:19&lt;06:43,  1.41s/it]\n\n  5%|‚ñå         | 15/300 [00:20&lt;06:43,  1.42s/it]\n\n  5%|‚ñå         | 16/300 [00:22&lt;06:40,  1.41s/it]\n\n  6%|‚ñå         | 17/300 [00:23&lt;06:37,  1.40s/it]\n\n  6%|‚ñå         | 18/300 [00:25&lt;06:35,  1.40s/it]\n\n  6%|‚ñã         | 19/300 [00:26&lt;06:33,  1.40s/it]\n\n  7%|‚ñã         | 20/300 [00:27&lt;06:31,  1.40s/it]\n\n  7%|‚ñã         | 21/300 [00:29&lt;06:29,  1.40s/it]\n\n  7%|‚ñã         | 22/300 [00:30&lt;06:29,  1.40s/it]\n\n  8%|‚ñä         | 23/300 [00:32&lt;06:29,  1.41s/it]\n\n  8%|‚ñä         | 24/300 [00:33&lt;06:27,  1.41s/it]\n\n  8%|‚ñä         | 25/300 [00:34&lt;06:26,  1.41s/it]\n\n  9%|‚ñä         | 26/300 [00:36&lt;06:27,  1.41s/it]\n\n  9%|‚ñâ         | 27/300 [00:37&lt;06:24,  1.41s/it]\n\n  9%|‚ñâ         | 28/300 [00:39&lt;06:22,  1.41s/it]\n\n 10%|‚ñâ         | 29/300 [00:40&lt;06:21,  1.41s/it]\n\n 10%|‚ñà         | 30/300 [00:41&lt;06:19,  1.40s/it]\n\n 10%|‚ñà         | 31/300 [00:43&lt;06:18,  1.41s/it]\n\n 11%|‚ñà         | 32/300 [00:44&lt;06:16,  1.40s/it]\n\n 11%|‚ñà         | 33/300 [00:46&lt;06:20,  1.42s/it]\n\n 11%|‚ñà‚ñè        | 34/300 [00:47&lt;06:16,  1.42s/it]\n\n 12%|‚ñà‚ñè        | 35/300 [00:49&lt;06:15,  1.42s/it]\n\n 12%|‚ñà‚ñè        | 36/300 [00:50&lt;06:12,  1.41s/it]\n\n 12%|‚ñà‚ñè        | 37/300 [00:51&lt;06:09,  1.41s/it]\n\n 13%|‚ñà‚ñé        | 38/300 [00:53&lt;06:07,  1.40s/it]\n\n 13%|‚ñà‚ñé        | 39/300 [00:54&lt;06:05,  1.40s/it]\n\n 13%|‚ñà‚ñé        | 40/300 [00:56&lt;06:06,  1.41s/it]\n\n 14%|‚ñà‚ñé        | 41/300 [00:57&lt;06:04,  1.41s/it]\n\n 14%|‚ñà‚ñç        | 42/300 [00:58&lt;06:04,  1.41s/it]\n\n 14%|‚ñà‚ñç        | 43/300 [01:00&lt;06:02,  1.41s/it]\n\n 15%|‚ñà‚ñç        | 44/300 [01:01&lt;06:00,  1.41s/it]\n\n 15%|‚ñà‚ñå        | 45/300 [01:03&lt;05:58,  1.41s/it]\n\n 15%|‚ñà‚ñå        | 46/300 [01:04&lt;05:56,  1.40s/it]\n\n 16%|‚ñà‚ñå        | 47/300 [01:05&lt;05:55,  1.40s/it]\n\n 16%|‚ñà‚ñå        | 48/300 [01:07&lt;05:53,  1.40s/it]\n\n 16%|‚ñà‚ñã        | 49/300 [01:08&lt;05:52,  1.40s/it]\n\n 17%|‚ñà‚ñã        | 50/300 [01:10&lt;05:50,  1.40s/it]\n\n 17%|‚ñà‚ñã        | 51/300 [01:11&lt;05:49,  1.41s/it]\n\n 17%|‚ñà‚ñã        | 52/300 [01:12&lt;05:47,  1.40s/it]\n\n 18%|‚ñà‚ñä        | 53/300 [01:14&lt;05:46,  1.40s/it]\n\n 18%|‚ñà‚ñä        | 54/300 [01:15&lt;05:47,  1.41s/it]\n\n 18%|‚ñà‚ñä        | 55/300 [01:17&lt;05:46,  1.42s/it]\n\n 19%|‚ñà‚ñä        | 56/300 [01:18&lt;05:43,  1.41s/it]\n\n 19%|‚ñà‚ñâ        | 57/300 [01:19&lt;05:41,  1.41s/it]\n\n 19%|‚ñà‚ñâ        | 58/300 [01:21&lt;05:39,  1.40s/it]\n\n 20%|‚ñà‚ñâ        | 59/300 [01:22&lt;05:40,  1.41s/it]\n\n 20%|‚ñà‚ñà        | 60/300 [01:24&lt;05:39,  1.41s/it]\n\n 20%|‚ñà‚ñà        | 61/300 [01:25&lt;05:36,  1.41s/it]\n\n 21%|‚ñà‚ñà        | 62/300 [01:26&lt;05:33,  1.40s/it]\n\n 21%|‚ñà‚ñà        | 63/300 [01:28&lt;05:32,  1.40s/it]\n\n 21%|‚ñà‚ñà‚ñè       | 64/300 [01:29&lt;05:30,  1.40s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 65/300 [01:31&lt;05:32,  1.41s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 66/300 [01:32&lt;05:30,  1.41s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 67/300 [01:34&lt;05:27,  1.41s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 68/300 [01:35&lt;05:25,  1.40s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 69/300 [01:36&lt;05:23,  1.40s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 70/300 [01:38&lt;05:23,  1.41s/it]\n\n 24%|‚ñà‚ñà‚ñé       | 71/300 [01:39&lt;05:21,  1.40s/it]\n\n 24%|‚ñà‚ñà‚ñç       | 72/300 [01:41&lt;05:19,  1.40s/it]\n\n 24%|‚ñà‚ñà‚ñç       | 73/300 [01:42&lt;05:19,  1.41s/it]\n\n 25%|‚ñà‚ñà‚ñç       | 74/300 [01:43&lt;05:17,  1.41s/it]\n\n 25%|‚ñà‚ñà‚ñå       | 75/300 [01:45&lt;05:20,  1.42s/it]\n\n 25%|‚ñà‚ñà‚ñå       | 76/300 [01:46&lt;05:18,  1.42s/it]\n\n 26%|‚ñà‚ñà‚ñå       | 77/300 [01:48&lt;05:15,  1.41s/it]\n\n 26%|‚ñà‚ñà‚ñå       | 78/300 [01:49&lt;05:12,  1.41s/it]\n\n 26%|‚ñà‚ñà‚ñã       | 79/300 [01:50&lt;05:10,  1.40s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 80/300 [01:52&lt;05:10,  1.41s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 81/300 [01:53&lt;05:07,  1.40s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 82/300 [01:55&lt;05:07,  1.41s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 83/300 [01:56&lt;05:05,  1.41s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 84/300 [01:57&lt;05:04,  1.41s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 85/300 [01:59&lt;05:02,  1.41s/it]\n\n 29%|‚ñà‚ñà‚ñä       | 86/300 [02:00&lt;05:01,  1.41s/it]\n\n 29%|‚ñà‚ñà‚ñâ       | 87/300 [02:02&lt;04:59,  1.41s/it]\n\n 29%|‚ñà‚ñà‚ñâ       | 88/300 [02:03&lt;04:58,  1.41s/it]\n\n 30%|‚ñà‚ñà‚ñâ       | 89/300 [02:05&lt;04:56,  1.41s/it]\n\n 30%|‚ñà‚ñà‚ñà       | 90/300 [02:06&lt;04:53,  1.40s/it]\n\n 30%|‚ñà‚ñà‚ñà       | 91/300 [02:07&lt;04:53,  1.40s/it]\n\n 31%|‚ñà‚ñà‚ñà       | 92/300 [02:09&lt;04:53,  1.41s/it]\n\n 31%|‚ñà‚ñà‚ñà       | 93/300 [02:10&lt;04:51,  1.41s/it]\n\n 31%|‚ñà‚ñà‚ñà‚ñè      | 94/300 [02:12&lt;04:56,  1.44s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 95/300 [02:13&lt;04:53,  1.43s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 96/300 [02:14&lt;04:51,  1.43s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 97/300 [02:16&lt;04:56,  1.46s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 98/300 [02:17&lt;04:52,  1.45s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 99/300 [02:19&lt;04:47,  1.43s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 100/300 [02:20&lt;04:44,  1.42s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñé      | 101/300 [02:22&lt;04:41,  1.41s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñç      | 102/300 [02:23&lt;04:38,  1.41s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñç      | 103/300 [02:24&lt;04:38,  1.42s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñç      | 104/300 [02:26&lt;04:36,  1.41s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñå      | 105/300 [02:27&lt;04:35,  1.41s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñå      | 106/300 [02:29&lt;04:35,  1.42s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñå      | 107/300 [02:30&lt;04:32,  1.41s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñå      | 108/300 [02:31&lt;04:29,  1.41s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñã      | 109/300 [02:33&lt;04:29,  1.41s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 110/300 [02:34&lt;04:26,  1.40s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 111/300 [02:36&lt;04:25,  1.40s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 112/300 [02:37&lt;04:23,  1.40s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 113/300 [02:39&lt;04:22,  1.40s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 114/300 [02:40&lt;04:21,  1.41s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 115/300 [02:41&lt;04:20,  1.41s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñä      | 116/300 [02:43&lt;04:18,  1.41s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñâ      | 117/300 [02:44&lt;04:17,  1.41s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñâ      | 118/300 [02:46&lt;04:15,  1.40s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñâ      | 119/300 [02:47&lt;04:13,  1.40s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñà      | 120/300 [02:48&lt;04:13,  1.41s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñà      | 121/300 [02:50&lt;04:11,  1.40s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà      | 122/300 [02:51&lt;04:10,  1.41s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà      | 123/300 [02:53&lt;04:09,  1.41s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 124/300 [02:54&lt;04:10,  1.42s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 125/300 [02:55&lt;04:07,  1.42s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 126/300 [02:57&lt;04:06,  1.41s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 127/300 [02:58&lt;04:05,  1.42s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 128/300 [03:00&lt;04:03,  1.42s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 129/300 [03:01&lt;04:02,  1.42s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 130/300 [03:03&lt;04:00,  1.42s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 131/300 [03:04&lt;03:58,  1.41s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 132/300 [03:05&lt;03:55,  1.40s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 133/300 [03:07&lt;03:54,  1.41s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 134/300 [03:08&lt;03:55,  1.42s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 135/300 [03:10&lt;03:53,  1.42s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 136/300 [03:11&lt;03:53,  1.43s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 137/300 [03:12&lt;03:51,  1.42s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 138/300 [03:14&lt;03:49,  1.41s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 139/300 [03:15&lt;03:48,  1.42s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 140/300 [03:17&lt;03:46,  1.42s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 141/300 [03:18&lt;03:44,  1.41s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 142/300 [03:19&lt;03:42,  1.41s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 143/300 [03:21&lt;03:41,  1.41s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 144/300 [03:22&lt;03:41,  1.42s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 145/300 [03:24&lt;03:39,  1.42s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 146/300 [03:25&lt;03:39,  1.42s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 147/300 [03:27&lt;03:37,  1.42s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 148/300 [03:28&lt;03:35,  1.42s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 149/300 [03:29&lt;03:33,  1.41s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 150/300 [03:31&lt;03:31,  1.41s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 151/300 [03:32&lt;03:30,  1.42s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 152/300 [03:34&lt;03:28,  1.41s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 153/300 [03:35&lt;03:27,  1.41s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 154/300 [03:36&lt;03:27,  1.42s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 155/300 [03:38&lt;03:26,  1.42s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 156/300 [03:39&lt;03:24,  1.42s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 157/300 [03:41&lt;03:21,  1.41s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 158/300 [03:42&lt;03:19,  1.41s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 159/300 [03:43&lt;03:17,  1.40s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 160/300 [03:45&lt;03:16,  1.40s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 161/300 [03:46&lt;03:14,  1.40s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 162/300 [03:48&lt;03:13,  1.40s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 163/300 [03:49&lt;03:12,  1.40s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 164/300 [03:51&lt;03:16,  1.44s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 165/300 [03:52&lt;03:14,  1.44s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 166/300 [03:53&lt;03:11,  1.43s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 167/300 [03:55&lt;03:09,  1.42s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 168/300 [03:56&lt;03:06,  1.41s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 169/300 [03:58&lt;03:05,  1.41s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 170/300 [03:59&lt;03:03,  1.41s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 171/300 [04:00&lt;03:01,  1.41s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 172/300 [04:02&lt;02:59,  1.40s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 173/300 [04:03&lt;02:57,  1.40s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 174/300 [04:05&lt;02:57,  1.41s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 175/300 [04:06&lt;02:56,  1.42s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 176/300 [04:08&lt;02:55,  1.41s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 177/300 [04:09&lt;02:53,  1.41s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 178/300 [04:10&lt;02:52,  1.41s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 179/300 [04:12&lt;02:50,  1.41s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 180/300 [04:13&lt;02:48,  1.40s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 181/300 [04:15&lt;02:46,  1.40s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 182/300 [04:16&lt;02:45,  1.40s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 183/300 [04:17&lt;02:44,  1.41s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 184/300 [04:19&lt;02:42,  1.40s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 185/300 [04:20&lt;02:41,  1.40s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 186/300 [04:22&lt;02:39,  1.40s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 187/300 [04:23&lt;02:38,  1.40s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 188/300 [04:24&lt;02:36,  1.40s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 189/300 [04:26&lt;02:35,  1.40s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 190/300 [04:27&lt;02:37,  1.44s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 191/300 [04:29&lt;02:34,  1.42s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 192/300 [04:30&lt;02:33,  1.42s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 193/300 [04:32&lt;02:32,  1.43s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 194/300 [04:33&lt;02:31,  1.43s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 195/300 [04:34&lt;02:30,  1.43s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 196/300 [04:36&lt;02:28,  1.43s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 197/300 [04:37&lt;02:25,  1.42s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 198/300 [04:39&lt;02:23,  1.41s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 199/300 [04:40&lt;02:22,  1.41s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 200/300 [04:41&lt;02:21,  1.42s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 201/300 [04:43&lt;02:19,  1.41s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 202/300 [04:44&lt;02:18,  1.41s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 203/300 [04:46&lt;02:16,  1.41s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 204/300 [04:47&lt;02:14,  1.41s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 205/300 [04:48&lt;02:13,  1.41s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 206/300 [04:50&lt;02:11,  1.40s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 207/300 [04:51&lt;02:10,  1.40s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 208/300 [04:53&lt;02:09,  1.40s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 209/300 [04:54&lt;02:07,  1.40s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 210/300 [04:55&lt;02:06,  1.41s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 211/300 [04:57&lt;02:05,  1.40s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 212/300 [04:58&lt;02:04,  1.41s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 213/300 [05:00&lt;02:02,  1.41s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 214/300 [05:01&lt;02:01,  1.41s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 215/300 [05:03&lt;01:59,  1.41s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 216/300 [05:04&lt;01:57,  1.40s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 217/300 [05:05&lt;01:56,  1.40s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 218/300 [05:07&lt;01:54,  1.40s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 219/300 [05:08&lt;01:53,  1.40s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 220/300 [05:10&lt;01:53,  1.41s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 221/300 [05:11&lt;01:51,  1.41s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 222/300 [05:12&lt;01:49,  1.40s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 223/300 [05:14&lt;01:48,  1.41s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 224/300 [05:15&lt;01:47,  1.41s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 225/300 [05:17&lt;01:46,  1.42s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 226/300 [05:18&lt;01:44,  1.42s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 227/300 [05:19&lt;01:42,  1.41s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 228/300 [05:21&lt;01:41,  1.41s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 229/300 [05:22&lt;01:39,  1.40s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 230/300 [05:24&lt;01:38,  1.41s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 231/300 [05:25&lt;01:37,  1.41s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 232/300 [05:26&lt;01:35,  1.41s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 233/300 [05:28&lt;01:33,  1.40s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 234/300 [05:29&lt;01:33,  1.42s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 235/300 [05:31&lt;01:32,  1.42s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 236/300 [05:32&lt;01:30,  1.41s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 237/300 [05:34&lt;01:28,  1.40s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 238/300 [05:35&lt;01:27,  1.41s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 239/300 [05:36&lt;01:25,  1.40s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 240/300 [05:38&lt;01:24,  1.40s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 241/300 [05:39&lt;01:22,  1.40s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 242/300 [05:41&lt;01:21,  1.40s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 243/300 [05:42&lt;01:20,  1.41s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 244/300 [05:43&lt;01:18,  1.41s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 245/300 [05:45&lt;01:17,  1.41s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 246/300 [05:46&lt;01:16,  1.41s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 247/300 [05:48&lt;01:15,  1.42s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 248/300 [05:49&lt;01:13,  1.41s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 249/300 [05:50&lt;01:11,  1.40s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 250/300 [05:52&lt;01:10,  1.40s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 251/300 [05:53&lt;01:09,  1.41s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 252/300 [05:55&lt;01:07,  1.41s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 253/300 [05:56&lt;01:06,  1.41s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 254/300 [05:57&lt;01:05,  1.41s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 255/300 [05:59&lt;01:03,  1.42s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 256/300 [06:00&lt;01:02,  1.42s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 257/300 [06:02&lt;01:00,  1.41s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 258/300 [06:03&lt;00:59,  1.41s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 259/300 [06:05&lt;00:57,  1.41s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 260/300 [06:06&lt;00:56,  1.41s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 261/300 [06:07&lt;00:54,  1.40s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 262/300 [06:09&lt;00:53,  1.41s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 263/300 [06:10&lt;00:52,  1.41s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 264/300 [06:12&lt;00:51,  1.42s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 265/300 [06:13&lt;00:49,  1.42s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 266/300 [06:14&lt;00:47,  1.41s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 267/300 [06:16&lt;00:46,  1.40s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 268/300 [06:17&lt;00:44,  1.40s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 269/300 [06:19&lt;00:43,  1.40s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 270/300 [06:20&lt;00:42,  1.40s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 271/300 [06:21&lt;00:40,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 272/300 [06:23&lt;00:39,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 273/300 [06:24&lt;00:37,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 274/300 [06:26&lt;00:36,  1.40s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 275/300 [06:27&lt;00:35,  1.40s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 276/300 [06:28&lt;00:33,  1.41s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 277/300 [06:30&lt;00:32,  1.41s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 278/300 [06:31&lt;00:31,  1.41s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 279/300 [06:33&lt;00:29,  1.41s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 280/300 [06:34&lt;00:28,  1.42s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 281/300 [06:36&lt;00:26,  1.42s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 282/300 [06:37&lt;00:25,  1.41s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 283/300 [06:38&lt;00:24,  1.46s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 284/300 [06:40&lt;00:23,  1.44s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 285/300 [06:41&lt;00:21,  1.43s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 286/300 [06:43&lt;00:19,  1.42s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 287/300 [06:44&lt;00:18,  1.43s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 288/300 [06:46&lt;00:17,  1.42s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 289/300 [06:47&lt;00:15,  1.42s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 290/300 [06:48&lt;00:14,  1.41s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 291/300 [06:50&lt;00:12,  1.41s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 292/300 [06:51&lt;00:11,  1.40s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 293/300 [06:53&lt;00:09,  1.41s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 294/300 [06:54&lt;00:08,  1.42s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 295/300 [06:55&lt;00:07,  1.42s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 296/300 [06:57&lt;00:05,  1.41s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 297/300 [06:58&lt;00:04,  1.41s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 298/300 [07:00&lt;00:02,  1.40s/it]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 299/300 [07:01&lt;00:01,  1.40s/it]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [07:02&lt;00:00,  1.41s/it]\n\n5it [35:16, 423.51s/it]\n\n  0%|          | 0/300 [00:00&lt;?, ?it/s]\n\n  0%|          | 1/300 [00:00&lt;04:55,  1.01it/s]\n\n  1%|          | 2/300 [00:02&lt;06:18,  1.27s/it]\n\n  1%|          | 3/300 [00:03&lt;06:34,  1.33s/it]\n\n  1%|‚ñè         | 4/300 [00:05&lt;06:47,  1.38s/it]\n\n  2%|‚ñè         | 5/300 [00:06&lt;06:50,  1.39s/it]\n\n  2%|‚ñè         | 6/300 [00:08&lt;06:48,  1.39s/it]\n\n  2%|‚ñè         | 7/300 [00:09&lt;06:47,  1.39s/it]\n\n  3%|‚ñé         | 8/300 [00:10&lt;06:46,  1.39s/it]\n\n  3%|‚ñé         | 9/300 [00:12&lt;06:45,  1.39s/it]\n\n  3%|‚ñé         | 10/300 [00:13&lt;06:46,  1.40s/it]\n\n  4%|‚ñé         | 11/300 [00:15&lt;06:45,  1.40s/it]\n\n  4%|‚ñç         | 12/300 [00:16&lt;06:45,  1.41s/it]\n\n  4%|‚ñç         | 13/300 [00:17&lt;06:43,  1.40s/it]\n\n  5%|‚ñç         | 14/300 [00:19&lt;06:49,  1.43s/it]\n\n  5%|‚ñå         | 15/300 [00:20&lt;06:50,  1.44s/it]\n\n  5%|‚ñå         | 16/300 [00:22&lt;06:45,  1.43s/it]\n\n  6%|‚ñå         | 17/300 [00:23&lt;06:40,  1.41s/it]\n\n  6%|‚ñå         | 18/300 [00:25&lt;06:37,  1.41s/it]\n\n  6%|‚ñã         | 19/300 [00:26&lt;06:35,  1.41s/it]\n\n  7%|‚ñã         | 20/300 [00:27&lt;06:35,  1.41s/it]\n\n  7%|‚ñã         | 21/300 [00:29&lt;06:32,  1.41s/it]\n\n  7%|‚ñã         | 22/300 [00:30&lt;06:30,  1.40s/it]\n\n  8%|‚ñä         | 23/300 [00:32&lt;06:31,  1.41s/it]\n\n  8%|‚ñä         | 24/300 [00:33&lt;06:30,  1.41s/it]\n\n  8%|‚ñä         | 25/300 [00:34&lt;06:28,  1.41s/it]\n\n  9%|‚ñä         | 26/300 [00:36&lt;06:25,  1.41s/it]\n\n  9%|‚ñâ         | 27/300 [00:37&lt;06:25,  1.41s/it]\n\n  9%|‚ñâ         | 28/300 [00:39&lt;06:23,  1.41s/it]\n\n 10%|‚ñâ         | 29/300 [00:40&lt;06:22,  1.41s/it]\n\n 10%|‚ñà         | 30/300 [00:41&lt;06:19,  1.41s/it]\n\n 10%|‚ñà         | 31/300 [00:43&lt;06:18,  1.41s/it]\n\n 11%|‚ñà         | 32/300 [00:44&lt;06:18,  1.41s/it]\n\n 11%|‚ñà         | 33/300 [00:46&lt;06:15,  1.41s/it]\n\n 11%|‚ñà‚ñè        | 34/300 [00:47&lt;06:13,  1.40s/it]\n\n 12%|‚ñà‚ñè        | 35/300 [00:49&lt;06:12,  1.41s/it]\n\n 12%|‚ñà‚ñè        | 36/300 [00:50&lt;06:12,  1.41s/it]\n\n 12%|‚ñà‚ñè        | 37/300 [00:51&lt;06:11,  1.41s/it]\n\n 13%|‚ñà‚ñé        | 38/300 [00:53&lt;06:09,  1.41s/it]\n\n 13%|‚ñà‚ñé        | 39/300 [00:54&lt;06:06,  1.40s/it]\n\n 13%|‚ñà‚ñé        | 40/300 [00:56&lt;06:04,  1.40s/it]\n\n 14%|‚ñà‚ñé        | 41/300 [00:57&lt;06:02,  1.40s/it]\n\n 14%|‚ñà‚ñç        | 42/300 [00:58&lt;06:01,  1.40s/it]\n\n 14%|‚ñà‚ñç        | 43/300 [01:00&lt;06:00,  1.40s/it]\n\n 15%|‚ñà‚ñç        | 44/300 [01:01&lt;05:59,  1.40s/it]\n\n 15%|‚ñà‚ñå        | 45/300 [01:03&lt;05:56,  1.40s/it]\n\n 15%|‚ñà‚ñå        | 46/300 [01:04&lt;05:56,  1.40s/it]\n\n 16%|‚ñà‚ñå        | 47/300 [01:05&lt;05:57,  1.41s/it]\n\n 16%|‚ñà‚ñå        | 48/300 [01:07&lt;05:55,  1.41s/it]\n\n 16%|‚ñà‚ñã        | 49/300 [01:08&lt;05:53,  1.41s/it]\n\n 17%|‚ñà‚ñã        | 50/300 [01:10&lt;05:51,  1.41s/it]\n\n 17%|‚ñà‚ñã        | 51/300 [01:11&lt;05:50,  1.41s/it]\n\n 17%|‚ñà‚ñã        | 52/300 [01:12&lt;05:47,  1.40s/it]\n\n 18%|‚ñà‚ñä        | 53/300 [01:14&lt;05:45,  1.40s/it]\n\n 18%|‚ñà‚ñä        | 54/300 [01:15&lt;05:44,  1.40s/it]\n\n 18%|‚ñà‚ñä        | 55/300 [01:17&lt;05:43,  1.40s/it]\n\n 19%|‚ñà‚ñä        | 56/300 [01:18&lt;05:40,  1.40s/it]\n\n 19%|‚ñà‚ñâ        | 57/300 [01:19&lt;05:38,  1.39s/it]\n\n 19%|‚ñà‚ñâ        | 58/300 [01:21&lt;05:39,  1.40s/it]\n\n 20%|‚ñà‚ñâ        | 59/300 [01:22&lt;05:37,  1.40s/it]\n\n 20%|‚ñà‚ñà        | 60/300 [01:24&lt;05:38,  1.41s/it]\n\n 20%|‚ñà‚ñà        | 61/300 [01:25&lt;05:35,  1.40s/it]\n\n 21%|‚ñà‚ñà        | 62/300 [01:26&lt;05:33,  1.40s/it]\n\n 21%|‚ñà‚ñà        | 63/300 [01:28&lt;05:32,  1.40s/it]\n\n 21%|‚ñà‚ñà‚ñè       | 64/300 [01:29&lt;05:30,  1.40s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 65/300 [01:31&lt;05:29,  1.40s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 66/300 [01:32&lt;05:29,  1.41s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 67/300 [01:33&lt;05:27,  1.40s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 68/300 [01:35&lt;05:25,  1.40s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 69/300 [01:36&lt;05:23,  1.40s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 70/300 [01:38&lt;05:21,  1.40s/it]\n\n 24%|‚ñà‚ñà‚ñé       | 71/300 [01:39&lt;05:20,  1.40s/it]\n\n 24%|‚ñà‚ñà‚ñç       | 72/300 [01:40&lt;05:18,  1.40s/it]\n\n 24%|‚ñà‚ñà‚ñç       | 73/300 [01:42&lt;05:17,  1.40s/it]\n\n 25%|‚ñà‚ñà‚ñç       | 74/300 [01:43&lt;05:31,  1.47s/it]\n\n 25%|‚ñà‚ñà‚ñå       | 75/300 [01:45&lt;05:25,  1.45s/it]\n\n 25%|‚ñà‚ñà‚ñå       | 76/300 [01:46&lt;05:20,  1.43s/it]\n\n 26%|‚ñà‚ñà‚ñå       | 77/300 [01:48&lt;05:16,  1.42s/it]\n\n 26%|‚ñà‚ñà‚ñå       | 78/300 [01:49&lt;05:13,  1.41s/it]\n\n 26%|‚ñà‚ñà‚ñã       | 79/300 [01:50&lt;05:12,  1.41s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 80/300 [01:52&lt;05:09,  1.41s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 81/300 [01:53&lt;05:07,  1.40s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 82/300 [01:55&lt;05:05,  1.40s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 83/300 [01:56&lt;05:04,  1.40s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 84/300 [01:57&lt;05:03,  1.41s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 85/300 [01:59&lt;05:08,  1.44s/it]\n\n 29%|‚ñà‚ñà‚ñä       | 86/300 [02:00&lt;05:06,  1.43s/it]\n\n 29%|‚ñà‚ñà‚ñâ       | 87/300 [02:02&lt;05:02,  1.42s/it]\n\n 29%|‚ñà‚ñà‚ñâ       | 88/300 [02:03&lt;05:00,  1.42s/it]\n\n 30%|‚ñà‚ñà‚ñâ       | 89/300 [02:05&lt;04:57,  1.41s/it]\n\n 30%|‚ñà‚ñà‚ñà       | 90/300 [02:06&lt;04:56,  1.41s/it]\n\n 30%|‚ñà‚ñà‚ñà       | 91/300 [02:07&lt;04:53,  1.41s/it]\n\n 31%|‚ñà‚ñà‚ñà       | 92/300 [02:09&lt;04:51,  1.40s/it]\n\n 31%|‚ñà‚ñà‚ñà       | 93/300 [02:10&lt;04:51,  1.41s/it]\n\n 31%|‚ñà‚ñà‚ñà‚ñè      | 94/300 [02:12&lt;04:49,  1.40s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 95/300 [02:13&lt;04:48,  1.41s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 96/300 [02:14&lt;04:47,  1.41s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 97/300 [02:16&lt;04:44,  1.40s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 98/300 [02:17&lt;04:42,  1.40s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 99/300 [02:19&lt;04:41,  1.40s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 100/300 [02:20&lt;04:41,  1.41s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñé      | 101/300 [02:21&lt;04:39,  1.40s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñç      | 102/300 [02:23&lt;04:38,  1.40s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñç      | 103/300 [02:24&lt;04:36,  1.41s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñç      | 104/300 [02:26&lt;04:35,  1.40s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñå      | 105/300 [02:27&lt;04:34,  1.41s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñå      | 106/300 [02:28&lt;04:33,  1.41s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñå      | 107/300 [02:30&lt;04:31,  1.41s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñå      | 108/300 [02:31&lt;04:29,  1.40s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñã      | 109/300 [02:33&lt;04:27,  1.40s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 110/300 [02:34&lt;04:25,  1.40s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 111/300 [02:35&lt;04:24,  1.40s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 112/300 [02:37&lt;04:23,  1.40s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 113/300 [02:38&lt;04:22,  1.40s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 114/300 [02:40&lt;04:21,  1.40s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 115/300 [02:41&lt;04:19,  1.41s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñä      | 116/300 [02:42&lt;04:19,  1.41s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñâ      | 117/300 [02:44&lt;04:16,  1.40s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñâ      | 118/300 [02:45&lt;04:16,  1.41s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñâ      | 119/300 [02:47&lt;04:13,  1.40s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñà      | 120/300 [02:48&lt;04:11,  1.40s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñà      | 121/300 [02:49&lt;04:10,  1.40s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà      | 122/300 [02:51&lt;04:10,  1.40s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà      | 123/300 [02:52&lt;04:08,  1.41s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 124/300 [02:54&lt;04:07,  1.41s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 125/300 [02:55&lt;04:07,  1.41s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 126/300 [02:57&lt;04:05,  1.41s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 127/300 [02:58&lt;04:04,  1.41s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 128/300 [02:59&lt;04:01,  1.41s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 129/300 [03:01&lt;03:59,  1.40s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 130/300 [03:02&lt;03:59,  1.41s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 131/300 [03:04&lt;03:57,  1.40s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 132/300 [03:05&lt;03:55,  1.40s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 133/300 [03:06&lt;03:54,  1.40s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 134/300 [03:08&lt;03:57,  1.43s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 135/300 [03:09&lt;03:54,  1.42s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 136/300 [03:11&lt;03:53,  1.42s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 137/300 [03:12&lt;03:50,  1.41s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 138/300 [03:13&lt;03:48,  1.41s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 139/300 [03:15&lt;03:46,  1.40s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 140/300 [03:16&lt;03:45,  1.41s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 141/300 [03:18&lt;03:43,  1.40s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 142/300 [03:19&lt;03:41,  1.40s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 143/300 [03:20&lt;03:40,  1.40s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 144/300 [03:22&lt;03:39,  1.41s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 145/300 [03:23&lt;03:40,  1.42s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 146/300 [03:25&lt;03:38,  1.42s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 147/300 [03:26&lt;03:35,  1.41s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 148/300 [03:28&lt;03:33,  1.41s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 149/300 [03:29&lt;03:31,  1.40s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 150/300 [03:30&lt;03:29,  1.40s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 151/300 [03:32&lt;03:29,  1.41s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 152/300 [03:33&lt;03:28,  1.41s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 153/300 [03:35&lt;03:28,  1.42s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 154/300 [03:36&lt;03:27,  1.42s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 155/300 [03:37&lt;03:25,  1.42s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 156/300 [03:39&lt;03:24,  1.42s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 157/300 [03:40&lt;03:21,  1.41s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 158/300 [03:42&lt;03:21,  1.42s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 159/300 [03:43&lt;03:19,  1.41s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 160/300 [03:44&lt;03:17,  1.41s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 161/300 [03:46&lt;03:15,  1.40s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 162/300 [03:47&lt;03:13,  1.40s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 163/300 [03:49&lt;03:12,  1.41s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 164/300 [03:50&lt;03:11,  1.41s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 165/300 [03:52&lt;03:10,  1.41s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 166/300 [03:53&lt;03:08,  1.40s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 167/300 [03:54&lt;03:07,  1.41s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 168/300 [03:56&lt;03:10,  1.44s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 169/300 [03:57&lt;03:07,  1.43s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 170/300 [03:59&lt;03:04,  1.42s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 171/300 [04:00&lt;03:01,  1.41s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 172/300 [04:01&lt;03:00,  1.41s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 173/300 [04:03&lt;02:59,  1.41s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 174/300 [04:04&lt;02:58,  1.42s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 175/300 [04:06&lt;02:56,  1.41s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 176/300 [04:07&lt;02:54,  1.41s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 177/300 [04:08&lt;02:53,  1.41s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 178/300 [04:10&lt;02:52,  1.42s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 179/300 [04:11&lt;02:50,  1.41s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 180/300 [04:13&lt;02:48,  1.41s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 181/300 [04:14&lt;02:46,  1.40s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 182/300 [04:16&lt;02:48,  1.43s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 183/300 [04:17&lt;02:45,  1.42s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 184/300 [04:18&lt;02:44,  1.42s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 185/300 [04:20&lt;02:41,  1.41s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 186/300 [04:21&lt;02:40,  1.41s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 187/300 [04:23&lt;02:38,  1.40s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 188/300 [04:24&lt;02:37,  1.41s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 189/300 [04:25&lt;02:36,  1.41s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 190/300 [04:27&lt;02:35,  1.41s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 191/300 [04:28&lt;02:33,  1.41s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 192/300 [04:30&lt;02:31,  1.40s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 193/300 [04:31&lt;02:31,  1.42s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 194/300 [04:32&lt;02:29,  1.41s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 195/300 [04:34&lt;02:27,  1.41s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 196/300 [04:35&lt;02:26,  1.41s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 197/300 [04:37&lt;02:24,  1.40s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 198/300 [04:38&lt;02:23,  1.40s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 199/300 [04:39&lt;02:21,  1.40s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 200/300 [04:41&lt;02:20,  1.41s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 201/300 [04:42&lt;02:18,  1.40s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 202/300 [04:44&lt;02:17,  1.41s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 203/300 [04:45&lt;02:16,  1.41s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 204/300 [04:47&lt;02:14,  1.41s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 205/300 [04:48&lt;02:13,  1.40s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 206/300 [04:49&lt;02:11,  1.40s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 207/300 [04:51&lt;02:10,  1.40s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 208/300 [04:52&lt;02:09,  1.41s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 209/300 [04:54&lt;02:07,  1.40s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 210/300 [04:55&lt;02:05,  1.40s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 211/300 [04:56&lt;02:04,  1.40s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 212/300 [04:58&lt;02:02,  1.40s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 213/300 [04:59&lt;02:01,  1.40s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 214/300 [05:01&lt;02:00,  1.40s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 215/300 [05:02&lt;01:59,  1.40s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 216/300 [05:03&lt;01:58,  1.42s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 217/300 [05:05&lt;01:57,  1.41s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 218/300 [05:06&lt;01:55,  1.41s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 219/300 [05:08&lt;01:53,  1.40s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 220/300 [05:09&lt;01:52,  1.41s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 221/300 [05:10&lt;01:50,  1.40s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 222/300 [05:12&lt;01:49,  1.40s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 223/300 [05:13&lt;01:48,  1.41s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 224/300 [05:15&lt;01:46,  1.41s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 225/300 [05:16&lt;01:45,  1.41s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 226/300 [05:17&lt;01:43,  1.40s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 227/300 [05:19&lt;01:42,  1.40s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 228/300 [05:20&lt;01:40,  1.40s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 229/300 [05:22&lt;01:40,  1.41s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 230/300 [05:23&lt;01:38,  1.41s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 231/300 [05:24&lt;01:36,  1.41s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 232/300 [05:26&lt;01:35,  1.40s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 233/300 [05:27&lt;01:34,  1.40s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 234/300 [05:29&lt;01:32,  1.41s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 235/300 [05:30&lt;01:32,  1.42s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 236/300 [05:31&lt;01:30,  1.41s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 237/300 [05:33&lt;01:29,  1.41s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 238/300 [05:34&lt;01:27,  1.42s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 239/300 [05:36&lt;01:25,  1.41s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 240/300 [05:37&lt;01:24,  1.40s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 241/300 [05:39&lt;01:22,  1.40s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 242/300 [05:40&lt;01:21,  1.40s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 243/300 [05:41&lt;01:19,  1.40s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 244/300 [05:43&lt;01:18,  1.41s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 245/300 [05:44&lt;01:17,  1.41s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 246/300 [05:46&lt;01:15,  1.41s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 247/300 [05:47&lt;01:14,  1.40s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 248/300 [05:48&lt;01:13,  1.41s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 249/300 [05:50&lt;01:11,  1.41s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 250/300 [05:51&lt;01:10,  1.41s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 251/300 [05:53&lt;01:08,  1.41s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 252/300 [05:54&lt;01:07,  1.40s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 253/300 [05:55&lt;01:06,  1.41s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 254/300 [05:57&lt;01:04,  1.41s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 255/300 [05:58&lt;01:03,  1.41s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 256/300 [06:00&lt;01:01,  1.40s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 257/300 [06:01&lt;01:00,  1.40s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 258/300 [06:02&lt;00:58,  1.40s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 259/300 [06:04&lt;00:57,  1.40s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 260/300 [06:05&lt;00:56,  1.40s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 261/300 [06:07&lt;00:54,  1.40s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 262/300 [06:08&lt;00:54,  1.45s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 263/300 [06:10&lt;00:52,  1.43s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 264/300 [06:11&lt;00:51,  1.44s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 265/300 [06:12&lt;00:49,  1.43s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 266/300 [06:14&lt;00:48,  1.42s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 267/300 [06:15&lt;00:46,  1.42s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 268/300 [06:17&lt;00:45,  1.42s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 269/300 [06:18&lt;00:43,  1.41s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 270/300 [06:19&lt;00:42,  1.42s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 271/300 [06:21&lt;00:40,  1.41s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 272/300 [06:22&lt;00:39,  1.41s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 273/300 [06:24&lt;00:38,  1.43s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 274/300 [06:25&lt;00:37,  1.42s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 275/300 [06:27&lt;00:35,  1.41s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 276/300 [06:28&lt;00:33,  1.41s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 277/300 [06:29&lt;00:32,  1.40s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 278/300 [06:31&lt;00:30,  1.40s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 279/300 [06:32&lt;00:29,  1.40s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 280/300 [06:34&lt;00:28,  1.41s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 281/300 [06:35&lt;00:26,  1.41s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 282/300 [06:36&lt;00:25,  1.41s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 283/300 [06:38&lt;00:23,  1.40s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 284/300 [06:39&lt;00:22,  1.40s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 285/300 [06:41&lt;00:21,  1.40s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 286/300 [06:42&lt;00:19,  1.40s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 287/300 [06:43&lt;00:18,  1.40s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 288/300 [06:45&lt;00:16,  1.41s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 289/300 [06:46&lt;00:15,  1.40s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 290/300 [06:48&lt;00:13,  1.40s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 291/300 [06:49&lt;00:12,  1.40s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 292/300 [06:50&lt;00:11,  1.40s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 293/300 [06:52&lt;00:09,  1.40s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 294/300 [06:53&lt;00:08,  1.43s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 295/300 [06:55&lt;00:07,  1.42s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 296/300 [06:56&lt;00:05,  1.41s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 297/300 [06:57&lt;00:04,  1.41s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 298/300 [06:59&lt;00:02,  1.41s/it]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 299/300 [07:00&lt;00:01,  1.41s/it]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [07:02&lt;00:00,  1.41s/it]\n\n6it [42:19, 423.34s/it]\n\n  0%|          | 0/300 [00:00&lt;?, ?it/s]\n\n  0%|          | 1/300 [00:00&lt;04:55,  1.01it/s]\n\n  1%|          | 2/300 [00:02&lt;06:10,  1.24s/it]\n\n  1%|          | 3/300 [00:03&lt;06:29,  1.31s/it]\n\n  1%|‚ñè         | 4/300 [00:05&lt;06:39,  1.35s/it]\n\n  2%|‚ñè         | 5/300 [00:06&lt;06:41,  1.36s/it]\n\n  2%|‚ñè         | 6/300 [00:07&lt;06:43,  1.37s/it]\n\n  2%|‚ñè         | 7/300 [00:09&lt;06:43,  1.38s/it]\n\n  3%|‚ñé         | 8/300 [00:10&lt;06:43,  1.38s/it]\n\n  3%|‚ñé         | 9/300 [00:12&lt;06:42,  1.38s/it]\n\n  3%|‚ñé         | 10/300 [00:13&lt;06:41,  1.38s/it]\n\n  4%|‚ñé         | 11/300 [00:14&lt;06:41,  1.39s/it]\n\n  4%|‚ñç         | 12/300 [00:16&lt;06:40,  1.39s/it]\n\n  4%|‚ñç         | 13/300 [00:17&lt;06:40,  1.39s/it]\n\n  5%|‚ñç         | 14/300 [00:19&lt;06:45,  1.42s/it]\n\n  5%|‚ñå         | 15/300 [00:20&lt;06:41,  1.41s/it]\n\n  5%|‚ñå         | 16/300 [00:21&lt;06:38,  1.40s/it]\n\n  6%|‚ñå         | 17/300 [00:23&lt;06:37,  1.40s/it]\n\n  6%|‚ñå         | 18/300 [00:24&lt;06:35,  1.40s/it]\n\n  6%|‚ñã         | 19/300 [00:26&lt;06:32,  1.40s/it]\n\n  7%|‚ñã         | 20/300 [00:27&lt;06:30,  1.40s/it]\n\n  7%|‚ñã         | 21/300 [00:28&lt;06:29,  1.39s/it]\n\n  7%|‚ñã         | 22/300 [00:30&lt;06:28,  1.40s/it]\n\n  8%|‚ñä         | 23/300 [00:31&lt;06:26,  1.40s/it]\n\n  8%|‚ñä         | 24/300 [00:33&lt;06:29,  1.41s/it]\n\n  8%|‚ñä         | 25/300 [00:34&lt;06:26,  1.40s/it]\n\n  9%|‚ñä         | 26/300 [00:35&lt;06:23,  1.40s/it]\n\n  9%|‚ñâ         | 27/300 [00:37&lt;06:21,  1.40s/it]\n\n  9%|‚ñâ         | 28/300 [00:38&lt;06:19,  1.40s/it]\n\n 10%|‚ñâ         | 29/300 [00:40&lt;06:17,  1.39s/it]\n\n 10%|‚ñà         | 30/300 [00:41&lt;06:16,  1.39s/it]\n\n 10%|‚ñà         | 31/300 [00:42&lt;06:15,  1.39s/it]\n\n 11%|‚ñà         | 32/300 [00:44&lt;06:15,  1.40s/it]\n\n 11%|‚ñà         | 33/300 [00:45&lt;06:13,  1.40s/it]\n\n 11%|‚ñà‚ñè        | 34/300 [00:47&lt;06:12,  1.40s/it]\n\n 12%|‚ñà‚ñè        | 35/300 [00:48&lt;06:12,  1.41s/it]\n\n 12%|‚ñà‚ñè        | 36/300 [00:49&lt;06:10,  1.40s/it]\n\n 12%|‚ñà‚ñè        | 37/300 [00:51&lt;06:08,  1.40s/it]\n\n 13%|‚ñà‚ñé        | 38/300 [00:52&lt;06:06,  1.40s/it]\n\n 13%|‚ñà‚ñé        | 39/300 [00:54&lt;06:04,  1.40s/it]\n\n 13%|‚ñà‚ñé        | 40/300 [00:55&lt;06:03,  1.40s/it]\n\n 14%|‚ñà‚ñé        | 41/300 [00:56&lt;06:01,  1.40s/it]\n\n 14%|‚ñà‚ñç        | 42/300 [00:58&lt;06:00,  1.40s/it]\n\n 14%|‚ñà‚ñç        | 43/300 [00:59&lt;05:59,  1.40s/it]\n\n 15%|‚ñà‚ñç        | 44/300 [01:01&lt;06:00,  1.41s/it]\n\n 15%|‚ñà‚ñå        | 45/300 [01:02&lt;05:58,  1.41s/it]\n\n 15%|‚ñà‚ñå        | 46/300 [01:03&lt;05:57,  1.41s/it]\n\n 16%|‚ñà‚ñå        | 47/300 [01:05&lt;05:54,  1.40s/it]\n\n 16%|‚ñà‚ñå        | 48/300 [01:06&lt;05:53,  1.40s/it]\n\n 16%|‚ñà‚ñã        | 49/300 [01:08&lt;05:51,  1.40s/it]\n\n 17%|‚ñà‚ñã        | 50/300 [01:09&lt;05:49,  1.40s/it]\n\n 17%|‚ñà‚ñã        | 51/300 [01:10&lt;05:47,  1.40s/it]\n\n 17%|‚ñà‚ñã        | 52/300 [01:12&lt;05:47,  1.40s/it]\n\n 18%|‚ñà‚ñä        | 53/300 [01:13&lt;05:46,  1.40s/it]\n\n 18%|‚ñà‚ñä        | 54/300 [01:15&lt;05:46,  1.41s/it]\n\n 18%|‚ñà‚ñä        | 55/300 [01:16&lt;05:52,  1.44s/it]\n\n 19%|‚ñà‚ñä        | 56/300 [01:18&lt;05:47,  1.43s/it]\n\n 19%|‚ñà‚ñâ        | 57/300 [01:19&lt;05:44,  1.42s/it]\n\n 19%|‚ñà‚ñâ        | 58/300 [01:20&lt;05:41,  1.41s/it]\n\n 20%|‚ñà‚ñâ        | 59/300 [01:22&lt;05:40,  1.41s/it]\n\n 20%|‚ñà‚ñà        | 60/300 [01:23&lt;05:37,  1.40s/it]\n\n 20%|‚ñà‚ñà        | 61/300 [01:25&lt;05:36,  1.41s/it]\n\n 21%|‚ñà‚ñà        | 62/300 [01:26&lt;05:37,  1.42s/it]\n\n 21%|‚ñà‚ñà        | 63/300 [01:27&lt;05:34,  1.41s/it]\n\n 21%|‚ñà‚ñà‚ñè       | 64/300 [01:29&lt;05:33,  1.41s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 65/300 [01:30&lt;05:30,  1.41s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 66/300 [01:32&lt;05:28,  1.40s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 67/300 [01:33&lt;05:26,  1.40s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 68/300 [01:34&lt;05:24,  1.40s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 69/300 [01:36&lt;05:22,  1.40s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 70/300 [01:37&lt;05:21,  1.40s/it]\n\n 24%|‚ñà‚ñà‚ñé       | 71/300 [01:39&lt;05:19,  1.40s/it]\n\n 24%|‚ñà‚ñà‚ñç       | 72/300 [01:40&lt;05:18,  1.40s/it]\n\n 24%|‚ñà‚ñà‚ñç       | 73/300 [01:41&lt;05:17,  1.40s/it]\n\n 25%|‚ñà‚ñà‚ñç       | 74/300 [01:43&lt;05:17,  1.41s/it]\n\n 25%|‚ñà‚ñà‚ñå       | 75/300 [01:44&lt;05:15,  1.40s/it]\n\n 25%|‚ñà‚ñà‚ñå       | 76/300 [01:46&lt;05:14,  1.40s/it]\n\n 26%|‚ñà‚ñà‚ñå       | 77/300 [01:47&lt;05:12,  1.40s/it]\n\n 26%|‚ñà‚ñà‚ñå       | 78/300 [01:48&lt;05:10,  1.40s/it]\n\n 26%|‚ñà‚ñà‚ñã       | 79/300 [01:50&lt;05:08,  1.40s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 80/300 [01:51&lt;05:06,  1.39s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 81/300 [01:53&lt;05:06,  1.40s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 82/300 [01:54&lt;05:04,  1.40s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 83/300 [01:55&lt;05:04,  1.41s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 84/300 [01:57&lt;05:03,  1.41s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 85/300 [01:58&lt;05:01,  1.40s/it]\n\n 29%|‚ñà‚ñà‚ñä       | 86/300 [02:00&lt;04:59,  1.40s/it]\n\n 29%|‚ñà‚ñà‚ñâ       | 87/300 [02:01&lt;04:57,  1.40s/it]\n\n 29%|‚ñà‚ñà‚ñâ       | 88/300 [02:02&lt;04:56,  1.40s/it]\n\n 30%|‚ñà‚ñà‚ñâ       | 89/300 [02:04&lt;04:55,  1.40s/it]\n\n 30%|‚ñà‚ñà‚ñà       | 90/300 [02:05&lt;04:54,  1.40s/it]\n\n 30%|‚ñà‚ñà‚ñà       | 91/300 [02:07&lt;04:52,  1.40s/it]\n\n 31%|‚ñà‚ñà‚ñà       | 92/300 [02:08&lt;04:54,  1.42s/it]\n\n 31%|‚ñà‚ñà‚ñà       | 93/300 [02:10&lt;04:52,  1.41s/it]\n\n 31%|‚ñà‚ñà‚ñà‚ñè      | 94/300 [02:11&lt;04:51,  1.42s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 95/300 [02:12&lt;04:48,  1.41s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 96/300 [02:14&lt;04:48,  1.42s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 97/300 [02:15&lt;04:46,  1.41s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 98/300 [02:17&lt;04:46,  1.42s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 99/300 [02:18&lt;04:43,  1.41s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 100/300 [02:19&lt;04:43,  1.42s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñé      | 101/300 [02:21&lt;04:41,  1.41s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñç      | 102/300 [02:22&lt;04:39,  1.41s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñç      | 103/300 [02:24&lt;04:37,  1.41s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñç      | 104/300 [02:25&lt;04:36,  1.41s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñå      | 105/300 [02:26&lt;04:33,  1.40s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñå      | 106/300 [02:28&lt;04:31,  1.40s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñå      | 107/300 [02:29&lt;04:29,  1.40s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñå      | 108/300 [02:31&lt;04:28,  1.40s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñã      | 109/300 [02:32&lt;04:27,  1.40s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 110/300 [02:33&lt;04:26,  1.40s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 111/300 [02:35&lt;04:24,  1.40s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 112/300 [02:36&lt;04:23,  1.40s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 113/300 [02:38&lt;04:21,  1.40s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 114/300 [02:39&lt;04:21,  1.41s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 115/300 [02:40&lt;04:20,  1.41s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñä      | 116/300 [02:42&lt;04:17,  1.40s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñâ      | 117/300 [02:43&lt;04:15,  1.40s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñâ      | 118/300 [02:45&lt;04:13,  1.40s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñâ      | 119/300 [02:46&lt;04:12,  1.39s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñà      | 120/300 [02:47&lt;04:10,  1.39s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñà      | 121/300 [02:49&lt;04:09,  1.39s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà      | 122/300 [02:50&lt;04:07,  1.39s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà      | 123/300 [02:52&lt;04:07,  1.40s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 124/300 [02:53&lt;04:09,  1.42s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 125/300 [02:54&lt;04:06,  1.41s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 126/300 [02:56&lt;04:03,  1.40s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 127/300 [02:57&lt;04:01,  1.40s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 128/300 [02:59&lt;04:00,  1.40s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 129/300 [03:00&lt;03:59,  1.40s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 130/300 [03:01&lt;03:57,  1.40s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 131/300 [03:03&lt;03:57,  1.40s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 132/300 [03:04&lt;03:56,  1.41s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 133/300 [03:06&lt;03:54,  1.41s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 134/300 [03:07&lt;03:53,  1.41s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 135/300 [03:08&lt;03:51,  1.40s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 136/300 [03:10&lt;03:49,  1.40s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 137/300 [03:11&lt;03:47,  1.40s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 138/300 [03:13&lt;03:45,  1.39s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 139/300 [03:14&lt;03:44,  1.40s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 140/300 [03:15&lt;03:43,  1.39s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 141/300 [03:17&lt;03:41,  1.39s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 142/300 [03:18&lt;03:40,  1.39s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 143/300 [03:20&lt;03:41,  1.41s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 144/300 [03:21&lt;03:39,  1.41s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 145/300 [03:22&lt;03:37,  1.41s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 146/300 [03:24&lt;03:35,  1.40s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 147/300 [03:25&lt;03:33,  1.40s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 148/300 [03:27&lt;03:37,  1.43s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 149/300 [03:28&lt;03:34,  1.42s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 150/300 [03:30&lt;03:31,  1.41s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 151/300 [03:31&lt;03:29,  1.41s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 152/300 [03:32&lt;03:28,  1.41s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 153/300 [03:34&lt;03:26,  1.41s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 154/300 [03:35&lt;03:26,  1.41s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 155/300 [03:37&lt;03:24,  1.41s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 156/300 [03:38&lt;03:21,  1.40s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 157/300 [03:39&lt;03:20,  1.40s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 158/300 [03:41&lt;03:18,  1.40s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 159/300 [03:42&lt;03:16,  1.40s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 160/300 [03:44&lt;03:15,  1.39s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 161/300 [03:45&lt;03:13,  1.39s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 162/300 [03:46&lt;03:13,  1.40s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 163/300 [03:48&lt;03:12,  1.40s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 164/300 [03:49&lt;03:11,  1.41s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 165/300 [03:51&lt;03:09,  1.40s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 166/300 [03:52&lt;03:07,  1.40s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 167/300 [03:53&lt;03:05,  1.40s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 168/300 [03:55&lt;03:04,  1.40s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 169/300 [03:56&lt;03:02,  1.40s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 170/300 [03:58&lt;03:01,  1.39s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 171/300 [03:59&lt;02:59,  1.39s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 172/300 [04:00&lt;02:59,  1.40s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 173/300 [04:02&lt;02:58,  1.40s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 174/300 [04:03&lt;02:56,  1.40s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 175/300 [04:05&lt;02:54,  1.40s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 176/300 [04:06&lt;02:52,  1.39s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 177/300 [04:07&lt;02:51,  1.39s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 178/300 [04:09&lt;02:50,  1.39s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 179/300 [04:10&lt;02:48,  1.39s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 180/300 [04:11&lt;02:47,  1.39s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 181/300 [04:13&lt;02:46,  1.40s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 182/300 [04:14&lt;02:44,  1.40s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 183/300 [04:16&lt;02:44,  1.40s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 184/300 [04:17&lt;02:42,  1.40s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 185/300 [04:18&lt;02:40,  1.40s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 186/300 [04:20&lt;02:38,  1.39s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 187/300 [04:21&lt;02:37,  1.39s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 188/300 [04:23&lt;02:36,  1.39s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 189/300 [04:24&lt;02:34,  1.39s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 190/300 [04:25&lt;02:33,  1.40s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 191/300 [04:27&lt;02:32,  1.40s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 192/300 [04:28&lt;02:31,  1.41s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 193/300 [04:30&lt;02:30,  1.41s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 194/300 [04:31&lt;02:28,  1.40s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 195/300 [04:32&lt;02:26,  1.40s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 196/300 [04:34&lt;02:25,  1.40s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 197/300 [04:35&lt;02:23,  1.40s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 198/300 [04:37&lt;02:22,  1.40s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 199/300 [04:38&lt;02:20,  1.39s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 200/300 [04:39&lt;02:19,  1.39s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 201/300 [04:41&lt;02:18,  1.39s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 202/300 [04:42&lt;02:16,  1.40s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 203/300 [04:44&lt;02:15,  1.40s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 204/300 [04:45&lt;02:14,  1.40s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 205/300 [04:46&lt;02:13,  1.40s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 206/300 [04:48&lt;02:11,  1.40s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 207/300 [04:49&lt;02:09,  1.40s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 208/300 [04:51&lt;02:09,  1.40s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 209/300 [04:52&lt;02:07,  1.40s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 210/300 [04:53&lt;02:05,  1.40s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 211/300 [04:55&lt;02:04,  1.40s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 212/300 [04:56&lt;02:03,  1.40s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 213/300 [04:58&lt;02:02,  1.41s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 214/300 [04:59&lt;02:01,  1.41s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 215/300 [05:00&lt;01:59,  1.41s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 216/300 [05:02&lt;01:57,  1.40s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 217/300 [05:03&lt;01:56,  1.40s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 218/300 [05:05&lt;01:54,  1.40s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 219/300 [05:06&lt;01:53,  1.40s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 220/300 [05:07&lt;01:51,  1.40s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 221/300 [05:09&lt;01:50,  1.40s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 222/300 [05:10&lt;01:49,  1.40s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 223/300 [05:12&lt;01:48,  1.40s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 224/300 [05:13&lt;01:46,  1.40s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 225/300 [05:14&lt;01:44,  1.40s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 226/300 [05:16&lt;01:43,  1.40s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 227/300 [05:17&lt;01:41,  1.39s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 228/300 [05:19&lt;01:40,  1.39s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 229/300 [05:20&lt;01:38,  1.39s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 230/300 [05:21&lt;01:37,  1.39s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 231/300 [05:23&lt;01:36,  1.40s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 232/300 [05:24&lt;01:35,  1.40s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 233/300 [05:26&lt;01:34,  1.40s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 234/300 [05:27&lt;01:32,  1.40s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 235/300 [05:28&lt;01:31,  1.40s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 236/300 [05:30&lt;01:29,  1.40s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 237/300 [05:31&lt;01:30,  1.43s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 238/300 [05:33&lt;01:28,  1.43s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 239/300 [05:34&lt;01:26,  1.42s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 240/300 [05:36&lt;01:24,  1.41s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 241/300 [05:37&lt;01:22,  1.41s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 242/300 [05:38&lt;01:21,  1.41s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 243/300 [05:40&lt;01:20,  1.41s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 244/300 [05:41&lt;01:19,  1.41s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 245/300 [05:43&lt;01:17,  1.41s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 246/300 [05:44&lt;01:16,  1.41s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 247/300 [05:45&lt;01:14,  1.41s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 248/300 [05:47&lt;01:13,  1.41s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 249/300 [05:48&lt;01:11,  1.41s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 250/300 [05:50&lt;01:10,  1.41s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 251/300 [05:51&lt;01:08,  1.41s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 252/300 [05:52&lt;01:07,  1.41s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 253/300 [05:54&lt;01:06,  1.41s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 254/300 [05:55&lt;01:04,  1.40s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 255/300 [05:57&lt;01:02,  1.40s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 256/300 [05:58&lt;01:01,  1.39s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 257/300 [05:59&lt;00:59,  1.39s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 258/300 [06:01&lt;00:58,  1.39s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 259/300 [06:02&lt;00:56,  1.39s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 260/300 [06:04&lt;00:55,  1.39s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 261/300 [06:05&lt;00:54,  1.39s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 262/300 [06:06&lt;00:53,  1.40s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 263/300 [06:08&lt;00:51,  1.40s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 264/300 [06:09&lt;00:50,  1.40s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 265/300 [06:11&lt;00:48,  1.40s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 266/300 [06:12&lt;00:47,  1.39s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 267/300 [06:13&lt;00:46,  1.40s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 268/300 [06:15&lt;00:44,  1.40s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 269/300 [06:16&lt;00:43,  1.40s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 270/300 [06:18&lt;00:41,  1.40s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 271/300 [06:19&lt;00:40,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 272/300 [06:20&lt;00:39,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 273/300 [06:22&lt;00:37,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 274/300 [06:23&lt;00:36,  1.40s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 275/300 [06:25&lt;00:34,  1.40s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 276/300 [06:26&lt;00:33,  1.39s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 277/300 [06:27&lt;00:32,  1.40s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 278/300 [06:29&lt;00:30,  1.40s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 279/300 [06:30&lt;00:29,  1.40s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 280/300 [06:32&lt;00:27,  1.39s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 281/300 [06:33&lt;00:26,  1.41s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 282/300 [06:34&lt;00:25,  1.42s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 283/300 [06:36&lt;00:24,  1.41s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 284/300 [06:37&lt;00:22,  1.41s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 285/300 [06:39&lt;00:21,  1.40s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 286/300 [06:40&lt;00:19,  1.41s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 287/300 [06:41&lt;00:18,  1.40s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 288/300 [06:43&lt;00:16,  1.40s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 289/300 [06:44&lt;00:15,  1.40s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 290/300 [06:46&lt;00:13,  1.40s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 291/300 [06:47&lt;00:12,  1.40s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 292/300 [06:48&lt;00:11,  1.40s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 293/300 [06:50&lt;00:09,  1.40s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 294/300 [06:51&lt;00:08,  1.40s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 295/300 [06:53&lt;00:07,  1.40s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 296/300 [06:54&lt;00:05,  1.40s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 297/300 [06:55&lt;00:04,  1.40s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 298/300 [06:57&lt;00:02,  1.40s/it]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 299/300 [06:58&lt;00:01,  1.40s/it]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [07:00&lt;00:00,  1.40s/it]\n\n7it [49:20, 422.55s/it]\n\n  0%|          | 0/300 [00:00&lt;?, ?it/s]\n\n  0%|          | 1/300 [00:01&lt;04:58,  1.00it/s]\n\n  1%|          | 2/300 [00:02&lt;06:09,  1.24s/it]\n\n  1%|          | 3/300 [00:03&lt;06:29,  1.31s/it]\n\n  1%|‚ñè         | 4/300 [00:05&lt;06:37,  1.34s/it]\n\n  2%|‚ñè         | 5/300 [00:06&lt;06:40,  1.36s/it]\n\n  2%|‚ñè         | 6/300 [00:07&lt;06:43,  1.37s/it]\n\n  2%|‚ñè         | 7/300 [00:09&lt;06:44,  1.38s/it]\n\n  3%|‚ñé         | 8/300 [00:10&lt;06:43,  1.38s/it]\n\n  3%|‚ñé         | 9/300 [00:12&lt;06:44,  1.39s/it]\n\n  3%|‚ñé         | 10/300 [00:13&lt;06:45,  1.40s/it]\n\n  4%|‚ñé         | 11/300 [00:14&lt;06:45,  1.40s/it]\n\n  4%|‚ñç         | 12/300 [00:16&lt;06:45,  1.41s/it]\n\n  4%|‚ñç         | 13/300 [00:17&lt;06:43,  1.41s/it]\n\n  5%|‚ñç         | 14/300 [00:19&lt;06:41,  1.40s/it]\n\n  5%|‚ñå         | 15/300 [00:20&lt;06:42,  1.41s/it]\n\n  5%|‚ñå         | 16/300 [00:22&lt;06:39,  1.41s/it]\n\n  6%|‚ñå         | 17/300 [00:23&lt;06:37,  1.41s/it]\n\n  6%|‚ñå         | 18/300 [00:24&lt;06:36,  1.41s/it]\n\n  6%|‚ñã         | 19/300 [00:26&lt;06:35,  1.41s/it]\n\n  7%|‚ñã         | 20/300 [00:27&lt;06:34,  1.41s/it]\n\n  7%|‚ñã         | 21/300 [00:29&lt;06:33,  1.41s/it]\n\n  7%|‚ñã         | 22/300 [00:30&lt;06:31,  1.41s/it]\n\n  8%|‚ñä         | 23/300 [00:31&lt;06:28,  1.40s/it]\n\n  8%|‚ñä         | 24/300 [00:33&lt;06:26,  1.40s/it]\n\n  8%|‚ñä         | 25/300 [00:34&lt;06:24,  1.40s/it]\n\n  9%|‚ñä         | 26/300 [00:36&lt;06:22,  1.40s/it]\n\n  9%|‚ñâ         | 27/300 [00:37&lt;06:22,  1.40s/it]\n\n  9%|‚ñâ         | 28/300 [00:38&lt;06:19,  1.40s/it]\n\n 10%|‚ñâ         | 29/300 [00:40&lt;06:18,  1.40s/it]\n\n 10%|‚ñà         | 30/300 [00:41&lt;06:16,  1.40s/it]\n\n 10%|‚ñà         | 31/300 [00:43&lt;06:15,  1.40s/it]\n\n 11%|‚ñà         | 32/300 [00:44&lt;06:15,  1.40s/it]\n\n 11%|‚ñà         | 33/300 [00:45&lt;06:13,  1.40s/it]\n\n 11%|‚ñà‚ñè        | 34/300 [00:47&lt;06:11,  1.40s/it]\n\n 12%|‚ñà‚ñè        | 35/300 [00:48&lt;06:09,  1.39s/it]\n\n 12%|‚ñà‚ñè        | 36/300 [00:50&lt;06:08,  1.39s/it]\n\n 12%|‚ñà‚ñè        | 37/300 [00:51&lt;06:06,  1.40s/it]\n\n 13%|‚ñà‚ñé        | 38/300 [00:52&lt;06:05,  1.39s/it]\n\n 13%|‚ñà‚ñé        | 39/300 [00:54&lt;06:03,  1.39s/it]\n\n 13%|‚ñà‚ñé        | 40/300 [00:55&lt;06:02,  1.39s/it]\n\n 14%|‚ñà‚ñé        | 41/300 [00:57&lt;06:01,  1.40s/it]\n\n 14%|‚ñà‚ñç        | 42/300 [00:58&lt;06:00,  1.40s/it]\n\n 14%|‚ñà‚ñç        | 43/300 [00:59&lt;06:00,  1.40s/it]\n\n 15%|‚ñà‚ñç        | 44/300 [01:01&lt;05:57,  1.40s/it]\n\n 15%|‚ñà‚ñå        | 45/300 [01:02&lt;05:56,  1.40s/it]\n\n 15%|‚ñà‚ñå        | 46/300 [01:04&lt;05:54,  1.40s/it]\n\n 16%|‚ñà‚ñå        | 47/300 [01:05&lt;05:54,  1.40s/it]\n\n 16%|‚ñà‚ñå        | 48/300 [01:06&lt;05:52,  1.40s/it]\n\n 16%|‚ñà‚ñã        | 49/300 [01:08&lt;05:49,  1.39s/it]\n\n 17%|‚ñà‚ñã        | 50/300 [01:09&lt;05:51,  1.41s/it]\n\n 17%|‚ñà‚ñã        | 51/300 [01:11&lt;05:50,  1.41s/it]\n\n 17%|‚ñà‚ñã        | 52/300 [01:12&lt;06:01,  1.46s/it]\n\n 18%|‚ñà‚ñä        | 53/300 [01:13&lt;05:54,  1.44s/it]\n\n 18%|‚ñà‚ñä        | 54/300 [01:15&lt;05:51,  1.43s/it]\n\n 18%|‚ñà‚ñä        | 55/300 [01:16&lt;05:47,  1.42s/it]\n\n 19%|‚ñà‚ñä        | 56/300 [01:18&lt;05:44,  1.41s/it]\n\n 19%|‚ñà‚ñâ        | 57/300 [01:19&lt;05:41,  1.40s/it]\n\n 19%|‚ñà‚ñâ        | 58/300 [01:20&lt;05:39,  1.40s/it]\n\n 20%|‚ñà‚ñâ        | 59/300 [01:22&lt;05:36,  1.40s/it]\n\n 20%|‚ñà‚ñà        | 60/300 [01:23&lt;05:35,  1.40s/it]\n\n 20%|‚ñà‚ñà        | 61/300 [01:25&lt;05:34,  1.40s/it]\n\n 21%|‚ñà‚ñà        | 62/300 [01:26&lt;05:37,  1.42s/it]\n\n 21%|‚ñà‚ñà        | 63/300 [01:28&lt;05:34,  1.41s/it]\n\n 21%|‚ñà‚ñà‚ñè       | 64/300 [01:29&lt;05:32,  1.41s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 65/300 [01:30&lt;05:29,  1.40s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 66/300 [01:32&lt;05:27,  1.40s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 67/300 [01:33&lt;05:25,  1.40s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 68/300 [01:35&lt;05:25,  1.40s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 69/300 [01:36&lt;05:23,  1.40s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 70/300 [01:37&lt;05:23,  1.41s/it]\n\n 24%|‚ñà‚ñà‚ñé       | 71/300 [01:39&lt;05:22,  1.41s/it]\n\n 24%|‚ñà‚ñà‚ñç       | 72/300 [01:40&lt;05:20,  1.41s/it]\n\n 24%|‚ñà‚ñà‚ñç       | 73/300 [01:42&lt;05:18,  1.40s/it]\n\n 25%|‚ñà‚ñà‚ñç       | 74/300 [01:43&lt;05:16,  1.40s/it]\n\n 25%|‚ñà‚ñà‚ñå       | 75/300 [01:44&lt;05:15,  1.40s/it]\n\n 25%|‚ñà‚ñà‚ñå       | 76/300 [01:46&lt;05:13,  1.40s/it]\n\n 26%|‚ñà‚ñà‚ñå       | 77/300 [01:47&lt;05:11,  1.40s/it]\n\n 26%|‚ñà‚ñà‚ñå       | 78/300 [01:49&lt;05:10,  1.40s/it]\n\n 26%|‚ñà‚ñà‚ñã       | 79/300 [01:50&lt;05:08,  1.40s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 80/300 [01:51&lt;05:08,  1.40s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 81/300 [01:53&lt;05:08,  1.41s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 82/300 [01:54&lt;05:06,  1.41s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 83/300 [01:56&lt;05:04,  1.40s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 84/300 [01:57&lt;05:02,  1.40s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 85/300 [01:58&lt;05:00,  1.40s/it]\n\n 29%|‚ñà‚ñà‚ñä       | 86/300 [02:00&lt;04:58,  1.39s/it]\n\n 29%|‚ñà‚ñà‚ñâ       | 87/300 [02:01&lt;04:56,  1.39s/it]\n\n 29%|‚ñà‚ñà‚ñâ       | 88/300 [02:03&lt;04:55,  1.40s/it]\n\n 30%|‚ñà‚ñà‚ñâ       | 89/300 [02:04&lt;04:54,  1.39s/it]\n\n 30%|‚ñà‚ñà‚ñà       | 90/300 [02:05&lt;04:56,  1.41s/it]\n\n 30%|‚ñà‚ñà‚ñà       | 91/300 [02:07&lt;04:54,  1.41s/it]\n\n 31%|‚ñà‚ñà‚ñà       | 92/300 [02:08&lt;04:53,  1.41s/it]\n\n 31%|‚ñà‚ñà‚ñà       | 93/300 [02:10&lt;04:53,  1.42s/it]\n\n 31%|‚ñà‚ñà‚ñà‚ñè      | 94/300 [02:11&lt;04:50,  1.41s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 95/300 [02:12&lt;04:48,  1.41s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 96/300 [02:14&lt;04:46,  1.40s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 97/300 [02:15&lt;04:45,  1.41s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 98/300 [02:17&lt;04:43,  1.40s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 99/300 [02:18&lt;04:44,  1.41s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 100/300 [02:19&lt;04:42,  1.41s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñé      | 101/300 [02:21&lt;04:40,  1.41s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñç      | 102/300 [02:22&lt;04:38,  1.41s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñç      | 103/300 [02:24&lt;04:36,  1.40s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñç      | 104/300 [02:25&lt;04:34,  1.40s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñå      | 105/300 [02:26&lt;04:32,  1.40s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñå      | 106/300 [02:28&lt;04:32,  1.40s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñå      | 107/300 [02:29&lt;04:29,  1.40s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñå      | 108/300 [02:31&lt;04:28,  1.40s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñã      | 109/300 [02:32&lt;04:27,  1.40s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 110/300 [02:33&lt;04:26,  1.40s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 111/300 [02:35&lt;04:25,  1.41s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 112/300 [02:36&lt;04:23,  1.40s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 113/300 [02:38&lt;04:21,  1.40s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 114/300 [02:39&lt;04:19,  1.40s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 115/300 [02:40&lt;04:18,  1.40s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñä      | 116/300 [02:42&lt;04:16,  1.40s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñâ      | 117/300 [02:43&lt;04:16,  1.40s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñâ      | 118/300 [02:45&lt;04:14,  1.40s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñâ      | 119/300 [02:46&lt;04:13,  1.40s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñà      | 120/300 [02:47&lt;04:11,  1.40s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñà      | 121/300 [02:49&lt;04:10,  1.40s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà      | 122/300 [02:50&lt;04:09,  1.40s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà      | 123/300 [02:52&lt;04:08,  1.41s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 124/300 [02:53&lt;04:06,  1.40s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 125/300 [02:54&lt;04:05,  1.40s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 126/300 [02:56&lt;04:03,  1.40s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 127/300 [02:57&lt;04:01,  1.40s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 128/300 [02:59&lt;04:00,  1.40s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 129/300 [03:00&lt;03:59,  1.40s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 130/300 [03:02&lt;04:00,  1.41s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 131/300 [03:03&lt;03:58,  1.41s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 132/300 [03:04&lt;03:57,  1.42s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 133/300 [03:06&lt;03:55,  1.41s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 134/300 [03:07&lt;03:54,  1.41s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 135/300 [03:09&lt;03:52,  1.41s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 136/300 [03:10&lt;03:50,  1.40s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 137/300 [03:11&lt;03:48,  1.40s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 138/300 [03:13&lt;03:46,  1.40s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 139/300 [03:14&lt;03:46,  1.41s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 140/300 [03:16&lt;03:45,  1.41s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 141/300 [03:17&lt;03:51,  1.46s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 142/300 [03:19&lt;03:49,  1.45s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 143/300 [03:20&lt;03:45,  1.43s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 144/300 [03:21&lt;03:42,  1.42s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 145/300 [03:23&lt;03:39,  1.42s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 146/300 [03:24&lt;03:37,  1.41s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 147/300 [03:26&lt;03:35,  1.41s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 148/300 [03:27&lt;03:34,  1.41s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 149/300 [03:28&lt;03:33,  1.41s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 150/300 [03:30&lt;03:31,  1.41s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 151/300 [03:31&lt;03:29,  1.41s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 152/300 [03:33&lt;03:27,  1.40s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 153/300 [03:34&lt;03:25,  1.40s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 154/300 [03:35&lt;03:23,  1.40s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 155/300 [03:37&lt;03:22,  1.39s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 156/300 [03:38&lt;03:20,  1.39s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 157/300 [03:40&lt;03:19,  1.40s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 158/300 [03:41&lt;03:18,  1.40s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 159/300 [03:42&lt;03:18,  1.40s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 160/300 [03:44&lt;03:18,  1.42s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 161/300 [03:45&lt;03:15,  1.41s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 162/300 [03:47&lt;03:13,  1.41s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 163/300 [03:48&lt;03:11,  1.40s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 164/300 [03:49&lt;03:11,  1.41s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 165/300 [03:51&lt;03:09,  1.40s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 166/300 [03:52&lt;03:08,  1.41s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 167/300 [03:54&lt;03:06,  1.40s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 168/300 [03:55&lt;03:05,  1.40s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 169/300 [03:56&lt;03:03,  1.40s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 170/300 [03:58&lt;03:03,  1.41s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 171/300 [03:59&lt;03:01,  1.41s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 172/300 [04:01&lt;02:59,  1.40s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 173/300 [04:02&lt;02:58,  1.40s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 174/300 [04:03&lt;02:56,  1.40s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 175/300 [04:05&lt;02:54,  1.40s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 176/300 [04:06&lt;02:53,  1.40s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 177/300 [04:08&lt;02:52,  1.40s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 178/300 [04:09&lt;02:51,  1.41s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 179/300 [04:10&lt;02:50,  1.41s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 180/300 [04:12&lt;02:49,  1.42s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 181/300 [04:13&lt;02:47,  1.41s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 182/300 [04:15&lt;02:45,  1.41s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 183/300 [04:16&lt;02:44,  1.40s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 184/300 [04:18&lt;02:42,  1.40s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 185/300 [04:19&lt;02:40,  1.40s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 186/300 [04:20&lt;02:39,  1.40s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 187/300 [04:22&lt;02:37,  1.40s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 188/300 [04:23&lt;02:37,  1.40s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 189/300 [04:25&lt;02:37,  1.42s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 190/300 [04:26&lt;02:35,  1.41s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 191/300 [04:27&lt;02:33,  1.41s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 192/300 [04:29&lt;02:31,  1.40s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 193/300 [04:30&lt;02:29,  1.40s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 194/300 [04:32&lt;02:28,  1.40s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 195/300 [04:33&lt;02:26,  1.40s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 196/300 [04:34&lt;02:25,  1.40s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 197/300 [04:36&lt;02:23,  1.40s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 198/300 [04:37&lt;02:23,  1.40s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 199/300 [04:39&lt;02:22,  1.41s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 200/300 [04:40&lt;02:20,  1.41s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 201/300 [04:41&lt;02:18,  1.40s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 202/300 [04:43&lt;02:17,  1.40s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 203/300 [04:44&lt;02:15,  1.40s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 204/300 [04:46&lt;02:14,  1.40s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 205/300 [04:47&lt;02:13,  1.40s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 206/300 [04:48&lt;02:11,  1.40s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 207/300 [04:50&lt;02:10,  1.41s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 208/300 [04:51&lt;02:09,  1.40s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 209/300 [04:53&lt;02:07,  1.40s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 210/300 [04:54&lt;02:06,  1.40s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 211/300 [04:55&lt;02:04,  1.40s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 212/300 [04:57&lt;02:03,  1.40s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 213/300 [04:58&lt;02:01,  1.40s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 214/300 [05:00&lt;02:00,  1.41s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 215/300 [05:01&lt;01:59,  1.40s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 216/300 [05:02&lt;01:58,  1.41s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 217/300 [05:04&lt;01:57,  1.41s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 218/300 [05:05&lt;01:55,  1.41s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 219/300 [05:07&lt;01:54,  1.41s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 220/300 [05:08&lt;01:52,  1.41s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 221/300 [05:09&lt;01:50,  1.40s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 222/300 [05:11&lt;01:49,  1.40s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 223/300 [05:12&lt;01:47,  1.40s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 224/300 [05:14&lt;01:46,  1.40s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 225/300 [05:15&lt;01:44,  1.39s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 226/300 [05:16&lt;01:43,  1.39s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 227/300 [05:18&lt;01:41,  1.39s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 228/300 [05:19&lt;01:40,  1.39s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 229/300 [05:21&lt;01:39,  1.40s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 230/300 [05:22&lt;01:38,  1.40s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 231/300 [05:23&lt;01:36,  1.40s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 232/300 [05:25&lt;01:35,  1.41s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 233/300 [05:26&lt;01:33,  1.40s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 234/300 [05:28&lt;01:32,  1.40s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 235/300 [05:29&lt;01:30,  1.40s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 236/300 [05:30&lt;01:29,  1.40s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 237/300 [05:32&lt;01:28,  1.41s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 238/300 [05:33&lt;01:27,  1.41s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 239/300 [05:35&lt;01:25,  1.40s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 240/300 [05:36&lt;01:24,  1.41s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 241/300 [05:37&lt;01:22,  1.40s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 242/300 [05:39&lt;01:21,  1.40s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 243/300 [05:40&lt;01:19,  1.40s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 244/300 [05:42&lt;01:18,  1.40s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 245/300 [05:43&lt;01:16,  1.39s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 246/300 [05:44&lt;01:15,  1.40s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 247/300 [05:46&lt;01:14,  1.40s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 248/300 [05:47&lt;01:12,  1.40s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 249/300 [05:49&lt;01:11,  1.40s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 250/300 [05:50&lt;01:10,  1.40s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 251/300 [05:51&lt;01:08,  1.40s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 252/300 [05:53&lt;01:07,  1.40s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 253/300 [05:54&lt;01:05,  1.40s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 254/300 [05:56&lt;01:04,  1.40s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 255/300 [05:57&lt;01:03,  1.40s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 256/300 [05:58&lt;01:01,  1.40s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 257/300 [06:00&lt;01:00,  1.40s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 258/300 [06:01&lt;00:58,  1.40s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 259/300 [06:03&lt;00:57,  1.40s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 260/300 [06:04&lt;00:55,  1.40s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 261/300 [06:05&lt;00:54,  1.40s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 262/300 [06:07&lt;00:53,  1.41s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 263/300 [06:08&lt;00:51,  1.40s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 264/300 [06:10&lt;00:50,  1.40s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 265/300 [06:11&lt;00:50,  1.43s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 266/300 [06:12&lt;00:48,  1.42s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 267/300 [06:14&lt;00:46,  1.41s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 268/300 [06:15&lt;00:45,  1.41s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 269/300 [06:17&lt;00:43,  1.41s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 270/300 [06:18&lt;00:42,  1.41s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 271/300 [06:20&lt;00:40,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 272/300 [06:21&lt;00:39,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 273/300 [06:22&lt;00:37,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 274/300 [06:24&lt;00:36,  1.39s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 275/300 [06:25&lt;00:34,  1.39s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 276/300 [06:26&lt;00:33,  1.40s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 277/300 [06:28&lt;00:32,  1.40s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 278/300 [06:29&lt;00:30,  1.41s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 279/300 [06:31&lt;00:29,  1.41s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 280/300 [06:32&lt;00:28,  1.40s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 281/300 [06:34&lt;00:26,  1.40s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 282/300 [06:35&lt;00:25,  1.40s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 283/300 [06:36&lt;00:23,  1.40s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 284/300 [06:38&lt;00:22,  1.40s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 285/300 [06:39&lt;00:20,  1.40s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 286/300 [06:40&lt;00:19,  1.40s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 287/300 [06:42&lt;00:18,  1.40s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 288/300 [06:43&lt;00:16,  1.40s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 289/300 [06:45&lt;00:15,  1.40s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 290/300 [06:46&lt;00:13,  1.40s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 291/300 [06:47&lt;00:12,  1.39s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 292/300 [06:49&lt;00:11,  1.39s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 293/300 [06:50&lt;00:09,  1.39s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 294/300 [06:52&lt;00:08,  1.39s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 295/300 [06:53&lt;00:06,  1.39s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 296/300 [06:54&lt;00:05,  1.39s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 297/300 [06:56&lt;00:04,  1.39s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 298/300 [06:57&lt;00:02,  1.40s/it]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 299/300 [06:59&lt;00:01,  1.40s/it]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [07:00&lt;00:00,  1.40s/it]\n\n8it [56:22, 422.18s/it]\n\n  0%|          | 0/300 [00:00&lt;?, ?it/s]\n\n  0%|          | 1/300 [00:00&lt;04:55,  1.01it/s]\n\n  1%|          | 2/300 [00:02&lt;06:06,  1.23s/it]\n\n  1%|          | 3/300 [00:03&lt;06:28,  1.31s/it]\n\n  1%|‚ñè         | 4/300 [00:05&lt;06:37,  1.34s/it]\n\n  2%|‚ñè         | 5/300 [00:06&lt;06:41,  1.36s/it]\n\n  2%|‚ñè         | 6/300 [00:07&lt;06:45,  1.38s/it]\n\n  2%|‚ñè         | 7/300 [00:09&lt;06:47,  1.39s/it]\n\n  3%|‚ñé         | 8/300 [00:10&lt;06:49,  1.40s/it]\n\n  3%|‚ñé         | 9/300 [00:12&lt;06:47,  1.40s/it]\n\n  3%|‚ñé         | 10/300 [00:13&lt;06:44,  1.40s/it]\n\n  4%|‚ñé         | 11/300 [00:15&lt;06:43,  1.40s/it]\n\n  4%|‚ñç         | 12/300 [00:16&lt;06:41,  1.40s/it]\n\n  4%|‚ñç         | 13/300 [00:17&lt;06:41,  1.40s/it]\n\n  5%|‚ñç         | 14/300 [00:19&lt;06:40,  1.40s/it]\n\n  5%|‚ñå         | 15/300 [00:20&lt;06:39,  1.40s/it]\n\n  5%|‚ñå         | 16/300 [00:22&lt;06:39,  1.41s/it]\n\n  6%|‚ñå         | 17/300 [00:23&lt;06:37,  1.41s/it]\n\n  6%|‚ñå         | 18/300 [00:24&lt;06:38,  1.41s/it]\n\n  6%|‚ñã         | 19/300 [00:26&lt;06:35,  1.41s/it]\n\n  7%|‚ñã         | 20/300 [00:27&lt;06:32,  1.40s/it]\n\n  7%|‚ñã         | 21/300 [00:29&lt;06:35,  1.42s/it]\n\n  7%|‚ñã         | 22/300 [00:30&lt;06:32,  1.41s/it]\n\n  8%|‚ñä         | 23/300 [00:31&lt;06:30,  1.41s/it]\n\n  8%|‚ñä         | 24/300 [00:33&lt;06:28,  1.41s/it]\n\n  8%|‚ñä         | 25/300 [00:34&lt;06:25,  1.40s/it]\n\n  9%|‚ñä         | 26/300 [00:36&lt;06:24,  1.40s/it]\n\n  9%|‚ñâ         | 27/300 [00:37&lt;06:22,  1.40s/it]\n\n  9%|‚ñâ         | 28/300 [00:38&lt;06:20,  1.40s/it]\n\n 10%|‚ñâ         | 29/300 [00:40&lt;06:18,  1.40s/it]\n\n 10%|‚ñà         | 30/300 [00:41&lt;06:18,  1.40s/it]\n\n 10%|‚ñà         | 31/300 [00:43&lt;06:15,  1.40s/it]\n\n 11%|‚ñà         | 32/300 [00:44&lt;06:14,  1.40s/it]\n\n 11%|‚ñà         | 33/300 [00:45&lt;06:12,  1.39s/it]\n\n 11%|‚ñà‚ñè        | 34/300 [00:47&lt;06:11,  1.39s/it]\n\n 12%|‚ñà‚ñè        | 35/300 [00:48&lt;06:13,  1.41s/it]\n\n 12%|‚ñà‚ñè        | 36/300 [00:50&lt;06:13,  1.41s/it]\n\n 12%|‚ñà‚ñè        | 37/300 [00:51&lt;06:11,  1.41s/it]\n\n 13%|‚ñà‚ñé        | 38/300 [00:52&lt;06:09,  1.41s/it]\n\n 13%|‚ñà‚ñé        | 39/300 [00:54&lt;06:06,  1.40s/it]\n\n 13%|‚ñà‚ñé        | 40/300 [00:55&lt;06:04,  1.40s/it]\n\n 14%|‚ñà‚ñé        | 41/300 [00:57&lt;06:03,  1.41s/it]\n\n 14%|‚ñà‚ñç        | 42/300 [00:58&lt;06:01,  1.40s/it]\n\n 14%|‚ñà‚ñç        | 43/300 [00:59&lt;05:59,  1.40s/it]\n\n 15%|‚ñà‚ñç        | 44/300 [01:01&lt;05:57,  1.40s/it]\n\n 15%|‚ñà‚ñå        | 45/300 [01:02&lt;05:58,  1.41s/it]\n\n 15%|‚ñà‚ñå        | 46/300 [01:04&lt;05:56,  1.40s/it]\n\n 16%|‚ñà‚ñå        | 47/300 [01:05&lt;05:55,  1.40s/it]\n\n 16%|‚ñà‚ñå        | 48/300 [01:06&lt;05:52,  1.40s/it]\n\n 16%|‚ñà‚ñã        | 49/300 [01:08&lt;05:50,  1.40s/it]\n\n 17%|‚ñà‚ñã        | 50/300 [01:09&lt;05:50,  1.40s/it]\n\n 17%|‚ñà‚ñã        | 51/300 [01:11&lt;05:48,  1.40s/it]\n\n 17%|‚ñà‚ñã        | 52/300 [01:12&lt;05:46,  1.40s/it]\n\n 18%|‚ñà‚ñä        | 53/300 [01:13&lt;05:44,  1.40s/it]\n\n 18%|‚ñà‚ñä        | 54/300 [01:15&lt;05:42,  1.39s/it]\n\n 18%|‚ñà‚ñä        | 55/300 [01:16&lt;05:41,  1.39s/it]\n\n 19%|‚ñà‚ñä        | 56/300 [01:18&lt;05:41,  1.40s/it]\n\n 19%|‚ñà‚ñâ        | 57/300 [01:19&lt;05:43,  1.41s/it]\n\n 19%|‚ñà‚ñâ        | 58/300 [01:20&lt;05:40,  1.41s/it]\n\n 20%|‚ñà‚ñâ        | 59/300 [01:22&lt;05:38,  1.40s/it]\n\n 20%|‚ñà‚ñà        | 60/300 [01:23&lt;05:35,  1.40s/it]\n\n 20%|‚ñà‚ñà        | 61/300 [01:25&lt;05:42,  1.43s/it]\n\n 21%|‚ñà‚ñà        | 62/300 [01:26&lt;05:38,  1.42s/it]\n\n 21%|‚ñà‚ñà        | 63/300 [01:28&lt;05:34,  1.41s/it]\n\n 21%|‚ñà‚ñà‚ñè       | 64/300 [01:29&lt;05:33,  1.41s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 65/300 [01:30&lt;05:33,  1.42s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 66/300 [01:32&lt;05:30,  1.41s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 67/300 [01:33&lt;05:29,  1.41s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 68/300 [01:35&lt;05:27,  1.41s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 69/300 [01:36&lt;05:24,  1.41s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 70/300 [01:37&lt;05:22,  1.40s/it]\n\n 24%|‚ñà‚ñà‚ñé       | 71/300 [01:39&lt;05:21,  1.40s/it]\n\n 24%|‚ñà‚ñà‚ñç       | 72/300 [01:40&lt;05:21,  1.41s/it]\n\n 24%|‚ñà‚ñà‚ñç       | 73/300 [01:42&lt;05:18,  1.40s/it]\n\n 25%|‚ñà‚ñà‚ñç       | 74/300 [01:43&lt;05:16,  1.40s/it]\n\n 25%|‚ñà‚ñà‚ñå       | 75/300 [01:44&lt;05:18,  1.42s/it]\n\n 25%|‚ñà‚ñà‚ñå       | 76/300 [01:46&lt;05:18,  1.42s/it]\n\n 26%|‚ñà‚ñà‚ñå       | 77/300 [01:47&lt;05:15,  1.41s/it]\n\n 26%|‚ñà‚ñà‚ñå       | 78/300 [01:49&lt;05:13,  1.41s/it]\n\n 26%|‚ñà‚ñà‚ñã       | 79/300 [01:50&lt;05:10,  1.41s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 80/300 [01:52&lt;05:09,  1.41s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 81/300 [01:53&lt;05:07,  1.40s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 82/300 [01:54&lt;05:05,  1.40s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 83/300 [01:56&lt;05:03,  1.40s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 84/300 [01:57&lt;05:02,  1.40s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 85/300 [01:59&lt;05:02,  1.41s/it]\n\n 29%|‚ñà‚ñà‚ñä       | 86/300 [02:00&lt;05:02,  1.41s/it]\n\n 29%|‚ñà‚ñà‚ñâ       | 87/300 [02:01&lt;05:00,  1.41s/it]\n\n 29%|‚ñà‚ñà‚ñâ       | 88/300 [02:03&lt;04:58,  1.41s/it]\n\n 30%|‚ñà‚ñà‚ñâ       | 89/300 [02:04&lt;04:56,  1.40s/it]\n\n 30%|‚ñà‚ñà‚ñà       | 90/300 [02:06&lt;04:54,  1.40s/it]\n\n 30%|‚ñà‚ñà‚ñà       | 91/300 [02:07&lt;04:52,  1.40s/it]\n\n 31%|‚ñà‚ñà‚ñà       | 92/300 [02:08&lt;04:51,  1.40s/it]\n\n 31%|‚ñà‚ñà‚ñà       | 93/300 [02:10&lt;04:50,  1.40s/it]\n\n 31%|‚ñà‚ñà‚ñà‚ñè      | 94/300 [02:11&lt;04:51,  1.42s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 95/300 [02:13&lt;04:49,  1.41s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 96/300 [02:14&lt;04:47,  1.41s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 97/300 [02:15&lt;04:45,  1.40s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 98/300 [02:17&lt;04:43,  1.40s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 99/300 [02:18&lt;04:41,  1.40s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 100/300 [02:20&lt;04:42,  1.41s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñé      | 101/300 [02:21&lt;04:39,  1.41s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñç      | 102/300 [02:22&lt;04:37,  1.40s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñç      | 103/300 [02:24&lt;04:35,  1.40s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñç      | 104/300 [02:25&lt;04:34,  1.40s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñå      | 105/300 [02:27&lt;04:33,  1.40s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñå      | 106/300 [02:28&lt;04:34,  1.42s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñå      | 107/300 [02:29&lt;04:31,  1.41s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñå      | 108/300 [02:31&lt;04:31,  1.41s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñã      | 109/300 [02:32&lt;04:28,  1.40s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 110/300 [02:34&lt;04:27,  1.41s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 111/300 [02:35&lt;04:25,  1.41s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 112/300 [02:36&lt;04:23,  1.40s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 113/300 [02:38&lt;04:21,  1.40s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 114/300 [02:39&lt;04:20,  1.40s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 115/300 [02:41&lt;04:18,  1.40s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñä      | 116/300 [02:42&lt;04:19,  1.41s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñâ      | 117/300 [02:44&lt;04:17,  1.41s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñâ      | 118/300 [02:45&lt;04:16,  1.41s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñâ      | 119/300 [02:46&lt;04:13,  1.40s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñà      | 120/300 [02:48&lt;04:11,  1.40s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñà      | 121/300 [02:49&lt;04:11,  1.40s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà      | 122/300 [02:51&lt;04:09,  1.40s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà      | 123/300 [02:52&lt;04:08,  1.40s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 124/300 [02:53&lt;04:06,  1.40s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 125/300 [02:55&lt;04:04,  1.40s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 126/300 [02:56&lt;04:03,  1.40s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 127/300 [02:58&lt;04:02,  1.40s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 128/300 [02:59&lt;04:01,  1.40s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 129/300 [03:00&lt;03:59,  1.40s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 130/300 [03:02&lt;03:58,  1.40s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 131/300 [03:03&lt;03:56,  1.40s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 132/300 [03:05&lt;03:54,  1.40s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 133/300 [03:06&lt;03:53,  1.40s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 134/300 [03:07&lt;03:51,  1.39s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 135/300 [03:09&lt;03:49,  1.39s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 136/300 [03:10&lt;03:49,  1.40s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 137/300 [03:11&lt;03:47,  1.40s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 138/300 [03:13&lt;03:46,  1.40s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 139/300 [03:14&lt;03:44,  1.39s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 140/300 [03:16&lt;03:43,  1.40s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 141/300 [03:17&lt;03:41,  1.39s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 142/300 [03:18&lt;03:40,  1.39s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 143/300 [03:20&lt;03:38,  1.39s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 144/300 [03:21&lt;03:37,  1.39s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 145/300 [03:23&lt;03:37,  1.40s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 146/300 [03:24&lt;03:37,  1.41s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 147/300 [03:25&lt;03:35,  1.41s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 148/300 [03:27&lt;03:32,  1.40s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 149/300 [03:28&lt;03:31,  1.40s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 150/300 [03:30&lt;03:29,  1.40s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 151/300 [03:31&lt;03:28,  1.40s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 152/300 [03:32&lt;03:27,  1.40s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 153/300 [03:34&lt;03:26,  1.41s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 154/300 [03:35&lt;03:26,  1.41s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 155/300 [03:37&lt;03:24,  1.41s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 156/300 [03:38&lt;03:25,  1.43s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 157/300 [03:40&lt;03:22,  1.41s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 158/300 [03:41&lt;03:19,  1.41s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 159/300 [03:42&lt;03:17,  1.40s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 160/300 [03:44&lt;03:16,  1.40s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 161/300 [03:45&lt;03:14,  1.40s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 162/300 [03:47&lt;03:12,  1.40s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 163/300 [03:48&lt;03:11,  1.39s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 164/300 [03:49&lt;03:10,  1.40s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 165/300 [03:51&lt;03:09,  1.41s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 166/300 [03:52&lt;03:08,  1.41s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 167/300 [03:54&lt;03:06,  1.40s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 168/300 [03:55&lt;03:04,  1.40s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 169/300 [03:56&lt;03:03,  1.40s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 170/300 [03:58&lt;03:01,  1.40s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 171/300 [03:59&lt;02:59,  1.39s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 172/300 [04:01&lt;02:58,  1.40s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 173/300 [04:02&lt;02:57,  1.40s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 174/300 [04:03&lt;02:56,  1.40s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 175/300 [04:05&lt;02:55,  1.40s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 176/300 [04:06&lt;02:54,  1.40s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 177/300 [04:08&lt;02:52,  1.41s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 178/300 [04:09&lt;02:51,  1.40s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 179/300 [04:10&lt;02:49,  1.40s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 180/300 [04:12&lt;02:47,  1.40s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 181/300 [04:13&lt;02:47,  1.41s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 182/300 [04:15&lt;02:46,  1.41s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 183/300 [04:16&lt;02:44,  1.40s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 184/300 [04:17&lt;02:45,  1.42s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 185/300 [04:19&lt;02:48,  1.46s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 186/300 [04:20&lt;02:45,  1.45s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 187/300 [04:22&lt;02:41,  1.43s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 188/300 [04:23&lt;02:39,  1.42s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 189/300 [04:25&lt;02:37,  1.42s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 190/300 [04:26&lt;02:36,  1.42s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 191/300 [04:27&lt;02:33,  1.41s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 192/300 [04:29&lt;02:31,  1.40s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 193/300 [04:30&lt;02:31,  1.41s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 194/300 [04:32&lt;02:29,  1.41s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 195/300 [04:33&lt;02:28,  1.41s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 196/300 [04:35&lt;02:26,  1.41s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 197/300 [04:36&lt;02:24,  1.41s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 198/300 [04:37&lt;02:22,  1.40s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 199/300 [04:39&lt;02:21,  1.40s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 200/300 [04:40&lt;02:19,  1.40s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 201/300 [04:41&lt;02:18,  1.40s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 202/300 [04:43&lt;02:16,  1.40s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 203/300 [04:44&lt;02:15,  1.40s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 204/300 [04:46&lt;02:14,  1.40s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 205/300 [04:47&lt;02:13,  1.40s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 206/300 [04:48&lt;02:11,  1.40s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 207/300 [04:50&lt;02:10,  1.40s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 208/300 [04:51&lt;02:09,  1.41s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 209/300 [04:53&lt;02:07,  1.41s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 210/300 [04:54&lt;02:06,  1.40s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 211/300 [04:56&lt;02:04,  1.40s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 212/300 [04:57&lt;02:03,  1.40s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 213/300 [04:58&lt;02:03,  1.42s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 214/300 [05:00&lt;02:02,  1.42s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 215/300 [05:01&lt;02:00,  1.41s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 216/300 [05:03&lt;01:58,  1.41s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 217/300 [05:04&lt;01:56,  1.40s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 218/300 [05:05&lt;01:54,  1.40s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 219/300 [05:07&lt;01:53,  1.40s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 220/300 [05:08&lt;01:51,  1.40s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 221/300 [05:10&lt;01:50,  1.40s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 222/300 [05:11&lt;01:48,  1.40s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 223/300 [05:12&lt;01:48,  1.42s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 224/300 [05:14&lt;01:47,  1.41s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 225/300 [05:15&lt;01:45,  1.41s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 226/300 [05:17&lt;01:43,  1.40s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 227/300 [05:18&lt;01:42,  1.40s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 228/300 [05:19&lt;01:40,  1.40s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 229/300 [05:21&lt;01:39,  1.40s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 230/300 [05:22&lt;01:37,  1.39s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 231/300 [05:24&lt;01:36,  1.39s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 232/300 [05:25&lt;01:35,  1.41s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 233/300 [05:26&lt;01:33,  1.40s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 234/300 [05:28&lt;01:32,  1.41s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 235/300 [05:29&lt;01:31,  1.40s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 236/300 [05:31&lt;01:29,  1.40s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 237/300 [05:32&lt;01:28,  1.40s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 238/300 [05:33&lt;01:27,  1.40s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 239/300 [05:35&lt;01:25,  1.40s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 240/300 [05:36&lt;01:23,  1.40s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 241/300 [05:38&lt;01:22,  1.39s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 242/300 [05:39&lt;01:21,  1.41s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 243/300 [05:40&lt;01:20,  1.41s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 244/300 [05:42&lt;01:19,  1.41s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 245/300 [05:43&lt;01:17,  1.41s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 246/300 [05:45&lt;01:15,  1.40s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 247/300 [05:46&lt;01:14,  1.40s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 248/300 [05:47&lt;01:13,  1.41s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 249/300 [05:49&lt;01:11,  1.41s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 250/300 [05:50&lt;01:10,  1.42s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 251/300 [05:52&lt;01:09,  1.41s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 252/300 [05:53&lt;01:07,  1.41s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 253/300 [05:55&lt;01:06,  1.41s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 254/300 [05:56&lt;01:04,  1.40s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 255/300 [05:57&lt;01:03,  1.41s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 256/300 [05:59&lt;01:01,  1.40s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 257/300 [06:00&lt;01:00,  1.40s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 258/300 [06:02&lt;00:58,  1.40s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 259/300 [06:03&lt;00:57,  1.41s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 260/300 [06:04&lt;00:56,  1.41s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 261/300 [06:06&lt;00:54,  1.41s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 262/300 [06:07&lt;00:54,  1.42s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 263/300 [06:09&lt;00:52,  1.42s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 264/300 [06:10&lt;00:50,  1.41s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 265/300 [06:11&lt;00:49,  1.41s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 266/300 [06:13&lt;00:48,  1.42s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 267/300 [06:14&lt;00:46,  1.41s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 268/300 [06:16&lt;00:44,  1.40s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 269/300 [06:17&lt;00:43,  1.40s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 270/300 [06:18&lt;00:41,  1.40s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 271/300 [06:20&lt;00:40,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 272/300 [06:21&lt;00:39,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 273/300 [06:23&lt;00:37,  1.41s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 274/300 [06:24&lt;00:36,  1.41s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 275/300 [06:25&lt;00:35,  1.40s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 276/300 [06:27&lt;00:33,  1.40s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 277/300 [06:28&lt;00:32,  1.40s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 278/300 [06:30&lt;00:30,  1.40s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 279/300 [06:31&lt;00:29,  1.40s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 280/300 [06:32&lt;00:27,  1.40s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 281/300 [06:34&lt;00:26,  1.40s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 282/300 [06:35&lt;00:25,  1.41s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 283/300 [06:37&lt;00:23,  1.41s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 284/300 [06:38&lt;00:22,  1.41s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 285/300 [06:40&lt;00:21,  1.45s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 286/300 [06:41&lt;00:20,  1.43s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 287/300 [06:42&lt;00:18,  1.42s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 288/300 [06:44&lt;00:16,  1.41s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 289/300 [06:45&lt;00:15,  1.40s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 290/300 [06:47&lt;00:14,  1.41s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 291/300 [06:48&lt;00:12,  1.41s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 292/300 [06:49&lt;00:11,  1.42s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 293/300 [06:51&lt;00:09,  1.42s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 294/300 [06:52&lt;00:08,  1.41s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 295/300 [06:54&lt;00:07,  1.40s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 296/300 [06:55&lt;00:05,  1.40s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 297/300 [06:56&lt;00:04,  1.40s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 298/300 [06:58&lt;00:02,  1.40s/it]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 299/300 [06:59&lt;00:01,  1.40s/it]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [07:01&lt;00:00,  1.40s/it]\n\n9it [1:03:24, 422.12s/it]\n\n  0%|          | 0/300 [00:00&lt;?, ?it/s]\n\n  0%|          | 1/300 [00:01&lt;04:59,  1.00s/it]\n\n  1%|          | 2/300 [00:02&lt;06:08,  1.24s/it]\n\n  1%|          | 3/300 [00:03&lt;06:30,  1.31s/it]\n\n  1%|‚ñè         | 4/300 [00:05&lt;06:38,  1.34s/it]\n\n  2%|‚ñè         | 5/300 [00:06&lt;06:41,  1.36s/it]\n\n  2%|‚ñè         | 6/300 [00:07&lt;06:43,  1.37s/it]\n\n  2%|‚ñè         | 7/300 [00:09&lt;06:46,  1.39s/it]\n\n  3%|‚ñé         | 8/300 [00:10&lt;06:45,  1.39s/it]\n\n  3%|‚ñé         | 9/300 [00:12&lt;06:44,  1.39s/it]\n\n  3%|‚ñé         | 10/300 [00:13&lt;06:43,  1.39s/it]\n\n  4%|‚ñé         | 11/300 [00:14&lt;06:43,  1.40s/it]\n\n  4%|‚ñç         | 12/300 [00:16&lt;06:42,  1.40s/it]\n\n  4%|‚ñç         | 13/300 [00:17&lt;06:42,  1.40s/it]\n\n  5%|‚ñç         | 14/300 [00:19&lt;06:39,  1.40s/it]\n\n  5%|‚ñå         | 15/300 [00:20&lt;06:38,  1.40s/it]\n\n  5%|‚ñå         | 16/300 [00:21&lt;06:36,  1.40s/it]\n\n  6%|‚ñå         | 17/300 [00:23&lt;06:35,  1.40s/it]\n\n  6%|‚ñå         | 18/300 [00:24&lt;06:33,  1.39s/it]\n\n  6%|‚ñã         | 19/300 [00:26&lt;06:31,  1.39s/it]\n\n  7%|‚ñã         | 20/300 [00:27&lt;06:30,  1.39s/it]\n\n  7%|‚ñã         | 21/300 [00:28&lt;06:29,  1.39s/it]\n\n  7%|‚ñã         | 22/300 [00:30&lt;06:27,  1.39s/it]\n\n  8%|‚ñä         | 23/300 [00:31&lt;06:29,  1.41s/it]\n\n  8%|‚ñä         | 24/300 [00:33&lt;06:26,  1.40s/it]\n\n  8%|‚ñä         | 25/300 [00:34&lt;06:24,  1.40s/it]\n\n  9%|‚ñä         | 26/300 [00:35&lt;06:23,  1.40s/it]\n\n  9%|‚ñâ         | 27/300 [00:37&lt;06:21,  1.40s/it]\n\n  9%|‚ñâ         | 28/300 [00:38&lt;06:19,  1.39s/it]\n\n 10%|‚ñâ         | 29/300 [00:40&lt;06:17,  1.39s/it]\n\n 10%|‚ñà         | 30/300 [00:41&lt;06:15,  1.39s/it]\n\n 10%|‚ñà         | 31/300 [00:42&lt;06:19,  1.41s/it]\n\n 11%|‚ñà         | 32/300 [00:44&lt;06:18,  1.41s/it]\n\n 11%|‚ñà         | 33/300 [00:45&lt;06:16,  1.41s/it]\n\n 11%|‚ñà‚ñè        | 34/300 [00:47&lt;06:13,  1.40s/it]\n\n 12%|‚ñà‚ñè        | 35/300 [00:48&lt;06:10,  1.40s/it]\n\n 12%|‚ñà‚ñè        | 36/300 [00:49&lt;06:09,  1.40s/it]\n\n 12%|‚ñà‚ñè        | 37/300 [00:51&lt;06:07,  1.40s/it]\n\n 13%|‚ñà‚ñé        | 38/300 [00:52&lt;06:05,  1.39s/it]\n\n 13%|‚ñà‚ñé        | 39/300 [00:54&lt;06:06,  1.40s/it]\n\n 13%|‚ñà‚ñé        | 40/300 [00:55&lt;06:03,  1.40s/it]\n\n 14%|‚ñà‚ñé        | 41/300 [00:56&lt;06:03,  1.40s/it]\n\n 14%|‚ñà‚ñç        | 42/300 [00:58&lt;06:03,  1.41s/it]\n\n 14%|‚ñà‚ñç        | 43/300 [00:59&lt;06:01,  1.41s/it]\n\n 15%|‚ñà‚ñç        | 44/300 [01:01&lt;05:58,  1.40s/it]\n\n 15%|‚ñà‚ñå        | 45/300 [01:02&lt;05:58,  1.40s/it]\n\n 15%|‚ñà‚ñå        | 46/300 [01:03&lt;05:55,  1.40s/it]\n\n 16%|‚ñà‚ñå        | 47/300 [01:05&lt;05:53,  1.40s/it]\n\n 16%|‚ñà‚ñå        | 48/300 [01:06&lt;05:51,  1.39s/it]\n\n 16%|‚ñà‚ñã        | 49/300 [01:08&lt;05:49,  1.39s/it]\n\n 17%|‚ñà‚ñã        | 50/300 [01:09&lt;05:49,  1.40s/it]\n\n 17%|‚ñà‚ñã        | 51/300 [01:10&lt;05:48,  1.40s/it]\n\n 17%|‚ñà‚ñã        | 52/300 [01:12&lt;05:46,  1.40s/it]\n\n 18%|‚ñà‚ñä        | 53/300 [01:13&lt;05:44,  1.39s/it]\n\n 18%|‚ñà‚ñä        | 54/300 [01:15&lt;05:43,  1.40s/it]\n\n 18%|‚ñà‚ñä        | 55/300 [01:16&lt;05:41,  1.39s/it]\n\n 19%|‚ñà‚ñä        | 56/300 [01:17&lt;05:39,  1.39s/it]\n\n 19%|‚ñà‚ñâ        | 57/300 [01:19&lt;05:37,  1.39s/it]\n\n 19%|‚ñà‚ñâ        | 58/300 [01:20&lt;05:36,  1.39s/it]\n\n 20%|‚ñà‚ñâ        | 59/300 [01:22&lt;05:35,  1.39s/it]\n\n 20%|‚ñà‚ñà        | 60/300 [01:23&lt;05:34,  1.39s/it]\n\n 20%|‚ñà‚ñà        | 61/300 [01:24&lt;05:36,  1.41s/it]\n\n 21%|‚ñà‚ñà        | 62/300 [01:26&lt;05:34,  1.41s/it]\n\n 21%|‚ñà‚ñà        | 63/300 [01:27&lt;05:32,  1.40s/it]\n\n 21%|‚ñà‚ñà‚ñè       | 64/300 [01:29&lt;05:33,  1.41s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 65/300 [01:30&lt;05:30,  1.41s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 66/300 [01:32&lt;05:30,  1.41s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 67/300 [01:33&lt;05:27,  1.41s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 68/300 [01:34&lt;05:26,  1.41s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 69/300 [01:36&lt;05:25,  1.41s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 70/300 [01:37&lt;05:23,  1.41s/it]\n\n 24%|‚ñà‚ñà‚ñé       | 71/300 [01:39&lt;05:24,  1.42s/it]\n\n 24%|‚ñà‚ñà‚ñç       | 72/300 [01:40&lt;05:23,  1.42s/it]\n\n 24%|‚ñà‚ñà‚ñç       | 73/300 [01:41&lt;05:20,  1.41s/it]\n\n 25%|‚ñà‚ñà‚ñç       | 74/300 [01:43&lt;05:17,  1.40s/it]\n\n 25%|‚ñà‚ñà‚ñå       | 75/300 [01:44&lt;05:15,  1.40s/it]\n\n 25%|‚ñà‚ñà‚ñå       | 76/300 [01:46&lt;05:13,  1.40s/it]\n\n 26%|‚ñà‚ñà‚ñå       | 77/300 [01:47&lt;05:11,  1.40s/it]\n\n 26%|‚ñà‚ñà‚ñå       | 78/300 [01:48&lt;05:12,  1.41s/it]\n\n 26%|‚ñà‚ñà‚ñã       | 79/300 [01:50&lt;05:10,  1.40s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 80/300 [01:51&lt;05:10,  1.41s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 81/300 [01:53&lt;05:09,  1.41s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 82/300 [01:54&lt;05:06,  1.41s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 83/300 [01:55&lt;05:04,  1.40s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 84/300 [01:57&lt;05:02,  1.40s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 85/300 [01:58&lt;05:02,  1.41s/it]\n\n 29%|‚ñà‚ñà‚ñä       | 86/300 [02:00&lt;04:59,  1.40s/it]\n\n 29%|‚ñà‚ñà‚ñâ       | 87/300 [02:01&lt;04:57,  1.40s/it]\n\n 29%|‚ñà‚ñà‚ñâ       | 88/300 [02:02&lt;04:55,  1.39s/it]\n\n 30%|‚ñà‚ñà‚ñâ       | 89/300 [02:04&lt;04:55,  1.40s/it]\n\n 30%|‚ñà‚ñà‚ñà       | 90/300 [02:05&lt;04:53,  1.40s/it]\n\n 30%|‚ñà‚ñà‚ñà       | 91/300 [02:07&lt;04:53,  1.40s/it]\n\n 31%|‚ñà‚ñà‚ñà       | 92/300 [02:08&lt;04:52,  1.41s/it]\n\n 31%|‚ñà‚ñà‚ñà       | 93/300 [02:09&lt;04:49,  1.40s/it]\n\n 31%|‚ñà‚ñà‚ñà‚ñè      | 94/300 [02:11&lt;04:48,  1.40s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 95/300 [02:12&lt;04:47,  1.40s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 96/300 [02:14&lt;04:45,  1.40s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 97/300 [02:15&lt;04:43,  1.40s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 98/300 [02:16&lt;04:42,  1.40s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 99/300 [02:18&lt;04:40,  1.40s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 100/300 [02:19&lt;04:39,  1.40s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñé      | 101/300 [02:21&lt;04:39,  1.41s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñç      | 102/300 [02:22&lt;04:37,  1.40s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñç      | 103/300 [02:23&lt;04:35,  1.40s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñç      | 104/300 [02:25&lt;04:34,  1.40s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñå      | 105/300 [02:26&lt;04:39,  1.43s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñå      | 106/300 [02:28&lt;04:36,  1.42s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñå      | 107/300 [02:29&lt;04:32,  1.41s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñå      | 108/300 [02:31&lt;04:31,  1.41s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñã      | 109/300 [02:32&lt;04:28,  1.41s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 110/300 [02:33&lt;04:29,  1.42s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 111/300 [02:35&lt;04:27,  1.41s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 112/300 [02:36&lt;04:25,  1.41s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 113/300 [02:38&lt;04:23,  1.41s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 114/300 [02:39&lt;04:20,  1.40s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 115/300 [02:40&lt;04:19,  1.40s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñä      | 116/300 [02:42&lt;04:18,  1.40s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñâ      | 117/300 [02:43&lt;04:16,  1.40s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñâ      | 118/300 [02:45&lt;04:15,  1.40s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñâ      | 119/300 [02:46&lt;04:15,  1.41s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñà      | 120/300 [02:47&lt;04:13,  1.41s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñà      | 121/300 [02:49&lt;04:11,  1.41s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà      | 122/300 [02:50&lt;04:09,  1.40s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà      | 123/300 [02:52&lt;04:07,  1.40s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 124/300 [02:53&lt;04:07,  1.41s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 125/300 [02:54&lt;04:05,  1.40s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 126/300 [02:56&lt;04:03,  1.40s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 127/300 [02:57&lt;04:01,  1.40s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 128/300 [02:59&lt;04:00,  1.40s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 129/300 [03:00&lt;04:00,  1.41s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 130/300 [03:01&lt;03:59,  1.41s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 131/300 [03:03&lt;03:57,  1.40s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 132/300 [03:04&lt;03:55,  1.40s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 133/300 [03:06&lt;03:53,  1.40s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 134/300 [03:07&lt;03:51,  1.40s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 135/300 [03:08&lt;03:51,  1.40s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 136/300 [03:10&lt;03:50,  1.41s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 137/300 [03:11&lt;03:48,  1.40s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 138/300 [03:13&lt;03:47,  1.40s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 139/300 [03:14&lt;03:45,  1.40s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 140/300 [03:15&lt;03:45,  1.41s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 141/300 [03:17&lt;03:43,  1.41s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 142/300 [03:18&lt;03:41,  1.40s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 143/300 [03:20&lt;03:39,  1.40s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 144/300 [03:21&lt;03:37,  1.40s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 145/300 [03:22&lt;03:36,  1.39s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 146/300 [03:24&lt;03:34,  1.39s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 147/300 [03:25&lt;03:34,  1.40s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 148/300 [03:27&lt;03:32,  1.40s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 149/300 [03:28&lt;03:31,  1.40s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 150/300 [03:29&lt;03:30,  1.40s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 151/300 [03:31&lt;03:28,  1.40s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 152/300 [03:32&lt;03:26,  1.40s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 153/300 [03:34&lt;03:26,  1.40s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 154/300 [03:35&lt;03:25,  1.40s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 155/300 [03:36&lt;03:23,  1.40s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 156/300 [03:38&lt;03:21,  1.40s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 157/300 [03:39&lt;03:19,  1.40s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 158/300 [03:41&lt;03:19,  1.40s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 159/300 [03:42&lt;03:18,  1.41s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 160/300 [03:43&lt;03:17,  1.41s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 161/300 [03:45&lt;03:14,  1.40s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 162/300 [03:46&lt;03:13,  1.40s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 163/300 [03:48&lt;03:11,  1.40s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 164/300 [03:49&lt;03:10,  1.40s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 165/300 [03:50&lt;03:08,  1.40s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 166/300 [03:52&lt;03:07,  1.40s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 167/300 [03:53&lt;03:05,  1.40s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 168/300 [03:55&lt;03:05,  1.40s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 169/300 [03:56&lt;03:05,  1.42s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 170/300 [03:58&lt;03:03,  1.41s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 171/300 [03:59&lt;03:01,  1.41s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 172/300 [04:00&lt;02:59,  1.41s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 173/300 [04:02&lt;02:57,  1.40s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 174/300 [04:03&lt;02:56,  1.40s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 175/300 [04:04&lt;02:54,  1.40s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 176/300 [04:06&lt;02:53,  1.40s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 177/300 [04:07&lt;02:52,  1.40s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 178/300 [04:09&lt;02:51,  1.40s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 179/300 [04:10&lt;02:50,  1.41s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 180/300 [04:12&lt;02:48,  1.41s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 181/300 [04:13&lt;02:47,  1.40s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 182/300 [04:14&lt;02:45,  1.40s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 183/300 [04:16&lt;02:43,  1.40s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 184/300 [04:17&lt;02:41,  1.40s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 185/300 [04:19&lt;02:41,  1.40s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 186/300 [04:20&lt;02:39,  1.40s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 187/300 [04:21&lt;02:37,  1.40s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 188/300 [04:23&lt;02:36,  1.40s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 189/300 [04:24&lt;02:35,  1.40s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 190/300 [04:26&lt;02:34,  1.40s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 191/300 [04:27&lt;02:32,  1.40s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 192/300 [04:28&lt;02:30,  1.40s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 193/300 [04:30&lt;02:29,  1.39s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 194/300 [04:31&lt;02:28,  1.40s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 195/300 [04:32&lt;02:26,  1.40s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 196/300 [04:34&lt;02:25,  1.39s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 197/300 [04:35&lt;02:23,  1.40s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 198/300 [04:37&lt;02:23,  1.41s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 199/300 [04:38&lt;02:22,  1.41s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 200/300 [04:40&lt;02:20,  1.40s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 201/300 [04:41&lt;02:18,  1.40s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 202/300 [04:42&lt;02:17,  1.41s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 203/300 [04:44&lt;02:15,  1.40s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 204/300 [04:45&lt;02:14,  1.40s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 205/300 [04:47&lt;02:15,  1.43s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 206/300 [04:48&lt;02:13,  1.42s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 207/300 [04:49&lt;02:11,  1.42s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 208/300 [04:51&lt;02:10,  1.42s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 209/300 [04:52&lt;02:09,  1.42s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 210/300 [04:54&lt;02:07,  1.42s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 211/300 [04:55&lt;02:05,  1.41s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 212/300 [04:56&lt;02:03,  1.41s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 213/300 [04:58&lt;02:02,  1.41s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 214/300 [04:59&lt;02:00,  1.40s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 215/300 [05:01&lt;01:58,  1.40s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 216/300 [05:02&lt;01:57,  1.40s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 217/300 [05:03&lt;01:57,  1.41s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 218/300 [05:05&lt;01:55,  1.41s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 219/300 [05:06&lt;01:54,  1.41s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 220/300 [05:08&lt;01:52,  1.41s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 221/300 [05:09&lt;01:50,  1.40s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 222/300 [05:10&lt;01:49,  1.40s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 223/300 [05:12&lt;01:47,  1.40s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 224/300 [05:13&lt;01:46,  1.40s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 225/300 [05:15&lt;01:44,  1.40s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 226/300 [05:16&lt;01:43,  1.40s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 227/300 [05:17&lt;01:41,  1.40s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 228/300 [05:19&lt;01:40,  1.40s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 229/300 [05:20&lt;01:39,  1.40s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 230/300 [05:22&lt;01:38,  1.41s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 231/300 [05:23&lt;01:36,  1.40s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 232/300 [05:25&lt;01:35,  1.41s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 233/300 [05:26&lt;01:34,  1.40s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 234/300 [05:27&lt;01:32,  1.40s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 235/300 [05:29&lt;01:30,  1.40s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 236/300 [05:30&lt;01:29,  1.39s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 237/300 [05:31&lt;01:28,  1.40s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 238/300 [05:33&lt;01:26,  1.40s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 239/300 [05:34&lt;01:25,  1.40s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 240/300 [05:36&lt;01:24,  1.40s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 241/300 [05:37&lt;01:23,  1.41s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 242/300 [05:39&lt;01:21,  1.40s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 243/300 [05:40&lt;01:20,  1.40s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 244/300 [05:41&lt;01:18,  1.41s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 245/300 [05:43&lt;01:16,  1.40s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 246/300 [05:44&lt;01:15,  1.40s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 247/300 [05:46&lt;01:14,  1.40s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 248/300 [05:47&lt;01:13,  1.41s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 249/300 [05:48&lt;01:11,  1.41s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 250/300 [05:50&lt;01:10,  1.41s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 251/300 [05:51&lt;01:08,  1.41s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 252/300 [05:53&lt;01:07,  1.40s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 253/300 [05:54&lt;01:05,  1.40s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 254/300 [05:55&lt;01:04,  1.40s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 255/300 [05:57&lt;01:02,  1.40s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 256/300 [05:58&lt;01:01,  1.40s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 257/300 [06:00&lt;01:00,  1.41s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 258/300 [06:01&lt;00:58,  1.40s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 259/300 [06:02&lt;00:57,  1.40s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 260/300 [06:04&lt;00:55,  1.40s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 261/300 [06:05&lt;00:54,  1.40s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 262/300 [06:07&lt;00:53,  1.40s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 263/300 [06:08&lt;00:51,  1.40s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 264/300 [06:09&lt;00:50,  1.40s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 265/300 [06:11&lt;00:48,  1.40s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 266/300 [06:12&lt;00:47,  1.40s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 267/300 [06:14&lt;00:46,  1.41s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 268/300 [06:15&lt;00:45,  1.43s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 269/300 [06:16&lt;00:43,  1.41s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 270/300 [06:18&lt;00:42,  1.41s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 271/300 [06:19&lt;00:40,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 272/300 [06:21&lt;00:39,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 273/300 [06:22&lt;00:37,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 274/300 [06:23&lt;00:36,  1.41s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 275/300 [06:25&lt;00:35,  1.40s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 276/300 [06:26&lt;00:33,  1.40s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 277/300 [06:28&lt;00:32,  1.40s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 278/300 [06:29&lt;00:30,  1.40s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 279/300 [06:30&lt;00:29,  1.40s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 280/300 [06:32&lt;00:28,  1.40s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 281/300 [06:33&lt;00:26,  1.40s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 282/300 [06:35&lt;00:25,  1.40s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 283/300 [06:36&lt;00:23,  1.40s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 284/300 [06:37&lt;00:22,  1.40s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 285/300 [06:39&lt;00:21,  1.40s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 286/300 [06:40&lt;00:19,  1.41s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 287/300 [06:42&lt;00:18,  1.41s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 288/300 [06:43&lt;00:16,  1.41s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 289/300 [06:44&lt;00:15,  1.40s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 290/300 [06:46&lt;00:13,  1.40s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 291/300 [06:47&lt;00:12,  1.40s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 292/300 [06:49&lt;00:11,  1.40s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 293/300 [06:50&lt;00:09,  1.39s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 294/300 [06:52&lt;00:08,  1.43s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 295/300 [06:53&lt;00:07,  1.42s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 296/300 [06:54&lt;00:05,  1.41s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 297/300 [06:56&lt;00:04,  1.41s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 298/300 [06:57&lt;00:02,  1.41s/it]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 299/300 [06:59&lt;00:01,  1.41s/it]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [07:00&lt;00:00,  1.40s/it]\n\n10it [1:10:25, 421.87s/it]\n\n  0%|          | 0/300 [00:00&lt;?, ?it/s]\n\n  0%|          | 1/300 [00:00&lt;04:56,  1.01it/s]\n\n  1%|          | 2/300 [00:02&lt;06:07,  1.23s/it]\n\n  1%|          | 3/300 [00:03&lt;06:28,  1.31s/it]\n\n  1%|‚ñè         | 4/300 [00:05&lt;06:37,  1.34s/it]\n\n  2%|‚ñè         | 5/300 [00:06&lt;06:40,  1.36s/it]\n\n  2%|‚ñè         | 6/300 [00:07&lt;06:43,  1.37s/it]\n\n  2%|‚ñè         | 7/300 [00:09&lt;06:48,  1.40s/it]\n\n  3%|‚ñé         | 8/300 [00:10&lt;06:48,  1.40s/it]\n\n  3%|‚ñé         | 9/300 [00:12&lt;06:46,  1.40s/it]\n\n  3%|‚ñé         | 10/300 [00:13&lt;06:43,  1.39s/it]\n\n  4%|‚ñé         | 11/300 [00:14&lt;06:42,  1.39s/it]\n\n  4%|‚ñç         | 12/300 [00:16&lt;06:40,  1.39s/it]\n\n  4%|‚ñç         | 13/300 [00:17&lt;06:39,  1.39s/it]\n\n  5%|‚ñç         | 14/300 [00:19&lt;06:37,  1.39s/it]\n\n  5%|‚ñå         | 15/300 [00:20&lt;06:39,  1.40s/it]\n\n  5%|‚ñå         | 16/300 [00:21&lt;06:39,  1.41s/it]\n\n  6%|‚ñå         | 17/300 [00:23&lt;06:38,  1.41s/it]\n\n  6%|‚ñå         | 18/300 [00:24&lt;06:37,  1.41s/it]\n\n  6%|‚ñã         | 19/300 [00:26&lt;06:36,  1.41s/it]\n\n  7%|‚ñã         | 20/300 [00:27&lt;06:33,  1.41s/it]\n\n  7%|‚ñã         | 21/300 [00:29&lt;06:32,  1.41s/it]\n\n  7%|‚ñã         | 22/300 [00:30&lt;06:29,  1.40s/it]\n\n  8%|‚ñä         | 23/300 [00:31&lt;06:27,  1.40s/it]\n\n  8%|‚ñä         | 24/300 [00:33&lt;06:24,  1.39s/it]\n\n  8%|‚ñä         | 25/300 [00:34&lt;06:25,  1.40s/it]\n\n  9%|‚ñä         | 26/300 [00:36&lt;06:24,  1.40s/it]\n\n  9%|‚ñâ         | 27/300 [00:37&lt;06:26,  1.42s/it]\n\n  9%|‚ñâ         | 28/300 [00:38&lt;06:23,  1.41s/it]\n\n 10%|‚ñâ         | 29/300 [00:40&lt;06:21,  1.41s/it]\n\n 10%|‚ñà         | 30/300 [00:41&lt;06:18,  1.40s/it]\n\n 10%|‚ñà         | 31/300 [00:43&lt;06:17,  1.40s/it]\n\n 11%|‚ñà         | 32/300 [00:44&lt;06:14,  1.40s/it]\n\n 11%|‚ñà         | 33/300 [00:45&lt;06:12,  1.40s/it]\n\n 11%|‚ñà‚ñè        | 34/300 [00:47&lt;06:10,  1.39s/it]\n\n 12%|‚ñà‚ñè        | 35/300 [00:48&lt;06:09,  1.39s/it]\n\n 12%|‚ñà‚ñè        | 36/300 [00:50&lt;06:10,  1.40s/it]\n\n 12%|‚ñà‚ñè        | 37/300 [00:51&lt;06:10,  1.41s/it]\n\n 13%|‚ñà‚ñé        | 38/300 [00:52&lt;06:08,  1.41s/it]\n\n 13%|‚ñà‚ñé        | 39/300 [00:54&lt;06:05,  1.40s/it]\n\n 13%|‚ñà‚ñé        | 40/300 [00:55&lt;06:03,  1.40s/it]\n\n 14%|‚ñà‚ñé        | 41/300 [00:57&lt;06:01,  1.40s/it]\n\n 14%|‚ñà‚ñç        | 42/300 [00:58&lt;05:58,  1.39s/it]\n\n 14%|‚ñà‚ñç        | 43/300 [00:59&lt;05:59,  1.40s/it]\n\n 15%|‚ñà‚ñç        | 44/300 [01:01&lt;05:57,  1.40s/it]\n\n 15%|‚ñà‚ñå        | 45/300 [01:02&lt;05:56,  1.40s/it]\n\n 15%|‚ñà‚ñå        | 46/300 [01:04&lt;05:55,  1.40s/it]\n\n 16%|‚ñà‚ñå        | 47/300 [01:05&lt;05:54,  1.40s/it]\n\n 16%|‚ñà‚ñå        | 48/300 [01:06&lt;05:53,  1.40s/it]\n\n 16%|‚ñà‚ñã        | 49/300 [01:08&lt;05:50,  1.40s/it]\n\n 17%|‚ñà‚ñã        | 50/300 [01:09&lt;05:48,  1.39s/it]\n\n 17%|‚ñà‚ñã        | 51/300 [01:11&lt;05:50,  1.41s/it]\n\n 17%|‚ñà‚ñã        | 52/300 [01:12&lt;05:48,  1.41s/it]\n\n 18%|‚ñà‚ñä        | 53/300 [01:13&lt;05:46,  1.40s/it]\n\n 18%|‚ñà‚ñä        | 54/300 [01:15&lt;05:44,  1.40s/it]\n\n 18%|‚ñà‚ñä        | 55/300 [01:16&lt;05:42,  1.40s/it]\n\n 19%|‚ñà‚ñä        | 56/300 [01:18&lt;05:43,  1.41s/it]\n\n 19%|‚ñà‚ñâ        | 57/300 [01:19&lt;05:44,  1.42s/it]\n\n 19%|‚ñà‚ñâ        | 58/300 [01:20&lt;05:40,  1.41s/it]\n\n 20%|‚ñà‚ñâ        | 59/300 [01:22&lt;05:38,  1.40s/it]\n\n 20%|‚ñà‚ñà        | 60/300 [01:23&lt;05:38,  1.41s/it]\n\n 20%|‚ñà‚ñà        | 61/300 [01:25&lt;05:35,  1.40s/it]\n\n 21%|‚ñà‚ñà        | 62/300 [01:26&lt;05:35,  1.41s/it]\n\n 21%|‚ñà‚ñà        | 63/300 [01:27&lt;05:32,  1.40s/it]\n\n 21%|‚ñà‚ñà‚ñè       | 64/300 [01:29&lt;05:30,  1.40s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 65/300 [01:30&lt;05:31,  1.41s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 66/300 [01:32&lt;05:31,  1.42s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 67/300 [01:33&lt;05:30,  1.42s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 68/300 [01:35&lt;05:29,  1.42s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 69/300 [01:36&lt;05:25,  1.41s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 70/300 [01:37&lt;05:25,  1.42s/it]\n\n 24%|‚ñà‚ñà‚ñé       | 71/300 [01:39&lt;05:22,  1.41s/it]\n\n 24%|‚ñà‚ñà‚ñç       | 72/300 [01:40&lt;05:20,  1.41s/it]\n\n 24%|‚ñà‚ñà‚ñç       | 73/300 [01:42&lt;05:17,  1.40s/it]\n\n 25%|‚ñà‚ñà‚ñç       | 74/300 [01:43&lt;05:15,  1.40s/it]\n\n 25%|‚ñà‚ñà‚ñå       | 75/300 [01:44&lt;05:15,  1.40s/it]\n\n 25%|‚ñà‚ñà‚ñå       | 76/300 [01:46&lt;05:14,  1.40s/it]\n\n 26%|‚ñà‚ñà‚ñå       | 77/300 [01:47&lt;05:12,  1.40s/it]\n\n 26%|‚ñà‚ñà‚ñå       | 78/300 [01:49&lt;05:09,  1.40s/it]\n\n 26%|‚ñà‚ñà‚ñã       | 79/300 [01:50&lt;05:07,  1.39s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 80/300 [01:51&lt;05:07,  1.40s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 81/300 [01:53&lt;05:06,  1.40s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 82/300 [01:54&lt;05:06,  1.41s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 83/300 [01:56&lt;05:04,  1.40s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 84/300 [01:57&lt;05:01,  1.40s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 85/300 [01:58&lt;05:02,  1.41s/it]\n\n 29%|‚ñà‚ñà‚ñä       | 86/300 [02:00&lt;05:01,  1.41s/it]\n\n 29%|‚ñà‚ñà‚ñâ       | 87/300 [02:01&lt;04:58,  1.40s/it]\n\n 29%|‚ñà‚ñà‚ñâ       | 88/300 [02:03&lt;04:56,  1.40s/it]\n\n 30%|‚ñà‚ñà‚ñâ       | 89/300 [02:04&lt;04:54,  1.40s/it]\n\n 30%|‚ñà‚ñà‚ñà       | 90/300 [02:05&lt;04:53,  1.40s/it]\n\n 30%|‚ñà‚ñà‚ñà       | 91/300 [02:07&lt;04:51,  1.39s/it]\n\n 31%|‚ñà‚ñà‚ñà       | 92/300 [02:08&lt;04:49,  1.39s/it]\n\n 31%|‚ñà‚ñà‚ñà       | 93/300 [02:09&lt;04:48,  1.39s/it]\n\n 31%|‚ñà‚ñà‚ñà‚ñè      | 94/300 [02:11&lt;04:48,  1.40s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 95/300 [02:12&lt;04:46,  1.40s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 96/300 [02:14&lt;04:45,  1.40s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 97/300 [02:15&lt;04:49,  1.43s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 98/300 [02:17&lt;04:45,  1.42s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 99/300 [02:18&lt;04:42,  1.41s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 100/300 [02:19&lt;04:41,  1.41s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñé      | 101/300 [02:21&lt;04:39,  1.40s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñç      | 102/300 [02:22&lt;04:37,  1.40s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñç      | 103/300 [02:24&lt;04:37,  1.41s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñç      | 104/300 [02:25&lt;04:36,  1.41s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñå      | 105/300 [02:26&lt;04:35,  1.41s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñå      | 106/300 [02:28&lt;04:35,  1.42s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñå      | 107/300 [02:29&lt;04:32,  1.41s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñå      | 108/300 [02:31&lt;04:30,  1.41s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñã      | 109/300 [02:32&lt;04:27,  1.40s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 110/300 [02:33&lt;04:27,  1.41s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 111/300 [02:35&lt;04:24,  1.40s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 112/300 [02:36&lt;04:22,  1.40s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 113/300 [02:38&lt;04:20,  1.39s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 114/300 [02:39&lt;04:19,  1.40s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 115/300 [02:40&lt;04:19,  1.40s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñä      | 116/300 [02:42&lt;04:18,  1.40s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñâ      | 117/300 [02:43&lt;04:16,  1.40s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñâ      | 118/300 [02:45&lt;04:16,  1.41s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñâ      | 119/300 [02:46&lt;04:14,  1.40s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñà      | 120/300 [02:47&lt;04:12,  1.40s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñà      | 121/300 [02:49&lt;04:10,  1.40s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà      | 122/300 [02:50&lt;04:10,  1.40s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà      | 123/300 [02:52&lt;04:08,  1.41s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 124/300 [02:53&lt;04:09,  1.42s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 125/300 [02:55&lt;04:07,  1.41s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 126/300 [02:56&lt;04:05,  1.41s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 127/300 [02:57&lt;04:03,  1.40s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 128/300 [02:59&lt;04:01,  1.41s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 129/300 [03:00&lt;04:00,  1.41s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 130/300 [03:02&lt;03:58,  1.40s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 131/300 [03:03&lt;03:56,  1.40s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 132/300 [03:04&lt;03:54,  1.40s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 133/300 [03:06&lt;03:53,  1.40s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 134/300 [03:07&lt;03:54,  1.41s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 135/300 [03:09&lt;03:52,  1.41s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 136/300 [03:10&lt;03:51,  1.41s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 137/300 [03:11&lt;03:48,  1.40s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 138/300 [03:13&lt;03:46,  1.40s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 139/300 [03:14&lt;03:45,  1.40s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 140/300 [03:16&lt;03:44,  1.40s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 141/300 [03:17&lt;03:42,  1.40s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 142/300 [03:18&lt;03:40,  1.40s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 143/300 [03:20&lt;03:38,  1.39s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 144/300 [03:21&lt;03:40,  1.41s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 145/300 [03:23&lt;03:38,  1.41s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 146/300 [03:24&lt;03:37,  1.41s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 147/300 [03:25&lt;03:34,  1.40s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 148/300 [03:27&lt;03:35,  1.41s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 149/300 [03:28&lt;03:32,  1.41s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 150/300 [03:30&lt;03:30,  1.41s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 151/300 [03:31&lt;03:29,  1.40s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 152/300 [03:32&lt;03:28,  1.41s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 153/300 [03:34&lt;03:26,  1.41s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 154/300 [03:35&lt;03:27,  1.42s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 155/300 [03:37&lt;03:25,  1.42s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 156/300 [03:38&lt;03:23,  1.41s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 157/300 [03:40&lt;03:21,  1.41s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 158/300 [03:41&lt;03:19,  1.41s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 159/300 [03:42&lt;03:18,  1.41s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 160/300 [03:44&lt;03:16,  1.40s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 161/300 [03:45&lt;03:14,  1.40s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 162/300 [03:46&lt;03:13,  1.40s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 163/300 [03:48&lt;03:13,  1.42s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 164/300 [03:49&lt;03:12,  1.41s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 165/300 [03:51&lt;03:12,  1.42s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 166/300 [03:52&lt;03:09,  1.42s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 167/300 [03:54&lt;03:07,  1.41s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 168/300 [03:55&lt;03:05,  1.40s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 169/300 [03:56&lt;03:03,  1.40s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 170/300 [03:58&lt;03:01,  1.40s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 171/300 [03:59&lt;03:00,  1.40s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 172/300 [04:01&lt;02:58,  1.39s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 173/300 [04:02&lt;02:57,  1.40s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 174/300 [04:03&lt;02:56,  1.40s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 175/300 [04:05&lt;02:55,  1.41s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 176/300 [04:06&lt;02:54,  1.40s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 177/300 [04:08&lt;02:53,  1.41s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 178/300 [04:09&lt;02:52,  1.41s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 179/300 [04:10&lt;02:49,  1.40s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 180/300 [04:12&lt;02:48,  1.40s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 181/300 [04:13&lt;02:46,  1.40s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 182/300 [04:15&lt;02:45,  1.40s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 183/300 [04:16&lt;02:43,  1.40s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 184/300 [04:17&lt;02:43,  1.41s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 185/300 [04:19&lt;02:42,  1.41s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 186/300 [04:20&lt;02:44,  1.44s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 187/300 [04:22&lt;02:41,  1.43s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 188/300 [04:23&lt;02:38,  1.41s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 189/300 [04:25&lt;02:36,  1.41s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 190/300 [04:26&lt;02:35,  1.41s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 191/300 [04:27&lt;02:33,  1.41s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 192/300 [04:29&lt;02:31,  1.40s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 193/300 [04:30&lt;02:30,  1.40s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 194/300 [04:32&lt;02:29,  1.41s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 195/300 [04:33&lt;02:29,  1.43s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 196/300 [04:34&lt;02:27,  1.42s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 197/300 [04:36&lt;02:25,  1.41s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 198/300 [04:37&lt;02:23,  1.41s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 199/300 [04:39&lt;02:22,  1.41s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 200/300 [04:40&lt;02:20,  1.40s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 201/300 [04:41&lt;02:18,  1.40s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 202/300 [04:43&lt;02:18,  1.41s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 203/300 [04:44&lt;02:16,  1.41s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 204/300 [04:46&lt;02:15,  1.41s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 205/300 [04:47&lt;02:14,  1.42s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 206/300 [04:49&lt;02:12,  1.41s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 207/300 [04:50&lt;02:11,  1.41s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 208/300 [04:51&lt;02:09,  1.41s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 209/300 [04:53&lt;02:07,  1.41s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 210/300 [04:54&lt;02:07,  1.41s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 211/300 [04:56&lt;02:05,  1.41s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 212/300 [04:57&lt;02:03,  1.40s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 213/300 [04:58&lt;02:02,  1.41s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 214/300 [05:00&lt;02:01,  1.41s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 215/300 [05:01&lt;02:00,  1.41s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 216/300 [05:03&lt;01:58,  1.41s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 217/300 [05:04&lt;01:56,  1.41s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 218/300 [05:05&lt;01:55,  1.41s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 219/300 [05:07&lt;01:53,  1.40s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 220/300 [05:08&lt;01:52,  1.41s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 221/300 [05:10&lt;01:51,  1.41s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 222/300 [05:11&lt;01:49,  1.41s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 223/300 [05:13&lt;01:49,  1.42s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 224/300 [05:14&lt;01:48,  1.42s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 225/300 [05:15&lt;01:46,  1.42s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 226/300 [05:17&lt;01:44,  1.42s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 227/300 [05:18&lt;01:42,  1.41s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 228/300 [05:20&lt;01:41,  1.41s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 229/300 [05:21&lt;01:39,  1.40s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 230/300 [05:22&lt;01:38,  1.40s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 231/300 [05:24&lt;01:36,  1.40s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 232/300 [05:25&lt;01:36,  1.42s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 233/300 [05:27&lt;01:34,  1.41s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 234/300 [05:28&lt;01:34,  1.43s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 235/300 [05:29&lt;01:32,  1.42s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 236/300 [05:31&lt;01:30,  1.41s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 237/300 [05:32&lt;01:29,  1.41s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 238/300 [05:34&lt;01:27,  1.41s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 239/300 [05:35&lt;01:25,  1.41s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 240/300 [05:36&lt;01:24,  1.40s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 241/300 [05:38&lt;01:23,  1.41s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 242/300 [05:39&lt;01:21,  1.41s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 243/300 [05:41&lt;01:20,  1.41s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 244/300 [05:42&lt;01:20,  1.43s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 245/300 [05:44&lt;01:17,  1.42s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 246/300 [05:45&lt;01:16,  1.41s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 247/300 [05:46&lt;01:14,  1.41s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 248/300 [05:48&lt;01:13,  1.41s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 249/300 [05:49&lt;01:11,  1.40s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 250/300 [05:51&lt;01:10,  1.40s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 251/300 [05:52&lt;01:08,  1.40s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 252/300 [05:53&lt;01:07,  1.41s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 253/300 [05:55&lt;01:06,  1.40s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 254/300 [05:56&lt;01:04,  1.41s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 255/300 [05:58&lt;01:03,  1.41s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 256/300 [05:59&lt;01:02,  1.41s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 257/300 [06:00&lt;01:00,  1.41s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 258/300 [06:02&lt;00:59,  1.41s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 259/300 [06:03&lt;00:58,  1.42s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 260/300 [06:05&lt;00:56,  1.41s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 261/300 [06:06&lt;00:54,  1.41s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 262/300 [06:08&lt;00:53,  1.41s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 263/300 [06:09&lt;00:52,  1.41s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 264/300 [06:10&lt;00:51,  1.42s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 265/300 [06:12&lt;00:49,  1.42s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 266/300 [06:13&lt;00:47,  1.41s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 267/300 [06:15&lt;00:46,  1.41s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 268/300 [06:16&lt;00:45,  1.41s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 269/300 [06:17&lt;00:43,  1.41s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 270/300 [06:19&lt;00:42,  1.41s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 271/300 [06:20&lt;00:40,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 272/300 [06:22&lt;00:39,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 273/300 [06:23&lt;00:37,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 274/300 [06:24&lt;00:36,  1.41s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 275/300 [06:26&lt;00:35,  1.40s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 276/300 [06:27&lt;00:33,  1.40s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 277/300 [06:29&lt;00:32,  1.40s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 278/300 [06:30&lt;00:30,  1.40s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 279/300 [06:31&lt;00:29,  1.40s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 280/300 [06:33&lt;00:28,  1.40s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 281/300 [06:34&lt;00:26,  1.41s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 282/300 [06:36&lt;00:25,  1.41s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 283/300 [06:37&lt;00:23,  1.41s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 284/300 [06:38&lt;00:22,  1.41s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 285/300 [06:40&lt;00:21,  1.40s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 286/300 [06:41&lt;00:19,  1.41s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 287/300 [06:43&lt;00:18,  1.40s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 288/300 [06:44&lt;00:16,  1.40s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 289/300 [06:45&lt;00:15,  1.40s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 290/300 [06:47&lt;00:13,  1.40s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 291/300 [06:48&lt;00:12,  1.40s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 292/300 [06:50&lt;00:11,  1.40s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 293/300 [06:51&lt;00:09,  1.40s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 294/300 [06:52&lt;00:08,  1.40s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 295/300 [06:54&lt;00:07,  1.40s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 296/300 [06:55&lt;00:05,  1.40s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 297/300 [06:57&lt;00:04,  1.40s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 298/300 [06:58&lt;00:02,  1.41s/it]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 299/300 [06:59&lt;00:01,  1.40s/it]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [07:01&lt;00:00,  1.40s/it]\n\n11it [1:17:27, 421.96s/it]\n\n  0%|          | 0/300 [00:00&lt;?, ?it/s]\n\n  0%|          | 1/300 [00:00&lt;04:56,  1.01it/s]\n\n  1%|          | 2/300 [00:02&lt;06:14,  1.26s/it]\n\n  1%|          | 3/300 [00:03&lt;06:34,  1.33s/it]\n\n  1%|‚ñè         | 4/300 [00:05&lt;06:40,  1.35s/it]\n\n  2%|‚ñè         | 5/300 [00:06&lt;06:43,  1.37s/it]\n\n  2%|‚ñè         | 6/300 [00:08&lt;06:56,  1.42s/it]\n\n  2%|‚ñè         | 7/300 [00:09&lt;06:53,  1.41s/it]\n\n  3%|‚ñé         | 8/300 [00:10&lt;06:51,  1.41s/it]\n\n  3%|‚ñé         | 9/300 [00:12&lt;06:48,  1.40s/it]\n\n  3%|‚ñé         | 10/300 [00:13&lt;06:47,  1.41s/it]\n\n  4%|‚ñé         | 11/300 [00:15&lt;06:47,  1.41s/it]\n\n  4%|‚ñç         | 12/300 [00:16&lt;06:45,  1.41s/it]\n\n  4%|‚ñç         | 13/300 [00:18&lt;06:46,  1.41s/it]\n\n  5%|‚ñç         | 14/300 [00:19&lt;06:42,  1.41s/it]\n\n  5%|‚ñå         | 15/300 [00:20&lt;06:39,  1.40s/it]\n\n  5%|‚ñå         | 16/300 [00:22&lt;06:37,  1.40s/it]\n\n  6%|‚ñå         | 17/300 [00:23&lt;06:36,  1.40s/it]\n\n  6%|‚ñå         | 18/300 [00:24&lt;06:33,  1.40s/it]\n\n  6%|‚ñã         | 19/300 [00:26&lt;06:31,  1.39s/it]\n\n  7%|‚ñã         | 20/300 [00:27&lt;06:30,  1.39s/it]\n\n  7%|‚ñã         | 21/300 [00:29&lt;06:28,  1.39s/it]\n\n  7%|‚ñã         | 22/300 [00:30&lt;06:28,  1.40s/it]\n\n  8%|‚ñä         | 23/300 [00:31&lt;06:29,  1.40s/it]\n\n  8%|‚ñä         | 24/300 [00:33&lt;06:28,  1.41s/it]\n\n  8%|‚ñä         | 25/300 [00:34&lt;06:25,  1.40s/it]\n\n  9%|‚ñä         | 26/300 [00:36&lt;06:23,  1.40s/it]\n\n  9%|‚ñâ         | 27/300 [00:37&lt;06:20,  1.40s/it]\n\n  9%|‚ñâ         | 28/300 [00:38&lt;06:21,  1.40s/it]\n\n 10%|‚ñâ         | 29/300 [00:40&lt;06:19,  1.40s/it]\n\n 10%|‚ñà         | 30/300 [00:41&lt;06:17,  1.40s/it]\n\n 10%|‚ñà         | 31/300 [00:43&lt;06:18,  1.41s/it]\n\n 11%|‚ñà         | 32/300 [00:44&lt;06:16,  1.41s/it]\n\n 11%|‚ñà         | 33/300 [00:45&lt;06:15,  1.41s/it]\n\n 11%|‚ñà‚ñè        | 34/300 [00:47&lt;06:14,  1.41s/it]\n\n 12%|‚ñà‚ñè        | 35/300 [00:48&lt;06:14,  1.41s/it]\n\n 12%|‚ñà‚ñè        | 36/300 [00:50&lt;06:11,  1.41s/it]\n\n 12%|‚ñà‚ñè        | 37/300 [00:51&lt;06:12,  1.41s/it]\n\n 13%|‚ñà‚ñé        | 38/300 [00:53&lt;06:08,  1.41s/it]\n\n 13%|‚ñà‚ñé        | 39/300 [00:54&lt;06:06,  1.40s/it]\n\n 13%|‚ñà‚ñé        | 40/300 [00:55&lt;06:04,  1.40s/it]\n\n 14%|‚ñà‚ñé        | 41/300 [00:57&lt;06:03,  1.40s/it]\n\n 14%|‚ñà‚ñç        | 42/300 [00:58&lt;06:02,  1.41s/it]\n\n 14%|‚ñà‚ñç        | 43/300 [01:00&lt;06:06,  1.43s/it]\n\n 15%|‚ñà‚ñç        | 44/300 [01:01&lt;06:02,  1.42s/it]\n\n 15%|‚ñà‚ñå        | 45/300 [01:02&lt;05:59,  1.41s/it]\n\n 15%|‚ñà‚ñå        | 46/300 [01:04&lt;05:56,  1.40s/it]\n\n 16%|‚ñà‚ñå        | 47/300 [01:05&lt;05:53,  1.40s/it]\n\n 16%|‚ñà‚ñå        | 48/300 [01:07&lt;05:52,  1.40s/it]\n\n 16%|‚ñà‚ñã        | 49/300 [01:08&lt;05:50,  1.39s/it]\n\n 17%|‚ñà‚ñã        | 50/300 [01:09&lt;05:50,  1.40s/it]\n\n 17%|‚ñà‚ñã        | 51/300 [01:11&lt;05:51,  1.41s/it]\n\n 17%|‚ñà‚ñã        | 52/300 [01:12&lt;05:48,  1.41s/it]\n\n 18%|‚ñà‚ñä        | 53/300 [01:14&lt;05:48,  1.41s/it]\n\n 18%|‚ñà‚ñä        | 54/300 [01:15&lt;05:45,  1.41s/it]\n\n 18%|‚ñà‚ñä        | 55/300 [01:16&lt;05:43,  1.40s/it]\n\n 19%|‚ñà‚ñä        | 56/300 [01:18&lt;05:41,  1.40s/it]\n\n 19%|‚ñà‚ñâ        | 57/300 [01:19&lt;05:39,  1.40s/it]\n\n 19%|‚ñà‚ñâ        | 58/300 [01:21&lt;05:38,  1.40s/it]\n\n 20%|‚ñà‚ñâ        | 59/300 [01:22&lt;05:36,  1.40s/it]\n\n 20%|‚ñà‚ñà        | 60/300 [01:23&lt;05:34,  1.40s/it]\n\n 20%|‚ñà‚ñà        | 61/300 [01:25&lt;05:35,  1.40s/it]\n\n 21%|‚ñà‚ñà        | 62/300 [01:26&lt;05:34,  1.41s/it]\n\n 21%|‚ñà‚ñà        | 63/300 [01:28&lt;05:33,  1.41s/it]\n\n 21%|‚ñà‚ñà‚ñè       | 64/300 [01:29&lt;05:31,  1.41s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 65/300 [01:30&lt;05:29,  1.40s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 66/300 [01:32&lt;05:27,  1.40s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 67/300 [01:33&lt;05:26,  1.40s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 68/300 [01:35&lt;05:25,  1.40s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 69/300 [01:36&lt;05:24,  1.40s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 70/300 [01:37&lt;05:23,  1.41s/it]\n\n 24%|‚ñà‚ñà‚ñé       | 71/300 [01:39&lt;05:21,  1.40s/it]\n\n 24%|‚ñà‚ñà‚ñç       | 72/300 [01:40&lt;05:19,  1.40s/it]\n\n 24%|‚ñà‚ñà‚ñç       | 73/300 [01:42&lt;05:17,  1.40s/it]\n\n 25%|‚ñà‚ñà‚ñç       | 74/300 [01:43&lt;05:14,  1.39s/it]\n\n 25%|‚ñà‚ñà‚ñå       | 75/300 [01:44&lt;05:13,  1.39s/it]\n\n 25%|‚ñà‚ñà‚ñå       | 76/300 [01:46&lt;05:12,  1.40s/it]\n\n 26%|‚ñà‚ñà‚ñå       | 77/300 [01:47&lt;05:10,  1.39s/it]\n\n 26%|‚ñà‚ñà‚ñå       | 78/300 [01:49&lt;05:11,  1.40s/it]\n\n 26%|‚ñà‚ñà‚ñã       | 79/300 [01:50&lt;05:09,  1.40s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 80/300 [01:51&lt;05:09,  1.41s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 81/300 [01:53&lt;05:08,  1.41s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 82/300 [01:54&lt;05:09,  1.42s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 83/300 [01:56&lt;05:05,  1.41s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 84/300 [01:57&lt;05:05,  1.42s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 85/300 [01:59&lt;05:03,  1.41s/it]\n\n 29%|‚ñà‚ñà‚ñä       | 86/300 [02:00&lt;05:01,  1.41s/it]\n\n 29%|‚ñà‚ñà‚ñâ       | 87/300 [02:01&lt;04:58,  1.40s/it]\n\n 29%|‚ñà‚ñà‚ñâ       | 88/300 [02:03&lt;04:56,  1.40s/it]\n\n 30%|‚ñà‚ñà‚ñâ       | 89/300 [02:04&lt;04:55,  1.40s/it]\n\n 30%|‚ñà‚ñà‚ñà       | 90/300 [02:06&lt;04:59,  1.43s/it]\n\n 30%|‚ñà‚ñà‚ñà       | 91/300 [02:07&lt;04:55,  1.42s/it]\n\n 31%|‚ñà‚ñà‚ñà       | 92/300 [02:08&lt;04:54,  1.41s/it]\n\n 31%|‚ñà‚ñà‚ñà       | 93/300 [02:10&lt;04:51,  1.41s/it]\n\n 31%|‚ñà‚ñà‚ñà‚ñè      | 94/300 [02:11&lt;04:49,  1.40s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 95/300 [02:13&lt;04:46,  1.40s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 96/300 [02:14&lt;04:47,  1.41s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 97/300 [02:15&lt;04:44,  1.40s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 98/300 [02:17&lt;04:42,  1.40s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 99/300 [02:18&lt;04:40,  1.39s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 100/300 [02:20&lt;04:41,  1.41s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñé      | 101/300 [02:21&lt;04:40,  1.41s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñç      | 102/300 [02:22&lt;04:38,  1.41s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñç      | 103/300 [02:24&lt;04:36,  1.40s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñç      | 104/300 [02:25&lt;04:34,  1.40s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñå      | 105/300 [02:27&lt;04:32,  1.40s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñå      | 106/300 [02:28&lt;04:30,  1.39s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñå      | 107/300 [02:29&lt;04:28,  1.39s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñå      | 108/300 [02:31&lt;04:27,  1.39s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñã      | 109/300 [02:32&lt;04:28,  1.41s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 110/300 [02:34&lt;04:27,  1.41s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 111/300 [02:35&lt;04:27,  1.41s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 112/300 [02:36&lt;04:24,  1.41s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 113/300 [02:38&lt;04:23,  1.41s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 114/300 [02:39&lt;04:21,  1.40s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 115/300 [02:41&lt;04:18,  1.40s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñä      | 116/300 [02:42&lt;04:18,  1.40s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñâ      | 117/300 [02:43&lt;04:15,  1.40s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñâ      | 118/300 [02:45&lt;04:15,  1.41s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñâ      | 119/300 [02:46&lt;04:14,  1.40s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñà      | 120/300 [02:48&lt;04:12,  1.40s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñà      | 121/300 [02:49&lt;04:11,  1.40s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà      | 122/300 [02:50&lt;04:08,  1.40s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà      | 123/300 [02:52&lt;04:07,  1.40s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 124/300 [02:53&lt;04:05,  1.40s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 125/300 [02:55&lt;04:03,  1.39s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 126/300 [02:56&lt;04:03,  1.40s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 127/300 [02:57&lt;04:01,  1.40s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 128/300 [02:59&lt;04:01,  1.40s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 129/300 [03:00&lt;04:00,  1.41s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 130/300 [03:02&lt;04:07,  1.45s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 131/300 [03:03&lt;04:03,  1.44s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 132/300 [03:05&lt;04:00,  1.43s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 133/300 [03:06&lt;03:56,  1.42s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 134/300 [03:07&lt;03:54,  1.41s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 135/300 [03:09&lt;03:53,  1.41s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 136/300 [03:10&lt;03:50,  1.41s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 137/300 [03:12&lt;03:48,  1.40s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 138/300 [03:13&lt;03:47,  1.40s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 139/300 [03:14&lt;03:45,  1.40s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 140/300 [03:16&lt;03:46,  1.42s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 141/300 [03:17&lt;03:45,  1.42s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 142/300 [03:19&lt;03:42,  1.41s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 143/300 [03:20&lt;03:40,  1.40s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 144/300 [03:21&lt;03:38,  1.40s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 145/300 [03:23&lt;03:36,  1.40s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 146/300 [03:24&lt;03:35,  1.40s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 147/300 [03:26&lt;03:34,  1.40s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 148/300 [03:27&lt;03:32,  1.40s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 149/300 [03:28&lt;03:31,  1.40s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 150/300 [03:30&lt;03:30,  1.40s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 151/300 [03:31&lt;03:28,  1.40s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 152/300 [03:33&lt;03:26,  1.40s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 153/300 [03:34&lt;03:25,  1.40s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 154/300 [03:35&lt;03:23,  1.39s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 155/300 [03:37&lt;03:22,  1.40s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 156/300 [03:38&lt;03:22,  1.41s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 157/300 [03:40&lt;03:20,  1.40s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 158/300 [03:41&lt;03:19,  1.41s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 159/300 [03:42&lt;03:18,  1.40s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 160/300 [03:44&lt;03:16,  1.41s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 161/300 [03:45&lt;03:16,  1.41s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 162/300 [03:47&lt;03:13,  1.40s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 163/300 [03:48&lt;03:11,  1.40s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 164/300 [03:49&lt;03:09,  1.40s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 165/300 [03:51&lt;03:08,  1.39s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 166/300 [03:52&lt;03:07,  1.40s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 167/300 [03:54&lt;03:06,  1.40s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 168/300 [03:55&lt;03:07,  1.42s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 169/300 [03:57&lt;03:05,  1.42s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 170/300 [03:58&lt;03:03,  1.41s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 171/300 [03:59&lt;03:02,  1.42s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 172/300 [04:01&lt;03:00,  1.41s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 173/300 [04:02&lt;02:58,  1.40s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 174/300 [04:04&lt;02:56,  1.40s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 175/300 [04:05&lt;02:54,  1.40s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 176/300 [04:06&lt;02:53,  1.40s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 177/300 [04:08&lt;02:51,  1.40s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 178/300 [04:09&lt;02:51,  1.41s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 179/300 [04:11&lt;02:51,  1.42s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 180/300 [04:12&lt;02:49,  1.41s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 181/300 [04:13&lt;02:47,  1.41s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 182/300 [04:15&lt;02:45,  1.40s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 183/300 [04:16&lt;02:43,  1.40s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 184/300 [04:18&lt;02:41,  1.40s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 185/300 [04:19&lt;02:40,  1.40s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 186/300 [04:20&lt;02:39,  1.40s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 187/300 [04:22&lt;02:38,  1.40s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 188/300 [04:23&lt;02:37,  1.41s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 189/300 [04:25&lt;02:36,  1.41s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 190/300 [04:26&lt;02:34,  1.40s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 191/300 [04:27&lt;02:32,  1.40s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 192/300 [04:29&lt;02:31,  1.40s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 193/300 [04:30&lt;02:29,  1.40s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 194/300 [04:32&lt;02:27,  1.39s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 195/300 [04:33&lt;02:26,  1.39s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 196/300 [04:34&lt;02:24,  1.39s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 197/300 [04:36&lt;02:23,  1.39s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 198/300 [04:37&lt;02:23,  1.41s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 199/300 [04:39&lt;02:22,  1.41s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 200/300 [04:40&lt;02:20,  1.40s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 201/300 [04:41&lt;02:18,  1.40s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 202/300 [04:43&lt;02:17,  1.40s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 203/300 [04:44&lt;02:16,  1.41s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 204/300 [04:46&lt;02:15,  1.41s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 205/300 [04:47&lt;02:13,  1.40s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 206/300 [04:48&lt;02:12,  1.41s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 207/300 [04:50&lt;02:10,  1.41s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 208/300 [04:51&lt;02:09,  1.40s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 209/300 [04:53&lt;02:08,  1.41s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 210/300 [04:54&lt;02:06,  1.40s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 211/300 [04:55&lt;02:04,  1.40s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 212/300 [04:57&lt;02:03,  1.41s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 213/300 [04:58&lt;02:01,  1.40s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 214/300 [05:00&lt;02:00,  1.40s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 215/300 [05:01&lt;01:58,  1.40s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 216/300 [05:02&lt;01:57,  1.40s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 217/300 [05:04&lt;01:55,  1.40s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 218/300 [05:05&lt;01:54,  1.39s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 219/300 [05:07&lt;01:53,  1.40s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 220/300 [05:08&lt;01:52,  1.40s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 221/300 [05:09&lt;01:50,  1.40s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 222/300 [05:11&lt;01:49,  1.40s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 223/300 [05:12&lt;01:47,  1.40s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 224/300 [05:14&lt;01:45,  1.39s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 225/300 [05:15&lt;01:44,  1.40s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 226/300 [05:16&lt;01:43,  1.40s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 227/300 [05:18&lt;01:42,  1.40s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 228/300 [05:19&lt;01:41,  1.41s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 229/300 [05:21&lt;01:40,  1.41s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 230/300 [05:22&lt;01:40,  1.43s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 231/300 [05:24&lt;01:38,  1.43s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 232/300 [05:25&lt;01:36,  1.42s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 233/300 [05:26&lt;01:35,  1.42s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 234/300 [05:28&lt;01:33,  1.42s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 235/300 [05:29&lt;01:31,  1.41s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 236/300 [05:31&lt;01:30,  1.41s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 237/300 [05:32&lt;01:28,  1.41s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 238/300 [05:33&lt;01:27,  1.40s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 239/300 [05:35&lt;01:26,  1.41s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 240/300 [05:36&lt;01:24,  1.41s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 241/300 [05:38&lt;01:22,  1.41s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 242/300 [05:39&lt;01:21,  1.40s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 243/300 [05:40&lt;01:19,  1.40s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 244/300 [05:42&lt;01:18,  1.40s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 245/300 [05:43&lt;01:16,  1.40s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 246/300 [05:45&lt;01:15,  1.40s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 247/300 [05:46&lt;01:13,  1.40s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 248/300 [05:47&lt;01:13,  1.41s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 249/300 [05:49&lt;01:11,  1.40s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 250/300 [05:50&lt;01:09,  1.40s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 251/300 [05:52&lt;01:08,  1.40s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 252/300 [05:53&lt;01:07,  1.40s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 253/300 [05:54&lt;01:05,  1.39s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 254/300 [05:56&lt;01:04,  1.39s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 255/300 [05:57&lt;01:02,  1.40s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 256/300 [05:59&lt;01:01,  1.40s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 257/300 [06:00&lt;01:00,  1.40s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 258/300 [06:01&lt;00:59,  1.41s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 259/300 [06:03&lt;00:57,  1.41s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 260/300 [06:04&lt;00:56,  1.41s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 261/300 [06:06&lt;00:54,  1.40s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 262/300 [06:07&lt;00:53,  1.41s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 263/300 [06:08&lt;00:51,  1.40s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 264/300 [06:10&lt;00:50,  1.41s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 265/300 [06:11&lt;00:49,  1.40s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 266/300 [06:13&lt;00:47,  1.41s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 267/300 [06:14&lt;00:46,  1.41s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 268/300 [06:16&lt;00:45,  1.41s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 269/300 [06:17&lt;00:43,  1.41s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 270/300 [06:18&lt;00:42,  1.41s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 271/300 [06:20&lt;00:40,  1.41s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 272/300 [06:21&lt;00:39,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 273/300 [06:23&lt;00:37,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 274/300 [06:24&lt;00:36,  1.40s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 275/300 [06:25&lt;00:35,  1.40s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 276/300 [06:27&lt;00:33,  1.41s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 277/300 [06:28&lt;00:32,  1.41s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 278/300 [06:30&lt;00:31,  1.41s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 279/300 [06:31&lt;00:29,  1.41s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 280/300 [06:32&lt;00:28,  1.41s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 281/300 [06:34&lt;00:26,  1.40s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 282/300 [06:35&lt;00:25,  1.41s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 283/300 [06:37&lt;00:23,  1.40s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 284/300 [06:38&lt;00:22,  1.40s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 285/300 [06:39&lt;00:21,  1.40s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 286/300 [06:41&lt;00:19,  1.40s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 287/300 [06:42&lt;00:18,  1.40s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 288/300 [06:44&lt;00:17,  1.42s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 289/300 [06:45&lt;00:15,  1.41s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 290/300 [06:46&lt;00:14,  1.41s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 291/300 [06:48&lt;00:12,  1.40s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 292/300 [06:49&lt;00:11,  1.41s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 293/300 [06:51&lt;00:09,  1.41s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 294/300 [06:52&lt;00:08,  1.41s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 295/300 [06:53&lt;00:07,  1.40s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 296/300 [06:55&lt;00:05,  1.40s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 297/300 [06:56&lt;00:04,  1.40s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 298/300 [06:58&lt;00:02,  1.40s/it]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 299/300 [06:59&lt;00:01,  1.40s/it]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [07:00&lt;00:00,  1.40s/it]\n\n12it [1:24:29, 421.90s/it]\n\n  0%|          | 0/300 [00:00&lt;?, ?it/s]\n\n  0%|          | 1/300 [00:00&lt;04:53,  1.02it/s]\n\n  1%|          | 2/300 [00:02&lt;06:03,  1.22s/it]\n\n  1%|          | 3/300 [00:03&lt;06:25,  1.30s/it]\n\n  1%|‚ñè         | 4/300 [00:05&lt;06:34,  1.33s/it]\n\n  2%|‚ñè         | 5/300 [00:06&lt;06:39,  1.35s/it]\n\n  2%|‚ñè         | 6/300 [00:07&lt;06:44,  1.38s/it]\n\n  2%|‚ñè         | 7/300 [00:09&lt;06:48,  1.40s/it]\n\n  3%|‚ñé         | 8/300 [00:10&lt;06:47,  1.39s/it]\n\n  3%|‚ñé         | 9/300 [00:12&lt;06:46,  1.40s/it]\n\n  3%|‚ñé         | 10/300 [00:13&lt;06:44,  1.40s/it]\n\n  4%|‚ñé         | 11/300 [00:14&lt;06:43,  1.40s/it]\n\n  4%|‚ñç         | 12/300 [00:16&lt;06:43,  1.40s/it]\n\n  4%|‚ñç         | 13/300 [00:17&lt;06:40,  1.39s/it]\n\n  5%|‚ñç         | 14/300 [00:19&lt;06:39,  1.40s/it]\n\n  5%|‚ñå         | 15/300 [00:20&lt;06:37,  1.39s/it]\n\n  5%|‚ñå         | 16/300 [00:21&lt;06:36,  1.40s/it]\n\n  6%|‚ñå         | 17/300 [00:23&lt;06:41,  1.42s/it]\n\n  6%|‚ñå         | 18/300 [00:24&lt;06:38,  1.41s/it]\n\n  6%|‚ñã         | 19/300 [00:26&lt;06:35,  1.41s/it]\n\n  7%|‚ñã         | 20/300 [00:27&lt;06:33,  1.40s/it]\n\n  7%|‚ñã         | 21/300 [00:29&lt;06:30,  1.40s/it]\n\n  7%|‚ñã         | 22/300 [00:30&lt;06:37,  1.43s/it]\n\n  8%|‚ñä         | 23/300 [00:31&lt;06:32,  1.42s/it]\n\n  8%|‚ñä         | 24/300 [00:33&lt;06:29,  1.41s/it]\n\n  8%|‚ñä         | 25/300 [00:34&lt;06:27,  1.41s/it]\n\n  9%|‚ñä         | 26/300 [00:36&lt;06:26,  1.41s/it]\n\n  9%|‚ñâ         | 27/300 [00:37&lt;06:24,  1.41s/it]\n\n  9%|‚ñâ         | 28/300 [00:38&lt;06:21,  1.40s/it]\n\n 10%|‚ñâ         | 29/300 [00:40&lt;06:18,  1.40s/it]\n\n 10%|‚ñà         | 30/300 [00:41&lt;06:16,  1.39s/it]\n\n 10%|‚ñà         | 31/300 [00:43&lt;06:15,  1.40s/it]\n\n 11%|‚ñà         | 32/300 [00:44&lt;06:14,  1.40s/it]\n\n 11%|‚ñà         | 33/300 [00:45&lt;06:13,  1.40s/it]\n\n 11%|‚ñà‚ñè        | 34/300 [00:47&lt;06:12,  1.40s/it]\n\n 12%|‚ñà‚ñè        | 35/300 [00:48&lt;06:13,  1.41s/it]\n\n 12%|‚ñà‚ñè        | 36/300 [00:50&lt;06:10,  1.40s/it]\n\n 12%|‚ñà‚ñè        | 37/300 [00:51&lt;06:12,  1.41s/it]\n\n 13%|‚ñà‚ñé        | 38/300 [00:52&lt;06:09,  1.41s/it]\n\n 13%|‚ñà‚ñé        | 39/300 [00:54&lt;06:06,  1.40s/it]\n\n 13%|‚ñà‚ñé        | 40/300 [00:55&lt;06:04,  1.40s/it]\n\n 14%|‚ñà‚ñé        | 41/300 [00:57&lt;06:02,  1.40s/it]\n\n 14%|‚ñà‚ñç        | 42/300 [00:58&lt;06:00,  1.40s/it]\n\n 14%|‚ñà‚ñç        | 43/300 [00:59&lt;05:59,  1.40s/it]\n\n 15%|‚ñà‚ñç        | 44/300 [01:01&lt;05:58,  1.40s/it]\n\n 15%|‚ñà‚ñå        | 45/300 [01:02&lt;05:57,  1.40s/it]\n\n 15%|‚ñà‚ñå        | 46/300 [01:04&lt;05:55,  1.40s/it]\n\n 16%|‚ñà‚ñå        | 47/300 [01:05&lt;05:54,  1.40s/it]\n\n 16%|‚ñà‚ñå        | 48/300 [01:06&lt;05:51,  1.40s/it]\n\n 16%|‚ñà‚ñã        | 49/300 [01:08&lt;05:50,  1.40s/it]\n\n 17%|‚ñà‚ñã        | 50/300 [01:09&lt;05:51,  1.41s/it]\n\n 17%|‚ñà‚ñã        | 51/300 [01:11&lt;05:49,  1.40s/it]\n\n 17%|‚ñà‚ñã        | 52/300 [01:12&lt;05:48,  1.41s/it]\n\n 18%|‚ñà‚ñä        | 53/300 [01:13&lt;05:47,  1.41s/it]\n\n 18%|‚ñà‚ñä        | 54/300 [01:15&lt;05:46,  1.41s/it]\n\n 18%|‚ñà‚ñä        | 55/300 [01:16&lt;05:51,  1.43s/it]\n\n 19%|‚ñà‚ñä        | 56/300 [01:18&lt;05:47,  1.43s/it]\n\n 19%|‚ñà‚ñâ        | 57/300 [01:19&lt;05:43,  1.41s/it]\n\n 19%|‚ñà‚ñâ        | 58/300 [01:21&lt;05:40,  1.41s/it]\n\n 20%|‚ñà‚ñâ        | 59/300 [01:22&lt;05:40,  1.41s/it]\n\n 20%|‚ñà‚ñà        | 60/300 [01:23&lt;05:38,  1.41s/it]\n\n 20%|‚ñà‚ñà        | 61/300 [01:25&lt;05:35,  1.40s/it]\n\n 21%|‚ñà‚ñà        | 62/300 [01:26&lt;05:32,  1.40s/it]\n\n 21%|‚ñà‚ñà        | 63/300 [01:28&lt;05:30,  1.40s/it]\n\n 21%|‚ñà‚ñà‚ñè       | 64/300 [01:29&lt;05:30,  1.40s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 65/300 [01:30&lt;05:28,  1.40s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 66/300 [01:32&lt;05:27,  1.40s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 67/300 [01:33&lt;05:25,  1.40s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 68/300 [01:35&lt;05:24,  1.40s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 69/300 [01:36&lt;05:24,  1.40s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 70/300 [01:37&lt;05:23,  1.40s/it]\n\n 24%|‚ñà‚ñà‚ñé       | 71/300 [01:39&lt;05:20,  1.40s/it]\n\n 24%|‚ñà‚ñà‚ñç       | 72/300 [01:40&lt;05:18,  1.40s/it]\n\n 24%|‚ñà‚ñà‚ñç       | 73/300 [01:42&lt;05:16,  1.39s/it]\n\n 25%|‚ñà‚ñà‚ñç       | 74/300 [01:43&lt;05:17,  1.41s/it]\n\n 25%|‚ñà‚ñà‚ñå       | 75/300 [01:44&lt;05:18,  1.41s/it]\n\n 25%|‚ñà‚ñà‚ñå       | 76/300 [01:46&lt;05:15,  1.41s/it]\n\n 26%|‚ñà‚ñà‚ñå       | 77/300 [01:47&lt;05:13,  1.41s/it]\n\n 26%|‚ñà‚ñà‚ñå       | 78/300 [01:49&lt;05:10,  1.40s/it]\n\n 26%|‚ñà‚ñà‚ñã       | 79/300 [01:50&lt;05:08,  1.40s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 80/300 [01:51&lt;05:06,  1.39s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 81/300 [01:53&lt;05:07,  1.40s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 82/300 [01:54&lt;05:05,  1.40s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 83/300 [01:56&lt;05:03,  1.40s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 84/300 [01:57&lt;05:01,  1.40s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 85/300 [01:58&lt;05:00,  1.40s/it]\n\n 29%|‚ñà‚ñà‚ñä       | 86/300 [02:00&lt;05:02,  1.41s/it]\n\n 29%|‚ñà‚ñà‚ñâ       | 87/300 [02:01&lt;04:59,  1.41s/it]\n\n 29%|‚ñà‚ñà‚ñâ       | 88/300 [02:03&lt;04:57,  1.40s/it]\n\n 30%|‚ñà‚ñà‚ñâ       | 89/300 [02:04&lt;04:56,  1.40s/it]\n\n 30%|‚ñà‚ñà‚ñà       | 90/300 [02:05&lt;04:55,  1.41s/it]\n\n 30%|‚ñà‚ñà‚ñà       | 91/300 [02:07&lt;04:52,  1.40s/it]\n\n 31%|‚ñà‚ñà‚ñà       | 92/300 [02:08&lt;04:51,  1.40s/it]\n\n 31%|‚ñà‚ñà‚ñà       | 93/300 [02:10&lt;04:49,  1.40s/it]\n\n 31%|‚ñà‚ñà‚ñà‚ñè      | 94/300 [02:11&lt;04:50,  1.41s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 95/300 [02:12&lt;04:48,  1.41s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 96/300 [02:14&lt;04:47,  1.41s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 97/300 [02:15&lt;04:46,  1.41s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 98/300 [02:17&lt;04:44,  1.41s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 99/300 [02:18&lt;04:42,  1.40s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 100/300 [02:19&lt;04:40,  1.40s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñé      | 101/300 [02:21&lt;04:40,  1.41s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñç      | 102/300 [02:22&lt;04:38,  1.40s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñç      | 103/300 [02:24&lt;04:36,  1.40s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñç      | 104/300 [02:25&lt;04:38,  1.42s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñå      | 105/300 [02:27&lt;04:35,  1.41s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñå      | 106/300 [02:28&lt;04:34,  1.41s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñå      | 107/300 [02:29&lt;04:31,  1.41s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñå      | 108/300 [02:31&lt;04:29,  1.40s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñã      | 109/300 [02:32&lt;04:27,  1.40s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 110/300 [02:34&lt;04:26,  1.40s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 111/300 [02:35&lt;04:24,  1.40s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 112/300 [02:36&lt;04:23,  1.40s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 113/300 [02:38&lt;04:24,  1.41s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 114/300 [02:39&lt;04:22,  1.41s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 115/300 [02:41&lt;04:21,  1.41s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñä      | 116/300 [02:42&lt;04:19,  1.41s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñâ      | 117/300 [02:43&lt;04:17,  1.41s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñâ      | 118/300 [02:45&lt;04:16,  1.41s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñâ      | 119/300 [02:46&lt;04:14,  1.41s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñà      | 120/300 [02:48&lt;04:13,  1.41s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñà      | 121/300 [02:49&lt;04:11,  1.40s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà      | 122/300 [02:51&lt;04:16,  1.44s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà      | 123/300 [02:52&lt;04:12,  1.43s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 124/300 [02:53&lt;04:11,  1.43s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 125/300 [02:55&lt;04:09,  1.43s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 126/300 [02:56&lt;04:07,  1.42s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 127/300 [02:58&lt;04:05,  1.42s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 128/300 [02:59&lt;04:02,  1.41s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 129/300 [03:00&lt;03:59,  1.40s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 130/300 [03:02&lt;03:57,  1.40s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 131/300 [03:03&lt;03:56,  1.40s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 132/300 [03:05&lt;03:55,  1.40s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 133/300 [03:06&lt;03:55,  1.41s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 134/300 [03:07&lt;03:53,  1.41s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 135/300 [03:09&lt;03:52,  1.41s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 136/300 [03:10&lt;03:49,  1.40s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 137/300 [03:12&lt;03:48,  1.40s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 138/300 [03:13&lt;03:46,  1.40s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 139/300 [03:14&lt;03:44,  1.40s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 140/300 [03:16&lt;03:43,  1.40s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 141/300 [03:17&lt;03:42,  1.40s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 142/300 [03:19&lt;03:40,  1.40s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 143/300 [03:20&lt;03:39,  1.40s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 144/300 [03:21&lt;03:41,  1.42s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 145/300 [03:23&lt;03:39,  1.42s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 146/300 [03:24&lt;03:38,  1.42s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 147/300 [03:26&lt;03:35,  1.41s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 148/300 [03:27&lt;03:33,  1.40s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 149/300 [03:28&lt;03:31,  1.40s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 150/300 [03:30&lt;03:29,  1.40s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 151/300 [03:31&lt;03:28,  1.40s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 152/300 [03:33&lt;03:27,  1.40s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 153/300 [03:34&lt;03:26,  1.40s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 154/300 [03:35&lt;03:25,  1.41s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 155/300 [03:37&lt;03:26,  1.43s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 156/300 [03:38&lt;03:24,  1.42s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 157/300 [03:40&lt;03:21,  1.41s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 158/300 [03:41&lt;03:20,  1.41s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 159/300 [03:43&lt;03:17,  1.40s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 160/300 [03:44&lt;03:15,  1.40s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 161/300 [03:45&lt;03:14,  1.40s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 162/300 [03:47&lt;03:12,  1.39s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 163/300 [03:48&lt;03:14,  1.42s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 164/300 [03:50&lt;03:12,  1.42s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 165/300 [03:51&lt;03:10,  1.41s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 166/300 [03:52&lt;03:10,  1.42s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 167/300 [03:54&lt;03:08,  1.41s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 168/300 [03:55&lt;03:07,  1.42s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 169/300 [03:57&lt;03:05,  1.42s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 170/300 [03:58&lt;03:03,  1.41s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 171/300 [03:59&lt;03:01,  1.41s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 172/300 [04:01&lt;03:00,  1.41s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 173/300 [04:02&lt;02:59,  1.41s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 174/300 [04:04&lt;02:58,  1.42s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 175/300 [04:05&lt;02:56,  1.41s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 176/300 [04:07&lt;02:54,  1.40s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 177/300 [04:08&lt;02:52,  1.40s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 178/300 [04:09&lt;02:50,  1.40s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 179/300 [04:11&lt;02:48,  1.39s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 180/300 [04:12&lt;02:47,  1.39s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 181/300 [04:13&lt;02:45,  1.39s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 182/300 [04:15&lt;02:46,  1.41s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 183/300 [04:16&lt;02:44,  1.41s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 184/300 [04:18&lt;02:43,  1.41s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 185/300 [04:19&lt;02:42,  1.41s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 186/300 [04:21&lt;02:40,  1.41s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 187/300 [04:22&lt;02:38,  1.40s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 188/300 [04:23&lt;02:36,  1.40s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 189/300 [04:25&lt;02:35,  1.40s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 190/300 [04:26&lt;02:33,  1.40s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 191/300 [04:28&lt;02:32,  1.39s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 192/300 [04:29&lt;02:30,  1.40s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 193/300 [04:30&lt;02:29,  1.40s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 194/300 [04:32&lt;02:28,  1.40s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 195/300 [04:33&lt;02:27,  1.40s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 196/300 [04:35&lt;02:25,  1.40s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 197/300 [04:36&lt;02:25,  1.41s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 198/300 [04:37&lt;02:23,  1.40s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 199/300 [04:39&lt;02:22,  1.41s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 200/300 [04:40&lt;02:21,  1.41s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 201/300 [04:42&lt;02:19,  1.41s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 202/300 [04:43&lt;02:17,  1.40s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 203/300 [04:44&lt;02:16,  1.40s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 204/300 [04:46&lt;02:14,  1.40s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 205/300 [04:47&lt;02:12,  1.40s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 206/300 [04:49&lt;02:11,  1.40s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 207/300 [04:50&lt;02:09,  1.40s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 208/300 [04:51&lt;02:08,  1.39s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 209/300 [04:53&lt;02:06,  1.39s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 210/300 [04:54&lt;02:05,  1.39s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 211/300 [04:56&lt;02:06,  1.42s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 212/300 [04:57&lt;02:04,  1.42s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 213/300 [04:58&lt;02:02,  1.41s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 214/300 [05:00&lt;02:01,  1.41s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 215/300 [05:01&lt;01:59,  1.40s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 216/300 [05:03&lt;01:57,  1.40s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 217/300 [05:04&lt;01:55,  1.40s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 218/300 [05:05&lt;01:54,  1.39s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 219/300 [05:07&lt;01:52,  1.39s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 220/300 [05:08&lt;01:51,  1.39s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 221/300 [05:10&lt;01:49,  1.39s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 222/300 [05:11&lt;01:48,  1.40s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 223/300 [05:12&lt;01:48,  1.41s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 224/300 [05:14&lt;01:47,  1.41s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 225/300 [05:15&lt;01:45,  1.41s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 226/300 [05:17&lt;01:44,  1.41s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 227/300 [05:18&lt;01:42,  1.40s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 228/300 [05:19&lt;01:41,  1.41s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 229/300 [05:21&lt;01:39,  1.40s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 230/300 [05:22&lt;01:37,  1.40s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 231/300 [05:24&lt;01:36,  1.40s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 232/300 [05:25&lt;01:35,  1.40s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 233/300 [05:26&lt;01:33,  1.40s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 234/300 [05:28&lt;01:32,  1.40s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 235/300 [05:29&lt;01:30,  1.40s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 236/300 [05:31&lt;01:29,  1.40s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 237/300 [05:32&lt;01:27,  1.39s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 238/300 [05:33&lt;01:27,  1.41s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 239/300 [05:35&lt;01:25,  1.40s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 240/300 [05:36&lt;01:24,  1.40s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 241/300 [05:38&lt;01:23,  1.41s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 242/300 [05:39&lt;01:22,  1.42s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 243/300 [05:41&lt;01:21,  1.42s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 244/300 [05:42&lt;01:19,  1.43s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 245/300 [05:43&lt;01:17,  1.42s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 246/300 [05:45&lt;01:16,  1.41s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 247/300 [05:46&lt;01:14,  1.41s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 248/300 [05:48&lt;01:12,  1.40s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 249/300 [05:49&lt;01:11,  1.40s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 250/300 [05:50&lt;01:10,  1.41s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 251/300 [05:52&lt;01:08,  1.40s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 252/300 [05:53&lt;01:07,  1.41s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 253/300 [05:55&lt;01:06,  1.41s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 254/300 [05:56&lt;01:04,  1.41s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 255/300 [05:57&lt;01:03,  1.41s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 256/300 [05:59&lt;01:01,  1.40s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 257/300 [06:00&lt;01:00,  1.40s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 258/300 [06:02&lt;00:58,  1.40s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 259/300 [06:03&lt;00:57,  1.40s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 260/300 [06:04&lt;00:56,  1.40s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 261/300 [06:06&lt;00:54,  1.41s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 262/300 [06:07&lt;00:53,  1.40s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 263/300 [06:09&lt;00:52,  1.41s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 264/300 [06:10&lt;00:50,  1.40s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 265/300 [06:11&lt;00:49,  1.40s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 266/300 [06:13&lt;00:47,  1.40s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 267/300 [06:14&lt;00:46,  1.41s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 268/300 [06:16&lt;00:45,  1.41s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 269/300 [06:17&lt;00:43,  1.40s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 270/300 [06:18&lt;00:41,  1.40s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 271/300 [06:20&lt;00:40,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 272/300 [06:21&lt;00:39,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 273/300 [06:23&lt;00:38,  1.41s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 274/300 [06:24&lt;00:36,  1.40s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 275/300 [06:25&lt;00:34,  1.40s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 276/300 [06:27&lt;00:33,  1.40s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 277/300 [06:28&lt;00:32,  1.40s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 278/300 [06:30&lt;00:30,  1.40s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 279/300 [06:31&lt;00:29,  1.41s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 280/300 [06:32&lt;00:28,  1.40s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 281/300 [06:34&lt;00:26,  1.41s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 282/300 [06:35&lt;00:25,  1.41s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 283/300 [06:37&lt;00:23,  1.40s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 284/300 [06:38&lt;00:22,  1.41s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 285/300 [06:40&lt;00:21,  1.40s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 286/300 [06:41&lt;00:19,  1.41s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 287/300 [06:42&lt;00:18,  1.40s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 288/300 [06:44&lt;00:16,  1.40s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 289/300 [06:45&lt;00:15,  1.40s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 290/300 [06:47&lt;00:14,  1.40s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 291/300 [06:48&lt;00:12,  1.41s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 292/300 [06:49&lt;00:11,  1.43s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 293/300 [06:51&lt;00:09,  1.42s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 294/300 [06:52&lt;00:08,  1.41s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 295/300 [06:54&lt;00:07,  1.40s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 296/300 [06:55&lt;00:05,  1.41s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 297/300 [06:56&lt;00:04,  1.40s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 298/300 [06:58&lt;00:02,  1.41s/it]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 299/300 [06:59&lt;00:01,  1.40s/it]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [07:01&lt;00:00,  1.40s/it]\n\n13it [1:31:31, 421.92s/it]\n\n  0%|          | 0/300 [00:00&lt;?, ?it/s]\n\n  0%|          | 1/300 [00:00&lt;04:56,  1.01it/s]\n\n  1%|          | 2/300 [00:02&lt;06:08,  1.24s/it]\n\n  1%|          | 3/300 [00:03&lt;06:29,  1.31s/it]\n\n  1%|‚ñè         | 4/300 [00:05&lt;06:37,  1.34s/it]\n\n  2%|‚ñè         | 5/300 [00:06&lt;06:40,  1.36s/it]\n\n  2%|‚ñè         | 6/300 [00:07&lt;06:43,  1.37s/it]\n\n  2%|‚ñè         | 7/300 [00:09&lt;06:44,  1.38s/it]\n\n  3%|‚ñé         | 8/300 [00:10&lt;06:43,  1.38s/it]\n\n  3%|‚ñé         | 9/300 [00:12&lt;06:42,  1.38s/it]\n\n  3%|‚ñé         | 10/300 [00:13&lt;06:49,  1.41s/it]\n\n  4%|‚ñé         | 11/300 [00:15&lt;06:49,  1.42s/it]\n\n  4%|‚ñç         | 12/300 [00:16&lt;06:49,  1.42s/it]\n\n  4%|‚ñç         | 13/300 [00:17&lt;06:45,  1.41s/it]\n\n  5%|‚ñç         | 14/300 [00:19&lt;06:41,  1.41s/it]\n\n  5%|‚ñå         | 15/300 [00:20&lt;06:40,  1.40s/it]\n\n  5%|‚ñå         | 16/300 [00:22&lt;06:37,  1.40s/it]\n\n  6%|‚ñå         | 17/300 [00:23&lt;06:35,  1.40s/it]\n\n  6%|‚ñå         | 18/300 [00:24&lt;06:33,  1.40s/it]\n\n  6%|‚ñã         | 19/300 [00:26&lt;06:34,  1.40s/it]\n\n  7%|‚ñã         | 20/300 [00:27&lt;06:32,  1.40s/it]\n\n  7%|‚ñã         | 21/300 [00:29&lt;06:33,  1.41s/it]\n\n  7%|‚ñã         | 22/300 [00:30&lt;06:31,  1.41s/it]\n\n  8%|‚ñä         | 23/300 [00:31&lt;06:30,  1.41s/it]\n\n  8%|‚ñä         | 24/300 [00:33&lt;06:27,  1.40s/it]\n\n  8%|‚ñä         | 25/300 [00:34&lt;06:25,  1.40s/it]\n\n  9%|‚ñä         | 26/300 [00:36&lt;06:34,  1.44s/it]\n\n  9%|‚ñâ         | 27/300 [00:37&lt;06:29,  1.43s/it]\n\n  9%|‚ñâ         | 28/300 [00:39&lt;06:25,  1.42s/it]\n\n 10%|‚ñâ         | 29/300 [00:40&lt;06:24,  1.42s/it]\n\n 10%|‚ñà         | 30/300 [00:41&lt;06:23,  1.42s/it]\n\n 10%|‚ñà         | 31/300 [00:43&lt;06:23,  1.42s/it]\n\n 11%|‚ñà         | 32/300 [00:44&lt;06:19,  1.42s/it]\n\n 11%|‚ñà         | 33/300 [00:46&lt;06:16,  1.41s/it]\n\n 11%|‚ñà‚ñè        | 34/300 [00:47&lt;06:13,  1.40s/it]\n\n 12%|‚ñà‚ñè        | 35/300 [00:48&lt;06:11,  1.40s/it]\n\n 12%|‚ñà‚ñè        | 36/300 [00:50&lt;06:09,  1.40s/it]\n\n 12%|‚ñà‚ñè        | 37/300 [00:51&lt;06:08,  1.40s/it]\n\n 13%|‚ñà‚ñé        | 38/300 [00:53&lt;06:06,  1.40s/it]\n\n 13%|‚ñà‚ñé        | 39/300 [00:54&lt;06:05,  1.40s/it]\n\n 13%|‚ñà‚ñé        | 40/300 [00:55&lt;06:04,  1.40s/it]\n\n 14%|‚ñà‚ñé        | 41/300 [00:57&lt;06:07,  1.42s/it]\n\n 14%|‚ñà‚ñç        | 42/300 [00:58&lt;06:04,  1.41s/it]\n\n 14%|‚ñà‚ñç        | 43/300 [01:00&lt;06:01,  1.41s/it]\n\n 15%|‚ñà‚ñç        | 44/300 [01:01&lt;05:59,  1.40s/it]\n\n 15%|‚ñà‚ñå        | 45/300 [01:02&lt;05:57,  1.40s/it]\n\n 15%|‚ñà‚ñå        | 46/300 [01:04&lt;05:55,  1.40s/it]\n\n 16%|‚ñà‚ñå        | 47/300 [01:05&lt;05:54,  1.40s/it]\n\n 16%|‚ñà‚ñå        | 48/300 [01:07&lt;05:53,  1.40s/it]\n\n 16%|‚ñà‚ñã        | 49/300 [01:08&lt;05:51,  1.40s/it]\n\n 17%|‚ñà‚ñã        | 50/300 [01:09&lt;05:50,  1.40s/it]\n\n 17%|‚ñà‚ñã        | 51/300 [01:11&lt;05:49,  1.40s/it]\n\n 17%|‚ñà‚ñã        | 52/300 [01:12&lt;05:48,  1.40s/it]\n\n 18%|‚ñà‚ñä        | 53/300 [01:14&lt;05:45,  1.40s/it]\n\n 18%|‚ñà‚ñä        | 54/300 [01:15&lt;05:43,  1.40s/it]\n\n 18%|‚ñà‚ñä        | 55/300 [01:16&lt;05:41,  1.40s/it]\n\n 19%|‚ñà‚ñä        | 56/300 [01:18&lt;05:40,  1.39s/it]\n\n 19%|‚ñà‚ñâ        | 57/300 [01:19&lt;05:40,  1.40s/it]\n\n 19%|‚ñà‚ñâ        | 58/300 [01:21&lt;05:37,  1.40s/it]\n\n 20%|‚ñà‚ñâ        | 59/300 [01:22&lt;05:36,  1.40s/it]\n\n 20%|‚ñà‚ñà        | 60/300 [01:23&lt;05:38,  1.41s/it]\n\n 20%|‚ñà‚ñà        | 61/300 [01:25&lt;05:36,  1.41s/it]\n\n 21%|‚ñà‚ñà        | 62/300 [01:26&lt;05:34,  1.41s/it]\n\n 21%|‚ñà‚ñà        | 63/300 [01:28&lt;05:33,  1.41s/it]\n\n 21%|‚ñà‚ñà‚ñè       | 64/300 [01:29&lt;05:31,  1.40s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 65/300 [01:30&lt;05:30,  1.40s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 66/300 [01:32&lt;05:27,  1.40s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 67/300 [01:33&lt;05:25,  1.40s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 68/300 [01:35&lt;05:23,  1.39s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 69/300 [01:36&lt;05:27,  1.42s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 70/300 [01:38&lt;05:29,  1.43s/it]\n\n 24%|‚ñà‚ñà‚ñé       | 71/300 [01:39&lt;05:25,  1.42s/it]\n\n 24%|‚ñà‚ñà‚ñç       | 72/300 [01:40&lt;05:23,  1.42s/it]\n\n 24%|‚ñà‚ñà‚ñç       | 73/300 [01:42&lt;05:20,  1.41s/it]\n\n 25%|‚ñà‚ñà‚ñç       | 74/300 [01:43&lt;05:19,  1.41s/it]\n\n 25%|‚ñà‚ñà‚ñå       | 75/300 [01:45&lt;05:16,  1.41s/it]\n\n 25%|‚ñà‚ñà‚ñå       | 76/300 [01:46&lt;05:15,  1.41s/it]\n\n 26%|‚ñà‚ñà‚ñå       | 77/300 [01:47&lt;05:15,  1.41s/it]\n\n 26%|‚ñà‚ñà‚ñå       | 78/300 [01:49&lt;05:12,  1.41s/it]\n\n 26%|‚ñà‚ñà‚ñã       | 79/300 [01:50&lt;05:10,  1.41s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 80/300 [01:52&lt;05:13,  1.42s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 81/300 [01:53&lt;05:10,  1.42s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 82/300 [01:55&lt;05:10,  1.43s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 83/300 [01:56&lt;05:06,  1.41s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 84/300 [01:57&lt;05:04,  1.41s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 85/300 [01:59&lt;05:02,  1.41s/it]\n\n 29%|‚ñà‚ñà‚ñä       | 86/300 [02:00&lt;05:01,  1.41s/it]\n\n 29%|‚ñà‚ñà‚ñâ       | 87/300 [02:02&lt;04:58,  1.40s/it]\n\n 29%|‚ñà‚ñà‚ñâ       | 88/300 [02:03&lt;04:57,  1.40s/it]\n\n 30%|‚ñà‚ñà‚ñâ       | 89/300 [02:04&lt;04:58,  1.41s/it]\n\n 30%|‚ñà‚ñà‚ñà       | 90/300 [02:06&lt;04:57,  1.42s/it]\n\n 30%|‚ñà‚ñà‚ñà       | 91/300 [02:07&lt;04:54,  1.41s/it]\n\n 31%|‚ñà‚ñà‚ñà       | 92/300 [02:09&lt;04:52,  1.41s/it]\n\n 31%|‚ñà‚ñà‚ñà       | 93/300 [02:10&lt;04:50,  1.40s/it]\n\n 31%|‚ñà‚ñà‚ñà‚ñè      | 94/300 [02:11&lt;04:48,  1.40s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 95/300 [02:13&lt;04:46,  1.40s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 96/300 [02:14&lt;04:48,  1.41s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 97/300 [02:16&lt;04:45,  1.41s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 98/300 [02:17&lt;04:43,  1.41s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 99/300 [02:18&lt;04:42,  1.40s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 100/300 [02:20&lt;04:41,  1.41s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñé      | 101/300 [02:21&lt;04:41,  1.41s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñç      | 102/300 [02:23&lt;04:40,  1.42s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñç      | 103/300 [02:24&lt;04:38,  1.41s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñç      | 104/300 [02:25&lt;04:38,  1.42s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñå      | 105/300 [02:27&lt;04:35,  1.41s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñå      | 106/300 [02:28&lt;04:33,  1.41s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñå      | 107/300 [02:30&lt;04:31,  1.41s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñå      | 108/300 [02:31&lt;04:32,  1.42s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñã      | 109/300 [02:33&lt;04:32,  1.43s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 110/300 [02:34&lt;04:29,  1.42s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 111/300 [02:35&lt;04:27,  1.41s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 112/300 [02:37&lt;04:24,  1.41s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 113/300 [02:38&lt;04:22,  1.40s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 114/300 [02:40&lt;04:20,  1.40s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 115/300 [02:41&lt;04:25,  1.43s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñä      | 116/300 [02:42&lt;04:21,  1.42s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñâ      | 117/300 [02:44&lt;04:18,  1.41s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñâ      | 118/300 [02:45&lt;04:16,  1.41s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñâ      | 119/300 [02:47&lt;04:14,  1.41s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñà      | 120/300 [02:48&lt;04:14,  1.41s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñà      | 121/300 [02:50&lt;04:12,  1.41s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà      | 122/300 [02:51&lt;04:10,  1.41s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà      | 123/300 [02:52&lt;04:08,  1.40s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 124/300 [02:54&lt;04:10,  1.42s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 125/300 [02:55&lt;04:06,  1.41s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 126/300 [02:57&lt;04:04,  1.41s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 127/300 [02:58&lt;04:04,  1.42s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 128/300 [02:59&lt;04:04,  1.42s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 129/300 [03:01&lt;04:01,  1.41s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 130/300 [03:02&lt;03:59,  1.41s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 131/300 [03:04&lt;03:59,  1.41s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 132/300 [03:05&lt;03:56,  1.41s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 133/300 [03:06&lt;03:55,  1.41s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 134/300 [03:08&lt;03:52,  1.40s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 135/300 [03:09&lt;03:50,  1.40s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 136/300 [03:11&lt;03:49,  1.40s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 137/300 [03:12&lt;03:47,  1.40s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 138/300 [03:13&lt;03:48,  1.41s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 139/300 [03:15&lt;03:46,  1.41s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 140/300 [03:16&lt;03:44,  1.40s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 141/300 [03:18&lt;03:42,  1.40s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 142/300 [03:19&lt;03:40,  1.40s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 143/300 [03:20&lt;03:39,  1.40s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 144/300 [03:22&lt;03:38,  1.40s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 145/300 [03:23&lt;03:36,  1.40s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 146/300 [03:25&lt;03:34,  1.40s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 147/300 [03:26&lt;03:33,  1.40s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 148/300 [03:27&lt;03:32,  1.40s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 149/300 [03:29&lt;03:32,  1.40s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 150/300 [03:30&lt;03:30,  1.40s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 151/300 [03:32&lt;03:29,  1.41s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 152/300 [03:33&lt;03:28,  1.41s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 153/300 [03:34&lt;03:26,  1.40s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 154/300 [03:36&lt;03:24,  1.40s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 155/300 [03:37&lt;03:25,  1.42s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 156/300 [03:39&lt;03:23,  1.41s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 157/300 [03:40&lt;03:22,  1.41s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 158/300 [03:42&lt;03:21,  1.42s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 159/300 [03:43&lt;03:19,  1.42s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 160/300 [03:44&lt;03:17,  1.41s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 161/300 [03:46&lt;03:15,  1.40s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 162/300 [03:47&lt;03:13,  1.40s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 163/300 [03:49&lt;03:13,  1.41s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 164/300 [03:50&lt;03:11,  1.41s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 165/300 [03:51&lt;03:09,  1.40s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 166/300 [03:53&lt;03:08,  1.40s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 167/300 [03:54&lt;03:08,  1.42s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 168/300 [03:56&lt;03:07,  1.42s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 169/300 [03:57&lt;03:05,  1.42s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 170/300 [03:58&lt;03:04,  1.42s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 171/300 [04:00&lt;03:02,  1.41s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 172/300 [04:01&lt;03:00,  1.41s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 173/300 [04:03&lt;02:58,  1.40s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 174/300 [04:04&lt;02:56,  1.40s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 175/300 [04:05&lt;02:54,  1.40s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 176/300 [04:07&lt;02:53,  1.40s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 177/300 [04:08&lt;02:52,  1.40s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 178/300 [04:10&lt;02:51,  1.41s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 179/300 [04:11&lt;02:50,  1.41s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 180/300 [04:12&lt;02:48,  1.40s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 181/300 [04:14&lt;02:46,  1.40s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 182/300 [04:15&lt;02:44,  1.39s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 183/300 [04:17&lt;02:42,  1.39s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 184/300 [04:18&lt;02:42,  1.40s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 185/300 [04:19&lt;02:41,  1.40s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 186/300 [04:21&lt;02:39,  1.40s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 187/300 [04:22&lt;02:38,  1.40s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 188/300 [04:24&lt;02:37,  1.40s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 189/300 [04:25&lt;02:35,  1.40s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 190/300 [04:27&lt;02:35,  1.41s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 191/300 [04:28&lt;02:33,  1.41s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 192/300 [04:29&lt;02:32,  1.41s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 193/300 [04:31&lt;02:30,  1.41s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 194/300 [04:32&lt;02:28,  1.40s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 195/300 [04:34&lt;02:27,  1.40s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 196/300 [04:35&lt;02:25,  1.40s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 197/300 [04:36&lt;02:24,  1.40s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 198/300 [04:38&lt;02:23,  1.40s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 199/300 [04:39&lt;02:21,  1.40s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 200/300 [04:41&lt;02:19,  1.40s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 201/300 [04:42&lt;02:18,  1.40s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 202/300 [04:43&lt;02:17,  1.40s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 203/300 [04:45&lt;02:15,  1.40s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 204/300 [04:46&lt;02:14,  1.40s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 205/300 [04:48&lt;02:13,  1.40s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 206/300 [04:49&lt;02:13,  1.42s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 207/300 [04:50&lt;02:11,  1.41s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 208/300 [04:52&lt;02:09,  1.41s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 209/300 [04:53&lt;02:07,  1.40s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 210/300 [04:55&lt;02:05,  1.40s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 211/300 [04:56&lt;02:05,  1.41s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 212/300 [04:57&lt;02:03,  1.41s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 213/300 [04:59&lt;02:01,  1.40s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 214/300 [05:00&lt;02:00,  1.40s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 215/300 [05:02&lt;01:59,  1.40s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 216/300 [05:03&lt;01:57,  1.40s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 217/300 [05:04&lt;01:56,  1.40s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 218/300 [05:06&lt;01:56,  1.42s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 219/300 [05:07&lt;01:54,  1.42s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 220/300 [05:09&lt;01:52,  1.41s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 221/300 [05:10&lt;01:50,  1.40s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 222/300 [05:11&lt;01:50,  1.41s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 223/300 [05:13&lt;01:48,  1.41s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 224/300 [05:14&lt;01:46,  1.40s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 225/300 [05:16&lt;01:45,  1.40s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 226/300 [05:17&lt;01:44,  1.41s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 227/300 [05:18&lt;01:42,  1.40s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 228/300 [05:20&lt;01:41,  1.40s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 229/300 [05:21&lt;01:39,  1.40s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 230/300 [05:23&lt;01:38,  1.40s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 231/300 [05:24&lt;01:36,  1.40s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 232/300 [05:25&lt;01:34,  1.40s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 233/300 [05:27&lt;01:33,  1.39s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 234/300 [05:28&lt;01:31,  1.39s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 235/300 [05:30&lt;01:31,  1.40s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 236/300 [05:31&lt;01:30,  1.41s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 237/300 [05:32&lt;01:28,  1.41s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 238/300 [05:34&lt;01:27,  1.42s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 239/300 [05:35&lt;01:27,  1.44s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 240/300 [05:37&lt;01:26,  1.44s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 241/300 [05:38&lt;01:24,  1.43s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 242/300 [05:40&lt;01:22,  1.42s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 243/300 [05:41&lt;01:20,  1.41s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 244/300 [05:42&lt;01:18,  1.41s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 245/300 [05:44&lt;01:17,  1.40s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 246/300 [05:45&lt;01:17,  1.43s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 247/300 [05:47&lt;01:15,  1.42s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 248/300 [05:48&lt;01:13,  1.41s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 249/300 [05:50&lt;01:11,  1.41s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 250/300 [05:51&lt;01:10,  1.42s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 251/300 [05:52&lt;01:08,  1.41s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 252/300 [05:54&lt;01:07,  1.40s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 253/300 [05:55&lt;01:05,  1.40s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 254/300 [05:57&lt;01:04,  1.40s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 255/300 [05:58&lt;01:02,  1.40s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 256/300 [05:59&lt;01:01,  1.40s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 257/300 [06:01&lt;01:00,  1.40s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 258/300 [06:02&lt;00:58,  1.40s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 259/300 [06:04&lt;00:57,  1.40s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 260/300 [06:05&lt;00:55,  1.40s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 261/300 [06:06&lt;00:54,  1.40s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 262/300 [06:08&lt;00:52,  1.39s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 263/300 [06:09&lt;00:51,  1.39s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 264/300 [06:11&lt;00:50,  1.40s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 265/300 [06:12&lt;00:48,  1.40s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 266/300 [06:13&lt;00:47,  1.40s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 267/300 [06:15&lt;00:46,  1.41s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 268/300 [06:16&lt;00:45,  1.41s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 269/300 [06:18&lt;00:43,  1.41s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 270/300 [06:19&lt;00:42,  1.40s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 271/300 [06:20&lt;00:40,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 272/300 [06:22&lt;00:39,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 273/300 [06:23&lt;00:37,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 274/300 [06:25&lt;00:36,  1.40s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 275/300 [06:26&lt;00:34,  1.40s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 276/300 [06:27&lt;00:33,  1.40s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 277/300 [06:29&lt;00:32,  1.40s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 278/300 [06:30&lt;00:31,  1.42s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 279/300 [06:32&lt;00:29,  1.41s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 280/300 [06:33&lt;00:28,  1.42s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 281/300 [06:34&lt;00:26,  1.41s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 282/300 [06:36&lt;00:25,  1.40s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 283/300 [06:37&lt;00:23,  1.40s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 284/300 [06:39&lt;00:22,  1.40s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 285/300 [06:40&lt;00:21,  1.40s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 286/300 [06:41&lt;00:19,  1.40s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 287/300 [06:43&lt;00:18,  1.40s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 288/300 [06:44&lt;00:16,  1.40s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 289/300 [06:46&lt;00:15,  1.40s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 290/300 [06:47&lt;00:13,  1.40s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 291/300 [06:48&lt;00:12,  1.39s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 292/300 [06:50&lt;00:11,  1.40s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 293/300 [06:51&lt;00:09,  1.40s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 294/300 [06:53&lt;00:08,  1.40s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 295/300 [06:54&lt;00:07,  1.40s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 296/300 [06:55&lt;00:05,  1.42s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 297/300 [06:57&lt;00:04,  1.41s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 298/300 [06:58&lt;00:02,  1.41s/it]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 299/300 [07:00&lt;00:01,  1.41s/it]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [07:01&lt;00:00,  1.41s/it]\n\n14it [1:38:33, 422.08s/it]\n\n  0%|          | 0/300 [00:00&lt;?, ?it/s]\n\n  0%|          | 1/300 [00:00&lt;04:56,  1.01it/s]\n\n  1%|          | 2/300 [00:02&lt;06:16,  1.26s/it]\n\n  1%|          | 3/300 [00:03&lt;06:32,  1.32s/it]\n\n  1%|‚ñè         | 4/300 [00:05&lt;06:40,  1.35s/it]\n\n  2%|‚ñè         | 5/300 [00:06&lt;06:44,  1.37s/it]\n\n  2%|‚ñè         | 6/300 [00:08&lt;06:46,  1.38s/it]\n\n  2%|‚ñè         | 7/300 [00:09&lt;06:45,  1.38s/it]\n\n  3%|‚ñé         | 8/300 [00:10&lt;06:45,  1.39s/it]\n\n  3%|‚ñé         | 9/300 [00:12&lt;06:44,  1.39s/it]\n\n  3%|‚ñé         | 10/300 [00:13&lt;06:44,  1.39s/it]\n\n  4%|‚ñé         | 11/300 [00:15&lt;06:42,  1.39s/it]\n\n  4%|‚ñç         | 12/300 [00:16&lt;06:40,  1.39s/it]\n\n  4%|‚ñç         | 13/300 [00:17&lt;06:38,  1.39s/it]\n\n  5%|‚ñç         | 14/300 [00:19&lt;06:38,  1.39s/it]\n\n  5%|‚ñå         | 15/300 [00:20&lt;06:36,  1.39s/it]\n\n  5%|‚ñå         | 16/300 [00:21&lt;06:36,  1.40s/it]\n\n  6%|‚ñå         | 17/300 [00:23&lt;06:34,  1.39s/it]\n\n  6%|‚ñå         | 18/300 [00:24&lt;06:36,  1.40s/it]\n\n  6%|‚ñã         | 19/300 [00:26&lt;06:34,  1.40s/it]\n\n  7%|‚ñã         | 20/300 [00:27&lt;06:31,  1.40s/it]\n\n  7%|‚ñã         | 21/300 [00:28&lt;06:30,  1.40s/it]\n\n  7%|‚ñã         | 22/300 [00:30&lt;06:30,  1.41s/it]\n\n  8%|‚ñä         | 23/300 [00:31&lt;06:28,  1.40s/it]\n\n  8%|‚ñä         | 24/300 [00:33&lt;06:26,  1.40s/it]\n\n  8%|‚ñä         | 25/300 [00:34&lt;06:24,  1.40s/it]\n\n  9%|‚ñä         | 26/300 [00:35&lt;06:23,  1.40s/it]\n\n  9%|‚ñâ         | 27/300 [00:37&lt;06:21,  1.40s/it]\n\n  9%|‚ñâ         | 28/300 [00:38&lt;06:19,  1.40s/it]\n\n 10%|‚ñâ         | 29/300 [00:40&lt;06:17,  1.39s/it]\n\n 10%|‚ñà         | 30/300 [00:41&lt;06:15,  1.39s/it]\n\n 10%|‚ñà         | 31/300 [00:42&lt;06:13,  1.39s/it]\n\n 11%|‚ñà         | 32/300 [00:44&lt;06:12,  1.39s/it]\n\n 11%|‚ñà         | 33/300 [00:45&lt;06:10,  1.39s/it]\n\n 11%|‚ñà‚ñè        | 34/300 [00:47&lt;06:10,  1.39s/it]\n\n 12%|‚ñà‚ñè        | 35/300 [00:48&lt;06:20,  1.44s/it]\n\n 12%|‚ñà‚ñè        | 36/300 [00:50&lt;06:16,  1.43s/it]\n\n 12%|‚ñà‚ñè        | 37/300 [00:51&lt;06:12,  1.42s/it]\n\n 13%|‚ñà‚ñé        | 38/300 [00:52&lt;06:09,  1.41s/it]\n\n 13%|‚ñà‚ñé        | 39/300 [00:54&lt;06:07,  1.41s/it]\n\n 13%|‚ñà‚ñé        | 40/300 [00:55&lt;06:04,  1.40s/it]\n\n 14%|‚ñà‚ñé        | 41/300 [00:57&lt;06:04,  1.41s/it]\n\n 14%|‚ñà‚ñç        | 42/300 [00:58&lt;06:02,  1.41s/it]\n\n 14%|‚ñà‚ñç        | 43/300 [00:59&lt;06:02,  1.41s/it]\n\n 15%|‚ñà‚ñç        | 44/300 [01:01&lt;06:00,  1.41s/it]\n\n 15%|‚ñà‚ñå        | 45/300 [01:02&lt;05:59,  1.41s/it]\n\n 15%|‚ñà‚ñå        | 46/300 [01:04&lt;05:56,  1.40s/it]\n\n 16%|‚ñà‚ñå        | 47/300 [01:05&lt;05:57,  1.41s/it]\n\n 16%|‚ñà‚ñå        | 48/300 [01:06&lt;05:54,  1.41s/it]\n\n 16%|‚ñà‚ñã        | 49/300 [01:08&lt;05:51,  1.40s/it]\n\n 17%|‚ñà‚ñã        | 50/300 [01:09&lt;05:49,  1.40s/it]\n\n 17%|‚ñà‚ñã        | 51/300 [01:11&lt;05:47,  1.40s/it]\n\n 17%|‚ñà‚ñã        | 52/300 [01:12&lt;05:47,  1.40s/it]\n\n 18%|‚ñà‚ñä        | 53/300 [01:13&lt;05:46,  1.40s/it]\n\n 18%|‚ñà‚ñä        | 54/300 [01:15&lt;05:47,  1.41s/it]\n\n 18%|‚ñà‚ñä        | 55/300 [01:16&lt;05:47,  1.42s/it]\n\n 19%|‚ñà‚ñä        | 56/300 [01:18&lt;05:44,  1.41s/it]\n\n 19%|‚ñà‚ñâ        | 57/300 [01:19&lt;05:44,  1.42s/it]\n\n 19%|‚ñà‚ñâ        | 58/300 [01:20&lt;05:40,  1.41s/it]\n\n 20%|‚ñà‚ñâ        | 59/300 [01:22&lt;05:39,  1.41s/it]\n\n 20%|‚ñà‚ñà        | 60/300 [01:23&lt;05:36,  1.40s/it]\n\n 20%|‚ñà‚ñà        | 61/300 [01:25&lt;05:34,  1.40s/it]\n\n 21%|‚ñà‚ñà        | 62/300 [01:26&lt;05:32,  1.40s/it]\n\n 21%|‚ñà‚ñà        | 63/300 [01:27&lt;05:30,  1.40s/it]\n\n 21%|‚ñà‚ñà‚ñè       | 64/300 [01:29&lt;05:29,  1.40s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 65/300 [01:30&lt;05:28,  1.40s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 66/300 [01:32&lt;05:26,  1.40s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 67/300 [01:33&lt;05:25,  1.40s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 68/300 [01:34&lt;05:23,  1.39s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 69/300 [01:36&lt;05:21,  1.39s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 70/300 [01:37&lt;05:20,  1.39s/it]\n\n 24%|‚ñà‚ñà‚ñé       | 71/300 [01:39&lt;05:18,  1.39s/it]\n\n 24%|‚ñà‚ñà‚ñç       | 72/300 [01:40&lt;05:17,  1.39s/it]\n\n 24%|‚ñà‚ñà‚ñç       | 73/300 [01:41&lt;05:16,  1.39s/it]\n\n 25%|‚ñà‚ñà‚ñç       | 74/300 [01:43&lt;05:17,  1.40s/it]\n\n 25%|‚ñà‚ñà‚ñå       | 75/300 [01:44&lt;05:18,  1.41s/it]\n\n 25%|‚ñà‚ñà‚ñå       | 76/300 [01:46&lt;05:14,  1.41s/it]\n\n 26%|‚ñà‚ñà‚ñå       | 77/300 [01:47&lt;05:13,  1.41s/it]\n\n 26%|‚ñà‚ñà‚ñå       | 78/300 [01:48&lt;05:10,  1.40s/it]\n\n 26%|‚ñà‚ñà‚ñã       | 79/300 [01:50&lt;05:08,  1.40s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 80/300 [01:51&lt;05:06,  1.39s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 81/300 [01:53&lt;05:05,  1.39s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 82/300 [01:54&lt;05:03,  1.39s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 83/300 [01:55&lt;05:02,  1.39s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 84/300 [01:57&lt;05:01,  1.40s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 85/300 [01:58&lt;05:01,  1.40s/it]\n\n 29%|‚ñà‚ñà‚ñä       | 86/300 [02:00&lt;04:59,  1.40s/it]\n\n 29%|‚ñà‚ñà‚ñâ       | 87/300 [02:01&lt;04:57,  1.40s/it]\n\n 29%|‚ñà‚ñà‚ñâ       | 88/300 [02:02&lt;04:57,  1.40s/it]\n\n 30%|‚ñà‚ñà‚ñâ       | 89/300 [02:04&lt;04:54,  1.40s/it]\n\n 30%|‚ñà‚ñà‚ñà       | 90/300 [02:05&lt;04:55,  1.41s/it]\n\n 30%|‚ñà‚ñà‚ñà       | 91/300 [02:07&lt;04:52,  1.40s/it]\n\n 31%|‚ñà‚ñà‚ñà       | 92/300 [02:08&lt;04:50,  1.40s/it]\n\n 31%|‚ñà‚ñà‚ñà       | 93/300 [02:09&lt;04:53,  1.42s/it]\n\n 31%|‚ñà‚ñà‚ñà‚ñè      | 94/300 [02:11&lt;04:52,  1.42s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 95/300 [02:12&lt;04:50,  1.41s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 96/300 [02:14&lt;04:47,  1.41s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 97/300 [02:15&lt;04:44,  1.40s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 98/300 [02:17&lt;04:45,  1.41s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 99/300 [02:18&lt;04:43,  1.41s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 100/300 [02:19&lt;04:41,  1.41s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñé      | 101/300 [02:21&lt;04:40,  1.41s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñç      | 102/300 [02:22&lt;04:39,  1.41s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñç      | 103/300 [02:24&lt;04:37,  1.41s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñç      | 104/300 [02:25&lt;04:35,  1.41s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñå      | 105/300 [02:26&lt;04:33,  1.40s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñå      | 106/300 [02:28&lt;04:34,  1.41s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñå      | 107/300 [02:29&lt;04:32,  1.41s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñå      | 108/300 [02:31&lt;04:30,  1.41s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñã      | 109/300 [02:32&lt;04:29,  1.41s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 110/300 [02:33&lt;04:27,  1.41s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 111/300 [02:35&lt;04:25,  1.40s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 112/300 [02:36&lt;04:23,  1.40s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 113/300 [02:38&lt;04:22,  1.40s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 114/300 [02:39&lt;04:22,  1.41s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 115/300 [02:40&lt;04:20,  1.41s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñä      | 116/300 [02:42&lt;04:18,  1.41s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñâ      | 117/300 [02:43&lt;04:19,  1.42s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñâ      | 118/300 [02:45&lt;04:16,  1.41s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñâ      | 119/300 [02:46&lt;04:13,  1.40s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñà      | 120/300 [02:47&lt;04:11,  1.40s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñà      | 121/300 [02:49&lt;04:10,  1.40s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà      | 122/300 [02:50&lt;04:08,  1.40s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà      | 123/300 [02:52&lt;04:07,  1.40s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 124/300 [02:53&lt;04:11,  1.43s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 125/300 [02:55&lt;04:07,  1.42s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 126/300 [02:56&lt;04:05,  1.41s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 127/300 [02:57&lt;04:03,  1.41s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 128/300 [02:59&lt;04:01,  1.40s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 129/300 [03:00&lt;03:59,  1.40s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 130/300 [03:02&lt;03:57,  1.40s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 131/300 [03:03&lt;03:56,  1.40s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 132/300 [03:04&lt;03:56,  1.41s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 133/300 [03:06&lt;03:54,  1.41s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 134/300 [03:07&lt;03:53,  1.40s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 135/300 [03:09&lt;03:51,  1.40s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 136/300 [03:10&lt;03:49,  1.40s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 137/300 [03:11&lt;03:47,  1.40s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 138/300 [03:13&lt;03:46,  1.40s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 139/300 [03:14&lt;03:44,  1.40s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 140/300 [03:16&lt;03:43,  1.40s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 141/300 [03:17&lt;03:41,  1.39s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 142/300 [03:18&lt;03:40,  1.40s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 143/300 [03:20&lt;03:42,  1.42s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 144/300 [03:21&lt;03:41,  1.42s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 145/300 [03:23&lt;03:38,  1.41s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 146/300 [03:24&lt;03:38,  1.42s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 147/300 [03:25&lt;03:36,  1.41s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 148/300 [03:27&lt;03:33,  1.41s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 149/300 [03:28&lt;03:31,  1.40s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 150/300 [03:30&lt;03:29,  1.40s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 151/300 [03:31&lt;03:27,  1.40s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 152/300 [03:32&lt;03:26,  1.40s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 153/300 [03:34&lt;03:25,  1.40s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 154/300 [03:35&lt;03:23,  1.40s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 155/300 [03:37&lt;03:22,  1.40s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 156/300 [03:38&lt;03:23,  1.41s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 157/300 [03:39&lt;03:20,  1.40s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 158/300 [03:41&lt;03:20,  1.41s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 159/300 [03:42&lt;03:22,  1.44s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 160/300 [03:44&lt;03:19,  1.42s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 161/300 [03:45&lt;03:17,  1.42s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 162/300 [03:47&lt;03:15,  1.42s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 163/300 [03:48&lt;03:13,  1.41s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 164/300 [03:49&lt;03:12,  1.41s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 165/300 [03:51&lt;03:10,  1.41s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 166/300 [03:52&lt;03:08,  1.40s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 167/300 [03:54&lt;03:06,  1.40s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 168/300 [03:55&lt;03:05,  1.40s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 169/300 [03:56&lt;03:03,  1.40s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 170/300 [03:58&lt;03:02,  1.40s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 171/300 [03:59&lt;03:00,  1.40s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 172/300 [04:01&lt;02:59,  1.40s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 173/300 [04:02&lt;02:58,  1.40s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 174/300 [04:03&lt;02:57,  1.41s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 175/300 [04:05&lt;02:55,  1.41s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 176/300 [04:06&lt;02:55,  1.42s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 177/300 [04:08&lt;02:53,  1.41s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 178/300 [04:09&lt;02:51,  1.40s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 179/300 [04:10&lt;02:49,  1.40s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 180/300 [04:12&lt;02:47,  1.40s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 181/300 [04:13&lt;02:47,  1.41s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 182/300 [04:15&lt;02:46,  1.41s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 183/300 [04:16&lt;02:44,  1.41s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 184/300 [04:17&lt;02:42,  1.40s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 185/300 [04:19&lt;02:41,  1.40s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 186/300 [04:20&lt;02:39,  1.40s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 187/300 [04:22&lt;02:38,  1.40s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 188/300 [04:23&lt;02:37,  1.40s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 189/300 [04:24&lt;02:35,  1.40s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 190/300 [04:26&lt;02:33,  1.40s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 191/300 [04:27&lt;02:32,  1.40s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 192/300 [04:29&lt;02:33,  1.42s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 193/300 [04:30&lt;02:32,  1.42s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 194/300 [04:32&lt;02:30,  1.42s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 195/300 [04:33&lt;02:27,  1.41s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 196/300 [04:34&lt;02:25,  1.40s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 197/300 [04:36&lt;02:24,  1.40s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 198/300 [04:37&lt;02:22,  1.40s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 199/300 [04:39&lt;02:22,  1.41s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 200/300 [04:40&lt;02:20,  1.40s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 201/300 [04:41&lt;02:18,  1.40s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 202/300 [04:43&lt;02:19,  1.42s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 203/300 [04:44&lt;02:17,  1.42s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 204/300 [04:46&lt;02:16,  1.42s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 205/300 [04:47&lt;02:14,  1.41s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 206/300 [04:48&lt;02:12,  1.41s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 207/300 [04:50&lt;02:10,  1.41s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 208/300 [04:51&lt;02:09,  1.40s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 209/300 [04:53&lt;02:07,  1.41s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 210/300 [04:54&lt;02:06,  1.40s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 211/300 [04:55&lt;02:04,  1.40s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 212/300 [04:57&lt;02:03,  1.40s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 213/300 [04:58&lt;02:02,  1.40s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 214/300 [05:00&lt;02:00,  1.40s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 215/300 [05:01&lt;01:59,  1.41s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 216/300 [05:02&lt;01:58,  1.41s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 217/300 [05:04&lt;01:56,  1.41s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 218/300 [05:05&lt;01:55,  1.41s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 219/300 [05:07&lt;01:53,  1.41s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 220/300 [05:08&lt;01:53,  1.41s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 221/300 [05:10&lt;01:51,  1.41s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 222/300 [05:11&lt;01:51,  1.42s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 223/300 [05:12&lt;01:49,  1.42s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 224/300 [05:14&lt;01:47,  1.42s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 225/300 [05:15&lt;01:45,  1.41s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 226/300 [05:17&lt;01:43,  1.40s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 227/300 [05:18&lt;01:43,  1.42s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 228/300 [05:19&lt;01:41,  1.41s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 229/300 [05:21&lt;01:39,  1.40s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 230/300 [05:22&lt;01:39,  1.42s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 231/300 [05:24&lt;01:37,  1.41s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 232/300 [05:25&lt;01:35,  1.41s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 233/300 [05:26&lt;01:34,  1.41s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 234/300 [05:28&lt;01:33,  1.41s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 235/300 [05:29&lt;01:31,  1.40s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 236/300 [05:31&lt;01:30,  1.42s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 237/300 [05:32&lt;01:28,  1.41s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 238/300 [05:33&lt;01:26,  1.40s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 239/300 [05:35&lt;01:25,  1.41s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 240/300 [05:36&lt;01:24,  1.40s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 241/300 [05:38&lt;01:23,  1.42s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 242/300 [05:39&lt;01:22,  1.43s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 243/300 [05:41&lt;01:20,  1.42s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 244/300 [05:42&lt;01:18,  1.41s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 245/300 [05:43&lt;01:17,  1.41s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 246/300 [05:45&lt;01:15,  1.40s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 247/300 [05:46&lt;01:13,  1.40s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 248/300 [05:48&lt;01:12,  1.40s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 249/300 [05:49&lt;01:11,  1.40s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 250/300 [05:50&lt;01:10,  1.40s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 251/300 [05:52&lt;01:08,  1.40s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 252/300 [05:53&lt;01:07,  1.41s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 253/300 [05:55&lt;01:06,  1.40s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 254/300 [05:56&lt;01:04,  1.40s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 255/300 [05:57&lt;01:02,  1.40s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 256/300 [05:59&lt;01:01,  1.40s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 257/300 [06:00&lt;01:00,  1.40s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 258/300 [06:02&lt;00:58,  1.40s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 259/300 [06:03&lt;00:58,  1.43s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 260/300 [06:05&lt;00:57,  1.43s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 261/300 [06:06&lt;00:55,  1.42s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 262/300 [06:07&lt;00:53,  1.42s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 263/300 [06:09&lt;00:52,  1.41s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 264/300 [06:10&lt;00:50,  1.42s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 265/300 [06:12&lt;00:49,  1.41s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 266/300 [06:13&lt;00:47,  1.40s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 267/300 [06:14&lt;00:46,  1.40s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 268/300 [06:16&lt;00:44,  1.40s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 269/300 [06:17&lt;00:43,  1.39s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 270/300 [06:18&lt;00:41,  1.40s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 271/300 [06:20&lt;00:41,  1.42s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 272/300 [06:21&lt;00:39,  1.42s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 273/300 [06:23&lt;00:38,  1.41s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 274/300 [06:24&lt;00:36,  1.41s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 275/300 [06:26&lt;00:35,  1.41s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 276/300 [06:27&lt;00:33,  1.40s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 277/300 [06:28&lt;00:32,  1.40s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 278/300 [06:30&lt;00:30,  1.40s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 279/300 [06:31&lt;00:29,  1.40s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 280/300 [06:33&lt;00:28,  1.42s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 281/300 [06:34&lt;00:26,  1.41s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 282/300 [06:35&lt;00:25,  1.41s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 283/300 [06:37&lt;00:23,  1.40s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 284/300 [06:38&lt;00:22,  1.40s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 285/300 [06:40&lt;00:20,  1.40s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 286/300 [06:41&lt;00:19,  1.40s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 287/300 [06:42&lt;00:18,  1.39s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 288/300 [06:44&lt;00:16,  1.39s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 289/300 [06:45&lt;00:15,  1.41s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 290/300 [06:47&lt;00:14,  1.41s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 291/300 [06:48&lt;00:12,  1.41s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 292/300 [06:49&lt;00:11,  1.40s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 293/300 [06:51&lt;00:09,  1.40s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 294/300 [06:52&lt;00:08,  1.40s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 295/300 [06:54&lt;00:06,  1.40s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 296/300 [06:55&lt;00:05,  1.39s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 297/300 [06:56&lt;00:04,  1.39s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 298/300 [06:58&lt;00:02,  1.39s/it]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 299/300 [06:59&lt;00:01,  1.40s/it]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [07:01&lt;00:00,  1.40s/it]\n\n15it [1:45:35, 422.05s/it]\n\n  0%|          | 0/300 [00:00&lt;?, ?it/s]\n\n  0%|          | 1/300 [00:00&lt;04:55,  1.01it/s]\n\n  1%|          | 2/300 [00:02&lt;06:05,  1.23s/it]\n\n  1%|          | 3/300 [00:03&lt;06:26,  1.30s/it]\n\n  1%|‚ñè         | 4/300 [00:05&lt;06:34,  1.33s/it]\n\n  2%|‚ñè         | 5/300 [00:06&lt;06:38,  1.35s/it]\n\n  2%|‚ñè         | 6/300 [00:07&lt;06:40,  1.36s/it]\n\n  2%|‚ñè         | 7/300 [00:09&lt;06:42,  1.37s/it]\n\n  3%|‚ñé         | 8/300 [00:10&lt;06:41,  1.38s/it]\n\n  3%|‚ñé         | 9/300 [00:12&lt;06:46,  1.40s/it]\n\n  3%|‚ñé         | 10/300 [00:13&lt;06:46,  1.40s/it]\n\n  4%|‚ñé         | 11/300 [00:14&lt;06:43,  1.40s/it]\n\n  4%|‚ñç         | 12/300 [00:16&lt;06:41,  1.39s/it]\n\n  4%|‚ñç         | 13/300 [00:17&lt;06:40,  1.39s/it]\n\n  5%|‚ñç         | 14/300 [00:19&lt;06:40,  1.40s/it]\n\n  5%|‚ñå         | 15/300 [00:20&lt;06:38,  1.40s/it]\n\n  5%|‚ñå         | 16/300 [00:21&lt;06:35,  1.39s/it]\n\n  6%|‚ñå         | 17/300 [00:23&lt;06:34,  1.39s/it]\n\n  6%|‚ñå         | 18/300 [00:24&lt;06:33,  1.40s/it]\n\n  6%|‚ñã         | 19/300 [00:26&lt;06:34,  1.40s/it]\n\n  7%|‚ñã         | 20/300 [00:27&lt;06:33,  1.40s/it]\n\n  7%|‚ñã         | 21/300 [00:28&lt;06:30,  1.40s/it]\n\n  7%|‚ñã         | 22/300 [00:30&lt;06:27,  1.39s/it]\n\n  8%|‚ñä         | 23/300 [00:31&lt;06:25,  1.39s/it]\n\n  8%|‚ñä         | 24/300 [00:33&lt;06:24,  1.39s/it]\n\n  8%|‚ñä         | 25/300 [00:34&lt;06:22,  1.39s/it]\n\n  9%|‚ñä         | 26/300 [00:35&lt;06:21,  1.39s/it]\n\n  9%|‚ñâ         | 27/300 [00:37&lt;06:20,  1.39s/it]\n\n  9%|‚ñâ         | 28/300 [00:38&lt;06:19,  1.40s/it]\n\n 10%|‚ñâ         | 29/300 [00:40&lt;06:17,  1.39s/it]\n\n 10%|‚ñà         | 30/300 [00:41&lt;06:17,  1.40s/it]\n\n 10%|‚ñà         | 31/300 [00:42&lt;06:16,  1.40s/it]\n\n 11%|‚ñà         | 32/300 [00:44&lt;06:15,  1.40s/it]\n\n 11%|‚ñà         | 33/300 [00:45&lt;06:13,  1.40s/it]\n\n 11%|‚ñà‚ñè        | 34/300 [00:47&lt;06:13,  1.40s/it]\n\n 12%|‚ñà‚ñè        | 35/300 [00:48&lt;06:11,  1.40s/it]\n\n 12%|‚ñà‚ñè        | 36/300 [00:49&lt;06:09,  1.40s/it]\n\n 12%|‚ñà‚ñè        | 37/300 [00:51&lt;06:08,  1.40s/it]\n\n 13%|‚ñà‚ñé        | 38/300 [00:52&lt;06:07,  1.40s/it]\n\n 13%|‚ñà‚ñé        | 39/300 [00:54&lt;06:05,  1.40s/it]\n\n 13%|‚ñà‚ñé        | 40/300 [00:55&lt;06:04,  1.40s/it]\n\n 14%|‚ñà‚ñé        | 41/300 [00:56&lt;06:03,  1.40s/it]\n\n 14%|‚ñà‚ñç        | 42/300 [00:58&lt;06:01,  1.40s/it]\n\n 14%|‚ñà‚ñç        | 43/300 [00:59&lt;05:59,  1.40s/it]\n\n 15%|‚ñà‚ñç        | 44/300 [01:01&lt;05:58,  1.40s/it]\n\n 15%|‚ñà‚ñå        | 45/300 [01:02&lt;05:55,  1.40s/it]\n\n 15%|‚ñà‚ñå        | 46/300 [01:03&lt;05:54,  1.39s/it]\n\n 16%|‚ñà‚ñå        | 47/300 [01:05&lt;05:54,  1.40s/it]\n\n 16%|‚ñà‚ñå        | 48/300 [01:06&lt;05:53,  1.40s/it]\n\n 16%|‚ñà‚ñã        | 49/300 [01:08&lt;05:52,  1.40s/it]\n\n 17%|‚ñà‚ñã        | 50/300 [01:09&lt;05:50,  1.40s/it]\n\n 17%|‚ñà‚ñã        | 51/300 [01:10&lt;05:48,  1.40s/it]\n\n 17%|‚ñà‚ñã        | 52/300 [01:12&lt;05:46,  1.40s/it]\n\n 18%|‚ñà‚ñä        | 53/300 [01:13&lt;05:43,  1.39s/it]\n\n 18%|‚ñà‚ñä        | 54/300 [01:15&lt;05:42,  1.39s/it]\n\n 18%|‚ñà‚ñä        | 55/300 [01:16&lt;05:39,  1.39s/it]\n\n 19%|‚ñà‚ñä        | 56/300 [01:17&lt;05:38,  1.39s/it]\n\n 19%|‚ñà‚ñâ        | 57/300 [01:19&lt;05:37,  1.39s/it]\n\n 19%|‚ñà‚ñâ        | 58/300 [01:20&lt;05:42,  1.41s/it]\n\n 20%|‚ñà‚ñâ        | 59/300 [01:22&lt;05:39,  1.41s/it]\n\n 20%|‚ñà‚ñà        | 60/300 [01:23&lt;05:36,  1.40s/it]\n\n 20%|‚ñà‚ñà        | 61/300 [01:24&lt;05:34,  1.40s/it]\n\n 21%|‚ñà‚ñà        | 62/300 [01:26&lt;05:32,  1.40s/it]\n\n 21%|‚ñà‚ñà        | 63/300 [01:27&lt;05:29,  1.39s/it]\n\n 21%|‚ñà‚ñà‚ñè       | 64/300 [01:29&lt;05:32,  1.41s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 65/300 [01:30&lt;05:29,  1.40s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 66/300 [01:31&lt;05:27,  1.40s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 67/300 [01:33&lt;05:33,  1.43s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 68/300 [01:34&lt;05:34,  1.44s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 69/300 [01:36&lt;05:31,  1.44s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 70/300 [01:37&lt;05:26,  1.42s/it]\n\n 24%|‚ñà‚ñà‚ñé       | 71/300 [01:39&lt;05:23,  1.41s/it]\n\n 24%|‚ñà‚ñà‚ñç       | 72/300 [01:40&lt;05:21,  1.41s/it]\n\n 24%|‚ñà‚ñà‚ñç       | 73/300 [01:41&lt;05:19,  1.41s/it]\n\n 25%|‚ñà‚ñà‚ñç       | 74/300 [01:43&lt;05:16,  1.40s/it]\n\n 25%|‚ñà‚ñà‚ñå       | 75/300 [01:44&lt;05:13,  1.40s/it]\n\n 25%|‚ñà‚ñà‚ñå       | 76/300 [01:45&lt;05:11,  1.39s/it]\n\n 26%|‚ñà‚ñà‚ñå       | 77/300 [01:47&lt;05:12,  1.40s/it]\n\n 26%|‚ñà‚ñà‚ñå       | 78/300 [01:48&lt;05:10,  1.40s/it]\n\n 26%|‚ñà‚ñà‚ñã       | 79/300 [01:50&lt;05:09,  1.40s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 80/300 [01:51&lt;05:07,  1.40s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 81/300 [01:52&lt;05:05,  1.39s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 82/300 [01:54&lt;05:05,  1.40s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 83/300 [01:55&lt;05:02,  1.40s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 84/300 [01:57&lt;05:01,  1.40s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 85/300 [01:58&lt;04:59,  1.40s/it]\n\n 29%|‚ñà‚ñà‚ñä       | 86/300 [01:59&lt;04:57,  1.39s/it]\n\n 29%|‚ñà‚ñà‚ñâ       | 87/300 [02:01&lt;04:56,  1.39s/it]\n\n 29%|‚ñà‚ñà‚ñâ       | 88/300 [02:02&lt;04:55,  1.39s/it]\n\n 30%|‚ñà‚ñà‚ñâ       | 89/300 [02:04&lt;04:53,  1.39s/it]\n\n 30%|‚ñà‚ñà‚ñà       | 90/300 [02:05&lt;04:52,  1.39s/it]\n\n 30%|‚ñà‚ñà‚ñà       | 91/300 [02:06&lt;04:50,  1.39s/it]\n\n 31%|‚ñà‚ñà‚ñà       | 92/300 [02:08&lt;04:50,  1.40s/it]\n\n 31%|‚ñà‚ñà‚ñà       | 93/300 [02:09&lt;04:48,  1.39s/it]\n\n 31%|‚ñà‚ñà‚ñà‚ñè      | 94/300 [02:11&lt;04:46,  1.39s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 95/300 [02:12&lt;04:45,  1.39s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 96/300 [02:13&lt;04:43,  1.39s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 97/300 [02:15&lt;04:43,  1.39s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 98/300 [02:16&lt;04:43,  1.40s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 99/300 [02:18&lt;04:43,  1.41s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 100/300 [02:19&lt;04:40,  1.40s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñé      | 101/300 [02:20&lt;04:38,  1.40s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñç      | 102/300 [02:22&lt;04:37,  1.40s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñç      | 103/300 [02:23&lt;04:35,  1.40s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñç      | 104/300 [02:25&lt;04:34,  1.40s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñå      | 105/300 [02:26&lt;04:31,  1.39s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñå      | 106/300 [02:27&lt;04:29,  1.39s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñå      | 107/300 [02:29&lt;04:29,  1.39s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñå      | 108/300 [02:30&lt;04:28,  1.40s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñã      | 109/300 [02:32&lt;04:27,  1.40s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 110/300 [02:33&lt;04:26,  1.40s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 111/300 [02:34&lt;04:24,  1.40s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 112/300 [02:36&lt;04:23,  1.40s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 113/300 [02:37&lt;04:23,  1.41s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 114/300 [02:39&lt;04:20,  1.40s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 115/300 [02:40&lt;04:18,  1.40s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñä      | 116/300 [02:41&lt;04:16,  1.40s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñâ      | 117/300 [02:43&lt;04:15,  1.40s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñâ      | 118/300 [02:44&lt;04:16,  1.41s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñâ      | 119/300 [02:46&lt;04:15,  1.41s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñà      | 120/300 [02:47&lt;04:13,  1.41s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñà      | 121/300 [02:48&lt;04:10,  1.40s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà      | 122/300 [02:50&lt;04:08,  1.40s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà      | 123/300 [02:51&lt;04:06,  1.39s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 124/300 [02:53&lt;04:06,  1.40s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 125/300 [02:54&lt;04:03,  1.39s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 126/300 [02:55&lt;04:02,  1.39s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 127/300 [02:57&lt;04:02,  1.40s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 128/300 [02:58&lt;04:02,  1.41s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 129/300 [03:00&lt;04:00,  1.40s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 130/300 [03:01&lt;03:59,  1.41s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 131/300 [03:02&lt;03:56,  1.40s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 132/300 [03:04&lt;03:54,  1.40s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 133/300 [03:05&lt;03:52,  1.39s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 134/300 [03:07&lt;03:50,  1.39s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 135/300 [03:08&lt;03:49,  1.39s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 136/300 [03:09&lt;03:49,  1.40s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 137/300 [03:11&lt;03:48,  1.40s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 138/300 [03:12&lt;03:46,  1.40s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 139/300 [03:14&lt;03:46,  1.41s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 140/300 [03:15&lt;03:44,  1.41s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 141/300 [03:16&lt;03:42,  1.40s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 142/300 [03:18&lt;03:41,  1.40s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 143/300 [03:19&lt;03:39,  1.40s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 144/300 [03:21&lt;03:37,  1.40s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 145/300 [03:22&lt;03:35,  1.39s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 146/300 [03:23&lt;03:34,  1.39s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 147/300 [03:25&lt;03:33,  1.39s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 148/300 [03:26&lt;03:33,  1.40s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 149/300 [03:28&lt;03:32,  1.41s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 150/300 [03:29&lt;03:30,  1.40s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 151/300 [03:30&lt;03:27,  1.39s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 152/300 [03:32&lt;03:27,  1.40s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 153/300 [03:33&lt;03:25,  1.40s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 154/300 [03:35&lt;03:23,  1.39s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 155/300 [03:36&lt;03:21,  1.39s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 156/300 [03:37&lt;03:20,  1.39s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 157/300 [03:39&lt;03:21,  1.41s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 158/300 [03:40&lt;03:21,  1.42s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 159/300 [03:42&lt;03:19,  1.41s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 160/300 [03:43&lt;03:16,  1.41s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 161/300 [03:44&lt;03:14,  1.40s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 162/300 [03:46&lt;03:12,  1.40s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 163/300 [03:47&lt;03:10,  1.39s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 164/300 [03:49&lt;03:09,  1.39s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 165/300 [03:50&lt;03:07,  1.39s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 166/300 [03:51&lt;03:06,  1.39s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 167/300 [03:53&lt;03:10,  1.43s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 168/300 [03:54&lt;03:08,  1.43s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 169/300 [03:56&lt;03:06,  1.42s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 170/300 [03:57&lt;03:03,  1.41s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 171/300 [03:58&lt;03:01,  1.40s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 172/300 [04:00&lt;02:59,  1.41s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 173/300 [04:01&lt;02:57,  1.40s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 174/300 [04:03&lt;02:55,  1.40s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 175/300 [04:04&lt;02:54,  1.39s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 176/300 [04:05&lt;02:53,  1.40s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 177/300 [04:07&lt;02:51,  1.39s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 178/300 [04:08&lt;02:51,  1.41s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 179/300 [04:10&lt;02:50,  1.41s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 180/300 [04:11&lt;02:48,  1.40s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 181/300 [04:12&lt;02:46,  1.40s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 182/300 [04:14&lt;02:45,  1.40s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 183/300 [04:15&lt;02:43,  1.39s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 184/300 [04:17&lt;02:41,  1.39s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 185/300 [04:18&lt;02:39,  1.39s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 186/300 [04:19&lt;02:38,  1.39s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 187/300 [04:21&lt;02:38,  1.40s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 188/300 [04:22&lt;02:37,  1.40s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 189/300 [04:24&lt;02:35,  1.40s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 190/300 [04:25&lt;02:33,  1.40s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 191/300 [04:26&lt;02:32,  1.40s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 192/300 [04:28&lt;02:30,  1.40s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 193/300 [04:29&lt;02:29,  1.40s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 194/300 [04:31&lt;02:28,  1.40s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 195/300 [04:32&lt;02:26,  1.40s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 196/300 [04:33&lt;02:25,  1.40s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 197/300 [04:35&lt;02:23,  1.40s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 198/300 [04:36&lt;02:22,  1.40s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 199/300 [04:38&lt;02:21,  1.40s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 200/300 [04:39&lt;02:19,  1.39s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 201/300 [04:40&lt;02:17,  1.39s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 202/300 [04:42&lt;02:16,  1.39s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 203/300 [04:43&lt;02:14,  1.39s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 204/300 [04:45&lt;02:13,  1.39s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 205/300 [04:46&lt;02:12,  1.39s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 206/300 [04:47&lt;02:10,  1.39s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 207/300 [04:49&lt;02:09,  1.39s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 208/300 [04:50&lt;02:07,  1.39s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 209/300 [04:51&lt;02:06,  1.39s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 210/300 [04:53&lt;02:05,  1.39s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 211/300 [04:54&lt;02:04,  1.39s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 212/300 [04:56&lt;02:02,  1.39s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 213/300 [04:57&lt;02:01,  1.39s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 214/300 [04:58&lt;01:59,  1.39s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 215/300 [05:00&lt;01:58,  1.39s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 216/300 [05:01&lt;01:56,  1.39s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 217/300 [05:03&lt;01:55,  1.39s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 218/300 [05:04&lt;01:55,  1.41s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 219/300 [05:05&lt;01:53,  1.40s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 220/300 [05:07&lt;01:51,  1.40s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 221/300 [05:08&lt;01:50,  1.40s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 222/300 [05:10&lt;01:48,  1.40s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 223/300 [05:11&lt;01:47,  1.39s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 224/300 [05:12&lt;01:46,  1.40s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 225/300 [05:14&lt;01:44,  1.40s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 226/300 [05:15&lt;01:43,  1.40s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 227/300 [05:17&lt;01:42,  1.40s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 228/300 [05:18&lt;01:40,  1.40s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 229/300 [05:19&lt;01:39,  1.40s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 230/300 [05:21&lt;01:37,  1.39s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 231/300 [05:22&lt;01:36,  1.39s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 232/300 [05:24&lt;01:34,  1.40s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 233/300 [05:25&lt;01:33,  1.39s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 234/300 [05:26&lt;01:31,  1.39s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 235/300 [05:28&lt;01:30,  1.40s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 236/300 [05:29&lt;01:29,  1.40s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 237/300 [05:31&lt;01:27,  1.40s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 238/300 [05:32&lt;01:26,  1.40s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 239/300 [05:33&lt;01:24,  1.39s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 240/300 [05:35&lt;01:23,  1.39s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 241/300 [05:36&lt;01:22,  1.39s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 242/300 [05:38&lt;01:20,  1.40s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 243/300 [05:39&lt;01:19,  1.39s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 244/300 [05:40&lt;01:17,  1.39s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 245/300 [05:42&lt;01:16,  1.39s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 246/300 [05:43&lt;01:15,  1.40s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 247/300 [05:45&lt;01:14,  1.40s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 248/300 [05:46&lt;01:12,  1.40s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 249/300 [05:47&lt;01:11,  1.39s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 250/300 [05:49&lt;01:09,  1.40s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 251/300 [05:50&lt;01:08,  1.40s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 252/300 [05:52&lt;01:07,  1.40s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 253/300 [05:53&lt;01:05,  1.40s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 254/300 [05:54&lt;01:04,  1.40s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 255/300 [05:56&lt;01:03,  1.40s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 256/300 [05:57&lt;01:03,  1.44s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 257/300 [05:59&lt;01:01,  1.43s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 258/300 [06:00&lt;00:59,  1.42s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 259/300 [06:01&lt;00:58,  1.41s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 260/300 [06:03&lt;00:56,  1.42s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 261/300 [06:04&lt;00:55,  1.41s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 262/300 [06:06&lt;00:53,  1.41s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 263/300 [06:07&lt;00:51,  1.40s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 264/300 [06:08&lt;00:50,  1.41s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 265/300 [06:10&lt;00:49,  1.41s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 266/300 [06:11&lt;00:47,  1.41s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 267/300 [06:13&lt;00:46,  1.41s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 268/300 [06:14&lt;00:45,  1.41s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 269/300 [06:16&lt;00:43,  1.40s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 270/300 [06:17&lt;00:42,  1.40s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 271/300 [06:18&lt;00:40,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 272/300 [06:20&lt;00:39,  1.41s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 273/300 [06:21&lt;00:37,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 274/300 [06:23&lt;00:36,  1.40s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 275/300 [06:24&lt;00:35,  1.40s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 276/300 [06:25&lt;00:33,  1.41s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 277/300 [06:27&lt;00:32,  1.41s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 278/300 [06:28&lt;00:30,  1.41s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 279/300 [06:30&lt;00:29,  1.40s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 280/300 [06:31&lt;00:28,  1.40s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 281/300 [06:32&lt;00:26,  1.40s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 282/300 [06:34&lt;00:25,  1.40s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 283/300 [06:35&lt;00:23,  1.39s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 284/300 [06:37&lt;00:22,  1.40s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 285/300 [06:38&lt;00:20,  1.40s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 286/300 [06:39&lt;00:19,  1.40s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 287/300 [06:41&lt;00:18,  1.41s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 288/300 [06:42&lt;00:16,  1.41s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 289/300 [06:44&lt;00:15,  1.40s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 290/300 [06:45&lt;00:14,  1.41s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 291/300 [06:46&lt;00:12,  1.40s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 292/300 [06:48&lt;00:11,  1.40s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 293/300 [06:49&lt;00:09,  1.40s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 294/300 [06:51&lt;00:08,  1.40s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 295/300 [06:52&lt;00:07,  1.41s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 296/300 [06:53&lt;00:05,  1.42s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 297/300 [06:55&lt;00:04,  1.42s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 298/300 [06:56&lt;00:02,  1.41s/it]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 299/300 [06:58&lt;00:01,  1.41s/it]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [06:59&lt;00:00,  1.40s/it]\n\n16it [1:52:36, 421.56s/it]\n\n  0%|          | 0/300 [00:00&lt;?, ?it/s]\n\n  0%|          | 1/300 [00:00&lt;04:54,  1.01it/s]\n\n  1%|          | 2/300 [00:02&lt;06:11,  1.25s/it]\n\n  1%|          | 3/300 [00:03&lt;06:30,  1.31s/it]\n\n  1%|‚ñè         | 4/300 [00:05&lt;06:44,  1.37s/it]\n\n  2%|‚ñè         | 5/300 [00:06&lt;06:47,  1.38s/it]\n\n  2%|‚ñè         | 6/300 [00:08&lt;06:48,  1.39s/it]\n\n  2%|‚ñè         | 7/300 [00:09&lt;06:50,  1.40s/it]\n\n  3%|‚ñé         | 8/300 [00:10&lt;06:48,  1.40s/it]\n\n  3%|‚ñé         | 9/300 [00:12&lt;06:48,  1.40s/it]\n\n  3%|‚ñé         | 10/300 [00:13&lt;06:46,  1.40s/it]\n\n  4%|‚ñé         | 11/300 [00:15&lt;06:45,  1.40s/it]\n\n  4%|‚ñç         | 12/300 [00:16&lt;06:43,  1.40s/it]\n\n  4%|‚ñç         | 13/300 [00:17&lt;06:43,  1.40s/it]\n\n  5%|‚ñç         | 14/300 [00:19&lt;06:41,  1.40s/it]\n\n  5%|‚ñå         | 15/300 [00:20&lt;06:42,  1.41s/it]\n\n  5%|‚ñå         | 16/300 [00:22&lt;06:45,  1.43s/it]\n\n  6%|‚ñå         | 17/300 [00:23&lt;06:41,  1.42s/it]\n\n  6%|‚ñå         | 18/300 [00:25&lt;06:38,  1.41s/it]\n\n  6%|‚ñã         | 19/300 [00:26&lt;06:34,  1.41s/it]\n\n  7%|‚ñã         | 20/300 [00:27&lt;06:33,  1.41s/it]\n\n  7%|‚ñã         | 21/300 [00:29&lt;06:30,  1.40s/it]\n\n  7%|‚ñã         | 22/300 [00:30&lt;06:28,  1.40s/it]\n\n  8%|‚ñä         | 23/300 [00:31&lt;06:25,  1.39s/it]\n\n  8%|‚ñä         | 24/300 [00:33&lt;06:25,  1.40s/it]\n\n  8%|‚ñä         | 25/300 [00:34&lt;06:24,  1.40s/it]\n\n  9%|‚ñä         | 26/300 [00:36&lt;06:23,  1.40s/it]\n\n  9%|‚ñâ         | 27/300 [00:37&lt;06:22,  1.40s/it]\n\n  9%|‚ñâ         | 28/300 [00:38&lt;06:20,  1.40s/it]\n\n 10%|‚ñâ         | 29/300 [00:40&lt;06:18,  1.39s/it]\n\n 10%|‚ñà         | 30/300 [00:41&lt;06:16,  1.39s/it]\n\n 10%|‚ñà         | 31/300 [00:43&lt;06:15,  1.39s/it]\n\n 11%|‚ñà         | 32/300 [00:44&lt;06:13,  1.39s/it]\n\n 11%|‚ñà         | 33/300 [00:45&lt;06:12,  1.40s/it]\n\n 11%|‚ñà‚ñè        | 34/300 [00:47&lt;06:11,  1.40s/it]\n\n 12%|‚ñà‚ñè        | 35/300 [00:48&lt;06:11,  1.40s/it]\n\n 12%|‚ñà‚ñè        | 36/300 [00:50&lt;06:13,  1.41s/it]\n\n 12%|‚ñà‚ñè        | 37/300 [00:51&lt;06:11,  1.41s/it]\n\n 13%|‚ñà‚ñé        | 38/300 [00:53&lt;06:09,  1.41s/it]\n\n 13%|‚ñà‚ñé        | 39/300 [00:54&lt;06:06,  1.40s/it]\n\n 13%|‚ñà‚ñé        | 40/300 [00:55&lt;06:04,  1.40s/it]\n\n 14%|‚ñà‚ñé        | 41/300 [00:57&lt;06:02,  1.40s/it]\n\n 14%|‚ñà‚ñç        | 42/300 [00:58&lt;06:00,  1.40s/it]\n\n 14%|‚ñà‚ñç        | 43/300 [00:59&lt;06:00,  1.40s/it]\n\n 15%|‚ñà‚ñç        | 44/300 [01:01&lt;05:58,  1.40s/it]\n\n 15%|‚ñà‚ñå        | 45/300 [01:02&lt;06:00,  1.42s/it]\n\n 15%|‚ñà‚ñå        | 46/300 [01:04&lt;05:58,  1.41s/it]\n\n 16%|‚ñà‚ñå        | 47/300 [01:05&lt;05:59,  1.42s/it]\n\n 16%|‚ñà‚ñå        | 48/300 [01:07&lt;05:55,  1.41s/it]\n\n 16%|‚ñà‚ñã        | 49/300 [01:08&lt;05:52,  1.40s/it]\n\n 17%|‚ñà‚ñã        | 50/300 [01:09&lt;05:50,  1.40s/it]\n\n 17%|‚ñà‚ñã        | 51/300 [01:11&lt;05:51,  1.41s/it]\n\n 17%|‚ñà‚ñã        | 52/300 [01:12&lt;05:48,  1.41s/it]\n\n 18%|‚ñà‚ñä        | 53/300 [01:14&lt;05:46,  1.40s/it]\n\n 18%|‚ñà‚ñä        | 54/300 [01:15&lt;05:44,  1.40s/it]\n\n 18%|‚ñà‚ñä        | 55/300 [01:16&lt;05:42,  1.40s/it]\n\n 19%|‚ñà‚ñä        | 56/300 [01:18&lt;05:42,  1.40s/it]\n\n 19%|‚ñà‚ñâ        | 57/300 [01:19&lt;05:41,  1.40s/it]\n\n 19%|‚ñà‚ñâ        | 58/300 [01:21&lt;05:38,  1.40s/it]\n\n 20%|‚ñà‚ñâ        | 59/300 [01:22&lt;05:38,  1.41s/it]\n\n 20%|‚ñà‚ñà        | 60/300 [01:23&lt;05:36,  1.40s/it]\n\n 20%|‚ñà‚ñà        | 61/300 [01:25&lt;05:34,  1.40s/it]\n\n 21%|‚ñà‚ñà        | 62/300 [01:26&lt;05:32,  1.40s/it]\n\n 21%|‚ñà‚ñà        | 63/300 [01:28&lt;05:40,  1.44s/it]\n\n 21%|‚ñà‚ñà‚ñè       | 64/300 [01:29&lt;05:36,  1.43s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 65/300 [01:31&lt;05:33,  1.42s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 66/300 [01:32&lt;05:34,  1.43s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 67/300 [01:33&lt;05:30,  1.42s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 68/300 [01:35&lt;05:28,  1.41s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 69/300 [01:36&lt;05:25,  1.41s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 70/300 [01:38&lt;05:22,  1.40s/it]\n\n 24%|‚ñà‚ñà‚ñé       | 71/300 [01:39&lt;05:21,  1.40s/it]\n\n 24%|‚ñà‚ñà‚ñç       | 72/300 [01:40&lt;05:20,  1.40s/it]\n\n 24%|‚ñà‚ñà‚ñç       | 73/300 [01:42&lt;05:17,  1.40s/it]\n\n 25%|‚ñà‚ñà‚ñç       | 74/300 [01:43&lt;05:17,  1.40s/it]\n\n 25%|‚ñà‚ñà‚ñå       | 75/300 [01:45&lt;05:15,  1.40s/it]\n\n 25%|‚ñà‚ñà‚ñå       | 76/300 [01:46&lt;05:14,  1.40s/it]\n\n 26%|‚ñà‚ñà‚ñå       | 77/300 [01:47&lt;05:12,  1.40s/it]\n\n 26%|‚ñà‚ñà‚ñå       | 78/300 [01:49&lt;05:11,  1.40s/it]\n\n 26%|‚ñà‚ñà‚ñã       | 79/300 [01:50&lt;05:10,  1.41s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 80/300 [01:52&lt;05:08,  1.40s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 81/300 [01:53&lt;05:05,  1.40s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 82/300 [01:54&lt;05:04,  1.40s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 83/300 [01:56&lt;05:02,  1.39s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 84/300 [01:57&lt;05:04,  1.41s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 85/300 [01:59&lt;05:05,  1.42s/it]\n\n 29%|‚ñà‚ñà‚ñä       | 86/300 [02:00&lt;05:02,  1.41s/it]\n\n 29%|‚ñà‚ñà‚ñâ       | 87/300 [02:01&lt;04:59,  1.41s/it]\n\n 29%|‚ñà‚ñà‚ñâ       | 88/300 [02:03&lt;04:57,  1.40s/it]\n\n 30%|‚ñà‚ñà‚ñâ       | 89/300 [02:04&lt;04:56,  1.41s/it]\n\n 30%|‚ñà‚ñà‚ñà       | 90/300 [02:06&lt;04:54,  1.40s/it]\n\n 30%|‚ñà‚ñà‚ñà       | 91/300 [02:07&lt;04:54,  1.41s/it]\n\n 31%|‚ñà‚ñà‚ñà       | 92/300 [02:08&lt;04:52,  1.41s/it]\n\n 31%|‚ñà‚ñà‚ñà       | 93/300 [02:10&lt;04:50,  1.40s/it]\n\n 31%|‚ñà‚ñà‚ñà‚ñè      | 94/300 [02:11&lt;04:52,  1.42s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 95/300 [02:13&lt;04:50,  1.42s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 96/300 [02:14&lt;04:48,  1.41s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 97/300 [02:16&lt;04:46,  1.41s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 98/300 [02:17&lt;04:44,  1.41s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 99/300 [02:18&lt;04:41,  1.40s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 100/300 [02:20&lt;04:39,  1.40s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñé      | 101/300 [02:21&lt;04:38,  1.40s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñç      | 102/300 [02:22&lt;04:36,  1.40s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñç      | 103/300 [02:24&lt;04:36,  1.40s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñç      | 104/300 [02:25&lt;04:36,  1.41s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñå      | 105/300 [02:27&lt;04:34,  1.41s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñå      | 106/300 [02:28&lt;04:32,  1.40s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñå      | 107/300 [02:30&lt;04:30,  1.40s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñå      | 108/300 [02:31&lt;04:27,  1.40s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñã      | 109/300 [02:32&lt;04:26,  1.39s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 110/300 [02:34&lt;04:26,  1.40s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 111/300 [02:35&lt;04:24,  1.40s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 112/300 [02:37&lt;04:24,  1.41s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 113/300 [02:38&lt;04:22,  1.40s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 114/300 [02:39&lt;04:24,  1.42s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 115/300 [02:41&lt;04:22,  1.42s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñä      | 116/300 [02:42&lt;04:19,  1.41s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñâ      | 117/300 [02:44&lt;04:17,  1.41s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñâ      | 118/300 [02:45&lt;04:16,  1.41s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñâ      | 119/300 [02:46&lt;04:14,  1.41s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñà      | 120/300 [02:48&lt;04:12,  1.40s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñà      | 121/300 [02:49&lt;04:10,  1.40s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà      | 122/300 [02:51&lt;04:09,  1.40s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà      | 123/300 [02:52&lt;04:08,  1.40s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 124/300 [02:53&lt;04:09,  1.42s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 125/300 [02:55&lt;04:07,  1.41s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 126/300 [02:56&lt;04:04,  1.40s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 127/300 [02:58&lt;04:02,  1.40s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 128/300 [02:59&lt;04:01,  1.40s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 129/300 [03:00&lt;03:59,  1.40s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 130/300 [03:02&lt;03:57,  1.40s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 131/300 [03:03&lt;03:55,  1.39s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 132/300 [03:05&lt;03:54,  1.39s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 133/300 [03:06&lt;03:54,  1.41s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 134/300 [03:07&lt;03:54,  1.41s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 135/300 [03:09&lt;03:53,  1.41s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 136/300 [03:10&lt;03:50,  1.41s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 137/300 [03:12&lt;03:48,  1.40s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 138/300 [03:13&lt;03:46,  1.40s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 139/300 [03:14&lt;03:46,  1.41s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 140/300 [03:16&lt;03:45,  1.41s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 141/300 [03:17&lt;03:43,  1.40s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 142/300 [03:19&lt;03:41,  1.40s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 143/300 [03:20&lt;03:41,  1.41s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 144/300 [03:22&lt;03:39,  1.41s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 145/300 [03:23&lt;03:38,  1.41s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 146/300 [03:24&lt;03:36,  1.41s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 147/300 [03:26&lt;03:34,  1.40s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 148/300 [03:27&lt;03:33,  1.40s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 149/300 [03:29&lt;03:31,  1.40s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 150/300 [03:30&lt;03:31,  1.41s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 151/300 [03:31&lt;03:29,  1.41s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 152/300 [03:33&lt;03:34,  1.45s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 153/300 [03:34&lt;03:31,  1.44s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 154/300 [03:36&lt;03:29,  1.43s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 155/300 [03:37&lt;03:27,  1.43s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 156/300 [03:39&lt;03:24,  1.42s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 157/300 [03:40&lt;03:21,  1.41s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 158/300 [03:41&lt;03:19,  1.41s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 159/300 [03:43&lt;03:18,  1.41s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 160/300 [03:44&lt;03:16,  1.40s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 161/300 [03:46&lt;03:14,  1.40s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 162/300 [03:47&lt;03:13,  1.41s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 163/300 [03:48&lt;03:13,  1.41s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 164/300 [03:50&lt;03:12,  1.42s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 165/300 [03:51&lt;03:10,  1.41s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 166/300 [03:53&lt;03:07,  1.40s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 167/300 [03:54&lt;03:07,  1.41s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 168/300 [03:55&lt;03:06,  1.41s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 169/300 [03:57&lt;03:04,  1.41s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 170/300 [03:58&lt;03:02,  1.40s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 171/300 [04:00&lt;03:00,  1.40s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 172/300 [04:01&lt;02:59,  1.40s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 173/300 [04:02&lt;02:58,  1.40s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 174/300 [04:04&lt;02:57,  1.41s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 175/300 [04:05&lt;02:55,  1.41s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 176/300 [04:07&lt;02:54,  1.40s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 177/300 [04:08&lt;02:52,  1.40s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 178/300 [04:09&lt;02:50,  1.40s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 179/300 [04:11&lt;02:50,  1.41s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 180/300 [04:12&lt;02:48,  1.41s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 181/300 [04:14&lt;02:47,  1.41s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 182/300 [04:15&lt;02:46,  1.41s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 183/300 [04:17&lt;02:44,  1.41s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 184/300 [04:18&lt;02:43,  1.41s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 185/300 [04:19&lt;02:43,  1.42s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 186/300 [04:21&lt;02:40,  1.41s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 187/300 [04:22&lt;02:38,  1.40s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 188/300 [04:24&lt;02:37,  1.41s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 189/300 [04:25&lt;02:35,  1.40s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 190/300 [04:26&lt;02:34,  1.40s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 191/300 [04:28&lt;02:32,  1.40s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 192/300 [04:29&lt;02:31,  1.40s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 193/300 [04:31&lt;02:32,  1.42s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 194/300 [04:32&lt;02:30,  1.42s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 195/300 [04:33&lt;02:28,  1.41s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 196/300 [04:35&lt;02:26,  1.40s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 197/300 [04:36&lt;02:24,  1.40s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 198/300 [04:38&lt;02:22,  1.40s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 199/300 [04:39&lt;02:21,  1.40s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 200/300 [04:40&lt;02:21,  1.41s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 201/300 [04:42&lt;02:19,  1.41s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 202/300 [04:43&lt;02:17,  1.41s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 203/300 [04:45&lt;02:16,  1.41s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 204/300 [04:46&lt;02:15,  1.41s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 205/300 [04:47&lt;02:13,  1.40s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 206/300 [04:49&lt;02:12,  1.41s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 207/300 [04:50&lt;02:10,  1.40s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 208/300 [04:52&lt;02:08,  1.40s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 209/300 [04:53&lt;02:07,  1.40s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 210/300 [04:54&lt;02:06,  1.40s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 211/300 [04:56&lt;02:05,  1.41s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 212/300 [04:57&lt;02:04,  1.42s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 213/300 [04:59&lt;02:03,  1.42s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 214/300 [05:00&lt;02:01,  1.41s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 215/300 [05:02&lt;01:59,  1.41s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 216/300 [05:03&lt;01:57,  1.40s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 217/300 [05:04&lt;01:56,  1.40s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 218/300 [05:06&lt;01:54,  1.40s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 219/300 [05:07&lt;01:53,  1.40s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 220/300 [05:09&lt;01:51,  1.40s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 221/300 [05:10&lt;01:50,  1.40s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 222/300 [05:11&lt;01:49,  1.41s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 223/300 [05:13&lt;01:49,  1.42s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 224/300 [05:14&lt;01:47,  1.41s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 225/300 [05:16&lt;01:45,  1.41s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 226/300 [05:17&lt;01:43,  1.40s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 227/300 [05:18&lt;01:42,  1.40s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 228/300 [05:20&lt;01:40,  1.40s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 229/300 [05:21&lt;01:39,  1.40s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 230/300 [05:23&lt;01:37,  1.39s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 231/300 [05:24&lt;01:36,  1.40s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 232/300 [05:25&lt;01:35,  1.41s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 233/300 [05:27&lt;01:33,  1.40s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 234/300 [05:28&lt;01:32,  1.40s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 235/300 [05:30&lt;01:31,  1.40s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 236/300 [05:31&lt;01:29,  1.40s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 237/300 [05:32&lt;01:28,  1.40s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 238/300 [05:34&lt;01:26,  1.40s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 239/300 [05:35&lt;01:25,  1.39s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 240/300 [05:37&lt;01:23,  1.39s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 241/300 [05:38&lt;01:22,  1.40s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 242/300 [05:39&lt;01:21,  1.40s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 243/300 [05:41&lt;01:20,  1.42s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 244/300 [05:42&lt;01:19,  1.41s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 245/300 [05:44&lt;01:17,  1.41s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 246/300 [05:45&lt;01:16,  1.41s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 247/300 [05:46&lt;01:14,  1.41s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 248/300 [05:48&lt;01:13,  1.41s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 249/300 [05:49&lt;01:11,  1.41s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 250/300 [05:51&lt;01:10,  1.40s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 251/300 [05:52&lt;01:08,  1.40s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 252/300 [05:53&lt;01:07,  1.40s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 253/300 [05:55&lt;01:05,  1.40s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 254/300 [05:56&lt;01:04,  1.40s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 255/300 [05:58&lt;01:02,  1.40s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 256/300 [05:59&lt;01:01,  1.40s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 257/300 [06:00&lt;01:00,  1.40s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 258/300 [06:02&lt;00:58,  1.40s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 259/300 [06:03&lt;00:57,  1.40s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 260/300 [06:05&lt;00:56,  1.40s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 261/300 [06:06&lt;00:54,  1.40s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 262/300 [06:07&lt;00:53,  1.41s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 263/300 [06:09&lt;00:52,  1.41s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 264/300 [06:10&lt;00:50,  1.40s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 265/300 [06:12&lt;00:49,  1.40s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 266/300 [06:13&lt;00:47,  1.41s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 267/300 [06:14&lt;00:46,  1.41s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 268/300 [06:16&lt;00:44,  1.40s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 269/300 [06:17&lt;00:43,  1.40s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 270/300 [06:19&lt;00:41,  1.40s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 271/300 [06:20&lt;00:40,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 272/300 [06:22&lt;00:39,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 273/300 [06:23&lt;00:37,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 274/300 [06:24&lt;00:36,  1.40s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 275/300 [06:26&lt;00:34,  1.40s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 276/300 [06:27&lt;00:34,  1.43s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 277/300 [06:29&lt;00:32,  1.42s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 278/300 [06:30&lt;00:31,  1.41s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 279/300 [06:31&lt;00:29,  1.41s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 280/300 [06:33&lt;00:28,  1.41s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 281/300 [06:34&lt;00:26,  1.41s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 282/300 [06:36&lt;00:25,  1.41s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 283/300 [06:37&lt;00:23,  1.41s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 284/300 [06:38&lt;00:22,  1.41s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 285/300 [06:40&lt;00:21,  1.41s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 286/300 [06:41&lt;00:19,  1.40s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 287/300 [06:43&lt;00:18,  1.40s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 288/300 [06:44&lt;00:16,  1.40s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 289/300 [06:45&lt;00:15,  1.40s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 290/300 [06:47&lt;00:13,  1.40s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 291/300 [06:48&lt;00:12,  1.40s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 292/300 [06:50&lt;00:11,  1.41s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 293/300 [06:51&lt;00:09,  1.41s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 294/300 [06:52&lt;00:08,  1.41s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 295/300 [06:54&lt;00:07,  1.40s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 296/300 [06:55&lt;00:05,  1.40s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 297/300 [06:57&lt;00:04,  1.40s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 298/300 [06:58&lt;00:02,  1.40s/it]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 299/300 [06:59&lt;00:01,  1.40s/it]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [07:01&lt;00:00,  1.40s/it]\n\n17it [1:59:38, 421.75s/it]\n\n  0%|          | 0/300 [00:00&lt;?, ?it/s]\n\n  0%|          | 1/300 [00:01&lt;05:04,  1.02s/it]\n\n  1%|          | 2/300 [00:02&lt;06:11,  1.25s/it]\n\n  1%|          | 3/300 [00:03&lt;06:30,  1.31s/it]\n\n  1%|‚ñè         | 4/300 [00:05&lt;06:37,  1.34s/it]\n\n  2%|‚ñè         | 5/300 [00:06&lt;06:42,  1.36s/it]\n\n  2%|‚ñè         | 6/300 [00:08&lt;06:44,  1.37s/it]\n\n  2%|‚ñè         | 7/300 [00:09&lt;06:44,  1.38s/it]\n\n  3%|‚ñé         | 8/300 [00:10&lt;06:45,  1.39s/it]\n\n  3%|‚ñé         | 9/300 [00:12&lt;06:45,  1.39s/it]\n\n  3%|‚ñé         | 10/300 [00:13&lt;06:46,  1.40s/it]\n\n  4%|‚ñé         | 11/300 [00:15&lt;06:45,  1.40s/it]\n\n  4%|‚ñç         | 12/300 [00:16&lt;06:45,  1.41s/it]\n\n  4%|‚ñç         | 13/300 [00:17&lt;06:42,  1.40s/it]\n\n  5%|‚ñç         | 14/300 [00:19&lt;06:39,  1.40s/it]\n\n  5%|‚ñå         | 15/300 [00:20&lt;06:37,  1.40s/it]\n\n  5%|‚ñå         | 16/300 [00:22&lt;06:36,  1.40s/it]\n\n  6%|‚ñå         | 17/300 [00:23&lt;06:34,  1.39s/it]\n\n  6%|‚ñå         | 18/300 [00:24&lt;06:33,  1.40s/it]\n\n  6%|‚ñã         | 19/300 [00:26&lt;06:31,  1.39s/it]\n\n  7%|‚ñã         | 20/300 [00:27&lt;06:30,  1.40s/it]\n\n  7%|‚ñã         | 21/300 [00:29&lt;06:31,  1.40s/it]\n\n  7%|‚ñã         | 22/300 [00:30&lt;06:31,  1.41s/it]\n\n  8%|‚ñä         | 23/300 [00:31&lt;06:28,  1.40s/it]\n\n  8%|‚ñä         | 24/300 [00:33&lt;06:27,  1.40s/it]\n\n  8%|‚ñä         | 25/300 [00:34&lt;06:25,  1.40s/it]\n\n  9%|‚ñä         | 26/300 [00:36&lt;06:25,  1.41s/it]\n\n  9%|‚ñâ         | 27/300 [00:37&lt;06:23,  1.41s/it]\n\n  9%|‚ñâ         | 28/300 [00:38&lt;06:20,  1.40s/it]\n\n 10%|‚ñâ         | 29/300 [00:40&lt;06:18,  1.40s/it]\n\n 10%|‚ñà         | 30/300 [00:41&lt;06:20,  1.41s/it]\n\n 10%|‚ñà         | 31/300 [00:43&lt;06:18,  1.41s/it]\n\n 11%|‚ñà         | 32/300 [00:44&lt;06:20,  1.42s/it]\n\n 11%|‚ñà         | 33/300 [00:45&lt;06:16,  1.41s/it]\n\n 11%|‚ñà‚ñè        | 34/300 [00:47&lt;06:15,  1.41s/it]\n\n 12%|‚ñà‚ñè        | 35/300 [00:48&lt;06:13,  1.41s/it]\n\n 12%|‚ñà‚ñè        | 36/300 [00:50&lt;06:12,  1.41s/it]\n\n 12%|‚ñà‚ñè        | 37/300 [00:51&lt;06:09,  1.41s/it]\n\n 13%|‚ñà‚ñé        | 38/300 [00:52&lt;06:07,  1.40s/it]\n\n 13%|‚ñà‚ñé        | 39/300 [00:54&lt;06:05,  1.40s/it]\n\n 13%|‚ñà‚ñé        | 40/300 [00:55&lt;06:03,  1.40s/it]\n\n 14%|‚ñà‚ñé        | 41/300 [00:57&lt;06:04,  1.41s/it]\n\n 14%|‚ñà‚ñç        | 42/300 [00:58&lt;06:03,  1.41s/it]\n\n 14%|‚ñà‚ñç        | 43/300 [00:59&lt;06:04,  1.42s/it]\n\n 15%|‚ñà‚ñç        | 44/300 [01:01&lt;06:00,  1.41s/it]\n\n 15%|‚ñà‚ñå        | 45/300 [01:02&lt;05:59,  1.41s/it]\n\n 15%|‚ñà‚ñå        | 46/300 [01:04&lt;05:58,  1.41s/it]\n\n 16%|‚ñà‚ñå        | 47/300 [01:05&lt;05:55,  1.40s/it]\n\n 16%|‚ñà‚ñå        | 48/300 [01:06&lt;05:52,  1.40s/it]\n\n 16%|‚ñà‚ñã        | 49/300 [01:08&lt;05:51,  1.40s/it]\n\n 17%|‚ñà‚ñã        | 50/300 [01:09&lt;05:49,  1.40s/it]\n\n 17%|‚ñà‚ñã        | 51/300 [01:11&lt;05:48,  1.40s/it]\n\n 17%|‚ñà‚ñã        | 52/300 [01:12&lt;05:47,  1.40s/it]\n\n 18%|‚ñà‚ñä        | 53/300 [01:14&lt;05:48,  1.41s/it]\n\n 18%|‚ñà‚ñä        | 54/300 [01:15&lt;05:45,  1.40s/it]\n\n 18%|‚ñà‚ñä        | 55/300 [01:16&lt;05:43,  1.40s/it]\n\n 19%|‚ñà‚ñä        | 56/300 [01:18&lt;05:41,  1.40s/it]\n\n 19%|‚ñà‚ñâ        | 57/300 [01:19&lt;05:40,  1.40s/it]\n\n 19%|‚ñà‚ñâ        | 58/300 [01:21&lt;05:39,  1.40s/it]\n\n 20%|‚ñà‚ñâ        | 59/300 [01:22&lt;05:37,  1.40s/it]\n\n 20%|‚ñà‚ñà        | 60/300 [01:23&lt;05:38,  1.41s/it]\n\n 20%|‚ñà‚ñà        | 61/300 [01:25&lt;05:37,  1.41s/it]\n\n 21%|‚ñà‚ñà        | 62/300 [01:26&lt;05:35,  1.41s/it]\n\n 21%|‚ñà‚ñà        | 63/300 [01:28&lt;05:33,  1.41s/it]\n\n 21%|‚ñà‚ñà‚ñè       | 64/300 [01:29&lt;05:30,  1.40s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 65/300 [01:30&lt;05:28,  1.40s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 66/300 [01:32&lt;05:28,  1.41s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 67/300 [01:33&lt;05:26,  1.40s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 68/300 [01:35&lt;05:32,  1.43s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 69/300 [01:36&lt;05:28,  1.42s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 70/300 [01:37&lt;05:26,  1.42s/it]\n\n 24%|‚ñà‚ñà‚ñé       | 71/300 [01:39&lt;05:24,  1.42s/it]\n\n 24%|‚ñà‚ñà‚ñç       | 72/300 [01:40&lt;05:22,  1.41s/it]\n\n 24%|‚ñà‚ñà‚ñç       | 73/300 [01:42&lt;05:21,  1.42s/it]\n\n 25%|‚ñà‚ñà‚ñç       | 74/300 [01:43&lt;05:19,  1.41s/it]\n\n 25%|‚ñà‚ñà‚ñå       | 75/300 [01:45&lt;05:16,  1.41s/it]\n\n 25%|‚ñà‚ñà‚ñå       | 76/300 [01:46&lt;05:14,  1.40s/it]\n\n 26%|‚ñà‚ñà‚ñå       | 77/300 [01:47&lt;05:13,  1.40s/it]\n\n 26%|‚ñà‚ñà‚ñå       | 78/300 [01:49&lt;05:12,  1.41s/it]\n\n 26%|‚ñà‚ñà‚ñã       | 79/300 [01:50&lt;05:10,  1.40s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 80/300 [01:52&lt;05:11,  1.41s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 81/300 [01:53&lt;05:09,  1.41s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 82/300 [01:54&lt;05:07,  1.41s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 83/300 [01:56&lt;05:05,  1.41s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 84/300 [01:57&lt;05:03,  1.41s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 85/300 [01:59&lt;05:01,  1.40s/it]\n\n 29%|‚ñà‚ñà‚ñä       | 86/300 [02:00&lt;04:59,  1.40s/it]\n\n 29%|‚ñà‚ñà‚ñâ       | 87/300 [02:01&lt;04:57,  1.40s/it]\n\n 29%|‚ñà‚ñà‚ñâ       | 88/300 [02:03&lt;04:56,  1.40s/it]\n\n 30%|‚ñà‚ñà‚ñâ       | 89/300 [02:04&lt;04:55,  1.40s/it]\n\n 30%|‚ñà‚ñà‚ñà       | 90/300 [02:06&lt;04:55,  1.41s/it]\n\n 30%|‚ñà‚ñà‚ñà       | 91/300 [02:07&lt;04:53,  1.40s/it]\n\n 31%|‚ñà‚ñà‚ñà       | 92/300 [02:08&lt;04:53,  1.41s/it]\n\n 31%|‚ñà‚ñà‚ñà       | 93/300 [02:10&lt;04:51,  1.41s/it]\n\n 31%|‚ñà‚ñà‚ñà‚ñè      | 94/300 [02:11&lt;04:49,  1.41s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 95/300 [02:13&lt;04:47,  1.40s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 96/300 [02:14&lt;04:46,  1.40s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 97/300 [02:15&lt;04:44,  1.40s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 98/300 [02:17&lt;04:42,  1.40s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 99/300 [02:18&lt;04:40,  1.40s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 100/300 [02:20&lt;04:42,  1.41s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñé      | 101/300 [02:21&lt;04:40,  1.41s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñç      | 102/300 [02:22&lt;04:39,  1.41s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñç      | 103/300 [02:24&lt;04:37,  1.41s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñç      | 104/300 [02:25&lt;04:35,  1.40s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñå      | 105/300 [02:27&lt;04:33,  1.40s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñå      | 106/300 [02:28&lt;04:32,  1.40s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñå      | 107/300 [02:29&lt;04:30,  1.40s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñå      | 108/300 [02:31&lt;04:28,  1.40s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñã      | 109/300 [02:32&lt;04:27,  1.40s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 110/300 [02:34&lt;04:28,  1.41s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 111/300 [02:35&lt;04:27,  1.41s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 112/300 [02:37&lt;04:25,  1.41s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 113/300 [02:38&lt;04:23,  1.41s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 114/300 [02:39&lt;04:22,  1.41s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 115/300 [02:41&lt;04:20,  1.41s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñä      | 116/300 [02:42&lt;04:18,  1.41s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñâ      | 117/300 [02:44&lt;04:16,  1.40s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñâ      | 118/300 [02:45&lt;04:14,  1.40s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñâ      | 119/300 [02:46&lt;04:14,  1.41s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñà      | 120/300 [02:48&lt;04:13,  1.41s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñà      | 121/300 [02:49&lt;04:12,  1.41s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà      | 122/300 [02:51&lt;04:11,  1.41s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà      | 123/300 [02:52&lt;04:09,  1.41s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 124/300 [02:53&lt;04:07,  1.40s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 125/300 [02:55&lt;04:05,  1.40s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 126/300 [02:56&lt;04:04,  1.40s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 127/300 [02:58&lt;04:02,  1.40s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 128/300 [02:59&lt;04:00,  1.40s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 129/300 [03:00&lt;03:58,  1.40s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 130/300 [03:02&lt;03:58,  1.40s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 131/300 [03:03&lt;03:58,  1.41s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 132/300 [03:05&lt;03:56,  1.41s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 133/300 [03:06&lt;03:54,  1.40s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 134/300 [03:07&lt;03:52,  1.40s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 135/300 [03:09&lt;03:51,  1.40s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 136/300 [03:10&lt;03:49,  1.40s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 137/300 [03:12&lt;03:48,  1.40s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 138/300 [03:13&lt;03:47,  1.41s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 139/300 [03:14&lt;03:46,  1.41s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 140/300 [03:16&lt;03:45,  1.41s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 141/300 [03:17&lt;03:45,  1.42s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 142/300 [03:19&lt;03:43,  1.41s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 143/300 [03:20&lt;03:40,  1.41s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 144/300 [03:21&lt;03:39,  1.41s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 145/300 [03:23&lt;03:37,  1.40s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 146/300 [03:24&lt;03:36,  1.41s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 147/300 [03:26&lt;03:35,  1.41s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 148/300 [03:27&lt;03:33,  1.40s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 149/300 [03:28&lt;03:31,  1.40s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 150/300 [03:30&lt;03:32,  1.42s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 151/300 [03:31&lt;03:30,  1.41s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 152/300 [03:33&lt;03:28,  1.41s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 153/300 [03:34&lt;03:26,  1.41s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 154/300 [03:36&lt;03:24,  1.40s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 155/300 [03:37&lt;03:23,  1.40s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 156/300 [03:38&lt;03:22,  1.41s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 157/300 [03:40&lt;03:21,  1.41s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 158/300 [03:41&lt;03:19,  1.40s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 159/300 [03:43&lt;03:17,  1.40s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 160/300 [03:44&lt;03:16,  1.40s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 161/300 [03:45&lt;03:15,  1.41s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 162/300 [03:47&lt;03:13,  1.41s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 163/300 [03:48&lt;03:12,  1.40s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 164/300 [03:50&lt;03:10,  1.40s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 165/300 [03:51&lt;03:09,  1.40s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 166/300 [03:52&lt;03:07,  1.40s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 167/300 [03:54&lt;03:05,  1.40s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 168/300 [03:55&lt;03:04,  1.40s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 169/300 [03:57&lt;03:03,  1.40s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 170/300 [03:58&lt;03:02,  1.40s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 171/300 [03:59&lt;03:01,  1.40s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 172/300 [04:01&lt;02:59,  1.40s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 173/300 [04:02&lt;02:58,  1.40s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 174/300 [04:04&lt;02:56,  1.40s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 175/300 [04:05&lt;02:54,  1.40s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 176/300 [04:06&lt;02:53,  1.40s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 177/300 [04:08&lt;02:52,  1.40s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 178/300 [04:09&lt;02:50,  1.40s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 179/300 [04:11&lt;02:50,  1.41s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 180/300 [04:12&lt;02:48,  1.40s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 181/300 [04:13&lt;02:47,  1.40s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 182/300 [04:15&lt;02:45,  1.40s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 183/300 [04:16&lt;02:43,  1.40s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 184/300 [04:18&lt;02:42,  1.40s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 185/300 [04:19&lt;02:40,  1.40s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 186/300 [04:20&lt;02:39,  1.40s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 187/300 [04:22&lt;02:38,  1.40s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 188/300 [04:23&lt;02:36,  1.40s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 189/300 [04:25&lt;02:36,  1.41s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 190/300 [04:26&lt;02:34,  1.41s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 191/300 [04:27&lt;02:33,  1.41s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 192/300 [04:29&lt;02:35,  1.44s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 193/300 [04:30&lt;02:32,  1.42s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 194/300 [04:32&lt;02:30,  1.42s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 195/300 [04:33&lt;02:28,  1.42s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 196/300 [04:35&lt;02:27,  1.41s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 197/300 [04:36&lt;02:25,  1.41s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 198/300 [04:37&lt;02:23,  1.41s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 199/300 [04:39&lt;02:21,  1.41s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 200/300 [04:40&lt;02:20,  1.41s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 201/300 [04:42&lt;02:21,  1.42s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 202/300 [04:43&lt;02:18,  1.42s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 203/300 [04:44&lt;02:16,  1.41s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 204/300 [04:46&lt;02:14,  1.40s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 205/300 [04:47&lt;02:13,  1.40s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 206/300 [04:49&lt;02:11,  1.40s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 207/300 [04:50&lt;02:09,  1.40s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 208/300 [04:51&lt;02:08,  1.40s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 209/300 [04:53&lt;02:07,  1.40s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 210/300 [04:54&lt;02:06,  1.41s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 211/300 [04:56&lt;02:05,  1.41s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 212/300 [04:57&lt;02:03,  1.40s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 213/300 [04:58&lt;02:02,  1.40s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 214/300 [05:00&lt;02:00,  1.40s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 215/300 [05:01&lt;01:58,  1.40s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 216/300 [05:03&lt;01:58,  1.41s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 217/300 [05:04&lt;01:56,  1.40s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 218/300 [05:05&lt;01:55,  1.40s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 219/300 [05:07&lt;01:54,  1.41s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 220/300 [05:08&lt;01:53,  1.42s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 221/300 [05:10&lt;01:51,  1.42s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 222/300 [05:11&lt;01:49,  1.41s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 223/300 [05:13&lt;01:48,  1.40s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 224/300 [05:14&lt;01:46,  1.40s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 225/300 [05:15&lt;01:44,  1.40s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 226/300 [05:17&lt;01:43,  1.40s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 227/300 [05:18&lt;01:42,  1.40s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 228/300 [05:19&lt;01:40,  1.40s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 229/300 [05:21&lt;01:39,  1.40s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 230/300 [05:22&lt;01:38,  1.41s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 231/300 [05:24&lt;01:38,  1.42s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 232/300 [05:25&lt;01:36,  1.41s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 233/300 [05:27&lt;01:34,  1.41s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 234/300 [05:28&lt;01:33,  1.41s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 235/300 [05:29&lt;01:31,  1.41s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 236/300 [05:31&lt;01:29,  1.41s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 237/300 [05:32&lt;01:28,  1.40s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 238/300 [05:34&lt;01:26,  1.40s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 239/300 [05:35&lt;01:25,  1.40s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 240/300 [05:36&lt;01:24,  1.41s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 241/300 [05:38&lt;01:23,  1.41s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 242/300 [05:39&lt;01:21,  1.40s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 243/300 [05:41&lt;01:19,  1.40s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 244/300 [05:42&lt;01:18,  1.40s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 245/300 [05:43&lt;01:16,  1.40s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 246/300 [05:45&lt;01:15,  1.40s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 247/300 [05:46&lt;01:14,  1.40s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 248/300 [05:48&lt;01:12,  1.40s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 249/300 [05:49&lt;01:11,  1.40s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 250/300 [05:50&lt;01:10,  1.40s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 251/300 [05:52&lt;01:09,  1.41s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 252/300 [05:53&lt;01:07,  1.40s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 253/300 [05:55&lt;01:05,  1.40s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 254/300 [05:56&lt;01:04,  1.41s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 255/300 [05:57&lt;01:03,  1.40s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 256/300 [05:59&lt;01:01,  1.41s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 257/300 [06:00&lt;01:00,  1.40s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 258/300 [06:02&lt;00:58,  1.40s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 259/300 [06:03&lt;00:57,  1.40s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 260/300 [06:04&lt;00:56,  1.41s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 261/300 [06:06&lt;00:55,  1.41s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 262/300 [06:07&lt;00:53,  1.40s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 263/300 [06:09&lt;00:51,  1.40s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 264/300 [06:10&lt;00:50,  1.41s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 265/300 [06:11&lt;00:49,  1.40s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 266/300 [06:13&lt;00:47,  1.40s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 267/300 [06:14&lt;00:46,  1.40s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 268/300 [06:16&lt;00:44,  1.40s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 269/300 [06:17&lt;00:43,  1.40s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 270/300 [06:18&lt;00:42,  1.41s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 271/300 [06:20&lt;00:40,  1.41s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 272/300 [06:21&lt;00:39,  1.41s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 273/300 [06:23&lt;00:38,  1.41s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 274/300 [06:24&lt;00:36,  1.40s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 275/300 [06:26&lt;00:35,  1.41s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 276/300 [06:27&lt;00:33,  1.41s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 277/300 [06:28&lt;00:32,  1.40s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 278/300 [06:30&lt;00:30,  1.40s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 279/300 [06:31&lt;00:29,  1.40s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 280/300 [06:33&lt;00:28,  1.41s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 281/300 [06:34&lt;00:26,  1.41s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 282/300 [06:35&lt;00:25,  1.41s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 283/300 [06:37&lt;00:23,  1.41s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 284/300 [06:38&lt;00:22,  1.40s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 285/300 [06:40&lt;00:21,  1.40s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 286/300 [06:41&lt;00:19,  1.40s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 287/300 [06:42&lt;00:18,  1.40s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 288/300 [06:44&lt;00:16,  1.40s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 289/300 [06:45&lt;00:15,  1.41s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 290/300 [06:47&lt;00:14,  1.41s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 291/300 [06:48&lt;00:12,  1.41s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 292/300 [06:50&lt;00:11,  1.44s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 293/300 [06:51&lt;00:09,  1.43s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 294/300 [06:52&lt;00:08,  1.42s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 295/300 [06:54&lt;00:07,  1.42s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 296/300 [06:55&lt;00:05,  1.41s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 297/300 [06:57&lt;00:04,  1.40s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 298/300 [06:58&lt;00:02,  1.40s/it]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 299/300 [06:59&lt;00:01,  1.41s/it]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [07:01&lt;00:00,  1.40s/it]\n\n18it [2:06:40, 421.88s/it]\n\n  0%|          | 0/300 [00:00&lt;?, ?it/s]\n\n  0%|          | 1/300 [00:01&lt;05:03,  1.02s/it]\n\n  1%|          | 2/300 [00:02&lt;06:10,  1.24s/it]\n\n  1%|          | 3/300 [00:03&lt;06:30,  1.32s/it]\n\n  1%|‚ñè         | 4/300 [00:05&lt;06:38,  1.35s/it]\n\n  2%|‚ñè         | 5/300 [00:06&lt;06:42,  1.36s/it]\n\n  2%|‚ñè         | 6/300 [00:08&lt;06:44,  1.38s/it]\n\n  2%|‚ñè         | 7/300 [00:09&lt;06:45,  1.38s/it]\n\n  3%|‚ñé         | 8/300 [00:10&lt;06:48,  1.40s/it]\n\n  3%|‚ñé         | 9/300 [00:12&lt;06:49,  1.41s/it]\n\n  3%|‚ñé         | 10/300 [00:13&lt;06:48,  1.41s/it]\n\n  4%|‚ñé         | 11/300 [00:15&lt;06:47,  1.41s/it]\n\n  4%|‚ñç         | 12/300 [00:16&lt;06:46,  1.41s/it]\n\n  4%|‚ñç         | 13/300 [00:17&lt;06:44,  1.41s/it]\n\n  5%|‚ñç         | 14/300 [00:19&lt;06:41,  1.41s/it]\n\n  5%|‚ñå         | 15/300 [00:20&lt;06:39,  1.40s/it]\n\n  5%|‚ñå         | 16/300 [00:22&lt;06:37,  1.40s/it]\n\n  6%|‚ñå         | 17/300 [00:23&lt;06:35,  1.40s/it]\n\n  6%|‚ñå         | 18/300 [00:24&lt;06:36,  1.41s/it]\n\n  6%|‚ñã         | 19/300 [00:26&lt;06:36,  1.41s/it]\n\n  7%|‚ñã         | 20/300 [00:27&lt;06:34,  1.41s/it]\n\n  7%|‚ñã         | 21/300 [00:29&lt;06:33,  1.41s/it]\n\n  7%|‚ñã         | 22/300 [00:30&lt;06:31,  1.41s/it]\n\n  8%|‚ñä         | 23/300 [00:31&lt;06:29,  1.40s/it]\n\n  8%|‚ñä         | 24/300 [00:33&lt;06:26,  1.40s/it]\n\n  8%|‚ñä         | 25/300 [00:34&lt;06:25,  1.40s/it]\n\n  9%|‚ñä         | 26/300 [00:36&lt;06:23,  1.40s/it]\n\n  9%|‚ñâ         | 27/300 [00:37&lt;06:22,  1.40s/it]\n\n  9%|‚ñâ         | 28/300 [00:38&lt;06:22,  1.40s/it]\n\n 10%|‚ñâ         | 29/300 [00:40&lt;06:22,  1.41s/it]\n\n 10%|‚ñà         | 30/300 [00:41&lt;06:22,  1.42s/it]\n\n 10%|‚ñà         | 31/300 [00:43&lt;06:19,  1.41s/it]\n\n 11%|‚ñà         | 32/300 [00:44&lt;06:18,  1.41s/it]\n\n 11%|‚ñà         | 33/300 [00:46&lt;06:17,  1.41s/it]\n\n 11%|‚ñà‚ñè        | 34/300 [00:47&lt;06:14,  1.41s/it]\n\n 12%|‚ñà‚ñè        | 35/300 [00:48&lt;06:12,  1.41s/it]\n\n 12%|‚ñà‚ñè        | 36/300 [00:50&lt;06:10,  1.40s/it]\n\n 12%|‚ñà‚ñè        | 37/300 [00:51&lt;06:07,  1.40s/it]\n\n 13%|‚ñà‚ñé        | 38/300 [00:53&lt;06:08,  1.41s/it]\n\n 13%|‚ñà‚ñé        | 39/300 [00:54&lt;06:07,  1.41s/it]\n\n 13%|‚ñà‚ñé        | 40/300 [00:55&lt;06:06,  1.41s/it]\n\n 14%|‚ñà‚ñé        | 41/300 [00:57&lt;06:03,  1.41s/it]\n\n 14%|‚ñà‚ñç        | 42/300 [00:58&lt;06:02,  1.41s/it]\n\n 14%|‚ñà‚ñç        | 43/300 [01:00&lt;06:01,  1.41s/it]\n\n 15%|‚ñà‚ñç        | 44/300 [01:01&lt;05:58,  1.40s/it]\n\n 15%|‚ñà‚ñå        | 45/300 [01:02&lt;05:56,  1.40s/it]\n\n 15%|‚ñà‚ñå        | 46/300 [01:04&lt;05:55,  1.40s/it]\n\n 16%|‚ñà‚ñå        | 47/300 [01:05&lt;05:54,  1.40s/it]\n\n 16%|‚ñà‚ñå        | 48/300 [01:07&lt;05:53,  1.40s/it]\n\n 16%|‚ñà‚ñã        | 49/300 [01:08&lt;05:52,  1.40s/it]\n\n 17%|‚ñà‚ñã        | 50/300 [01:09&lt;05:51,  1.40s/it]\n\n 17%|‚ñà‚ñã        | 51/300 [01:11&lt;05:50,  1.41s/it]\n\n 17%|‚ñà‚ñã        | 52/300 [01:12&lt;05:47,  1.40s/it]\n\n 18%|‚ñà‚ñä        | 53/300 [01:14&lt;05:45,  1.40s/it]\n\n 18%|‚ñà‚ñä        | 54/300 [01:15&lt;05:43,  1.40s/it]\n\n 18%|‚ñà‚ñä        | 55/300 [01:16&lt;05:41,  1.40s/it]\n\n 19%|‚ñà‚ñä        | 56/300 [01:18&lt;05:40,  1.40s/it]\n\n 19%|‚ñà‚ñâ        | 57/300 [01:19&lt;05:39,  1.40s/it]\n\n 19%|‚ñà‚ñâ        | 58/300 [01:21&lt;05:37,  1.40s/it]\n\n 20%|‚ñà‚ñâ        | 59/300 [01:22&lt;05:37,  1.40s/it]\n\n 20%|‚ñà‚ñà        | 60/300 [01:23&lt;05:39,  1.42s/it]\n\n 20%|‚ñà‚ñà        | 61/300 [01:25&lt;05:37,  1.41s/it]\n\n 21%|‚ñà‚ñà        | 62/300 [01:26&lt;05:34,  1.41s/it]\n\n 21%|‚ñà‚ñà        | 63/300 [01:28&lt;05:32,  1.40s/it]\n\n 21%|‚ñà‚ñà‚ñè       | 64/300 [01:29&lt;05:30,  1.40s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 65/300 [01:30&lt;05:29,  1.40s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 66/300 [01:32&lt;05:29,  1.41s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 67/300 [01:33&lt;05:26,  1.40s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 68/300 [01:35&lt;05:26,  1.41s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 69/300 [01:36&lt;05:26,  1.42s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 70/300 [01:37&lt;05:24,  1.41s/it]\n\n 24%|‚ñà‚ñà‚ñé       | 71/300 [01:39&lt;05:22,  1.41s/it]\n\n 24%|‚ñà‚ñà‚ñç       | 72/300 [01:40&lt;05:19,  1.40s/it]\n\n 24%|‚ñà‚ñà‚ñç       | 73/300 [01:42&lt;05:17,  1.40s/it]\n\n 25%|‚ñà‚ñà‚ñç       | 74/300 [01:43&lt;05:15,  1.40s/it]\n\n 25%|‚ñà‚ñà‚ñå       | 75/300 [01:44&lt;05:15,  1.40s/it]\n\n 25%|‚ñà‚ñà‚ñå       | 76/300 [01:46&lt;05:13,  1.40s/it]\n\n 26%|‚ñà‚ñà‚ñå       | 77/300 [01:47&lt;05:12,  1.40s/it]\n\n 26%|‚ñà‚ñà‚ñå       | 78/300 [01:49&lt;05:12,  1.41s/it]\n\n 26%|‚ñà‚ñà‚ñã       | 79/300 [01:50&lt;05:10,  1.41s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 80/300 [01:52&lt;05:10,  1.41s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 81/300 [01:53&lt;05:07,  1.41s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 82/300 [01:54&lt;05:05,  1.40s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 83/300 [01:56&lt;05:04,  1.40s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 84/300 [01:57&lt;05:02,  1.40s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 85/300 [01:59&lt;05:00,  1.40s/it]\n\n 29%|‚ñà‚ñà‚ñä       | 86/300 [02:00&lt;04:58,  1.40s/it]\n\n 29%|‚ñà‚ñà‚ñâ       | 87/300 [02:01&lt;04:57,  1.40s/it]\n\n 29%|‚ñà‚ñà‚ñâ       | 88/300 [02:03&lt;04:57,  1.40s/it]\n\n 30%|‚ñà‚ñà‚ñâ       | 89/300 [02:04&lt;04:55,  1.40s/it]\n\n 30%|‚ñà‚ñà‚ñà       | 90/300 [02:06&lt;04:54,  1.40s/it]\n\n 30%|‚ñà‚ñà‚ñà       | 91/300 [02:07&lt;04:52,  1.40s/it]\n\n 31%|‚ñà‚ñà‚ñà       | 92/300 [02:08&lt;04:50,  1.40s/it]\n\n 31%|‚ñà‚ñà‚ñà       | 93/300 [02:10&lt;04:49,  1.40s/it]\n\n 31%|‚ñà‚ñà‚ñà‚ñè      | 94/300 [02:11&lt;04:48,  1.40s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 95/300 [02:13&lt;04:46,  1.40s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 96/300 [02:14&lt;04:45,  1.40s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 97/300 [02:15&lt;04:43,  1.40s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 98/300 [02:17&lt;04:42,  1.40s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 99/300 [02:18&lt;04:41,  1.40s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 100/300 [02:20&lt;04:40,  1.40s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñé      | 101/300 [02:21&lt;04:38,  1.40s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñç      | 102/300 [02:22&lt;04:37,  1.40s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñç      | 103/300 [02:24&lt;04:35,  1.40s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñç      | 104/300 [02:25&lt;04:33,  1.40s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñå      | 105/300 [02:26&lt;04:32,  1.40s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñå      | 106/300 [02:28&lt;04:31,  1.40s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñå      | 107/300 [02:29&lt;04:29,  1.40s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñå      | 108/300 [02:31&lt;04:30,  1.41s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñã      | 109/300 [02:32&lt;04:28,  1.40s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 110/300 [02:34&lt;04:29,  1.42s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 111/300 [02:35&lt;04:26,  1.41s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 112/300 [02:36&lt;04:32,  1.45s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 113/300 [02:38&lt;04:27,  1.43s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 114/300 [02:39&lt;04:23,  1.42s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 115/300 [02:41&lt;04:20,  1.41s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñä      | 116/300 [02:42&lt;04:18,  1.41s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñâ      | 117/300 [02:43&lt;04:17,  1.41s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñâ      | 118/300 [02:45&lt;04:15,  1.41s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñâ      | 119/300 [02:46&lt;04:15,  1.41s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñà      | 120/300 [02:48&lt;04:15,  1.42s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñà      | 121/300 [02:49&lt;04:12,  1.41s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà      | 122/300 [02:51&lt;04:10,  1.41s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà      | 123/300 [02:52&lt;04:08,  1.40s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 124/300 [02:53&lt;04:06,  1.40s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 125/300 [02:55&lt;04:05,  1.40s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 126/300 [02:56&lt;04:03,  1.40s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 127/300 [02:58&lt;04:02,  1.40s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 128/300 [02:59&lt;04:01,  1.40s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 129/300 [03:00&lt;04:00,  1.40s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 130/300 [03:02&lt;03:58,  1.40s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 131/300 [03:03&lt;03:56,  1.40s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 132/300 [03:05&lt;03:55,  1.40s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 133/300 [03:06&lt;03:53,  1.40s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 134/300 [03:07&lt;03:51,  1.40s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 135/300 [03:09&lt;03:50,  1.39s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 136/300 [03:10&lt;03:48,  1.39s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 137/300 [03:12&lt;03:48,  1.40s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 138/300 [03:13&lt;03:47,  1.40s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 139/300 [03:14&lt;03:46,  1.41s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 140/300 [03:16&lt;03:44,  1.40s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 141/300 [03:17&lt;03:42,  1.40s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 142/300 [03:19&lt;03:40,  1.40s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 143/300 [03:20&lt;03:40,  1.40s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 144/300 [03:21&lt;03:39,  1.41s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 145/300 [03:23&lt;03:37,  1.40s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 146/300 [03:24&lt;03:35,  1.40s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 147/300 [03:26&lt;03:34,  1.40s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 148/300 [03:27&lt;03:32,  1.40s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 149/300 [03:28&lt;03:33,  1.41s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 150/300 [03:30&lt;03:31,  1.41s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 151/300 [03:31&lt;03:29,  1.40s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 152/300 [03:33&lt;03:28,  1.41s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 153/300 [03:34&lt;03:26,  1.40s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 154/300 [03:35&lt;03:24,  1.40s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 155/300 [03:37&lt;03:22,  1.40s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 156/300 [03:38&lt;03:21,  1.40s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 157/300 [03:40&lt;03:20,  1.40s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 158/300 [03:41&lt;03:18,  1.40s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 159/300 [03:42&lt;03:20,  1.42s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 160/300 [03:44&lt;03:18,  1.41s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 161/300 [03:45&lt;03:15,  1.41s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 162/300 [03:47&lt;03:14,  1.41s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 163/300 [03:48&lt;03:12,  1.40s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 164/300 [03:49&lt;03:10,  1.40s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 165/300 [03:51&lt;03:09,  1.40s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 166/300 [03:52&lt;03:08,  1.41s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 167/300 [03:54&lt;03:06,  1.40s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 168/300 [03:55&lt;03:06,  1.41s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 169/300 [03:56&lt;03:04,  1.41s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 170/300 [03:58&lt;03:02,  1.41s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 171/300 [03:59&lt;03:01,  1.40s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 172/300 [04:01&lt;02:59,  1.40s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 173/300 [04:02&lt;02:57,  1.40s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 174/300 [04:03&lt;02:56,  1.40s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 175/300 [04:05&lt;02:54,  1.40s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 176/300 [04:06&lt;02:52,  1.40s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 177/300 [04:08&lt;02:51,  1.40s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 178/300 [04:09&lt;02:52,  1.42s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 179/300 [04:11&lt;02:50,  1.41s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 180/300 [04:12&lt;02:48,  1.41s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 181/300 [04:13&lt;02:46,  1.40s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 182/300 [04:15&lt;02:45,  1.40s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 183/300 [04:16&lt;02:43,  1.40s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 184/300 [04:18&lt;02:42,  1.40s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 185/300 [04:19&lt;02:41,  1.40s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 186/300 [04:20&lt;02:39,  1.40s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 187/300 [04:22&lt;02:37,  1.40s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 188/300 [04:23&lt;02:36,  1.40s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 189/300 [04:25&lt;02:36,  1.41s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 190/300 [04:26&lt;02:34,  1.41s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 191/300 [04:27&lt;02:33,  1.41s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 192/300 [04:29&lt;02:31,  1.41s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 193/300 [04:30&lt;02:30,  1.40s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 194/300 [04:32&lt;02:28,  1.40s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 195/300 [04:33&lt;02:27,  1.41s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 196/300 [04:34&lt;02:26,  1.40s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 197/300 [04:36&lt;02:24,  1.40s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 198/300 [04:37&lt;02:23,  1.41s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 199/300 [04:39&lt;02:22,  1.41s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 200/300 [04:40&lt;02:20,  1.41s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 201/300 [04:41&lt;02:19,  1.41s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 202/300 [04:43&lt;02:17,  1.40s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 203/300 [04:44&lt;02:15,  1.40s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 204/300 [04:46&lt;02:14,  1.40s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 205/300 [04:47&lt;02:12,  1.40s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 206/300 [04:48&lt;02:11,  1.39s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 207/300 [04:50&lt;02:09,  1.40s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 208/300 [04:51&lt;02:09,  1.41s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 209/300 [04:53&lt;02:08,  1.41s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 210/300 [04:54&lt;02:06,  1.41s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 211/300 [04:55&lt;02:05,  1.41s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 212/300 [04:57&lt;02:06,  1.44s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 213/300 [04:58&lt;02:04,  1.43s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 214/300 [05:00&lt;02:02,  1.42s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 215/300 [05:01&lt;02:00,  1.41s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 216/300 [05:03&lt;01:58,  1.41s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 217/300 [05:04&lt;01:57,  1.41s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 218/300 [05:05&lt;01:55,  1.41s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 219/300 [05:07&lt;01:53,  1.41s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 220/300 [05:08&lt;01:52,  1.40s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 221/300 [05:10&lt;01:50,  1.40s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 222/300 [05:11&lt;01:49,  1.40s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 223/300 [05:12&lt;01:47,  1.40s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 224/300 [05:14&lt;01:46,  1.40s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 225/300 [05:15&lt;01:44,  1.40s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 226/300 [05:17&lt;01:43,  1.40s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 227/300 [05:18&lt;01:43,  1.41s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 228/300 [05:19&lt;01:41,  1.41s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 229/300 [05:21&lt;01:39,  1.40s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 230/300 [05:22&lt;01:37,  1.40s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 231/300 [05:24&lt;01:36,  1.40s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 232/300 [05:25&lt;01:35,  1.40s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 233/300 [05:26&lt;01:33,  1.40s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 234/300 [05:28&lt;01:32,  1.40s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 235/300 [05:29&lt;01:31,  1.40s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 236/300 [05:31&lt;01:29,  1.40s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 237/300 [05:32&lt;01:28,  1.40s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 238/300 [05:33&lt;01:27,  1.41s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 239/300 [05:35&lt;01:25,  1.41s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 240/300 [05:36&lt;01:24,  1.41s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 241/300 [05:38&lt;01:22,  1.40s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 242/300 [05:39&lt;01:21,  1.40s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 243/300 [05:40&lt;01:20,  1.42s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 244/300 [05:42&lt;01:18,  1.41s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 245/300 [05:43&lt;01:17,  1.41s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 246/300 [05:45&lt;01:16,  1.41s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 247/300 [05:46&lt;01:15,  1.42s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 248/300 [05:47&lt;01:13,  1.41s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 249/300 [05:49&lt;01:11,  1.41s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 250/300 [05:50&lt;01:10,  1.40s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 251/300 [05:52&lt;01:08,  1.40s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 252/300 [05:53&lt;01:07,  1.41s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 253/300 [05:54&lt;01:05,  1.40s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 254/300 [05:56&lt;01:04,  1.40s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 255/300 [05:57&lt;01:03,  1.40s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 256/300 [05:59&lt;01:02,  1.41s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 257/300 [06:00&lt;01:00,  1.42s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 258/300 [06:02&lt;00:59,  1.41s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 259/300 [06:03&lt;00:57,  1.41s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 260/300 [06:04&lt;00:56,  1.40s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 261/300 [06:06&lt;00:54,  1.40s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 262/300 [06:07&lt;00:53,  1.40s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 263/300 [06:09&lt;00:51,  1.40s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 264/300 [06:10&lt;00:50,  1.40s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 265/300 [06:11&lt;00:48,  1.40s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 266/300 [06:13&lt;00:47,  1.41s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 267/300 [06:14&lt;00:46,  1.41s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 268/300 [06:16&lt;00:45,  1.42s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 269/300 [06:17&lt;00:43,  1.41s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 270/300 [06:18&lt;00:42,  1.40s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 271/300 [06:20&lt;00:40,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 272/300 [06:21&lt;00:39,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 273/300 [06:23&lt;00:37,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 274/300 [06:24&lt;00:36,  1.40s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 275/300 [06:25&lt;00:34,  1.39s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 276/300 [06:27&lt;00:33,  1.41s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 277/300 [06:28&lt;00:32,  1.41s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 278/300 [06:30&lt;00:30,  1.41s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 279/300 [06:31&lt;00:29,  1.41s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 280/300 [06:32&lt;00:28,  1.41s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 281/300 [06:34&lt;00:26,  1.40s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 282/300 [06:35&lt;00:25,  1.41s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 283/300 [06:37&lt;00:23,  1.40s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 284/300 [06:38&lt;00:22,  1.40s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 285/300 [06:39&lt;00:20,  1.40s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 286/300 [06:41&lt;00:19,  1.40s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 287/300 [06:42&lt;00:18,  1.41s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 288/300 [06:44&lt;00:16,  1.41s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 289/300 [06:45&lt;00:15,  1.41s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 290/300 [06:46&lt;00:14,  1.40s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 291/300 [06:48&lt;00:12,  1.40s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 292/300 [06:49&lt;00:11,  1.40s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 293/300 [06:51&lt;00:09,  1.40s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 294/300 [06:52&lt;00:08,  1.39s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 295/300 [06:53&lt;00:06,  1.40s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 296/300 [06:55&lt;00:05,  1.41s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 297/300 [06:56&lt;00:04,  1.42s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 298/300 [06:58&lt;00:02,  1.41s/it]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 299/300 [06:59&lt;00:01,  1.41s/it]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [07:00&lt;00:00,  1.40s/it]\n\n19it [2:13:42, 421.87s/it]\n\n  0%|          | 0/300 [00:00&lt;?, ?it/s]\n\n  0%|          | 1/300 [00:01&lt;05:29,  1.10s/it]\n\n  1%|          | 2/300 [00:02&lt;06:24,  1.29s/it]\n\n  1%|          | 3/300 [00:03&lt;06:37,  1.34s/it]\n\n  1%|‚ñè         | 4/300 [00:05&lt;06:42,  1.36s/it]\n\n  2%|‚ñè         | 5/300 [00:06&lt;06:47,  1.38s/it]\n\n  2%|‚ñè         | 6/300 [00:08&lt;06:49,  1.39s/it]\n\n  2%|‚ñè         | 7/300 [00:09&lt;06:50,  1.40s/it]\n\n  3%|‚ñé         | 8/300 [00:10&lt;06:50,  1.41s/it]\n\n  3%|‚ñé         | 9/300 [00:12&lt;06:47,  1.40s/it]\n\n  3%|‚ñé         | 10/300 [00:13&lt;06:45,  1.40s/it]\n\n  4%|‚ñé         | 11/300 [00:15&lt;06:43,  1.39s/it]\n\n  4%|‚ñç         | 12/300 [00:16&lt;06:42,  1.40s/it]\n\n  4%|‚ñç         | 13/300 [00:17&lt;06:40,  1.40s/it]\n\n  5%|‚ñç         | 14/300 [00:19&lt;06:41,  1.40s/it]\n\n  5%|‚ñå         | 15/300 [00:20&lt;06:44,  1.42s/it]\n\n  5%|‚ñå         | 16/300 [00:22&lt;06:42,  1.42s/it]\n\n  6%|‚ñå         | 17/300 [00:23&lt;06:40,  1.41s/it]\n\n  6%|‚ñå         | 18/300 [00:25&lt;06:37,  1.41s/it]\n\n  6%|‚ñã         | 19/300 [00:26&lt;06:34,  1.40s/it]\n\n  7%|‚ñã         | 20/300 [00:27&lt;06:32,  1.40s/it]\n\n  7%|‚ñã         | 21/300 [00:29&lt;06:32,  1.41s/it]\n\n  7%|‚ñã         | 22/300 [00:30&lt;06:29,  1.40s/it]\n\n  8%|‚ñä         | 23/300 [00:32&lt;06:30,  1.41s/it]\n\n  8%|‚ñä         | 24/300 [00:33&lt;06:27,  1.40s/it]\n\n  8%|‚ñä         | 25/300 [00:34&lt;06:25,  1.40s/it]\n\n  9%|‚ñä         | 26/300 [00:36&lt;06:27,  1.41s/it]\n\n  9%|‚ñâ         | 27/300 [00:37&lt;06:25,  1.41s/it]\n\n  9%|‚ñâ         | 28/300 [00:39&lt;06:22,  1.41s/it]\n\n 10%|‚ñâ         | 29/300 [00:40&lt;06:20,  1.40s/it]\n\n 10%|‚ñà         | 30/300 [00:41&lt;06:18,  1.40s/it]\n\n 10%|‚ñà         | 31/300 [00:43&lt;06:15,  1.40s/it]\n\n 11%|‚ñà         | 32/300 [00:44&lt;06:14,  1.40s/it]\n\n 11%|‚ñà         | 33/300 [00:46&lt;06:12,  1.39s/it]\n\n 11%|‚ñà‚ñè        | 34/300 [00:47&lt;06:12,  1.40s/it]\n\n 12%|‚ñà‚ñè        | 35/300 [00:48&lt;06:12,  1.40s/it]\n\n 12%|‚ñà‚ñè        | 36/300 [00:50&lt;06:10,  1.40s/it]\n\n 12%|‚ñà‚ñè        | 37/300 [00:51&lt;06:08,  1.40s/it]\n\n 13%|‚ñà‚ñé        | 38/300 [00:53&lt;06:08,  1.41s/it]\n\n 13%|‚ñà‚ñé        | 39/300 [00:54&lt;06:05,  1.40s/it]\n\n 13%|‚ñà‚ñé        | 40/300 [00:55&lt;06:04,  1.40s/it]\n\n 14%|‚ñà‚ñé        | 41/300 [00:57&lt;06:02,  1.40s/it]\n\n 14%|‚ñà‚ñç        | 42/300 [00:58&lt;06:00,  1.40s/it]\n\n 14%|‚ñà‚ñç        | 43/300 [01:00&lt;05:58,  1.40s/it]\n\n 15%|‚ñà‚ñç        | 44/300 [01:01&lt;05:59,  1.41s/it]\n\n 15%|‚ñà‚ñå        | 45/300 [01:02&lt;05:59,  1.41s/it]\n\n 15%|‚ñà‚ñå        | 46/300 [01:04&lt;05:58,  1.41s/it]\n\n 16%|‚ñà‚ñå        | 47/300 [01:05&lt;05:56,  1.41s/it]\n\n 16%|‚ñà‚ñå        | 48/300 [01:07&lt;05:54,  1.41s/it]\n\n 16%|‚ñà‚ñã        | 49/300 [01:08&lt;05:54,  1.41s/it]\n\n 17%|‚ñà‚ñã        | 50/300 [01:09&lt;05:51,  1.41s/it]\n\n 17%|‚ñà‚ñã        | 51/300 [01:11&lt;05:49,  1.40s/it]\n\n 17%|‚ñà‚ñã        | 52/300 [01:12&lt;05:48,  1.40s/it]\n\n 18%|‚ñà‚ñä        | 53/300 [01:14&lt;05:46,  1.40s/it]\n\n 18%|‚ñà‚ñä        | 54/300 [01:15&lt;05:44,  1.40s/it]\n\n 18%|‚ñà‚ñä        | 55/300 [01:16&lt;05:45,  1.41s/it]\n\n 19%|‚ñà‚ñä        | 56/300 [01:18&lt;05:44,  1.41s/it]\n\n 19%|‚ñà‚ñâ        | 57/300 [01:19&lt;05:41,  1.40s/it]\n\n 19%|‚ñà‚ñâ        | 58/300 [01:21&lt;05:40,  1.41s/it]\n\n 20%|‚ñà‚ñâ        | 59/300 [01:22&lt;05:37,  1.40s/it]\n\n 20%|‚ñà‚ñà        | 60/300 [01:23&lt;05:35,  1.40s/it]\n\n 20%|‚ñà‚ñà        | 61/300 [01:25&lt;05:33,  1.40s/it]\n\n 21%|‚ñà‚ñà        | 62/300 [01:26&lt;05:31,  1.39s/it]\n\n 21%|‚ñà‚ñà        | 63/300 [01:28&lt;05:31,  1.40s/it]\n\n 21%|‚ñà‚ñà‚ñè       | 64/300 [01:29&lt;05:31,  1.41s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 65/300 [01:31&lt;05:30,  1.40s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 66/300 [01:32&lt;05:30,  1.41s/it]\n\n 22%|‚ñà‚ñà‚ñè       | 67/300 [01:33&lt;05:27,  1.41s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 68/300 [01:35&lt;05:25,  1.40s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 69/300 [01:36&lt;05:24,  1.41s/it]\n\n 23%|‚ñà‚ñà‚ñé       | 70/300 [01:38&lt;05:23,  1.41s/it]\n\n 24%|‚ñà‚ñà‚ñé       | 71/300 [01:39&lt;05:21,  1.41s/it]\n\n 24%|‚ñà‚ñà‚ñç       | 72/300 [01:40&lt;05:19,  1.40s/it]\n\n 24%|‚ñà‚ñà‚ñç       | 73/300 [01:42&lt;05:17,  1.40s/it]\n\n 25%|‚ñà‚ñà‚ñç       | 74/300 [01:43&lt;05:19,  1.42s/it]\n\n 25%|‚ñà‚ñà‚ñå       | 75/300 [01:45&lt;05:17,  1.41s/it]\n\n 25%|‚ñà‚ñà‚ñå       | 76/300 [01:46&lt;05:16,  1.41s/it]\n\n 26%|‚ñà‚ñà‚ñå       | 77/300 [01:47&lt;05:13,  1.40s/it]\n\n 26%|‚ñà‚ñà‚ñå       | 78/300 [01:49&lt;05:11,  1.40s/it]\n\n 26%|‚ñà‚ñà‚ñã       | 79/300 [01:50&lt;05:09,  1.40s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 80/300 [01:52&lt;05:08,  1.40s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 81/300 [01:53&lt;05:05,  1.40s/it]\n\n 27%|‚ñà‚ñà‚ñã       | 82/300 [01:54&lt;05:04,  1.40s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 83/300 [01:56&lt;05:03,  1.40s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 84/300 [01:57&lt;05:02,  1.40s/it]\n\n 28%|‚ñà‚ñà‚ñä       | 85/300 [01:59&lt;05:01,  1.40s/it]\n\n 29%|‚ñà‚ñà‚ñä       | 86/300 [02:00&lt;05:01,  1.41s/it]\n\n 29%|‚ñà‚ñà‚ñâ       | 87/300 [02:01&lt;04:58,  1.40s/it]\n\n 29%|‚ñà‚ñà‚ñâ       | 88/300 [02:03&lt;04:57,  1.40s/it]\n\n 30%|‚ñà‚ñà‚ñâ       | 89/300 [02:04&lt;04:55,  1.40s/it]\n\n 30%|‚ñà‚ñà‚ñà       | 90/300 [02:06&lt;04:54,  1.40s/it]\n\n 30%|‚ñà‚ñà‚ñà       | 91/300 [02:07&lt;04:52,  1.40s/it]\n\n 31%|‚ñà‚ñà‚ñà       | 92/300 [02:08&lt;04:50,  1.40s/it]\n\n 31%|‚ñà‚ñà‚ñà       | 93/300 [02:10&lt;04:49,  1.40s/it]\n\n 31%|‚ñà‚ñà‚ñà‚ñè      | 94/300 [02:11&lt;04:48,  1.40s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 95/300 [02:13&lt;04:47,  1.40s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 96/300 [02:14&lt;04:48,  1.41s/it]\n\n 32%|‚ñà‚ñà‚ñà‚ñè      | 97/300 [02:15&lt;04:45,  1.41s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 98/300 [02:17&lt;04:43,  1.40s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 99/300 [02:18&lt;04:42,  1.40s/it]\n\n 33%|‚ñà‚ñà‚ñà‚ñé      | 100/300 [02:20&lt;04:46,  1.43s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñé      | 101/300 [02:21&lt;04:42,  1.42s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñç      | 102/300 [02:23&lt;04:40,  1.42s/it]\n\n 34%|‚ñà‚ñà‚ñà‚ñç      | 103/300 [02:24&lt;04:37,  1.41s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñç      | 104/300 [02:25&lt;04:38,  1.42s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñå      | 105/300 [02:27&lt;04:36,  1.42s/it]\n\n 35%|‚ñà‚ñà‚ñà‚ñå      | 106/300 [02:28&lt;04:34,  1.42s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñå      | 107/300 [02:30&lt;04:31,  1.41s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñå      | 108/300 [02:31&lt;04:29,  1.40s/it]\n\n 36%|‚ñà‚ñà‚ñà‚ñã      | 109/300 [02:32&lt;04:27,  1.40s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 110/300 [02:34&lt;04:25,  1.40s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 111/300 [02:35&lt;04:25,  1.40s/it]\n\n 37%|‚ñà‚ñà‚ñà‚ñã      | 112/300 [02:37&lt;04:24,  1.41s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 113/300 [02:38&lt;04:22,  1.40s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 114/300 [02:39&lt;04:21,  1.41s/it]\n\n 38%|‚ñà‚ñà‚ñà‚ñä      | 115/300 [02:41&lt;04:22,  1.42s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñä      | 116/300 [02:42&lt;04:20,  1.42s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñâ      | 117/300 [02:44&lt;04:17,  1.41s/it]\n\n 39%|‚ñà‚ñà‚ñà‚ñâ      | 118/300 [02:45&lt;04:16,  1.41s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñâ      | 119/300 [02:46&lt;04:14,  1.40s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñà      | 120/300 [02:48&lt;04:12,  1.40s/it]\n\n 40%|‚ñà‚ñà‚ñà‚ñà      | 121/300 [02:49&lt;04:10,  1.40s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà      | 122/300 [02:51&lt;04:08,  1.40s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà      | 123/300 [02:52&lt;04:08,  1.40s/it]\n\n 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 124/300 [02:53&lt;04:08,  1.41s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 125/300 [02:55&lt;04:07,  1.41s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 126/300 [02:56&lt;04:05,  1.41s/it]\n\n 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 127/300 [02:58&lt;04:03,  1.41s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 128/300 [02:59&lt;04:01,  1.41s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 129/300 [03:01&lt;04:00,  1.40s/it]\n\n 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 130/300 [03:02&lt;04:00,  1.42s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 131/300 [03:03&lt;03:57,  1.41s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 132/300 [03:05&lt;03:55,  1.40s/it]\n\n 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 133/300 [03:06&lt;03:56,  1.42s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 134/300 [03:08&lt;03:55,  1.42s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 135/300 [03:09&lt;03:55,  1.43s/it]\n\n 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 136/300 [03:10&lt;03:53,  1.42s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 137/300 [03:12&lt;03:50,  1.41s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 138/300 [03:13&lt;03:49,  1.41s/it]\n\n 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 139/300 [03:15&lt;03:46,  1.41s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 140/300 [03:16&lt;03:44,  1.40s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 141/300 [03:17&lt;03:42,  1.40s/it]\n\n 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 142/300 [03:19&lt;03:41,  1.40s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 143/300 [03:20&lt;03:39,  1.40s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 144/300 [03:22&lt;03:39,  1.41s/it]\n\n 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 145/300 [03:23&lt;03:38,  1.41s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 146/300 [03:24&lt;03:35,  1.40s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 147/300 [03:26&lt;03:35,  1.41s/it]\n\n 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 148/300 [03:27&lt;03:33,  1.41s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 149/300 [03:29&lt;03:31,  1.40s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 150/300 [03:30&lt;03:29,  1.40s/it]\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 151/300 [03:31&lt;03:28,  1.40s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 152/300 [03:33&lt;03:27,  1.40s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 153/300 [03:34&lt;03:26,  1.40s/it]\n\n 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 154/300 [03:36&lt;03:25,  1.41s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 155/300 [03:37&lt;03:25,  1.42s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 156/300 [03:39&lt;03:23,  1.41s/it]\n\n 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 157/300 [03:40&lt;03:20,  1.40s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 158/300 [03:41&lt;03:19,  1.40s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 159/300 [03:43&lt;03:18,  1.41s/it]\n\n 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 160/300 [03:44&lt;03:16,  1.40s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 161/300 [03:46&lt;03:14,  1.40s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 162/300 [03:47&lt;03:13,  1.40s/it]\n\n 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 163/300 [03:48&lt;03:11,  1.40s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 164/300 [03:50&lt;03:11,  1.40s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 165/300 [03:51&lt;03:11,  1.42s/it]\n\n 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 166/300 [03:53&lt;03:09,  1.41s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 167/300 [03:54&lt;03:07,  1.41s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 168/300 [03:55&lt;03:05,  1.41s/it]\n\n 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 169/300 [03:57&lt;03:03,  1.40s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 170/300 [03:58&lt;03:01,  1.40s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 171/300 [04:00&lt;03:00,  1.40s/it]\n\n 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 172/300 [04:01&lt;02:59,  1.40s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 173/300 [04:02&lt;02:58,  1.41s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 174/300 [04:04&lt;02:57,  1.41s/it]\n\n 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 175/300 [04:05&lt;02:56,  1.41s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 176/300 [04:07&lt;02:54,  1.40s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 177/300 [04:08&lt;02:52,  1.40s/it]\n\n 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 178/300 [04:09&lt;02:50,  1.40s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 179/300 [04:11&lt;02:48,  1.40s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 180/300 [04:12&lt;02:47,  1.40s/it]\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 181/300 [04:14&lt;02:45,  1.39s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 182/300 [04:15&lt;02:44,  1.40s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 183/300 [04:16&lt;02:43,  1.40s/it]\n\n 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 184/300 [04:18&lt;02:41,  1.40s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 185/300 [04:19&lt;02:43,  1.42s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 186/300 [04:21&lt;02:41,  1.41s/it]\n\n 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 187/300 [04:22&lt;02:38,  1.41s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 188/300 [04:23&lt;02:37,  1.40s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 189/300 [04:25&lt;02:38,  1.43s/it]\n\n 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 190/300 [04:26&lt;02:36,  1.43s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 191/300 [04:28&lt;02:34,  1.42s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 192/300 [04:29&lt;02:32,  1.41s/it]\n\n 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 193/300 [04:31&lt;02:30,  1.41s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 194/300 [04:32&lt;02:29,  1.41s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 195/300 [04:33&lt;02:29,  1.42s/it]\n\n 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 196/300 [04:35&lt;02:27,  1.41s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 197/300 [04:36&lt;02:25,  1.41s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 198/300 [04:38&lt;02:24,  1.41s/it]\n\n 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 199/300 [04:39&lt;02:22,  1.41s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 200/300 [04:40&lt;02:20,  1.40s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 201/300 [04:42&lt;02:20,  1.42s/it]\n\n 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 202/300 [04:43&lt;02:18,  1.41s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 203/300 [04:45&lt;02:16,  1.41s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 204/300 [04:46&lt;02:15,  1.42s/it]\n\n 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 205/300 [04:48&lt;02:15,  1.42s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 206/300 [04:49&lt;02:12,  1.41s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 207/300 [04:50&lt;02:11,  1.41s/it]\n\n 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 208/300 [04:52&lt;02:09,  1.41s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 209/300 [04:53&lt;02:07,  1.40s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 210/300 [04:55&lt;02:05,  1.40s/it]\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 211/300 [04:56&lt;02:04,  1.40s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 212/300 [04:57&lt;02:02,  1.39s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 213/300 [04:59&lt;02:01,  1.40s/it]\n\n 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 214/300 [05:00&lt;02:00,  1.40s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 215/300 [05:02&lt;01:59,  1.40s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 216/300 [05:03&lt;01:57,  1.40s/it]\n\n 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 217/300 [05:04&lt;01:55,  1.40s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 218/300 [05:06&lt;01:54,  1.40s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 219/300 [05:07&lt;01:52,  1.40s/it]\n\n 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 220/300 [05:08&lt;01:52,  1.40s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 221/300 [05:10&lt;01:50,  1.40s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 222/300 [05:11&lt;01:49,  1.40s/it]\n\n 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 223/300 [05:13&lt;01:48,  1.40s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 224/300 [05:14&lt;01:47,  1.42s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 225/300 [05:16&lt;01:46,  1.42s/it]\n\n 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 226/300 [05:17&lt;01:44,  1.41s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 227/300 [05:18&lt;01:42,  1.40s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 228/300 [05:20&lt;01:40,  1.40s/it]\n\n 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 229/300 [05:21&lt;01:39,  1.40s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 230/300 [05:23&lt;01:37,  1.40s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 231/300 [05:24&lt;01:36,  1.40s/it]\n\n 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 232/300 [05:25&lt;01:35,  1.40s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 233/300 [05:27&lt;01:33,  1.40s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 234/300 [05:28&lt;01:32,  1.40s/it]\n\n 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 235/300 [05:30&lt;01:31,  1.40s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 236/300 [05:31&lt;01:29,  1.41s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 237/300 [05:32&lt;01:28,  1.41s/it]\n\n 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 238/300 [05:34&lt;01:26,  1.40s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 239/300 [05:35&lt;01:26,  1.41s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 240/300 [05:37&lt;01:24,  1.41s/it]\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 241/300 [05:38&lt;01:22,  1.40s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 242/300 [05:39&lt;01:21,  1.41s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 243/300 [05:41&lt;01:20,  1.41s/it]\n\n 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 244/300 [05:42&lt;01:18,  1.41s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 245/300 [05:44&lt;01:18,  1.42s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 246/300 [05:45&lt;01:16,  1.41s/it]\n\n 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 247/300 [05:46&lt;01:14,  1.41s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 248/300 [05:48&lt;01:13,  1.40s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 249/300 [05:49&lt;01:11,  1.40s/it]\n\n 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 250/300 [05:51&lt;01:09,  1.40s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 251/300 [05:52&lt;01:08,  1.40s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 252/300 [05:53&lt;01:07,  1.41s/it]\n\n 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 253/300 [05:55&lt;01:06,  1.41s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 254/300 [05:56&lt;01:04,  1.41s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 255/300 [05:58&lt;01:03,  1.41s/it]\n\n 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 256/300 [05:59&lt;01:01,  1.41s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 257/300 [06:01&lt;01:00,  1.41s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 258/300 [06:02&lt;00:58,  1.40s/it]\n\n 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 259/300 [06:03&lt;00:57,  1.40s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 260/300 [06:05&lt;00:55,  1.40s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 261/300 [06:06&lt;00:54,  1.40s/it]\n\n 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 262/300 [06:08&lt;00:53,  1.40s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 263/300 [06:09&lt;00:52,  1.41s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 264/300 [06:10&lt;00:51,  1.42s/it]\n\n 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 265/300 [06:12&lt;00:49,  1.42s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 266/300 [06:13&lt;00:48,  1.41s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 267/300 [06:15&lt;00:46,  1.41s/it]\n\n 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 268/300 [06:16&lt;00:44,  1.40s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 269/300 [06:17&lt;00:43,  1.41s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 270/300 [06:19&lt;00:42,  1.41s/it]\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 271/300 [06:20&lt;00:40,  1.40s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 272/300 [06:22&lt;00:39,  1.42s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 273/300 [06:23&lt;00:38,  1.42s/it]\n\n 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 274/300 [06:24&lt;00:36,  1.41s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 275/300 [06:26&lt;00:35,  1.41s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 276/300 [06:27&lt;00:33,  1.41s/it]\n\n 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 277/300 [06:29&lt;00:32,  1.42s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 278/300 [06:30&lt;00:31,  1.42s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 279/300 [06:32&lt;00:29,  1.41s/it]\n\n 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 280/300 [06:33&lt;00:28,  1.41s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 281/300 [06:34&lt;00:26,  1.41s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 282/300 [06:36&lt;00:25,  1.42s/it]\n\n 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 283/300 [06:37&lt;00:24,  1.42s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 284/300 [06:39&lt;00:22,  1.41s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 285/300 [06:40&lt;00:21,  1.41s/it]\n\n 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 286/300 [06:41&lt;00:19,  1.41s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 287/300 [06:43&lt;00:18,  1.41s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 288/300 [06:44&lt;00:16,  1.41s/it]\n\n 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 289/300 [06:46&lt;00:15,  1.41s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 290/300 [06:47&lt;00:14,  1.41s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 291/300 [06:48&lt;00:12,  1.40s/it]\n\n 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 292/300 [06:50&lt;00:11,  1.41s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 293/300 [06:51&lt;00:09,  1.40s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 294/300 [06:53&lt;00:08,  1.41s/it]\n\n 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 295/300 [06:54&lt;00:07,  1.41s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 296/300 [06:56&lt;00:05,  1.42s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 297/300 [06:57&lt;00:04,  1.41s/it]\n\n 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 298/300 [06:58&lt;00:02,  1.40s/it]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 299/300 [07:00&lt;00:01,  1.41s/it]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [07:01&lt;00:00,  1.41s/it]\n\n20it [2:20:44, 422.25s/it]\n\n\n\n\n\n!cd 10_sub_for_validation/10_sub_for_validation/sub-ADNI002S4262/raw_data && ls\n\nground_truth_mask_sub-ADNI002S4262.nii.gz\ninterpretation_gradcam_mask_mask_T1_sub-ADNI002S4262_ses-M60.nii.gz\ninterpretation_guided_backprop_mask_T1_sub-ADNI002S4262_ses-M60.nii.gz\ninterpretation_mean_pertrub_mask_T1_sub-ADNI002S4262_ses-M60.nii.gz\nsub-ADNI002S4262_ses-M00_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz\nsub-ADNI002S4262_ses-M60_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz\n\n\n\nm_ad =  '10_sub_for_validation/10_sub_for_validation/sub-ADNI002S4262/raw_data/interpretation_mean_pertrub_mask_T1_sub-ADNI002S4262_ses-M60.nii.gz'\nmeanp_ad = nib.load(m_ad)\ndataset_img = data_test.__getitem__(2)\n\n\nfig, axes = plt.subplots(figsize=(16, 8))\n\n# roi_img = nib.Nifti1Image(interp_img, affine=np.eye(4))\nbim_img = nib.Nifti1Image(np.squeeze(dataset_img['image']).cpu().detach().numpy(), affine=np.eye(4))\nplotting.plot_roi(meanp_ad,bim_img, axes=axes, colorbar=True, cmap='jet', threshold=0.35, title='AD')\nplt.show()\n\n\n\n\n\n\n\n\nGround Truth\n\n!pip install antspyx\n\n\nCollecting antspyx\n\n  Downloading antspyx-0.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from antspyx) (2.1.4)\n\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from antspyx) (6.0.2)\n\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from antspyx) (1.26.4)\n\nRequirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from antspyx) (0.14.3)\n\nRequirement already satisfied: webcolors in /usr/local/lib/python3.10/dist-packages (from antspyx) (24.8.0)\n\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from antspyx) (3.7.1)\n\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from antspyx) (10.4.0)\n\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from antspyx) (2.32.3)\n\nRequirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;antspyx) (1.3.0)\n\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;antspyx) (0.12.1)\n\nRequirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;antspyx) (4.53.1)\n\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;antspyx) (1.4.7)\n\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;antspyx) (24.1)\n\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;antspyx) (3.1.4)\n\nRequirement already satisfied: python-dateutil&gt;=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib-&gt;antspyx) (2.8.2)\n\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;antspyx) (2024.2)\n\nRequirement already satisfied: tzdata&gt;=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;antspyx) (2024.1)\n\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;antspyx) (3.3.2)\n\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;antspyx) (3.10)\n\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;antspyx) (2.2.3)\n\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;antspyx) (2024.8.30)\n\nRequirement already satisfied: scipy!=1.9.2,&gt;=1.8 in /usr/local/lib/python3.10/dist-packages (from statsmodels-&gt;antspyx) (1.13.1)\n\nRequirement already satisfied: patsy&gt;=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels-&gt;antspyx) (0.5.6)\n\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy&gt;=0.5.6-&gt;statsmodels-&gt;antspyx) (1.16.0)\n\nDownloading antspyx-0.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.8 MB)\n\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18.8/18.8 MB 33.5 MB/s eta 0:00:00\n\nInstalling collected packages: antspyx\n\nSuccessfully installed antspyx-0.5.3\n\n\n\n\n\n!pip install torchio\n\n\nCollecting torchio\n\n  Downloading torchio-0.20.0-py2.py3-none-any.whl.metadata (50 kB)\n\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 50.6/50.6 kB 3.0 MB/s eta 0:00:00\n\nCollecting Deprecated (from torchio)\n\n  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n\nCollecting SimpleITK!=2.0.*,!=2.1.1.1 (from torchio)\n\n  Downloading SimpleITK-2.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n\nRequirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (from torchio) (4.10.0)\n\nRequirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (from torchio) (4.0.2)\n\nRequirement already satisfied: numpy&gt;=1.15 in /usr/local/lib/python3.10/dist-packages (from torchio) (1.26.4)\n\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torchio) (1.13.1)\n\nRequirement already satisfied: torch&gt;=1.1 in /usr/local/lib/python3.10/dist-packages (from torchio) (2.4.1+cu121)\n\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchio) (4.66.5)\n\nRequirement already satisfied: typer in /usr/local/lib/python3.10/dist-packages (from torchio) (0.12.5)\n\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.1-&gt;torchio) (3.16.1)\n\nRequirement already satisfied: typing-extensions&gt;=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.1-&gt;torchio) (4.12.2)\n\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.1-&gt;torchio) (1.13.3)\n\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.1-&gt;torchio) (3.3)\n\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.1-&gt;torchio) (3.1.4)\n\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.1-&gt;torchio) (2024.6.1)\n\nRequirement already satisfied: wrapt&lt;2,&gt;=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated-&gt;torchio) (1.16.0)\n\nRequirement already satisfied: packaging&gt;=17.0 in /usr/local/lib/python3.10/dist-packages (from nibabel-&gt;torchio) (24.1)\n\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nibabel-&gt;torchio) (71.0.4)\n\nRequirement already satisfied: click&gt;=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer-&gt;torchio) (8.1.7)\n\nRequirement already satisfied: shellingham&gt;=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer-&gt;torchio) (1.5.4)\n\nRequirement already satisfied: rich&gt;=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer-&gt;torchio) (13.8.1)\n\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich&gt;=10.11.0-&gt;typer-&gt;torchio) (3.0.0)\n\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich&gt;=10.11.0-&gt;typer-&gt;torchio) (2.18.0)\n\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;torch&gt;=1.1-&gt;torchio) (2.1.5)\n\nRequirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy-&gt;torch&gt;=1.1-&gt;torchio) (1.3.0)\n\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich&gt;=10.11.0-&gt;typer-&gt;torchio) (0.1.2)\n\nDownloading torchio-0.20.0-py2.py3-none-any.whl (175 kB)\n\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 175.3/175.3 kB 10.5 MB/s eta 0:00:00\n\nDownloading SimpleITK-2.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.4 MB)\n\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 52.4/52.4 MB 18.4 MB/s eta 0:00:00\n\nDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n\nInstalling collected packages: SimpleITK, Deprecated, torchio\n\nSuccessfully installed Deprecated-1.2.14 SimpleITK-2.4.0 torchio-0.20.0\n\n\n\n\n\nimport ants\nimport torchio as tio\nfrom torchio.transforms import HistogramStandardization\n\n\nad = f'10_sub_for_validation/10_sub_for_validation/sub-ADNI002S4262/raw_data/sub-ADNI002S4262_ses-M60_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz'\ncn = f'10_sub_for_validation/10_sub_for_validation/sub-ADNI002S4262/raw_data/sub-ADNI002S4262_ses-M00_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz'\nad_img = nib.load(ad)\ncn_img = nib.load(cn)\n\n\nref =  ants.image_read(ad)\ncn_caps =  ants.image_read(cn)\ncn_to_ref_caps = ants.registration(ref, cn_caps,'Rigid')\n# mask_to_ref = ants.apply_transforms(ref, mask, transformlist = [ad_to_ref['fwdtransforms'][0]])\n# ants.image_write(mask_to_ref, 'mask_ad_to_ref.nii.gz', ri=False)\ncn_reg_caps = ants.apply_transforms(ref, cn_caps,transformlist = [cn_to_ref_caps['fwdtransforms'][0]])\nants.image_write(cn_reg_caps, os.path.join(f'cn_00_to_ref_ad_60_wo_skull.nii.gz'), ri=False)\nants.plot(cn_reg_caps)\n\n\n\n\n\n\n\n\n\ncn_hist = f'cn_00_to_ref_ad_60_wo_skull.nii.gz'\nad_hist = f'10_sub_for_validation/10_sub_for_validation/sub-ADNI002S4262/raw_data/sub-ADNI002S4262_ses-M60_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz'\n\ndata = [ad_hist,cn_hist]\nlandmarks = HistogramStandardization.train(data)\nhist_standardize = tio.HistogramStandardization({'mri':landmarks})\nsubjects_list = []\nfor each in data:\n\n    subject = tio.Subject(\n        mri=tio.ScalarImage(each)\n    )\n    subjects_list.append(subject)\nfor i, sample in enumerate(subjects_list):\n    transformed = hist_standardize(sample)\n    print(str(transformed.mri.path).split('/')[-1])\n    transformed.mri.save('hist_{}'.format(str(transformed.mri.path).split('/')[-1]))\n\n/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n\n\n\n\n\nsub-ADNI002S4262_ses-M60_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz\ncn_00_to_ref_ad_60_wo_skull.nii.gz\n\n\n\n!ls\n\n10_sub_for_validation\n10_sub_for_validation.zip\ncn_00_to_ref_ad_60_wo_skull.nii.gz\ndrive\ngrad_cam_mean.png\nhist_cn_00_to_ref_ad_60_wo_skull.nii.gz\nhist_sub-ADNI002S4262_ses-M60_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz\nsample_data\n\n\n\nhist_cn = nib.load(f'hist_cn_00_to_ref_ad_60_wo_skull.nii.gz')\nhist_ad = nib.load(f'hist_sub-ADNI002S4262_ses-M60_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz')\n\n\nnp.sum(hist_cn.dataobj)\n\n111501900.0\n\n\n\nfig, axes = plt.subplots(figsize=(10,5))\n# roi_img = nib.Nifti1Image(mask_cn.dataobj, affine=np.eye(4))\n# bim_img  = nib.Nifti1Image(diff_ad_cn,affine=ad_img.affine)\nplotting.plot_roi(hist_ad, axes=axes,colorbar=True,cmap='gray')\nplt.show()\n\n\ndiff_hist_cn_ad = np.array(hist_cn.dataobj) - np.array(hist_ad.dataobj)\ndiff_hist_cn_ad = diff_hist_cn_ad.astype('int16')\n\n\ndiff_hist_cn_ad.min(), diff_hist_cn_ad.max(), np.array(hist_ad.dataobj).min(), np.array(hist_ad.dataobj).max()\n\n\nfig, axes = plt.subplots(figsize=(10,5))\n# roi_img = nib.Nifti1Image(mask_cn.dataobj, affine=np.eye(4))\nbim_img  = nib.Nifti1Image(diff_hist_cn_ad,affine=hist_ad.affine)\nplotting.plot_roi(bim_img,hist_ad, axes=axes,colorbar=True,cmap='jet')\nplt.show()\n\n\ndiff_hist_cn_ad[diff_hist_cn_ad &lt; 0] = 0\n\n\nfig, axes = plt.subplots(figsize=(10,5))\n# roi_img = nib.Nifti1Image(mask_cn.dataobj, affine=np.eye(4))\nbim_img  = nib.Nifti1Image(diff_hist_cn_ad,affine=hist_ad.affine)\nplotting.plot_roi(bim_img,hist_ad, axes=axes,colorbar=True,cmap='jet')\nplt.show()"
  },
  {
    "objectID": "projects/CLIP_simple_implementation.html",
    "href": "projects/CLIP_simple_implementation.html",
    "title": "Intro",
    "section": "",
    "text": "It was in January of 2021 that OpenAI announced two new models: DALL-E and CLIP, both multi-modality models connecting texts and images in some way. In this article we are going to implement CLIP model from scratch in PyTorch. OpenAI has open-sourced some of the code relating to CLIP model but I found it intimidating and it was far from something short and simple. I also came across a good tutorial inspired by CLIP model on Keras code examples and I translated some parts of it into PyTorch to build this tutorial totally with our beloved PyTorch!"
  },
  {
    "objectID": "projects/CLIP_simple_implementation.html#some-pre-preocessing",
    "href": "projects/CLIP_simple_implementation.html#some-pre-preocessing",
    "title": "Intro",
    "section": "Some pre-preocessing",
    "text": "Some pre-preocessing\n\nif dataset == \"8k\":\n  df = pd.read_csv(\"captions.txt\")\n  df['id'] = [id_ for id_ in range(df.shape[0] // 5) for _ in range(5)]\n  df.to_csv(\"captions.csv\", index=False)\n  df = pd.read_csv(\"captions.csv\")\n  image_path = \"/content/Images\"\n  captions_path = \"/content\"\nelif dataset == \"30k\":\n  df = pd.read_csv(\"/content/flickr30k_images/results.csv\", delimiter=\"|\")\n  df.columns = ['image', 'caption_number', 'caption']\n  df['caption'] = df['caption'].str.lstrip()\n  df['caption_number'] = df['caption_number'].str.lstrip()\n  df.loc[19999, 'caption_number'] = \"4\"\n  df.loc[19999, 'caption'] = \"A dog runs across the grass .\"\n  ids = [id_ for id_ in range(len(df) // 5) for _ in range(5)]\n  df['id'] = ids\n  df.to_csv(\"captions.csv\", index=False)\n  image_path = \"/content/flickr30k_images/flickr30k_images\"\n  captions_path = \"/content\"\n\ndf.head()"
  },
  {
    "objectID": "projects/CLIP_simple_implementation.html#config",
    "href": "projects/CLIP_simple_implementation.html#config",
    "title": "Intro",
    "section": "Config",
    "text": "Config\nA note on config and CFG: I wrote the codes with python scripts and then converted it into a Jupyter Notebook. So, in case of python scripts, config is a normal python file where I put all the hyperparameters and in the case of Jupyter Notebook, its a class defined in the beginning of the notebook to keep all the hyperparameters.\n\nclass CFG:\n    debug = False\n    image_path = image_path\n    captions_path = captions_path\n    batch_size = 32\n    num_workers = 2\n    head_lr = 1e-3\n    image_encoder_lr = 1e-4\n    text_encoder_lr = 1e-5\n    weight_decay = 1e-3\n    patience = 1\n    factor = 0.8\n    epochs = 4\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    model_name = 'resnet50'\n    image_embedding = 2048\n    text_encoder_model = \"distilbert-base-uncased\"\n    text_embedding = 768\n    text_tokenizer = \"distilbert-base-uncased\"\n    max_length = 200\n\n    pretrained = True # for both image encoder and text encoder\n    trainable = True # for both image encoder and text encoder\n    temperature = 1.0\n\n    # image size\n    size = 224\n\n    # for projection head; used for both image and text encoders\n    num_projection_layers = 1\n    projection_dim = 256\n    dropout = 0.1"
  },
  {
    "objectID": "projects/CLIP_simple_implementation.html#utils",
    "href": "projects/CLIP_simple_implementation.html#utils",
    "title": "Intro",
    "section": "Utils",
    "text": "Utils\n\nclass AvgMeter:\n    def __init__(self, name=\"Metric\"):\n        self.name = name\n        self.reset()\n\n    def reset(self):\n        self.avg, self.sum, self.count = [0] * 3\n\n    def update(self, val, count=1):\n        self.count += count\n        self.sum += val * count\n        self.avg = self.sum / self.count\n\n    def __repr__(self):\n        text = f\"{self.name}: {self.avg:.4f}\"\n        return text\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group[\"lr\"]"
  },
  {
    "objectID": "projects/CLIP_simple_implementation.html#dataset",
    "href": "projects/CLIP_simple_implementation.html#dataset",
    "title": "Intro",
    "section": "Dataset",
    "text": "Dataset\nAs you can see in the tittle image of this article, we need to encode both images and their describing texts. So, the dataset needs to return both images and texts. Of course we are not going to feed raw text to our text encoder! We will use DistilBERT model (which is smaller than BERT but performs nearly as well as BERT) from HuggingFace library as our text encoder; so, we need to tokenize the sentences (captions) with DistilBERT tokenizer and then feed the token ids (input_ids) and the attention masks to DistilBERT. Therefore, the dataset needs to take care of the tokenization as well. Below you can see the dataset‚Äôs code. Below that I‚Äôll explain the most important things that is happening in the code.\nIn the __init__ we receive a tokenizer object which is actually a HuggingFace tokinzer; this tokenizer will be loaded when running the model. We are padding and truncating the captions to a specified max_length. In the __getitem__ we will first load an encoded caption which is a dictionary with keys input_ids and attention_mask, make tensors out of its values and after that we will load the corresponding image, transform and augment it (if there is any!) and then we make it a tensor and put it in the dictionary with ‚Äúimage‚Äù as the key. Finally we put the raw text of the caption with the key ‚Äúcaption‚Äù in the dictionary only for visualization purposes.\nI did not use additional data augmentations but you can add them if you want to improve the model‚Äôs performance.\n\nclass CLIPDataset(torch.utils.data.Dataset):\n    def __init__(self, image_filenames, captions, tokenizer, transforms):\n        \"\"\"\n        image_filenames and cpations must have the same length; so, if there are\n        multiple captions for each image, the image_filenames must have repetitive\n        file names\n        \"\"\"\n\n        self.image_filenames = image_filenames\n        self.captions = list(captions)\n        self.encoded_captions = tokenizer(\n            list(captions), padding=True, truncation=True, max_length=CFG.max_length\n        )\n        self.transforms = transforms\n\n    def __getitem__(self, idx):\n        item = {\n            key: torch.tensor(values[idx])\n            for key, values in self.encoded_captions.items()\n        }\n\n        image = cv2.imread(f\"{CFG.image_path}/{self.image_filenames[idx]}\")\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = self.transforms(image=image)['image']\n        item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n        item['caption'] = self.captions[idx]\n\n        return item\n\n\n    def __len__(self):\n        return len(self.captions)\n\n\n\ndef get_transforms(mode=\"train\"):\n    if mode == \"train\":\n        return A.Compose(\n            [\n                A.Resize(CFG.size, CFG.size, always_apply=True),\n                A.Normalize(max_pixel_value=255.0, always_apply=True),\n            ]\n        )\n    else:\n        return A.Compose(\n            [\n                A.Resize(CFG.size, CFG.size, always_apply=True),\n                A.Normalize(max_pixel_value=255.0, always_apply=True),\n            ]\n        )"
  },
  {
    "objectID": "projects/CLIP_simple_implementation.html#image-encoder",
    "href": "projects/CLIP_simple_implementation.html#image-encoder",
    "title": "Intro",
    "section": "Image Encoder",
    "text": "Image Encoder\nThe image encoder code is straight forward. I‚Äôm using PyTorch Image Models library (timm) here which makes a lot of different image models available from ResNets to EfficientNets and many more. Here we will use a ResNet50 as our image encoder. You can easily use torchvision library to use ResNets if you don‚Äôt want to install a new library.\nThe code encodes each image to a fixed size vector with the size of the model‚Äôs output channels (in case of ResNet50 the vector size will be 2048). This is the output after the nn.AdaptiveAvgPool2d() layer.\n\nclass ImageEncoder(nn.Module):\n    \"\"\"\n    Encode images to a fixed size vector\n    \"\"\"\n\n    def __init__(\n        self, model_name=CFG.model_name, pretrained=CFG.pretrained, trainable=CFG.trainable\n    ):\n        super().__init__()\n        self.model = timm.create_model(\n            model_name, pretrained, num_classes=0, global_pool=\"avg\"\n        )\n        for p in self.model.parameters():\n            p.requires_grad = trainable\n\n    def forward(self, x):\n        return self.model(x)"
  },
  {
    "objectID": "projects/CLIP_simple_implementation.html#text-encoder",
    "href": "projects/CLIP_simple_implementation.html#text-encoder",
    "title": "Intro",
    "section": "Text Encoder",
    "text": "Text Encoder\nAs I mentioned before, I‚Äôll use DistilBERT as the text encoder. Like its bigger brother BERT, two special tokens will be added to the actual input tokens: CLS and SEP which mark the start and end of a sentence. To grab the whole representation of a sentence (as the related BERT and DistilBERT papers point out) we use the final representations of the CLS token and we hope that this representation captures the overall meaning of the sentence (caption). Thinking it in this way, it is similar to what we did to images and converted them into a fixed size vector.\nIn the case of DistilBERT (and also BERT) the output hidden representation for each token is a vector with size 768. So, the whole caption will be encoded in the CLS token representation whose size is 768.\n\nclass TextEncoder(nn.Module):\n    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable):\n        super().__init__()\n        if pretrained:\n            self.model = DistilBertModel.from_pretrained(model_name)\n        else:\n            self.model = DistilBertModel(config=DistilBertConfig())\n\n        for p in self.model.parameters():\n            p.requires_grad = trainable\n\n        # we are using the CLS token hidden representation as the sentence's embedding\n        self.target_token_idx = 0\n\n    def forward(self, input_ids, attention_mask):\n        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        last_hidden_state = output.last_hidden_state\n        return last_hidden_state[:, self.target_token_idx, :]"
  },
  {
    "objectID": "projects/CLIP_simple_implementation.html#projection-head",
    "href": "projects/CLIP_simple_implementation.html#projection-head",
    "title": "Intro",
    "section": "Projection Head",
    "text": "Projection Head\nI used Keras code example implementation of projection head to write the following in PyTorch. Now that we have encoded both our images and texts into fixed size vectors (2048 for image and 768 for text) we need to bring (project) them into a new world (!) with similar dimensions for both images and texts in order to be able to compare them and push apart the non-relevant image and texts and pull together those that match. So, the following code will bring the 2048 and 768 dimensional vectors into a 256 (projection_dim) dimensional world, where we can compare them.\n‚Äúembedding_dim‚Äù is the size of the input vector (2048 for images and 768 for texts) and ‚Äúprojection_dim‚Äù is the the size of the output vector which will be 256 for our case. For understanding the details of this part you can refer to the CLIP paper.\n\nclass ProjectionHead(nn.Module):\n    def __init__(\n        self,\n        embedding_dim,\n        projection_dim=CFG.projection_dim,\n        dropout=CFG.dropout\n    ):\n        super().__init__()\n        self.projection = nn.Linear(embedding_dim, projection_dim)\n        self.gelu = nn.GELU()\n        self.fc = nn.Linear(projection_dim, projection_dim)\n        self.dropout = nn.Dropout(dropout)\n        self.layer_norm = nn.LayerNorm(projection_dim)\n\n    def forward(self, x):\n        projected = self.projection(x)\n        x = self.gelu(projected)\n        x = self.fc(x)\n        x = self.dropout(x)\n        x = x + projected\n        x = self.layer_norm(x)\n        return x"
  },
  {
    "objectID": "projects/CLIP_simple_implementation.html#clip",
    "href": "projects/CLIP_simple_implementation.html#clip",
    "title": "Intro",
    "section": "CLIP",
    "text": "CLIP\nThis part is where all the fun happens! I‚Äôll also talk about the loss function here. I translated some of the code from Keras code examples into PyTorch for writing this part. Take a look at the code and then read the explanation below this code block.\nHere we will use the previous modules that we built to implement the main model. The __init__ function is self-explanatory. In the forward function, we first encode the images and texts separately into fixed size vectors (with different dimensionalities). After that, using separate projection modules we project them to that shared world (space) that I talked about previously. Here the encodings will become of similar shape (256 in our case). After that we will compute the loss. Again I recommend reading CLIP paper to get it better but I‚Äôll try my best to explain this part.\nIn Linear Algebra, one common way to measure if two vectors are of similar characteristics (they are like each other) is to calculate their dot product (multiplying the matching entries and take the sum of them); if the final number is big, they are alike and if it is small they are not (relatively speaking)!\nOkay! What I just said is the most important thing to have in mind to understand this loss function. Let‚Äôs continue. We talked about two vectors, but, what do we have here? We have image_embeddings, a matrix with shape (batch_size, 256) and text_embeddings with shape (batch_size, 256). Easy enough! it means we have two groups of vectors instead of two single vectors. How do we measure how similar two groups of vectors (two matrices) are to each other? Again, with dot product (@ operator in PyTorch does the dot product or matrix multiplication in this case). To be able to multiply these two matrices together, we transpose the second one. Okay, we get a matrix with shape (batch_size, batch_size) which we will call logits. (temperature is equal to 1.0 in our case, so, it does not make a difference. You can play with it and see what difference it makes. Also look at the paper to see why it is here!).\nI hope you are still with me! If not it‚Äôs okay, just review the code and check their shapes. Now that we have our logits, we need targets. I need to say that there is a more straight forward way to obtain targets but I had to do this for our case (I‚Äôll talk about why in a next paragraph).\nLet‚Äôs consider what we hope that this model learns: we want it to learn ‚Äúsimilar representations (vectors)‚Äù for a given image and the caption describing it. Meaning that either we give it an image or the text describing it, we want it to produce same 256 sized vectors for both.\n\nCheck the cell below this code block for the continue of the explanations\n\nclass CLIPModel(nn.Module):\n    def __init__(\n        self,\n        temperature=CFG.temperature,\n        image_embedding=CFG.image_embedding,\n        text_embedding=CFG.text_embedding,\n    ):\n        super().__init__()\n        self.image_encoder = ImageEncoder()\n        self.text_encoder = TextEncoder()\n        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n        self.temperature = temperature\n\n    def forward(self, batch):\n        # Getting Image and Text Features\n        image_features = self.image_encoder(batch[\"image\"])\n        text_features = self.text_encoder(\n            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n        )\n        # Getting Image and Text Embeddings (with same dimension)\n        image_embeddings = self.image_projection(image_features)\n        text_embeddings = self.text_projection(text_features)\n\n        # Calculating the Loss\n        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n        images_similarity = image_embeddings @ image_embeddings.T\n        texts_similarity = text_embeddings @ text_embeddings.T\n        targets = F.softmax(\n            (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n        )\n        texts_loss = cross_entropy(logits, targets, reduction='none')\n        images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n        loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)\n        return loss.mean()\n\n\ndef cross_entropy(preds, targets, reduction='none'):\n    log_softmax = nn.LogSoftmax(dim=-1)\n    loss = (-targets * log_softmax(preds)).sum(1)\n    if reduction == \"none\":\n        return loss\n    elif reduction == \"mean\":\n        return loss.mean()\n\nSo, in the best case scenario, text_embeddings and image_embedding matricies should be the same because they are describing similar things. Let‚Äôs think now: if this happens, what would the logits matrix be like? Let‚Äôs see with a simple example!\n\n# A simple Example\n\nbatch_size = 4\ndim = 256\nembeddings = torch.randn(batch_size, dim)\nout = embeddings @ embeddings.T\nprint(F.softmax(out, dim=-1))\n\nSo logits, in the best case, will be a matrix that if we take its softmax, will have 1.0s in the diagonal (An identity matrix to call it with fancy words!). As the loss function‚Äôs job is to make model‚Äôs predictions similar to targets (at least in most cases!), we want such a matrix as our target. That‚Äôs the reason why we are calculating images_similarity and texts_similarity matrices in the code block above.\nNow that we‚Äôve got our targets matrix, we will use simple cross entropy to calculate the actual loss. I‚Äôve written the full matrix form of cross entropy as a function which you can see in the bottom of the code block. Okay! We are done! Wasn‚Äôt it simple?! Alright, you can ignore the next paragraph but if you are curious, there is an important note in that.\nHere‚Äôs why I didn‚Äôt use a simpler approach: I need to admit that there‚Äôs a simpler way to calculate this loss in PyTorch; by doing this: nn.CrossEntropyLoss()(logits, torch.arange(batch_size)). Why I did not use it here? For 2 reasons. 1- The dataset we are using has multiple captions for a single image; so, there is the possibility that two identical images with their similar captions exist in a batch (it is rare but it can happen). Taking the loss with this easier method will ignore this possibility and the model learns to pull apart two representations (assume them different) that are actually the same. Obviously, we don‚Äôt want this to happen so I calculated the whole target matrix in a way that takes care of these edge cases. 2- Doing it the way I did, gave me a better understanding of what is happening in this loss function; so, I thought it would give you a better intuition as well!"
  },
  {
    "objectID": "projects/CLIP_simple_implementation.html#train",
    "href": "projects/CLIP_simple_implementation.html#train",
    "title": "Intro",
    "section": "Train",
    "text": "Train\nHere are some funtions to help us load train and valid dataloaders, our model and then train and evaluate our model on those. There‚Äôs not much going on here; just simple training loop and utility functions\n\ndef make_train_valid_dfs():\n    dataframe = pd.read_csv(f\"{CFG.captions_path}/captions.csv\")\n    max_id = dataframe[\"id\"].max() + 1 if not CFG.debug else 100\n    image_ids = np.arange(0, max_id)\n    np.random.seed(42)\n    valid_ids = np.random.choice(\n        image_ids, size=int(0.2 * len(image_ids)), replace=False\n    )\n    train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n    train_dataframe = dataframe[dataframe[\"id\"].isin(train_ids)].reset_index(drop=True)\n    valid_dataframe = dataframe[dataframe[\"id\"].isin(valid_ids)].reset_index(drop=True)\n    return train_dataframe, valid_dataframe\n\n\ndef build_loaders(dataframe, tokenizer, mode):\n    transforms = get_transforms(mode=mode)\n    dataset = CLIPDataset(\n        dataframe[\"image\"].values,\n        dataframe[\"caption\"].values,\n        tokenizer=tokenizer,\n        transforms=transforms,\n    )\n    dataloader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=CFG.batch_size,\n        num_workers=CFG.num_workers,\n        shuffle=True if mode == \"train\" else False,\n    )\n    return dataloader\n\nHere‚Äôs a handy function to train our model. There‚Äôs not much happening here; just loading the batches, feeding them to the model and stepping the optimizer and lr_scheduler.\n\ndef train_epoch(model, train_loader, optimizer, lr_scheduler, step):\n    loss_meter = AvgMeter()\n    tqdm_object = tqdm(train_loader, total=len(train_loader))\n    for batch in tqdm_object:\n        batch = {k: v.to(CFG.device) for k, v in batch.items() if k != \"caption\"}\n        loss = model(batch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        if step == \"batch\":\n            lr_scheduler.step()\n\n        count = batch[\"image\"].size(0)\n        loss_meter.update(loss.item(), count)\n\n        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))\n    return loss_meter\n\n\ndef valid_epoch(model, valid_loader):\n    loss_meter = AvgMeter()\n\n    tqdm_object = tqdm(valid_loader, total=len(valid_loader))\n    for batch in tqdm_object:\n        batch = {k: v.to(CFG.device) for k, v in batch.items() if k != \"caption\"}\n        loss = model(batch)\n\n        count = batch[\"image\"].size(0)\n        loss_meter.update(loss.item(), count)\n\n        tqdm_object.set_postfix(valid_loss=loss_meter.avg)\n    return loss_meter\n\n\ndef main():\n    train_df, valid_df = make_train_valid_dfs()\n    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n    train_loader = build_loaders(train_df, tokenizer, mode=\"train\")\n    valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n\n\n    model = CLIPModel().to(CFG.device)\n    params = [\n        {\"params\": model.image_encoder.parameters(), \"lr\": CFG.image_encoder_lr},\n        {\"params\": model.text_encoder.parameters(), \"lr\": CFG.text_encoder_lr},\n        {\"params\": itertools.chain(\n            model.image_projection.parameters(), model.text_projection.parameters()\n        ), \"lr\": CFG.head_lr, \"weight_decay\": CFG.weight_decay}\n    ]\n    optimizer = torch.optim.AdamW(params, weight_decay=0.)\n    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode=\"min\", patience=CFG.patience, factor=CFG.factor\n    )\n    step = \"epoch\"\n\n    best_loss = float('inf')\n    for epoch in range(CFG.epochs):\n        print(f\"Epoch: {epoch + 1}\")\n        model.train()\n        train_loss = train_epoch(model, train_loader, optimizer, lr_scheduler, step)\n        model.eval()\n        with torch.no_grad():\n            valid_loss = valid_epoch(model, valid_loader)\n\n        if valid_loss.avg &lt; best_loss:\n            best_loss = valid_loss.avg\n            torch.save(model.state_dict(), \"best.pt\")\n            print(\"Saved Best Model!\")\n\n        lr_scheduler.step(valid_loss.avg)\n\nRunning the next cell start training the model. Put the kernel on GPU mode. Every epoch should take about 8 minutes on GPU if you are using 8k version (even one epoch is enough!). It can take some seconds before training actually starts because we are going to encode all the captions once in the train and valid dataset, so please don‚Äôt stop it! Every thing is working fine.\n\nmain()"
  },
  {
    "objectID": "projects/CLIP_simple_implementation.html#inference",
    "href": "projects/CLIP_simple_implementation.html#inference",
    "title": "Intro",
    "section": "Inference",
    "text": "Inference\nOkay! We are done with training the model. Now, we need to do inference which in our case will be giving the model a piece of text and want it to retrieve the most relevant images from an unseen validation (or test) set.\n\nGetting Image Embeddings\nIn this function, we are loading the model that we saved after training, feeding it images in validation set and returning the image_embeddings with shape (valid_set_size, 256) and the model itself.\n\ndef get_image_embeddings(valid_df, model_path):\n    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n    valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n\n    model = CLIPModel().to(CFG.device)\n    model.load_state_dict(torch.load(model_path, map_location=CFG.device))\n    model.eval()\n\n    valid_image_embeddings = []\n    with torch.no_grad():\n        for batch in tqdm(valid_loader):\n            image_features = model.image_encoder(batch[\"image\"].to(CFG.device))\n            image_embeddings = model.image_projection(image_features)\n            valid_image_embeddings.append(image_embeddings)\n    return model, torch.cat(valid_image_embeddings)\n\n\n_, valid_df = make_train_valid_dfs()\nmodel, image_embeddings = get_image_embeddings(valid_df, \"best.pt\")\n\n\n\nFinding Matches\nThis function does the final task that we wished our model would be capable of: it gets the model, image_embeddings, and a text query. It will display the most relevant images from the validation set! Isn‚Äôt it amazing? Let‚Äôs see how it performs after all!\n\ndef find_matches(model, image_embeddings, query, image_filenames, n=9):\n    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n    encoded_query = tokenizer([query])\n    batch = {\n        key: torch.tensor(values).to(CFG.device)\n        for key, values in encoded_query.items()\n    }\n    with torch.no_grad():\n        text_features = model.text_encoder(\n            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n        )\n        text_embeddings = model.text_projection(text_features)\n\n    image_embeddings_n = F.normalize(image_embeddings, p=2, dim=-1)\n    text_embeddings_n = F.normalize(text_embeddings, p=2, dim=-1)\n    dot_similarity = text_embeddings_n @ image_embeddings_n.T\n\n    values, indices = torch.topk(dot_similarity.squeeze(0), n * 5)\n    matches = [image_filenames[idx] for idx in indices[::5]]\n\n    _, axes = plt.subplots(3, 3, figsize=(10, 10))\n    for match, ax in zip(matches, axes.flatten()):\n        image = cv2.imread(f\"{CFG.image_path}/{match}\")\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        ax.imshow(image)\n        ax.axis(\"off\")\n\n    plt.show()\n\nThis is how we use this function. Aaaannnndddd the results: (The results in the blog post and the one at the beginning of the notebook were achieved with training on the 30k version)\n\nfind_matches(model,\n             image_embeddings,\n             query=\"dogs on the grass\",\n             image_filenames=valid_df['image'].values,\n             n=9)"
  },
  {
    "objectID": "projects/CLIP_simple_implementation.html#final-words",
    "href": "projects/CLIP_simple_implementation.html#final-words",
    "title": "Intro",
    "section": "Final words",
    "text": "Final words\nI hope you have enjoyed this article. Implementing this paper was a really interesting experience for me."
  },
  {
    "objectID": "projects/GANs1.html",
    "href": "projects/GANs1.html",
    "title": "GANs",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nIn this seminar, you will learn how to\nRelated works and repositories to consider: * The seminal Generative Adversarial Networks paper by Ian Goodfellow et. al.; * NIPS 2016 tutorial on Generative Adversarial Networks by Ian Goodfellow (saturating and non-saturating GANs covered);  * Deep Generative Models course at MIPT, github. This seminar is partially based on the materials of this course."
  },
  {
    "objectID": "projects/GANs1.html#generative-modelling-problem",
    "href": "projects/GANs1.html#generative-modelling-problem",
    "title": "GANs",
    "section": "0. Generative modelling problem",
    "text": "0. Generative modelling problem\nWe have samples \\(\\{x_1, x_2, \\dots, x_n\\} = X \\sim \\mathbb{P}(x)\\). The distribution \\(\\mathbb{P} \\in \\mathcal{P}(\\mathbb{R}^{D_x})\\) is unknown.\nWe look for parametric approximation \\(p(x|\\theta), \\theta \\in \\Theta\\) of \\(\\mathbb{P}(x)\\), and pick up \\(\\theta^*\\) such that \\(\\mathbb{P}(x) \\approx p(x|\\theta^*)\\).\nWe want to estimate \\(\\hat{\\theta^*} = \\hat{\\theta^*}(X)\\) in order to\n\nsample from \\(p(x|\\hat{\\theta^*})\\)  (Primary goal) \nestimate the corresponding pdf.  (Less relevant) \n\n\n0.1 Latent Variable Model\n\n\n\nLikelihood\n\\(p(x |\\theta) = \\int\\limits_{\\mathbb{R}^{D_z}} p(x | z, \\theta) p_z(z| \\theta) dz\\)\n\nTypically, latent distribution \\(p_z(z| \\theta)\\) is simple and does not depend on \\(\\theta\\): \\(p_z(z| \\theta) = p_z(z)\\)\n\nSampling procedure\n\nSample \\(z^* \\sim p_z(z | \\theta)\\)\nSample \\(x \\sim p(x | z^*, \\theta)\\)"
  },
  {
    "objectID": "projects/GANs1.html#toy-2d-problem-setup",
    "href": "projects/GANs1.html#toy-2d-problem-setup",
    "title": "GANs",
    "section": "1. Toy \\(2D\\) problem setup",
    "text": "1. Toy \\(2D\\) problem setup\nSetting up reference distribution \\(\\mathbb{P}\\)\n\ndef generate_2d_data(size : int, var : float = 0.02) -&gt; np.ndarray:\n    scale = 2\n    centers = [\n        (1, 0),\n        (-1, 0),\n        (0, 1),\n        (0, -1),\n        (1. / np.sqrt(2), 1. / np.sqrt(2)),\n        (1. / np.sqrt(2), -1. / np.sqrt(2)),\n        (-1. / np.sqrt(2), 1. / np.sqrt(2)),\n        (-1. / np.sqrt(2), -1. / np.sqrt(2))\n    ]\n\n    centers = [(scale * x, scale * y) for x, y in centers]\n    dataset = []\n\n    for i in range(size):\n        point = np.random.randn(2) * var\n        center = centers[np.random.choice(np.arange(len(centers)))]\n        point[0] += center[0]\n        point[1] += center[1]\n        dataset.append(point)\n\n    dataset = np.array(dataset, dtype='float32')\n    dataset /= 1.414  # stdev\n\n    return dataset\n\nGenerate and visualize training samples \\(X \\sim \\mathbb{P}\\)\n\nCOUNT = 20000\n\ntrain_data = generate_2d_data(COUNT, var=0.1)\nvisualize_2d_samples(train_data[:1000], \"Train data\", colors='tomato')"
  },
  {
    "objectID": "projects/GANs1.html#gan-losses",
    "href": "projects/GANs1.html#gan-losses",
    "title": "GANs",
    "section": "2. GAN losses",
    "text": "2. GAN losses\nIn this part of the seminar you will get into details and implement Vanillan GAN and Non-saturating Vanilla GAN models. Then you will apply these models on the toy \\(2D\\) problem from above.\n\n2.0. GAN preliminaries\nGenerative Adversarial Networks parameterizes the distributions \\(p(x \\vert \\theta)\\) by means of\n\n\nSimple\n\n, pre-defined, typically  low-dimensional  latent distribution \\(p_z(z)\\), e.g., Gaussian or Uniform\nA parametric generator model \\(G_{\\theta} : \\mathbb{R}^{D_z} \\rightarrow \\mathbb{R}^{D_x}\\) which maps latent space to data space\n\nFormal probability model. Generator \\(G_{\\theta}\\) coupled with latent distribution \\(p_z(z)\\) forms the parametric distribution \\(p(x \\vert \\theta)\\) which is the distribution of points \\(x = G_{\\theta}(z)\\), where \\(z \\sim p_z(z)\\). I.e.,\n\\[\np(x \\vert \\theta) = p_z\\big(G_{\\theta}^{-1}(x)\\big).\n\\]\nüîé Remark. In the literature, \\(p(x \\vert \\theta)\\) typically called as pushforward of \\(p_z(z)\\) under \\(G_{\\theta}\\):\n\\[\np(\\cdot \\vert \\theta) = G_{\\theta}\\sharp p_z\n\\]\nSampling from \\(p(x \\vert \\theta)\\) procedure\n\nSample \\(z^* \\sim p_z(z)\\)\nObtain \\(x = G_{\\theta}(z^*)\\)\n\n\nTraining GANs\nWe need to adjust the optimal NN parameters \\(\\hat{\\theta^*}\\) based on samples \\(X \\sim \\mathbb{P}\\) such that \\(\\mathbb{P}(x) \\approx p(x|\\hat{\\theta^*})\\)\nThis can be done by choosing a discrepancy\n\\[\\mathfrak{D} : \\mathcal{P}(\\mathbb{R}^{D_x}) \\times \\mathcal{P}(\\mathbb{R}^{D_x}) \\rightarrow \\mathbb{R}\\]\nbetween probability distributions and minimizing \\(\\mathfrak{D}(p(x|\\theta), \\mathbb{P})\\) based on samples \\(X\\).\n‚ùî What discrepancies \\(\\mathfrak{D}\\) do you know? Which of them are used in generative modelling?\n\nIn GAN setup, the discrepancy is typically follow the variational manner, i.e., it is defined as an auxiliary optimization problem w.r.t. a discriminator (or critic) \\(D: \\mathbb{R}^{D_x} \\rightarrow \\mathbb{R}\\): \\[\\mathfrak{D}(p(x|\\theta), \\mathbb{P}) = \\sup_{D} V(G_{\\theta}, D)\\]\n\n\nDiscriminator \\(D\\) is parameterized as a NN: \\(D = D_{\\phi}, \\phi \\in \\Phi\\)\nTypically, the ‚Äúphysical‚Äù role of discriminator \\(D\\) is to distinguis real samples \\(X\\) from generated samples \\(X_{\\text{gen}} \\sim G_{\\theta}\\sharp p_z\\)\nThe resulting Generative Modelling problem is solved by \\(\\min - \\max\\) adversarial optimization problem:\n\n\\[\\min_{G_{\\theta}} \\max_{D_{\\phi}} V(G_{\\theta}, D_{\\phi}) = \\min_{\\theta \\in \\Theta} \\max_{\\phi \\in \\Phi} V(G_{\\theta}, D_{\\phi}) \\rightarrow \\hat{\\theta^*}, \\hat{\\phi^*}\\]\n\n\nParameterizing with Neural Networks\nNow let‚Äôs gradually move to practice. Define a class for MultiLayerPerceptron. It will be used for modelling generators \\(G_{\\theta}\\) and discriminators \\(D_{\\phi}\\) of our GAN models on the toy 2D problem.\n\nclass FullyConnectedMLP(nn.Module):\n\n    def __init__(\n        self,\n        input_dim : int,\n        hiddens : List[int], # hidden layer's dimensions\n        output_dim : int\n    ) -&gt; None:\n        super().__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.hiddens = hiddens\n\n        model = []\n        prev_h = input_dim\n        for h in hiddens:\n            model.append(nn.Linear(prev_h, h))\n            model.append(nn.ReLU())\n            prev_h = h\n        model.append(nn.Linear(hiddens[-1], output_dim))\n        self.net = nn.Sequential(*model)\n\n    def forward(self, x : torch.Tensor) -&gt; torch.Tensor:\n        batch_size = x.shape[0]\n        x = x.view(batch_size, -1)\n        return self.net(x).view(batch_size, self.output_dim)\n\n\n\n\n2.1 Vanilla GAN\nVanilla GAN objective\n\\[\\begin{align}\n\\min_G \\max_D V(G, D) &= \\min_G \\max_D \\big[\\mathbb{E}_{x \\sim \\mathbb{P}} \\log D(x) + \\mathbb{E}_{x_{\\text{gen}} \\sim p(x \\vert \\theta)} \\log(1 - D(x_{\\text{gen}}))\\big] \\\\\n&= \\min_G \\max_D \\big[\\mathbb{E}_{x \\sim \\mathbb{P}} \\log D(x) + \\mathbb{E}_{z \\sim p_z(z)} \\log(1 - D(G(z)))\\big]\n\\end{align}\\]\n\nGenerator: generative model \\(x = G(z)\\), which try to generate samples indistiguishable from real samples \\(X \\sim \\mathbb{P}\\)\nDiscriminator: a classifier \\(D(x) \\in [0, 1]\\), which distiguishes reals samples from generated samples\n\n‚ùî Which discrepancy \\(\\mathfrak{D}\\) is optimized in the internal maximization problem? [üí° Hint: Recall the lecture]\n\nSample estimate of Vanilla GAN objective:\n\nThe objective \\(V\\) is stochastically estimated by deriving a random batch \\(X_B \\subset X\\) and random batch sample \\(Z_B \\sim p_z\\):\n\n\\[\n\\hat{V}(G, D) = \\bigg[\\frac{1}{\\vert X_B \\vert} \\sum_{x \\in X_B} \\log D(x) + \\frac{1}{\\vert Z_B \\vert} \\sum_{z \\in Z_B} \\log(1 - D(G(z)))\\bigg].\n\\]\n\nThe problem is solved by alternating stochastic gradient descent-ascent steps w.r.t. parameters \\(\\theta\\) and \\(\\phi\\).\n\n\nImplementing Vanilla GAN for toy 2D problem\n\nAs latent distribution \\(p(z)\\) we will use \\(D_z\\)-dimensional Standard gaussian distribution. Choose \\(D_z\\) on your own. The author utilizes \\(D_z = 16\\).\n\n‚ùî What are the input and output dimensions of generator \\(G_\\theta\\) and discriminator \\(D_\\theta\\) models?\n\n\nDefine the Vanilla GAN generator \\(G :\\mathbb{R}^{D_z}\\rightarrow \\mathbb{R}^{2}\\). * It sufficies to use medium-size multi-layer perceptron with ReLU activations. * Take the advantage of FullyConnectedMLP class. * Implement sample method.\n\nclass VanillaGenerator(FullyConnectedMLP):\n\n    def sample(self, n : int) -&gt; torch.Tensor:\n        z = torch.randn(size=(n, self.input_dim)).to(\n            next(iter(self.parameters())))\n        return self.forward(z)\n\nGEN_HIDDENS = [128, 128, 128]\n\nG = VanillaGenerator(2, GEN_HIDDENS, 2).to(DEVICE)\n\nDefine the Vanilla GAN discriminator \\(D :\\mathbb{R}^{2}\\rightarrow \\mathbb{R}\\). * It sufficies to use medium-size multi-layer perceptron with ReLU activations. * Take the advantage of FullyConnectedMLP class. * Note that the output of \\(D\\) should be restricted to \\([0, 1]\\) segment.\n\nclass VanillaDiscriminator(FullyConnectedMLP):\n\n    def forward(self, z : torch.Tensor) -&gt; torch.Tensor:\n        x = super().forward(z)\n        return torch.sigmoid(x)\n\nDISCR_HIDDENS = [128, 128, 128]\n\nD = VanillaDiscriminator(2, DISCR_HIDDENS, 1).to(DEVICE)\n\n\n\nImplementing Vanilla GAN Loss\n\nNote, you will use the same function on images data problem\nIt is recommended to use torch.nn.functional.binary_cross_entropy (link to docs) (to eliminate possible inifinity or large values)\n\nImplement Vanilla GAN generator training step.\n\nThe function below should compute the stocastic estimate \\(\\hat{V}\\) of Vanilla GAN loss and perform a single gradient descent step w.r.t the parameter \\(\\theta\\) of the generator.\n\n\ndef vanilla_gen_step(\n    X : torch.Tensor, # random batch from the dataset\n    G : nn.Module, # generator model\n    D : nn.Module, # discriminator model\n    G_optim : torch.optim.Optimizer, # optimizer for generator (Adam/RMSprop/SGD etc.)\n) -&gt; torch.Tensor: # returns the value of loss to track the training statistics\n    G.train()\n    D.eval()\n    batch_size = X.size(0)\n    X_gen = G.sample(batch_size)\n    scores_gen = D(X_gen)\n    loss = - F.binary_cross_entropy(scores_gen, torch.zeros_like(scores_gen))\n    G_optim.zero_grad()\n    loss.backward()\n    G_optim.step()\n    return loss.item()\n\nImplement Vanilla GAN discriminator training step.\n\nThe function below should compute the stocastic estimate \\(\\hat{V}\\) of Vanilla GAN loss and perform a single gradient ascent step w.r.t the parameter \\(\\phi\\) of the discriminator.\n\n\ndef vanilla_discr_step(\n    X : torch.Tensor, # random batch from the dataset\n    G : nn.Module, # generator model\n    D : nn.Module, # discriminator model\n    D_optim : torch.optim.Optimizer, # optimizer for the discriminator (Adam/RMSprop/SGD etc.)\n) -&gt; torch.Tensor: # returns the value of loss to track the training statistics\n    G.eval()\n    D.train()\n    batch_size = X.size(0)\n    with torch.no_grad():\n        X_gen = G.sample(batch_size)\n    scores_gen = D(X_gen)\n    scores_real = D(X)\n    loss_gen = F.binary_cross_entropy(scores_gen, torch.zeros_like(scores_gen))\n    loss_real = F.binary_cross_entropy(scores_real, torch.ones_like(scores_real))\n    loss = loss_gen + loss_real\n\n    D_optim.zero_grad()\n    loss.backward()\n    D_optim.step()\n    return loss.item()\n\nImplement Vanilla GAN training loop\n\nIt is not possible (and, actually, is a bad strategy from the optimization perspectives) to solve the internal maximization problem \\(\\max_{\\phi} V(G_\\theta, D_\\phi)\\) for each particular generator \\(G_\\theta\\).\nInstead, we will perform a fixed number of discriminator_steps gradient ascent updates of the discriminator parameter \\(\\phi\\) per each gradient descent update of the generator parameter \\(\\theta\\)\nAfterwards we will see that setting discriminator_steps to large numbers will spoil the optimization and convergence.\n\n\ndef train_vanilla(\n    train_loader : DataLoader, # dataloader of training dataset\n    G : nn.Module, # generator model\n    D : nn.Module, # discriminator model\n    G_optim : torch.optim.Optimizer, # optimizer for the generator\n    D_optim : torch.optim.Optimizer, # optimizer for the discriminator\n    discriminator_steps : int, # number of discriminators steps per each generator step\n    n_epochs : int, # number of training epochs\n    diagnostic : GANDiagnosticCompanion, # tracking statistics & visualization\n    visualize_steps : int = 10, # for visualization purposes\n) -&gt; None:\n\n    G.train()\n    D.train()\n    step_i = 0\n    for epoch_i in tqdm(range(n_epochs)):\n        for batch_i, X in enumerate(train_loader):\n            X = X.to(DEVICE)\n\n            # DISCRIMINATOR UPDATE\n            d_loss = vanilla_discr_step(X, G, D, D_optim)\n            diagnostic.upd_d_loss(d_loss)\n\n            # GENERATOR UPDATE\n            if step_i % discriminator_steps == 0:\n                g_loss = vanilla_gen_step(X, G, D, G_optim)\n                diagnostic.upd_g_loss(g_loss)\n            step_i += 1\n\n        if visualize_steps and epoch_i % visualize_steps == 0:\n            print('Epoch {}'.format(epoch_i))\n            diagnostic.visualize()\n\n\n\nVanilla GAN training\nüîé Remark. It is recommended to use RMSprop optimizer when training a GAN model. Empirically, it was observed that this optimizer works more stable compared to Adam.\n\nBATCH_SIZE = 1024\nGEN_HIDDENS = [128, 128, 128]\nDISCR_HIDDENS = [128, 128, 128]\nDISCRIMINATOR_STEPS = 5 # 5- good, 15-mode not covered\nLR = 2e-4 # &lt; 1e-2\nN_EPOCHS = 1000 # change it if you want\n\ntrain_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n\nG = VanillaGenerator(16, GEN_HIDDENS, 2).to(DEVICE)\nD = VanillaDiscriminator(2, DISCR_HIDDENS, 1).to(DEVICE)\nG_optim = torch.optim.RMSprop(G.parameters(), lr=LR)\nD_optim = torch.optim.RMSprop(D.parameters(), lr=LR)\ndiagnostic = GANDiagnosticCompanion2D(G, D, train_data)\n\ntrain_losses = train_vanilla(\n    train_loader,\n    G,\n    D,\n    G_optim,\n    D_optim,\n    discriminator_steps=DISCRIMINATOR_STEPS,\n    n_epochs=N_EPOCHS,\n    diagnostic=diagnostic,\n    visualize_steps=100\n)\n\n  0%|                                                                                                                                              | 0/1000 [00:00&lt;?, ?it/s]\n\n\nEpoch 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                                                                      | 100/1000 [00:22&lt;02:53,  5.17it/s]\n\n\nEpoch 100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                                                         | 200/1000 [00:42&lt;02:39,  5.01it/s]\n\n\nEpoch 200\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                            | 300/1000 [01:05&lt;02:16,  5.12it/s]\n\n\nEpoch 300\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                               | 400/1000 [01:27&lt;01:58,  5.06it/s]\n\n\nEpoch 400\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                  | 500/1000 [01:49&lt;01:40,  4.98it/s]\n\n\nEpoch 500\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                    | 600/1000 [02:12&lt;01:17,  5.17it/s]\n\n\nEpoch 600\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                       | 700/1000 [02:33&lt;00:59,  5.04it/s]\n\n\nEpoch 700\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 800/1000 [02:55&lt;00:39,  5.04it/s]\n\n\nEpoch 800\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 900/1000 [03:17&lt;00:18,  5.44it/s]\n\n\nEpoch 900\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [03:38&lt;00:00,  4.57it/s]\n\n\n\ndiagnostic.visualize()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVanishing gradient problem [ saturation of \\(\\log(1 - D(G(z))\\) ]\nSee Towards Principled Methods for Training Generative Adversarial Networks paper by Martin Arjovsky for more details.\nProblem 1. When generated distribution \\(p(x \\vert \\theta)\\) and reference distribution \\(\\mathbb{P}\\) have disjoint compact supports (there are even softer conditions), then the perfect discriminator \\(D^* : \\mathbb{R}^{D_x} \\rightarrow [0, 1]\\) (which maximizes \\(V(G_\\theta, D)\\)) will perfectly distinguish generated and real samples and \\(\\nabla_x D^*(x) = 0\\) for \\(x \\sim \\mathbb{P}\\) or \\(x\\sim p(x \\vert \\theta)\\)\n\n\\(\\Rightarrow\\) any learning will stop!\nAt the beginning of GAN training it is typical that \\(p(x \\vert \\theta)\\) is not aligned with \\(\\mathbb{P}\\). \\(\\Rightarrow\\) learning may stuck if the discriminator is sub-optimal.\n\n\n\n\n\nWe will fix this problem later.\n\nProblem 2. (particular conclusion of Problem 1.). In the conditions of Problem 1.:\n\\[ \\lim\\limits_{\\Vert D - D^*\\Vert \\rightarrow 0} \\nabla_{\\theta} \\big\\{\\mathbb{E}_{z \\sim p_z(z)} \\log(1 - D(G_{\\theta}(z)))\\big\\} = 0,\\] i.e., if \\(p(x \\vert \\theta)\\) is not aligned with \\(\\mathbb{P}\\) then the closer discriminator \\(D\\) to the optimal one \\(D^*\\) the weaker the gradient of the objective \\(V(G_\\theta, D)\\) w.r.t. \\(\\theta\\) and the slower the optimization of the generator \\(G_\\theta\\). This effect is known as saturation of \\(\\log(1 - D(G(z))\\).\n\n\n\n\n\n\n2.2 Non-saturating GAN (‚Äúfixes‚Äù Problem 2)\n\nNS-GAN discriminator optimization\nThe same as Vanilla GAN disciminator optimization\n\n\n\nNS-GAN generator optimization\nVanilla GAN generator \\(G\\) gradient step update (for current distriminator \\(D\\)):\n\\[\\theta_{\\text{new}} = \\theta_{\\text{old}} - \\gamma \\nabla_{\\theta} \\big[\\mathbb{E}_{z \\sim p_z(z)} \\log(1 - D(G_{\\theta}(z)))\\big]\\Big|_{\\theta = \\theta_{\\text{old}}}\\]\n(this corresponds to minimization of \\(\\mathbb{E}_{z \\sim p_z(z)} \\log(1 - D(G_{\\theta}(z)))\\) w.r.t. \\(\\theta\\))\nNon-saturating GAN generator \\(G\\) gradient step update (for current distriminator \\(D\\)):\n\\[\\theta_{\\text{new}} = \\theta_{\\text{old}} + \\gamma \\nabla_{\\theta} \\big[\\mathbb{E}_{z \\sim p_z(z)} \\log(D(G_{\\theta}(z)))\\big]\\Big|_{\\theta = \\theta_{\\text{old}}}\\]\n(this corresponds to maximization of \\(\\mathbb{E}_{z \\sim p_z(z)} \\log(D(G_{\\theta}(z)))\\) w.r.t. \\(\\theta\\))\n\nGradients became much stronger\n\n\n\n\n\nLess stable training\n\n\n\nPractical aspects of NS GAN\n\nWhen implementing Vanilla GAN discriminator \\(D\\) we apply Sigmoid after the final layer:\n\n\\[\n\\sigma(x) = \\frac{1}{1 + e^{-x}}\n\\]\n\nLet us instead parameterize NS GAN discriminator \\(D\\) as arbitrary function \\(D : \\mathbb{R}^{D_x} \\rightarrow \\mathbb{R}\\) by explicitly introducing the Sigmoid in the loss function.\n\n‚ùî Exercise: Prove that optimization steps for the discriminator and the generator of NS GAN are as follows:\nüí° Hint: Recall that \\(\\text{Softplus}(x) = \\log(1 + \\exp(x))\\). Use the fact: \\(\\sigma(-x) = 1 - \\sigma(x)\\).\nNote: In what follows, discriminator \\(D_{\\phi}\\) is an arbitrary MLP (not restricted to produce numbers in \\([0, 1]\\) interval)!\n\nD update:\n\n\\[\n\\text{loss}_{D_{\\phi}} = \\frac{1}{\\vert X_{B} \\vert} \\sum\\limits_{x \\in X_B} \\text{Softplus}(-D_{\\phi}(x)) +  \\frac{1}{\\vert Z_B \\vert} \\sum\\limits_{z \\in Z_B} \\text{Softplus}(D_{\\phi}(G_{\\theta}(z)))  \\rightarrow \\min_{\\phi}\n\\]\n\nG update:\n\n\\[\n\\text{loss}_{G_{\\theta}} =  \\frac{1}{\\vert Z_B \\vert} \\sum\\limits_{z \\in Z_B} \\text{Softplus}(-D_{\\phi}(G_{\\theta}(z)))  \\rightarrow \\min_{\\theta}\n\\]\n\n\nImplementing NS GAN\nDefine the NS GAN generator \\(G :\\mathbb{R}^{D_z}\\rightarrow \\mathbb{R}^{2}\\) and discriminator \\(D : \\mathbb{R}^{2}\\rightarrow \\mathbb{R}\\)\n\nYou do not need to restrict the discriminator‚Äôs output to be in range \\([0, 1]\\).\n\n\nclass NSGenerator(VanillaGenerator):\n    pass\n\nGEN_HIDDENS = [128, 128, 128]\n\nG = NSGenerator(2, GEN_HIDDENS, 2).to(DEVICE)\n\nclass NSDiscriminator(FullyConnectedMLP):\n    pass\n\nDISCR_HIDDENS = [128, 128, 128]\n\nD = NSDiscriminator(2, DISCR_HIDDENS, 1).to(DEVICE)\n\n\n\nImplementing NS GAN Loss\nImplement NS GAN generator training step.\n\nThe function below should compute the stocastic estimate of Non-saturating GAN generator loss \\(\\frac{1}{\\vert Z_B \\vert} \\sum\\limits_{z \\in Z_B} \\text{Softplus}(-D_{\\phi}(G_{\\theta}(z)))\\), \\(Z_B \\sim p_z\\) and perform a single gradient descent step w.r.t the parameter \\(\\theta\\) of the generator.\n\n\ndef ns_gen_step(\n    X : torch.Tensor, # random batch from the dataset\n    G : nn.Module, # generator model\n    D : nn.Module, # discriminator model\n    G_optim : torch.optim.Optimizer, # optimizer for generator (Adam/RMSprop/SGD etc.)\n) -&gt; torch.Tensor: # returns the value of loss to track the training statistics\n    G.train()\n    D.eval()\n    batch_size = X.size(0)\n    X_gen = G.sample(batch_size)\n    scores_gen = D(X_gen)\n    loss = F.softplus(-scores_gen).mean()\n    G_optim.zero_grad()\n    loss.backward()\n    G_optim.step()\n    return loss.item()\n\n\n\n\\(R_1\\) regularizer (for discriminator)\nSee Mescheder et. al. for more details.\n\\[R_1(D_\\phi) = \\mathbb{E}_{x \\sim \\mathbb{P}} \\big[\\Vert \\nabla_x D_\\phi(x) \\Vert^2\\big]\\]\n\nThe stochastic estimate of \\(R_1\\) value is added with factor \\(\\lambda\\) to the discriminator loss:\n\n\\[\n\\text{loss}_{D_{\\phi}} = \\frac{1}{\\vert X_{B} \\vert} \\sum\\limits_{x \\in X_B} \\text{Softplus}(-D_{\\phi}(x)) +  \\frac{1}{\\vert Z_B \\vert} \\sum\\limits_{z \\in Z_B} \\text{Softplus}(D_{\\phi}(G_{\\theta}(z))) + \\lambda \\frac{1}{\\vert X_{B} \\vert} \\sum\\limits_{x \\in X_B} \\big[\\Vert \\nabla_x D_\\phi(x) \\Vert^2\\big] \\rightarrow \\min_{\\phi}\n\\]\n\nIt stabilizes the training. Large and Powerful GAN models, e.g., StyleGAN paper, utilizes Non-saturating GAN loss with \\(R_1\\) regularizer.\n\n\nImplement NS GAN discriminator training step with additional \\(R_1\\) regularizer.\n\nDo not forget to set the requries_grad flag of the input data batch (Why?)\nDo not forget to zero_grad the discriminator parameters after computing the gradient penalty (Why?)\nüí° Hint: Utilize torch.Tensor.backward(retain_graph=True, create_graph=True) function.\n\n‚ùî What do retain_graph and create_graph parameters mean? Why do we need to set the to True?\n\ndef ns_discr_step(\n    X : torch.Tensor, # random batch from the dataset\n    G : nn.Module, # generator model\n    D : nn.Module, # discriminator model\n    D_optim : torch.optim.Optimizer, # optimizer for generator (Adam/RMSprop/SGD etc.)\n    r1_regularizer : float = 1., # the factor of R_1 regularization\n) -&gt; torch.Tensor: # returns the value of loss to track the training statistics\n    G.eval()\n    D.train()\n    D_optim.zero_grad()\n    batch_size = X.size(0)\n    with torch.no_grad():\n        X_gen = G.sample(batch_size)\n    X.requires_grad_()\n    scores_gen = D(X_gen)\n    scores_real = D(X)\n    loss_gen = F.softplus(scores_gen).mean()\n    loss_real = F.softplus(-scores_real).mean()\n    scores_real.sum().backward(retain_graph=True, create_graph=True)\n    gradients = X.grad # the same shape as X\n    grad_penalty = (\n        gradients.view(gradients.size(0), -1).norm(2, dim=1) ** 2\n    ).mean()\n    D_optim.zero_grad()\n    loss = loss_gen + loss_real + r1_regularizer * grad_penalty\n\n    loss.backward()\n    D_optim.step()\n    gradients.detach_() # to avoid memory leak!\n    return loss.item()\n\nImplement NS GAN training loop\n\ndef train_ns(\n    train_loader : DataLoader, # dataloader of training dataset\n    G : nn.Module, # generator model\n    D : nn.Module, # discriminator model\n    G_optim : torch.optim.Optimizer, # optimizer for the generator\n    D_optim : torch.optim.Optimizer, # optimizer for the discriminator\n    discriminator_steps : int, # number of discriminators steps per each generator step\n    n_epochs : int, # number of training epochs\n    diagnostic : GANDiagnosticCompanion, # tracking statistics & visualization\n    r1_regularizer : float = 1., # the factor of R_1 regularization for the discriminator training\n    visualize_steps : int = 10, # for visualization purposes\n) -&gt; None:\n\n    G.train()\n    D.train()\n    step_i = 0\n    for epoch_i in tqdm(range(n_epochs)):\n        for batch_i, X in enumerate(train_loader):\n            X = X.to(DEVICE)\n\n            # DISCRIMINATOR UPDATE\n            d_loss = ns_discr_step(X, G, D, D_optim, r1_regularizer)\n            diagnostic.upd_d_loss(d_loss)\n\n            # GENERATOR UPDATE\n            if step_i % discriminator_steps == 0:\n                g_loss = ns_gen_step(X, G, D, G_optim)\n                diagnostic.upd_g_loss(g_loss)\n            step_i += 1\n\n        if visualize_steps and epoch_i % visualize_steps == 0:\n            print('Epoch {}'.format(epoch_i))\n            diagnostic.visualize()\n\n\n\nNS GAN training\n\nBATCH_SIZE = 1024\nGEN_HIDDENS = [128, 128, 128]\nDISCR_HIDDENS = [128, 128, 128]\nDISCRIMINATOR_STEPS = 5 # 2 - more or less, 5 - lose modes\nLR = 2e-4 # &lt; 1e-2\nR1_REGULARIZER = 0.1\nN_EPOCHS = 1000 # change it if you want\n\ntrain_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n\nG = NSGenerator(16, GEN_HIDDENS, 2).to(DEVICE)\nD = NSDiscriminator(2, DISCR_HIDDENS, 1).to(DEVICE)\nG_optim = torch.optim.RMSprop(G.parameters(), lr=LR)\nD_optim = torch.optim.RMSprop(D.parameters(), lr=LR)\ndiagnostic = GANDiagnosticCompanion2D(G, D, train_data)\n\ntrain_losses = train_ns(\n    train_loader,\n    G,\n    D,\n    G_optim,\n    D_optim,\n    discriminator_steps=DISCRIMINATOR_STEPS,\n    n_epochs=N_EPOCHS,\n    diagnostic=diagnostic,\n    r1_regularizer=R1_REGULARIZER,\n    visualize_steps=100\n)\n\n  0%|                                                                                                                                              | 0/1000 [00:00&lt;?, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at ../torch/csrc/autograd/engine.cpp:1151.)\n  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\n\nEpoch 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                                                                      | 100/1000 [00:38&lt;05:40,  2.64it/s]\n\n\nEpoch 100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                                                         | 200/1000 [01:18&lt;05:17,  2.52it/s]\n\n\nEpoch 200\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                            | 300/1000 [01:57&lt;04:17,  2.72it/s]\n\n\nEpoch 300\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                               | 400/1000 [02:34&lt;04:06,  2.43it/s]\n\n\nEpoch 400\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                  | 500/1000 [03:16&lt;03:18,  2.52it/s]\n\n\nEpoch 500\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                    | 600/1000 [03:55&lt;02:32,  2.62it/s]\n\n\nEpoch 600\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                       | 700/1000 [04:33&lt;01:45,  2.84it/s]\n\n\nEpoch 700\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 800/1000 [05:12&lt;01:15,  2.63it/s]\n\n\nEpoch 800\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 900/1000 [05:54&lt;00:38,  2.57it/s]\n\n\nEpoch 900\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [06:33&lt;00:00,  2.54it/s]\n\n\n\ndiagnostic.visualize()"
  },
  {
    "objectID": "projects/GANs1.html#gans-on-3times-16times-16-image-data",
    "href": "projects/GANs1.html#gans-on-3times-16times-16-image-data",
    "title": "GANs",
    "section": "3. GANs on \\(3\\times 16\\times 16\\) image data",
    "text": "3. GANs on \\(3\\times 16\\times 16\\) image data\nNow we apply our implemented Vanilla GAN and NS GAN R_1 approaches to solve the Generative Modelling problem on the \\(3\\times 16\\times 16\\) image data\n\n3.1 Image data problem setup\nAs the distribution \\(\\mathbb{P}\\) we aim to model, we consider Colored MNIST dataset downscaled to \\(16\\times 16\\) spatial resolutions. You will meet this distribution in the further seminars again üôÇ.\n\nLook into the code below carefully\nNote that Colored MNIST dataset produces images with the pixel range \\([-1, 1]\\).\n\n\nimport torchvision.datasets as TVdatasets\nfrom torchvision import transforms as TVtransforms\n\n# draw grayscaled image with random color\ndef random_color(im : torch.Tensor) -&gt; torch.Tensor:\n    hue = 360*np.random.rand()\n    d = (im *(hue%60)/60)\n    im_min, im_inc, im_dec = torch.zeros_like(im), d, im - d\n    H = round(hue/60) % 6\n    cmap = [[0, 3, 2], [2, 0, 3], [1, 0, 3], [1, 2, 0], [3, 1, 0], [0, 1, 2]]\n    return torch.cat((im, im_min, im_dec, im_inc), dim=0)[cmap[H]]\n\nclass CMNISTDataset(torch.utils.data.Dataset):\n\n    def __init__(\n        self,\n        train : bool =True,\n        spat_dim : Tuple[int, int] = (16, 16),\n        download : bool = False,\n        pix_range : Tuple[float, float] = (-1., 1.)\n    ) -&gt; None:\n        _m, _std = pix_range[0]/float(pix_range[0] - pix_range[1]), 1./float(pix_range[1] - pix_range[0])\n        TRANSFORM = TVtransforms.Compose([\n            TVtransforms.Resize(spat_dim),\n            TVtransforms.ToTensor(),\n            random_color,\n            TVtransforms.Normalize([_m],[_std])\n        ])\n        self.mnist = TVdatasets.MNIST(root='./data', train=train, download=download, transform=TRANSFORM)\n\n    def __len__(self):\n        return len(self.mnist)\n\n    def __getitem__(self, idx : int) -&gt; torch.Tensor:\n        return self.mnist[idx][0]\n\ncmnist_train = CMNISTDataset(train=True, download=True)\ncmnist_test = CMNISTDataset(train=False)\n\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9912422/9912422 [00:04&lt;00:00, 2364788.66it/s]\n\n\nExtracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28881/28881 [00:00&lt;00:00, 218773.77it/s]\n\n\nExtracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1648877/1648877 [00:01&lt;00:00, 1532206.17it/s]\n\n\nExtracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4542/4542 [00:00&lt;00:00, 4599355.09it/s]\n\n\nExtracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\n\n\n\n\n\nLet us take a look at the images from the datasets.\n\nREFERENCE_IMAGES = next(iter(DataLoader(cmnist_test, batch_size=10)))\nplot_images(REFERENCE_IMAGES)\n\n\n\n\n\n\n\n\n\n\n3.2 Implementing Vanilla GAN and NS GAN on image data\nWhen people deal with images, they typically use Convolutional Neural Networks. Implement the generic classes for ConvGenerator \\(G : \\mathbb{R}^{D_z} \\rightarrow \\mathbb{R}^{3 \\times 16 \\times 16}\\) and ConvDiscriminator \\(D : \\mathbb{R}^{3 \\times 16 \\times 16} \\rightarrow \\mathbb{R}\\) below. For now, do not restrict somehow the output of the ConvDiscriminator class\n\nConvGenerator\n\nTakes a latent sample (vector) of size (batch_size, input_size) as input, produces a tensor of shape (batch_size, 3, 16, 16) as output.\nThe output tensor values are in range \\([-1, 1]\\).\nImplements sample method. Choose standard Normal distribution as \\(p_z\\).\n\nüí° Hint: Start with linear block, then combine several nn.ConvTranspose2d (docs) blocks with BatchNorm2d and ReLU nonlinearity.\n\n\nConvDiscriminator\n\nFor now, do not restrict somehow the output of the ConvDiscriminator class.\nTakes an image tensor (batch_size, 3, 16, 16) as the input, produces scalar values of size (batch_size, 1) as output\n\nüí° Hint: Just combine several nn.Conv2d layers with LeakyReLU nonlinearity. Apply linear layer at the end.\n\nclass ConvGenerator(nn.Module):\n\n    def __init__(\n        self, input_size: int = 128,\n    ) -&gt; None:\n        super().__init__()\n        self.n_channels = 64\n        self.input_size = input_size\n        self.linear_block = nn.Sequential(\n            nn.Linear(input_size, 4 * 4 * 4 * self.n_channels),\n            nn.ReLU(True),\n        )\n        self.conv_block = nn.Sequential(\n            nn.ConvTranspose2d(4 * self.n_channels, 2 * self.n_channels, 2, stride=2),\n            nn.BatchNorm2d(2 * self.n_channels),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(2 * self.n_channels, self.n_channels, 2, stride=2),\n            nn.BatchNorm2d(self.n_channels),\n            nn.ReLU(True),\n            nn.Conv2d(self.n_channels, 3, 3, padding=1)\n        )\n        self.noise = torch.distributions.Normal(torch.tensor(0.0), torch.tensor(1.0))\n\n    def forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n        assert input.size(1) == self.input_size\n        output = self.linear_block(input)\n        output = output.view(-1, 4 * self.n_channels, 4, 4)\n        output = self.conv_block(output)\n        output = torch.tanh(output)\n        return output.view(-1, 3, 16, 16)\n\n    def sample(self, n_samples: int) -&gt; torch.Tensor:\n        z = self.noise.sample([n_samples, self.input_size]).to(next(iter(self.parameters())))\n        return self.forward(z)\n\nclass ConvDiscriminator(nn.Module):\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.n_channels = 64\n\n        self.net = nn.Sequential(\n            nn.Conv2d(3, self.n_channels, 3, stride=2, padding=1),\n            nn.LeakyReLU(),\n            nn.Conv2d(self.n_channels, 2 * self.n_channels, 3, stride=2, padding=1),\n            nn.LeakyReLU(),\n            nn.Conv2d(2 * self.n_channels, 4 * self.n_channels, 3, stride=2, padding=1),\n            nn.LeakyReLU(),\n        )\n        self.linear = nn.Linear(4 * 2 * 2 * self.n_channels, 1)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        output = self.net(x)\n        output = output.view(-1, 4 * 2 * 2 * self.n_channels)\n        output = self.linear(output)\n        return output\n\nCG = ConvGenerator()\nCD = ConvDiscriminator()\n\nConvDiscriminator for Vanilla GAN\nYou know what to do.\n\nclass VanillaConvDiscriminator(ConvDiscriminator):\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        output = super().forward(x)\n        return torch.sigmoid(output)\n\n\n\n\n3.3 Training on image data\n\nTraining Vanilla GAN on image data\n\nuse train_vanilla function you implement in the previous sections\ndo not forget that the discriminator‚Äôs outpus should be restricted\n\n\nBATCH_SIZE = 256\nDISCRIMINATOR_STEPS = 1 # &gt;=2 - no learning, saturation!, 1 - something manages to learn\nLR = 2e-4 # &lt; 1e-2\nN_EPOCHS = 10 # change it if you want\n\ntrain_mnist_loader = DataLoader(cmnist_train, batch_size=BATCH_SIZE, shuffle=True)\nG = ConvGenerator().to(DEVICE)\nD = VanillaConvDiscriminator().to(DEVICE)\nG_optim = torch.optim.RMSprop(G.parameters(), lr=LR)\nD_optim = torch.optim.RMSprop(D.parameters(), lr=LR)\ndiagnostic = GANDiagnosticCompanionImages(G, D, REFERENCE_IMAGES.numpy())\n\ntrain_losses = train_vanilla(\n    train_mnist_loader,\n    G,\n    D,\n    G_optim,\n    D_optim,\n    discriminator_steps=DISCRIMINATOR_STEPS,\n    n_epochs=N_EPOCHS,\n    diagnostic=diagnostic,\n    visualize_steps=1\n)\n\n  0%|                                                                                                                                                | 0/10 [00:00&lt;?, ?it/s]\n\n\nEpoch 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                                                          | 1/10 [00:30&lt;04:32, 30.32s/it]\n\n\nEpoch 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                                                            | 2/10 [01:00&lt;04:00, 30.05s/it]\n\n\nEpoch 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                                               | 3/10 [01:30&lt;03:29, 29.99s/it]\n\n\nEpoch 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                                 | 4/10 [02:00&lt;02:59, 29.97s/it]\n\n\nEpoch 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                    | 5/10 [02:29&lt;02:29, 29.93s/it]\n\n\nEpoch 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                      | 6/10 [02:59&lt;01:59, 29.94s/it]\n\n\nEpoch 6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                        | 7/10 [03:29&lt;01:29, 29.90s/it]\n\n\nEpoch 7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                           | 8/10 [03:59&lt;00:59, 29.83s/it]\n\n\nEpoch 8\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 9/10 [04:28&lt;00:29, 29.67s/it]\n\n\nEpoch 9\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [04:57&lt;00:00, 29.78s/it]\n\n\nDiagnostic after 10 epochs\n\ndiagnostic.visualize()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSamples after 70 epochs of a successful launch.\n\nGENERATED_SAMPLES = G.sample(10).detach().cpu()\nplot_images(GENERATED_SAMPLES, 'Final generated samples')\n\n\n\n\n\n\n\n\n\n\nTraining NS GAN on image data\n\nuse train_ns function you implement in the previous sections\n\n\nBATCH_SIZE = 256\nDISCRIMINATOR_STEPS = 5 # 5 is ok\nLR = 2e-3 # &lt; 1e-2\nR1_REGULARIZER = 0.1\nN_EPOCHS = 10 # change it if you want\n\ntrain_mnist_loader = DataLoader(cmnist_train, batch_size=BATCH_SIZE, shuffle=True)\nG = ConvGenerator().to(DEVICE)\nD = ConvDiscriminator().to(DEVICE)\nG_optim = torch.optim.RMSprop(G.parameters(), lr=LR)\nD_optim = torch.optim.RMSprop(D.parameters(), lr=LR)\ndiagnostic = GANDiagnosticCompanionImages(G, D, REFERENCE_IMAGES.numpy())\n\ntrain_losses = train_ns(\n    train_mnist_loader,\n    G,\n    D,\n    G_optim,\n    D_optim,\n    discriminator_steps=DISCRIMINATOR_STEPS,\n    n_epochs=N_EPOCHS,\n    diagnostic=diagnostic,\n    r1_regularizer=R1_REGULARIZER,\n    visualize_steps=1\n)\n\n  0%|                                                                                                                                                | 0/10 [00:00&lt;?, ?it/s]\n\n\nEpoch 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                                                          | 1/10 [00:29&lt;04:21, 29.01s/it]\n\n\nEpoch 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                                                            | 2/10 [00:58&lt;03:52, 29.09s/it]\n\n\nEpoch 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                                               | 3/10 [01:27&lt;03:25, 29.36s/it]\n\n\nEpoch 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                                 | 4/10 [01:57&lt;02:57, 29.51s/it]\n\n\nEpoch 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                    | 5/10 [02:26&lt;02:27, 29.47s/it]\n\n\nEpoch 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                      | 6/10 [02:56&lt;01:57, 29.43s/it]\n\n\nEpoch 6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                        | 7/10 [03:25&lt;01:28, 29.45s/it]\n\n\nEpoch 7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                           | 8/10 [03:55&lt;00:58, 29.45s/it]\n\n\nEpoch 8\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç             | 9/10 [04:24&lt;00:29, 29.45s/it]\n\n\nEpoch 9\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [04:53&lt;00:00, 29.40s/it]\n\n\nDiagnostic after 10 epochs\n\ndiagnostic.visualize()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSamples after 100 epochs\n\nGENERATED_SAMPLES = G.sample(10).detach().cpu()\nplot_images(GENERATED_SAMPLES, 'Final generated samples')\n\n\n\n\n\n\n\n\nConclusion\nWhen applying GANs on complex data (e.g., images) one need to properly regularize your models.\nüîé Remark. Except NS GAN with \\(R_1\\) regularization, a popular choise of well-behaving regularized GAN loss is Wasserstein GAN with Gradient Penalty. For more information, see original Wasserstein GAN paper and Improved Training of Wasserstein GANs paper (the latter introduces WGAN with GP)."
  },
  {
    "objectID": "projects/GANs1.html#i-know-everything-from-above-let-me-do-something-interesting.",
    "href": "projects/GANs1.html#i-know-everything-from-above-let-me-do-something-interesting.",
    "title": "GANs",
    "section": "I know everything from above, let me do something interesting.",
    "text": "I know everything from above, let me do something interesting.\n‚ùîAdditional exercise. Learn NS GAN \\(R_1\\) on Cifar10 dataset. Come up with your own architectures for the generator and the discriminator, download the dataset, tune hyperparameters and launch train_ns function. Demonstrate the samples."
  },
  {
    "objectID": "projects/DDPM_eng.html",
    "href": "projects/DDPM_eng.html",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "",
    "text": "Author: Addisu Amare"
  },
  {
    "objectID": "projects/DDPM_eng.html#another-view-to-vae-models",
    "href": "projects/DDPM_eng.html#another-view-to-vae-models",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "1. Another view to VAE models",
    "text": "1. Another view to VAE models\nQuestions for the audience\n\nWhat are discriminative models?\nWhat are generative models?\nWhat task do generative models solve?\n\nHaving defined the main task of generative models, it is logical to ask yourself the question - ‚ÄúHow do we actually evaluate the probability of objects from the training sample?‚Äù\n\n\n\ntitle\n\n\n\nThe principle of maximum likelihood:\n\nWhen we want to train a generative model \\(p(x|\\theta)\\), then the first attempt to do this is the basis for solving the so-called \\(\\textbf{MLE-problem}\\) or the problem of maximizing the likelihood of the model by selecting its parameters:\n\\[\\theta = \\arg\\max_{\\theta} \\log p(X|\\theta) = \\{ X = \\{ x_{i}\\}_{i=1}^{n}\\}= \\arg\\max_{\\theta} \\sum_{i=1}^{n} \\log p(x_{i}|\\theta)\\]\nQuestions for the audience Why are we not satisfied with this approach?\n\nLatent space:\n\nBefore we move on to another way of estimating the likelihood of data, we will recall the essence of latent representations. The main intuition of which is shown in the picture below:\n\n\n\ntitle\n\n\n\nModels of the variation encoder:\n\nAnother attempt to assess the likelihood is to introduce latent variables and consider the VAE model. When we are dealing with a variational autoencoder, we do not have access to an honest value of the logarithm of likelihood, so we optimize the corresponding lower bound, which we call as \\(\\textbf{ELBO}\\)\n\\[ \\log p(x|\\theta) \\geq \\mathcal{L}(\\theta,q) = \\int_{Z}q(z|x, \\phi)\\log\\frac{p(x,z|\\theta)}{q(z|x,\\phi)} dz\\]\n\nExpansion of the latent space: more is better than one\n\nWe know that the variational autoencoder has exactly one latent space. That is, we entered the latent space with an encoder and \\(\\bf{immediately}\\) leave it with the help of a decoder, while we do not explore the latent space in any way.\nHowever, let‚Äôs try to expand the number of latent spaces by introducing \\(T\\) consecutive latent spaces with corresponding decompositions of \\(f_{i}\\). That is, we use the encoder to move into the latent space and walk through the latent space for a certain number of steps\n\n\n\ntitle"
  },
  {
    "objectID": "projects/DDPM_eng.html#hierarchical-vae-models",
    "href": "projects/DDPM_eng.html#hierarchical-vae-models",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "2. Hierarchical VAE models",
    "text": "2. Hierarchical VAE models\nFor the simplicity, let me denote the follwoing:\n\nx = \\(x_{0}\\)\n\\(z_{1} = x_{1}\\)\n\\(z_{2} = x_{2}\\)\n\\(z_{T} = x_{T}\\)\n\nAfter introduction more convinient notation for \\(T\\) dimensional vectors, we should define transfromation functions between \\(f_{i}\\) between latent statements. Undoubtedly, one can consider neural networks for this purpose , however, do not worth to complicate our life\n\\(\\textbf{Assumption :}\\) Let these functions \\(f_{i}\\) are \\(\\textbf{not-learnable}\\) certain transfromations. Since we would like to add some stochasticity to the framework, one can consider fixed distributions as such transformations:\n\\[ q(x_{t}|x_{t_1}) = \\mathcal{N}(x_{t}| x_{t-1}, \\beta I) \\]\nNow, our method looks like \\(\\textbf{Brownian motion}\\) or \\(\\textbf{Random movements}\\).\n\n\n\ntitle\n\n\nHowever, if we will learn means of such transformations (normal distributions) , then we obtain method that is referred to as \\(\\textbf{Hierarchical VAE}\\). Please, see this \\(\\href{https://jmtomczak.github.io/blog/9/9_hierarchical_lvm_p1.html}{blog}\\) for best understanding of this concept\nThus, thanks to this copncept, we realize that one can whole latent spaces \\(Z = \\{ x_{1},...,x_{T}\\}\\) and then the corresponding \\(\\textbf{ELBO}\\) formula is:\n\\[ \\log p(x|\\theta) \\geq \\mathcal{L}(\\theta,q) = \\int_{Z}q(x_{1},...,x_{T}|x_{0}, \\phi)\\log\\frac{p(x_{0},x_{1},..,x_{T}|\\theta)}{q(x_{1},...,x_{T}|x_{0},\\phi)} dx_{1:T}\\]"
  },
  {
    "objectID": "projects/DDPM_eng.html#gaussian-diffusion-processes",
    "href": "projects/DDPM_eng.html#gaussian-diffusion-processes",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "3. Gaussian diffusion processes",
    "text": "3. Gaussian diffusion processes"
  },
  {
    "objectID": "projects/DDPM_eng.html#what-is-the-main-motivation-to-consider-many-steps-in-latent-space",
    "href": "projects/DDPM_eng.html#what-is-the-main-motivation-to-consider-many-steps-in-latent-space",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "1. What is the main motivation to consider many steps in latent space?",
    "text": "1. What is the main motivation to consider many steps in latent space?\n - answer is here"
  },
  {
    "objectID": "projects/DDPM_eng.html#what-components-do-lie-at-the-heart-of-the-loss-function-of-vae",
    "href": "projects/DDPM_eng.html#what-components-do-lie-at-the-heart-of-the-loss-function-of-vae",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "2. What components do lie at the heart of the loss function of VAE?",
    "text": "2. What components do lie at the heart of the loss function of VAE?\n - answer is here\n\n\n\ntitle\n\n\nBecause the quality of generation of the hierarchical model of the variation encoder is better than the usual one.\nA question to the audience\n\nthen what conclusion does the number of latent representations suggest?\n\nThen we realize that the more latent spaces we consider, the better and better the quality of the model.\nThen the question arises - ‚ÄúBut if you take a lot of such latent representations, what will happen?‚Äù\nAnd answering this question, let‚Äôs slightly correct our transitional density of the hierarchical model of the variation encoder:\n\\[ q(x_t | x_{t-1}) = \\mathcal{N}(x_t | x_{t-1}, \\beta I) \\to q(x_t | x_{t-1}) = \\mathcal{N}(x_t | \\sqrt{1-\\beta}x_{t-1}, \\beta I)\\]\nThen it turns out that such a transient density determines the Markov process (where the present \\(x_{t}\\) depends only on the past \\(x_{t-1}\\)):\n\\[ x_t = \\sqrt{1 - \\beta} x_{t-1} + \\sqrt{\\beta} \\epsilon \\]\nwhere $ $ is usually a standard normal random variable\nThus, our transition process between latent spaces is a Markov process, and here‚Äôs what‚Äôs interesting to say about it:\n\n\n\ntitle\n\n\n\nTheorem 1:\n\nGiven:\n\n$ x_0 (x) $\n$(0,1) $\n\n\nThen applying the Markov chain to an arbitrary distribution of \\(\\pi(x)\\)infinitely many times, we get \\(\\mathcal{N}(0,I)\\). Thus, \\(\\mathcal{N}(0,I)\\) is a stationary distribution of the chain. That is, the following condition will be fulfilled $ p_(x) = (0, I) = q(x |x‚Äô) p_(x‚Äô) dx‚Äô $\nIf we denote $ t ={s=1}^{t} (1 - _s) $. Then we can express the sample of the process at any point in time using:\n\n\\[ x_t = \\sqrt{\\overline{\\alpha}_t} x_0 + \\sqrt{1 - \\overline{\\alpha}_t} \\epsilon \\]\n\\[ q(x_t | x_0) = \\mathcal{N}(x_t | \\sqrt{\\overline{\\alpha}_t} x_0, (1 - \\overline{\\alpha}_t) I) \\]\nThis means that we can select any \\(x_t\\) using only \\(x_0\\).\n\n\n\nChessUrl\n\n\n\\(\\textbf{The essence of theorem 1}\\)\n\nThe Markov process that we are considering will transform any data distribution into a normal standard distribution in an infinitely long time\nThe process is the so-called \\(\\textbf{free simulation}\\), that is, taking any point \\(x_{0}\\) and any point in time \\(t\\), you can instantly find \\(x_{t}\\)\n\nWe have proved the existence of a stochastic transformation from data to noise.\nRemember that the diffusion process does not depend on the initial density of \\(\\pi(x)\\)(complex) and the only requirement is access to a sample from it. The main idea of diffusion models is to use any data distribution of our choice as a complex initial density and gradually noise them. Thus, we understand the direct diffusion process as:\n\\[ x_{0} \\sim p_{data}(x) \\implies \\mathcal{F}(x_{0}) = x_{T} : x_{T} \\sim \\mathcal{N}(0,I) \\]\nThe idea: We have an equation for the direct noise reduction process that looks like this:\n\\[ dx_t = \\sqrt{1 - \\beta_t} x_{t-1} + \\sqrt{\\beta} \\epsilon \\]\nThanks to this equation, you can construct \\(\\color{red}{\\textbf{!! Untrained !!}}\\) trajectory from data to noise.\nIf we are dealing with an ordinary differential equation (ODE), then we can run this ODE in reverse time and get trajectories from noise to data. However, our process is not defined by an ODE, but by some kind of complex Stochastic diff equation. And this means that I would like to learn how to unfold such random equations in time.\nMotivation for learning diffusion models\n\nThere is a process from data to noise\nI want to expand the process\nThe detailed process goes from noise to data\n\nAlso, one can compare architectures and concepts of another generative models:\n\nVAE\nFLOW-based models\nDiffusion models\nGAN\n\n\n\n\nChessUrl"
  },
  {
    "objectID": "projects/DDPM_eng.html#reverse-process",
    "href": "projects/DDPM_eng.html#reverse-process",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "4. Reverse process",
    "text": "4. Reverse process\n\\(\\textbf{Theoretical statetment:}\\) If forward process is represented as set of Gaussian condtional transfromtions \\(q(x_{t}|x_{t-1})\\), then the reverse process will be the same, but with unknown parameters \\(p{x_{t-1}|x_{t}}\\)\nThus, we have 2 joint distributions of latent codes:\nBy the property of Markovian chains, one can represent forward process as:\n\n\\(q(x_{1},...,x_{T}|x_{0}) = q( x_{T} | x_{T-1},x_{0}) q(x_{T-1} | x_{T-2},x_{0})...q(x_{2} | x_{1},x_{0})\\)\n\nBy the property of Markovian chains, one can represent reversed process as:\n\n\\(p(x_{1},...,x_{T}) = p(x_{T-1}|x_{T})p(x_{T-1}|x_{T})....p(x_{1}|x_{2})\\)\n\nNow, we pay our attention to the loss function:\n\\[\\int_{x_{1:T}}q(x_{1},...,x_{T}|x_{0})\\log\\frac{p(x_{0},x_{1},....,x_{T})}{q(x_{1},....,x_{T}|x_{0})}dx_{1: T}\\]\nThus, one can represent this loss as corresponding KL-divergenges:\n\\[ \\int_{x_{1:T}} q(x_{1},...,x_{T}|x_{0})\\log p(x_{0}|x_{1})dx_{1:T} + \\int_{x_{1:T}}q(x_{1},...,x_{T}|x_{0})\\log\\frac{p(x_{1},....,x_{T})}{q(x_{1},....,x_{T}|x_{0})}dx_{1: T} \\]\nNow , we should take into account the second term, that it is similar to minimization of KL-divergences, however, we chains in opposite directions. Then, one can represent forward Markov chains transformation probabilities in opposite direction via \\(\\textbf{Bayes Theorem}\\)\n\\(\\textbf{My desire:}\\) i would like to represent the forward markov chain as:\n\\(q(x_{1},...,x_{T}|x_{0}) = q(x_{T}|x_{0})q(x_{T-1}|x_{T},x_{0})q(x_{T-2}|x_{T-1},x_{0})...q(x_{1}|x_{2},x_{0})\\)\n\\(\\textbf{Bayes theorem:}\\)\n\\[ q(x_{t-1}|x_{t},x_{0}) = \\frac{q(x_{t}|x_{t-1},x_{0})q(x_{t-1}|x_{0})}{q(x_{t}|x_{0})}\\]\nClarifications:\n\nI have Markovian process,hence, there is no dependences on $x_{0} q(x_{t}|x_{t-1},x_{0}) q(x_{t}|x_{t-1}) $\nAll distributions are gaussian \\(\\implies\\) one can perform accurate bayesiam inference\n\nAt home, you can substitute corresponding probabilities and get formula for posterior:\n\\[q(x_{t-1}|x_{t},x_{0}) = \\mathcal{N}(x_{t-1}| \\hat{\\mu}_{t}(x_{t},x_{0}),\\hat{\\beta_{t}}I)\\]\n\nMean:\n\n\\[\\hat{\\mu}_{t}(x_{t},x_{0})= \\frac{\\sqrt{\\overline{\\alpha}_{t-1}}}{1 - \\overline{\\alpha_{t}}}\\beta x_{0} +\\frac{\\sqrt{1 - \\beta}(1 - \\overline{\\alpha_{t}} )}{1 - \\overline{\\alpha_{t}}}x_{t}  \\]\n\nVariance:\n\n\\[\\hat{\\beta_{t}} = \\beta_{t}(1 - \\overline{\\alpha_{t-1}}) \\frac{1}{1 - \\overline{\\alpha}_{t}}\\]"
  },
  {
    "objectID": "projects/DDPM_eng.html#the-loss-deriavation",
    "href": "projects/DDPM_eng.html#the-loss-deriavation",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "5. The loss deriavation",
    "text": "5. The loss deriavation\nNow, we return to ELBO:\n\\[ \\int q(x_{1}|x_{0}) \\log p(x_{0}|x_{1})dx_{1} + \\int q(x_{T}|x_{0})...q(x_{1}|x_{2},x_{0})\\log \\frac{p(x_{1}|x_{2})}{q(x_{1}|x_{2},x_{0})}\n\\frac{p(x_{T-1}|x_{T})}{q(x_{T-1}|x_{T},x_{0})}\\frac{....}{....} \\frac{p(x_{T})}{q(x_{T}|x_{0})}\\]\nThen:\n\\[ - \\sum_{t=1}^{T} \\mathbb{E}_{x_{1},..,x_{T}} KL(q(x_{t-1}|x_{t},x_{0}) || p(x_{t-1}|x_{t}))) - KL(q(x_{T}|x_{0}||p(x_{T})) + \\int q(x_{1}|x_{0}) \\log p(x_{0}|x_{1})dx_{1}\\]\n\nIf \\(x_{1} \\approx x_{0}\\), then const\nAs for certain KL in terminate state ?\n\n\\(\\textbf{Idea}\\): Minimization of KL divergences\n\nfor simplicity: \\(p_{\\theta}(x_{t-1}|x_{t}) = \\mathcal{N}(\\mu_{\\theta}(x_{t},t),\\hat{\\beta_{t}}I)\\)\nMean matching:\n\n\\[ KL(q(x_{t-1}|x_{t},x_{0})|| p_{\\theta}(x_{t-1}|x_{t})) = \\mathbb{E}_{x_{0},x_{1},...,x_{T},t}\\frac{1}{2\\hat{\\beta_{t}}}|| \\hat{\\mu}_{t}(x_{t},x_{0}) - \\mu_{\\theta}(x_{t},t)||_{2}^{2}\\]\n\n\n\nChessUrl"
  },
  {
    "objectID": "projects/DDPM_eng.html#re-parameterization-like-simplification",
    "href": "projects/DDPM_eng.html#re-parameterization-like-simplification",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "6. Re-parameterization like simplification",
    "text": "6. Re-parameterization like simplification\nWe would like to approximate \\(\\mu_{\\theta}\\) by \\(\\hat{\\mu_{t}}\\), but there is the problem! \\(\\hat{\\mu_{t}}(x_{0})\\), while trainable mean does not depend on \\(x_{0}\\). As a consequence of that, we cannot converge to 0 this problem:\n\\(\\textbf{Re-parametrization:}\\)\n\\[\\hat{\\mu}_{t}(x_{t},x_{0})= \\frac{\\sqrt{\\overline{\\alpha}_{t-1}}}{1 - \\overline{\\alpha_{t}}}\\beta x_{0} +\\frac{\\sqrt{1 - \\beta}(1 - \\overline{\\alpha_{t}} )}{1 - \\overline{\\alpha_{t}}}x_{t}  \\]\n\\[ \\mu_{theta}(x_{t},t) = \\frac{\\sqrt{\\overline{\\alpha}_{t-1}}}{1 - \\overline{\\alpha_{t}}}\\beta \\color{red}{x_{\\theta}(x_{t},t)} +\\frac{\\sqrt{1 - \\beta}(1 - \\overline{\\alpha_{t}} )}{1 - \\overline{\\alpha_{t}}}x_{t} \\]\n\nRed term is as estimatiomn for \\(x_{0}\\)\n\nThen:\n\\[\\frac{1}{2\\hat{\\beta}_{t}}||\\hat{\\mu_{t}}(x_{t},x_{0}) - \\mu_{\\theta}(x_{t},t)||_{2}^{2}\n= \\frac{1}{2\\hat{\\beta}_{t}} || \\frac{\\sqrt{\\overline{\\alpha}_{t-1}}}{1-\\overline{\\alpha_{t}}}\\beta (x_{0} - x_{\\theta}(x_{t},t))||^{2}_{2}\\]\n\\(\\textbf{Final reparamterization:}\\)\n\\[ x_{t} = \\sqrt{\\overline{\\alpha}_{t}}x_{0} + \\sqrt{1 - \\overline{\\alpha}_{t}}\\hat{\\epsilon} \\]\nThen, one can express:\n\\[x_{0} = \\frac{1}{\\sqrt{\\overline{\\alpha_{t}}}}(x_{t} - \\sqrt{1 - \\overline{\\alpha}_{t}}\\hat{\\epsilon} ) \\]\nThe, one can reparametrize predicted value as:\n\\[x_{\\theta} = \\frac{1}{\\sqrt{\\overline{\\alpha_{t}}}}(x_{t} - \\sqrt{1 - \\overline{\\alpha}_{t}} \\epsilon_{\\theta}(x_{t},t) \\]\nThus, the final loss function:\n\\[ \\mathcal{L}(\\theta) = \\sum_{i=1}^{n}\\sum_{t=2}^{T} \\frac{\\beta^{2}}{2\\hat{\\beta}_{t}(1-\\beta)(1- \\overline{\\alpha}_{t})} ||  \\frac{x^{i}_{t} - \\sqrt{\\overline{\\alpha}}_{t}x_{0}^{i}}{\\sqrt{1-\\overline{\\alpha}}_{t}} - \\epsilon_{\\theta}(x^{i}_{t},t) ||\\]"
  },
  {
    "objectID": "projects/DDPM_eng.html#coding-part",
    "href": "projects/DDPM_eng.html#coding-part",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "7. Coding part",
    "text": "7. Coding part\n\nimport torch\nfrom torch.nn import init\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import TensorDataset\nfrom sklearn.datasets import make_moons\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n7.1 models\nIt is importantly to point out that our neural network will predict \\(\\textbf{noise}\\).\nOur model is neural network, that has 2 inputs:\n\n\\(x_{t}\\) data\n\\(t\\) time like condition for more accurate prediction noise of \\(x_{t}\\)\n\nIt is worth to notice that we have the same model for each step , we do not train new model for each new step. \\(\\color{red}{One\\quad model\\quad for\\quad all\\quad steps!}\\)\nAlso, You should understand that \\(t\\) is like a value condition and in order to estimate his influence to corresponding noise as a condition , it would be great to create embeddings of the time by corresponding network below.\n\nclass SinusoidalEmbedding(nn.Module):\n    def __init__(self, size: int, scale: float = 1.0):\n        super().__init__()\n        self.size = size\n        self.scale = scale\n\n    def forward(self, x: torch.Tensor):\n        x = x * self.scale\n        half_size = self.size // 2\n        emb = torch.log(torch.Tensor([10000.0])) / (half_size - 1)\n        emb = torch.exp(-emb * torch.arange(half_size))\n        emb = x.unsqueeze(-1) * emb.unsqueeze(0)\n        emb = torch.cat((torch.sin(emb), torch.cos(emb)), dim=-1)\n        return emb\n\n    def __len__(self):\n        return self.size\n\n\nclass PositionalEmbedding(nn.Module):\n    def __init__(self, size: int, type: str, **kwargs):\n        super().__init__()\n\n        self.layer = SinusoidalEmbedding(size, **kwargs)\n\n    def forward(self, x: torch.Tensor):\n        return self.layer(x)\n\n\nclass Block(nn.Module):\n    def __init__(self, size: int):\n        super().__init__()\n\n        self.ff = nn.Linear(size, size)\n        self.act = nn.GELU()\n\n    def forward(self, x: torch.Tensor):\n        return x + self.act(self.ff(x))\n\n\nclass MLP(nn.Module):\n    def __init__(self, hidden_size: int = 128, hidden_layers: int = 3, emb_size: int = 128,\n                 time_emb: str = \"sinusoidal\", input_emb: str = \"sinusoidal\"):\n        super().__init__()\n\n        self.time_mlp = PositionalEmbedding(emb_size, time_emb)\n        self.input_mlp1 = PositionalEmbedding(emb_size, input_emb, scale=25.0)\n        self.input_mlp2 = PositionalEmbedding(emb_size, input_emb, scale=25.0)\n\n        concat_size = len(self.time_mlp.layer) + \\\n            len(self.input_mlp1.layer) + len(self.input_mlp2.layer)\n        layers = [nn.Linear(concat_size, hidden_size), nn.GELU()]\n\n        for _ in range(hidden_layers):\n            layers.append(Block(hidden_size))\n\n        layers.append(nn.Linear(hidden_size, 2))\n        self.joint_mlp = nn.Sequential(*layers)\n\n    def forward(self, x, t):\n        x1_emb = self.input_mlp1(x[:, 0])\n        x2_emb = self.input_mlp2(x[:, 1])\n        t_emb = self.time_mlp(t)\n        x = torch.cat((x1_emb, x2_emb, t_emb), dim=-1)\n        x = self.joint_mlp(x)\n        return x\n\n\n\n7.2 Data\nWe use simple two moons dataset as data.\n\ndef moons_dataset(n=8000):\n    X, _ = make_moons(n_samples=n, random_state=42, noise=0.03)\n    X[:, 0] = (X[:, 0] + 0.3) * 2 - 1\n    X[:, 1] = (X[:, 1] + 0.3) * 3 - 1\n    return TensorDataset(torch.from_numpy(X.astype(np.float32)))\n\n\n\n7.3 Gaussian Diffusion\nThis calss is composed of whole formulas from senimar , please familiari`e you with these formulas accurately.\n\nreconstruct x0:\n\n\\[x_{0} = \\frac{1}{\\sqrt{\\overline{\\alpha_{t}}}}(x_{t} - \\sqrt{1 - \\overline{\\alpha}_{t}}\\hat{\\epsilon} ) \\]\n\nq posterior:\n\n\\[\\hat{\\mu}_{t}(x_{t},x_{0})= \\frac{\\sqrt{\\overline{\\alpha}_{t-1}}}{1 - \\overline{\\alpha_{t}}}\\beta x_{0} +\\frac{\\sqrt{1 - \\beta}(1 - \\overline{\\alpha_{t}} )}{1 - \\overline{\\alpha_{t}}}x_{t}  \\]\n\nget_variance:\n\n\\[\\hat{\\beta_{t}} = \\beta_{t}(1 - \\overline{\\alpha_{t-1}}) \\frac{1}{1 - \\overline{\\alpha}_{t}}\\]\n\nstep:\n\n\nThe sampling moments of time to get \\(x_{t}\\)\nMake the prediction \\(x_{0}\\)\nMake the prediction for \\(x_{t-1}\\)\nCalculate noise\nMake one step of the backward process\n\n\nadd noise:\n\n\nThe expression noise through \\(x_{0}\\) and \\(x_{t}\\)\n\\(\\hat{\\epsilon} =  \\sqrt{\\overline{\\alpha_{t}}}\\frac{x_{0}}{\\sqrt{1 -\\overline{\\alpha_{t}}  }} - x_{t}\\frac{1}{\\sqrt{1- \\overline{\\alpha_{t}}}}\\)\n\n\nclass NoiseScheduler():\n    def __init__(self,\n                 num_timesteps=1000,\n                 beta_start=0.0001,\n                 beta_end=0.02,\n                 beta_schedule=\"linear\"):\n\n        self.num_timesteps = num_timesteps\n        if beta_schedule == \"linear\":\n            self.betas = torch.linspace(\n                beta_start, beta_end, num_timesteps, dtype=torch.float32)\n        elif beta_schedule == \"quadratic\":\n            self.betas = torch.linspace(\n                beta_start ** 0.5, beta_end ** 0.5, num_timesteps, dtype=torch.float32) ** 2\n\n        self.alphas = 1.0 - self.betas\n        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n        self.alphas_cumprod_prev = F.pad(\n            self.alphas_cumprod[:-1], (1, 0), value=1.)\n\n        # required for self.add_noise\n        self.sqrt_alphas_cumprod = self.alphas_cumprod ** 0.5\n        self.sqrt_one_minus_alphas_cumprod = (1 - self.alphas_cumprod) ** 0.5\n\n        # required for reconstruct_x0\n        self.sqrt_inv_alphas_cumprod = torch.sqrt(1 / self.alphas_cumprod)\n        self.sqrt_inv_alphas_cumprod_minus_one = torch.sqrt(\n            1 / self.alphas_cumprod - 1)\n\n        # required for q_posterior\n        self.posterior_mean_coef1 = self.betas * torch.sqrt(self.alphas_cumprod_prev) / (1. - self.alphas_cumprod)\n        self.posterior_mean_coef2 = (1. - self.alphas_cumprod_prev) * torch.sqrt(self.alphas) / (1. - self.alphas_cumprod)\n\n\n    def reconstruct_x0(self, x_t, t, noise):\n        s1 = self.sqrt_inv_alphas_cumprod[t]\n        s2 = self.sqrt_inv_alphas_cumprod_minus_one[t]\n        s1 = s1.reshape(-1, 1)\n        s2 = s2.reshape(-1, 1)\n        return s1 * x_t - s2 * noise\n\n    def q_posterior(self, x_0, x_t, t):\n        s1 = self.posterior_mean_coef1[t]\n        s2 = self.posterior_mean_coef2[t]\n        s1 = s1.reshape(-1, 1)\n        s2 = s2.reshape(-1, 1)\n        mu = s1 * x_0 + s2 * x_t\n        return mu\n\n    def get_variance(self, t):\n        if t == 0:\n            return 0\n\n        variance = self.betas[t] * (1. - self.alphas_cumprod_prev[t]) / (1. - self.alphas_cumprod[t])\n        variance = variance.clip(1e-20)\n        return variance\n\n    def step(self, model_output, timestep, sample):\n        t = timestep\n        pred_original_sample = self.reconstruct_x0(sample, t, model_output)\n        pred_prev_sample = self.q_posterior(pred_original_sample, sample, t)\n\n        variance = 0\n        if t &gt; 0:\n            noise = torch.randn_like(model_output)\n            variance = (self.get_variance(t) ** 0.5) * noise\n\n        pred_prev_sample = pred_prev_sample + variance\n\n        return pred_prev_sample\n\n    def add_noise(self, x_start, x_noise, timesteps):\n        s1 = self.sqrt_alphas_cumprod[timesteps]\n        s2 = self.sqrt_one_minus_alphas_cumprod[timesteps]\n\n        s1 = s1.reshape(-1, 1)\n        s2 = s2.reshape(-1, 1)\n\n        return s1 * x_start + s2 * x_noise\n\n    def __len__(self):\n        return self.num_timesteps\n\n\n\n7.4 Training\nWe choose hyper parameters for method and run it\n\nNUM_SAMPLES_DATA = 10_000\nBATCH_SIZE=128\n\nHIDDEN_SIZE = 128\nHIDDEN_LAYERS=3\nEMBEDDING_SIZE = 128\nTIME_EMBEDDING=\"sinusoidal\"\nINPUT_EMEDDING=\"sinusoidal\"\n\nNUM_TIMESTEPS = 50\nBETA_SCHEDULE = 'linear'\nLR = 5e-4\n\nNUM_EPOCHS=200\n\n\ndataset = moons_dataset(NUM_SAMPLES_DATA)\ndataloader = torch.utils.data.DataLoader(\n        dataset, batch_size=BATCH_SIZE , shuffle=True, drop_last=True)\n\nmodel = MLP(\n        hidden_size=HIDDEN_SIZE,\n        hidden_layers=HIDDEN_LAYERS,\n        emb_size=EMBEDDING_SIZE,\n        time_emb=TIME_EMBEDDING,\n        input_emb=INPUT_EMEDDING)\n\nnoise_scheduler = NoiseScheduler(\n        num_timesteps=NUM_TIMESTEPS,\n        beta_schedule=BETA_SCHEDULE)\n\noptimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr=LR,\n    )\n\n\nglobal_step = 0\nframes = []\nlosses = []\n\nfor epoch in tqdm(range(NUM_EPOCHS)):\n\n    model.train()\n\n\n    for step, batch in enumerate(dataloader):\n        batch = batch[0]\n        noise = torch.randn(batch.shape)\n        timesteps = torch.randint(a\n            0, noise_scheduler.num_timesteps, (batch.shape[0],)\n        ).long()\n\n        noisy = noise_scheduler.add_noise(batch, noise, timesteps)\n        noise_pred = model(noisy, timesteps)\n        loss = F.mse_loss(noise_pred, noise)\n        loss.backward(loss)\n\n        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        optimizer.zero_grad()\n        losses.append(loss.detach().item())\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:07&lt;00:00,  1.56it/s]\n\n\n\nmodel.eval()\nsample = torch.randn(1024, 2) # sampling from noise\ntimesteps = list(range(len(noise_scheduler)))[::-1]\nfor i, t in enumerate(tqdm(timesteps)):\n    t = torch.from_numpy(np.repeat(t,  1024)).long()\n    with torch.no_grad():\n        residual = model(sample, t)\n    sample = noise_scheduler.step(residual, t[0], sample)\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00&lt;00:00, 104.94it/s]\n\n\n\nplt.scatter(sample[:,0],sample[:,1], edgecolor='black', label=\"generated data\")\nplt.grid();\nplt.legend();\n\n\n\n\n\n\n\n\n\nmodel.eval()\ntraj = []\nsample = torch.randn(1024, 2) # sampling from noise\ntimesteps = list(range(len(noise_scheduler)))[::-1]\nfor i, t in enumerate(tqdm(timesteps)):\n    t = torch.from_numpy(np.repeat(t,  1024)).long()\n    with torch.no_grad():\n        residual = model(sample, t)\n    sample = noise_scheduler.step(residual, t[0], sample)\n    if t[0].item() % 5 == 0:\n        traj.append(sample.cpu())\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00&lt;00:00, 118.63it/s]\n\n\n\nfig,ax = plt.subplots(1,10,figsize=(40,4),dpi=200)\nfor idx in range(10):\n    ax[idx].scatter(traj[9-idx][:,0],traj[9-idx][:,1],edgecolor='black')\n    ax[idx].set_xticks([]);ax[idx].set_yticks([])\n    ax[idx].grid();\nfig.tight_layout(pad=0.01)"
  },
  {
    "objectID": "projects/RealNVP.html",
    "href": "projects/RealNVP.html",
    "title": "Addisu Amare",
    "section": "",
    "text": "REAL-NVP Modified\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_moons\nfrom collections import defaultdict\nfrom tqdm.notebook import tqdm\nfrom typing import Tuple\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as data\n\nUSE_CUDA = torch.cuda.is_available()\nWe have already familiarized ourselves with normalizing flows and REAL-nvp method. For best understanding ,please, write the paper RealNVP."
  },
  {
    "objectID": "projects/RealNVP.html#data",
    "href": "projects/RealNVP.html#data",
    "title": "Addisu Amare",
    "section": "1. Data",
    "text": "1. Data\n\nTICKS_FONT_SIZE = 12\nLEGEND_FONT_SIZE = 12\nLABEL_FONT_SIZE = 14\nTITLE_FONT_SIZE = 16\n\n\ndef visualize_2d_data(train_data, test_data, train_labels=None, test_labels=None):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n    ax1.set_title('train', fontsize=TITLE_FONT_SIZE)\n    ax1.scatter(train_data[:, 0], train_data[:, 1], s=1, c=train_labels)\n    ax1.tick_params(labelsize=LABEL_FONT_SIZE)\n    ax2.set_title('test', fontsize=TITLE_FONT_SIZE)\n    ax2.scatter(test_data[:, 0], test_data[:, 1], s=1, c=test_labels)\n    ax2.tick_params(labelsize=LABEL_FONT_SIZE)\n    plt.show()\n\n\ndef generate_moons_data(count: int) -&gt; tuple:\n    data, labels = make_moons(n_samples=count, noise=0.1)\n    data = data.astype(\"float32\")\n    split = int(0.8 * count)\n    train_data, test_data = data[:split], data[split:]\n    train_labels, test_labels = labels[:split], labels[split:]\n    return train_data, train_labels, test_data, test_labels\n\n\nCOUNT = 5000\n\ntrain_data, train_labels, test_data, test_labels = generate_moons_data(COUNT)\nvisualize_2d_data(train_data, test_data, train_labels, test_labels)"
  },
  {
    "objectID": "projects/RealNVP.html#transformation",
    "href": "projects/RealNVP.html#transformation",
    "title": "Addisu Amare",
    "section": "2. Transformation",
    "text": "2. Transformation\nREAL-NVP model is a sequence of the affine coupling layers.\nForward transform: \\[\n    \\begin{cases}\n        \\mathbf{y}_1 &= \\mathbf{x}_1; \\\\\n        \\mathbf{y}_2 &= \\mathbf{x}_2 \\odot \\exp (s(\\mathbf{x}_1)) + t(\\mathbf{x}_1).\n    \\end{cases}\n\\]\nInverse transform: \\[\n    \\begin{cases}\n        \\mathbf{x}_1 &= \\mathbf{y}_1; \\\\\n        \\mathbf{x}_2 &= (\\mathbf{y}_2 - t(\\mathbf{y}_1)) \\odot \\exp ( - s(\\mathbf{y}_1)).\n    \\end{cases}\n\\]\nHere \\(s(\\cdot)\\) and \\(t(\\cdot)\\) are outputs of neural network. In this task our networks will be fully connected MLP.\n\nclass FullyConnectedMLP(nn.Module):\n    def __init__(self, input_shape: int, hiddens: list, output_shape: int) -&gt; None:\n        assert isinstance(hiddens, list)\n        super().__init__()\n        self.input_shape = (input_shape,)\n        self.output_shape = (output_shape,)\n        self.hiddens = hiddens\n\n        model = []\n\n        # ====\n        # your code\n        # Stack Dense layers with ReLU activation.\n        # Note that you do not have to add relu after the last dense layer\n        prev_h = input_shape\n        for h in hiddens:\n            model.append(nn.Linear(prev_h, h))\n            model.append(nn.ReLU())\n            prev_h = h\n        model.append(nn.Linear(hiddens[-1], output_shape))\n        # ====\n        self.net = nn.Sequential(*model)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # ====\n        # your code\n        # apply network that was defined in __init__ and return the output\n        batch_size = x.shape[0]\n        x = x.view(batch_size, -1)\n        return self.net(x).view(batch_size, *self.output_shape)\n        # ====\n\n\nclass AffineCouplingLayer(nn.Module):\n    def __init__(self, parity_type: bool, n_hiddens: list) -&gt; None:\n        assert isinstance(parity_type, bool)\n        assert isinstance(n_hiddens, list)\n        super().__init__()\n        self.mask = self.build_mask(parity_type=parity_type)\n        self.mlp = FullyConnectedMLP(input_shape=2, hiddens=n_hiddens, output_shape=2)\n\n    def build_mask(self, parity_type: bool) -&gt; torch.Tensor:\n        # ====\n        # your code\n        # the mask is extremely simple\n        # it is a float tensor of two scalars (1.0 and 0.0)\n        # the partition_type defines the order of these two scalars\n        if parity_type:\n            return torch.FloatTensor([1.0, 0.0])\n        else:\n            return torch.FloatTensor([0.0, 1.0])\n        # ====\n\n    def forward(self, x: torch.Tensor, invert: bool = False) -&gt; tuple:\n        # ====\n        # your code\n        # 1) mask our input x, using self.mask\n        # 2) apply mlp to masked input to get s and t\n        batch_size = x.shape[0]\n        mask = self.mask.repeat(batch_size, 1).cuda()\n        x_masked = x * mask\n\n        s, t = self.mlp(x_masked).split(1, dim=1)\n        # ====\n\n        # we invert mask here\n        t = t * (1.0 - mask)\n        s = s * (1.0 - mask)\n\n        # ====\n        # your code\n        # apply forward (invert=False) or inverse (invert=True) transform (invert=False)\n        if invert:\n            x = (x - t) * torch.exp(-s)\n        else:\n            x = x * torch.exp(s) + t\n        # ====\n\n        # the output is transformed input\n        # and logarithm of jacobian (which equals to s)\n        return x, s"
  },
  {
    "objectID": "projects/RealNVP.html#realnvp",
    "href": "projects/RealNVP.html#realnvp",
    "title": "Addisu Amare",
    "section": "3. RealNVP",
    "text": "3. RealNVP\nWe are ready to define RealNVP model. The model objective is the negative value of log-likelihood. Log-likelihood is given by the change of variables (CoV) theorem: \\[\n    \\log p(\\mathbf{x}| \\boldsymbol{\\theta}) = \\log p(\\mathbf{z}) + \\log \\left|\\det \\left(  \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}} \\right) \\right| = \\log p(f(\\mathbf{x}, \\boldsymbol{\\theta})) + \\log \\left|\\det \\left( \\frac{\\partial f(\\mathbf{x}, \\boldsymbol{\\theta})}{\\partial \\mathbf{x}} \\right) \\right|.\n\\]\n\nclass RealNVP(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n\n        # base distribution p(z) is normal\n        self.prior = torch.distributions.Normal(torch.tensor(0.0), torch.tensor(1.0))\n        # ====\n        # your code\n        # apply sequence of AffineCouplingLayer with alternating parity_type\n        # 6 layers is sufficient (with 2 hidden layers in each affine layer)\n        self.transforms = nn.ModuleList(\n            [\n                AffineCouplingLayer(True, n_hiddens=[64, 64]),\n                AffineCouplingLayer(False, n_hiddens=[64, 64]),\n                AffineCouplingLayer(True, n_hiddens=[64, 64]),\n                AffineCouplingLayer(False, n_hiddens=[64, 64]),\n                AffineCouplingLayer(True, n_hiddens=[64, 64]),\n                AffineCouplingLayer(False, n_hiddens=[64, 64]),\n            ]\n        )\n        # ====\n\n    def forward(self, x: torch.Tensor, invert: bool = False) -&gt; tuple:\n        z = x\n        log_det = 0.0\n\n        # ====\n        # your code\n        # apply sequence of transforms and sum all of log_dets\n        # if invert == True, you have to apply transforms in reversed order (from last to first!) with invert=True flag\n        transforms = reversed(self.transforms) if invert else self.transforms\n        for transform in transforms:\n            z, delta_log_det = transform(z, invert=invert)\n            log_det += delta_log_det\n\n        # ====\n        return z, log_det\n\n    def log_prob(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # ====\n        # your code\n        # 1) make forward pass with right inverse flag\n        # 2) sum log_det with log of base distribution (log p(z)) - see the formula above\n        # 3) we will get tensor of shape [batch_size, 2] - sum it over the the last dimension\n        z, log_det = self.forward(x, invert=False)\n\n        return torch.sum(log_det, dim=1) + torch.sum(self.prior.log_prob(z), dim=1)\n        # ====\n\n    def loss(self, x: torch.Tensor) -&gt; dict:\n        log_prob = self.log_prob(x)\n        # log_prob should be a vector of batch_size\n        assert len(log_prob.shape) == 1\n        return {\"nll_loss\": -log_prob.mean()}\n\n    def sample(self, n: int) -&gt; torch.Tensor:\n        # ====\n        # your code\n        # 1) sample from the prior\n        # 2) apply the forward pass with the right inverse flag\n        # 3) return only the first output of the forward pass (the second is the log of determinant - we don't need it in sampling)\n        z = self.prior.sample([n, 2]).cuda()\n        return self.forward(z, invert=True)[0]\n        # ===="
  },
  {
    "objectID": "projects/RealNVP.html#training",
    "href": "projects/RealNVP.html#training",
    "title": "Addisu Amare",
    "section": "4. Training",
    "text": "4. Training\n\ndef train_epoch(\n    model: object,\n    train_loader: object,\n    optimizer: object,\n    use_cuda: bool,\n    loss_key: str = \"total\",\n) -&gt; defaultdict:\n    model.train()\n\n    stats = defaultdict(list)\n    for x in train_loader:\n        if use_cuda:\n            x = x.cuda()\n        losses = model.loss(x)\n        optimizer.zero_grad()\n        losses[loss_key].backward()\n        optimizer.step()\n\n        for k, v in losses.items():\n            stats[k].append(v.item())\n\n    return stats\n\n\ndef eval_model(model: object, data_loader: object, use_cuda: bool) -&gt; defaultdict:\n    model.eval()\n    stats = defaultdict(float)\n    with torch.no_grad():\n        for x in data_loader:\n            if use_cuda:\n                x = x.cuda()\n            losses = model.loss(x)\n            for k, v in losses.items():\n                stats[k] += v.item() * x.shape[0]\n\n        for k in stats.keys():\n            stats[k] /= len(data_loader.dataset)\n    return stats\n\n\ndef train_model(\n    model: object,\n    train_loader: object,\n    test_loader: object,\n    epochs: int,\n    lr: float,\n    use_tqdm: bool = False,\n    use_cuda: bool = False,\n    loss_key: str = \"total_loss\",\n) -&gt; Tuple[dict, dict]:\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    train_losses = defaultdict(list)\n    test_losses = defaultdict(list)\n    forrange = tqdm(range(epochs)) if use_tqdm else range(epochs)\n    if use_cuda:\n        model = model.cuda()\n\n    for epoch in forrange:\n        model.train()\n        train_loss = train_epoch(model, train_loader, optimizer, use_cuda, loss_key)\n        test_loss = eval_model(model, test_loader, use_cuda)\n\n        for k in train_loss.keys():\n            train_losses[k].extend(train_loss[k])\n            test_losses[k].append(test_loss[k])\n    return dict(train_losses), dict(test_losses)\n\n\n# ====\n# your code\n# choose these parameters\n\nBATCH_SIZE = 128  # any adequate value\nEPOCHS = 50  # &lt; 100\nLR = 0.001  # &lt; 1e-2\n# ====\n\nCOUNT = 5000\n\ntrain_data, train_labels, test_data, test_labels = generate_moons_data(COUNT)\n\nloader_args = dict(batch_size=BATCH_SIZE, shuffle=True)\ntrain_loader = torch.utils.data.DataLoader(train_data, **loader_args)\ntest_loader = torch.utils.data.DataLoader(test_data, **loader_args)\n\n# model\nmodel = RealNVP()\n\n \n\n\n# train\ntrain_losses, test_losses = train_model(\n    model,\n    train_loader,\n    test_loader,\n    epochs=EPOCHS,\n    lr=LR,\n    loss_key=\"nll_loss\",\n    use_cuda=USE_CUDA,\n    use_tqdm=True,\n)"
  },
  {
    "objectID": "projects/RealNVP.html#inference",
    "href": "projects/RealNVP.html#inference",
    "title": "Addisu Amare",
    "section": "5.inference",
    "text": "5.inference\n\ndef plot_training_curves(train_losses, test_losses, logscale_y=False, logscale_x=False):\n    n_train = len(train_losses[list(train_losses.keys())[0]])\n    n_test = len(test_losses[list(train_losses.keys())[0]])\n    x_train = np.linspace(0, n_test - 1, n_train)\n    x_test = np.arange(n_test)\n\n    plt.figure()\n    for key, value in train_losses.items():\n        plt.plot(x_train, value, label=key + '_train')\n\n    for key, value in test_losses.items():\n        plt.plot(x_test, value, label=key + '_test')\n\n    if logscale_y:\n        plt.semilogy()\n    \n    if logscale_x:\n        plt.semilogx()\n\n    plt.legend(fontsize=LEGEND_FONT_SIZE)\n    plt.xlabel('Epoch', fontsize=LABEL_FONT_SIZE)\n    plt.ylabel('Loss', fontsize=LABEL_FONT_SIZE)\n    plt.xticks(fontsize=TICKS_FONT_SIZE)\n    plt.yticks(fontsize=TICKS_FONT_SIZE)\n    plt.grid()\n    plt.show()\n\n\nplot_training_curves(train_losses, test_losses)\n\n\n\n\n\n\n\n\n\ndef visualize_2d_densities(x_grid, y_grid, densities, title, xlabel=None, ylabel=None):\n    densities = densities.reshape([y_grid.shape[0], y_grid.shape[1]])\n    plt.figure(figsize=(5, 5))\n    plt.pcolor(x_grid, y_grid, densities)\n    plt.pcolor(x_grid, y_grid, densities)\n\n    plt.title(title, fontsize=TITLE_FONT_SIZE)\n    plt.xticks(fontsize=TICKS_FONT_SIZE)\n    plt.yticks(fontsize=TICKS_FONT_SIZE)\n    if xlabel is not None:\n        plt.xlabel(xlabel, fontsize=LABEL_FONT_SIZE)\n    if ylabel is not None:\n        plt.ylabel(ylabel, fontsize=LABEL_FONT_SIZE)\n    plt.show()\n    \n\n\ndef visualize_2d_samples(data, title, labels=None, xlabel=None, ylabel=None):\n    plt.figure(figsize=(5, 5))\n    plt.scatter(data[:, 0], data[:, 1], s=1, c=labels)\n    plt.title(title, fontsize=TITLE_FONT_SIZE)\n    plt.xticks(fontsize=TICKS_FONT_SIZE)\n    plt.yticks(fontsize=TICKS_FONT_SIZE)\n    if xlabel is not None:\n        plt.xlabel(xlabel, fontsize=LABEL_FONT_SIZE)\n    if ylabel is not None:\n        plt.ylabel(ylabel, fontsize=LABEL_FONT_SIZE)\n    plt.show()\n\n\n# Samples\n\nx_samples = model.sample(4000).cpu().detach().numpy()\nvisualize_2d_samples(x_samples, title=\"Data space\", xlabel=\"x1\", ylabel=\"x2\")\n\n# Density\ndx, dy = 0.025, 0.025\nx_lim = (-1.5, 2.5)\ny_lim = (-1, 1.5)\ny, x = np.mgrid[slice(y_lim[0], y_lim[1] + dy, dy), slice(x_lim[0], x_lim[1] + dx, dx)]\nmesh_xs = torch.FloatTensor(np.stack([x, y], axis=2).reshape(-1, 2))\n\nif USE_CUDA:\n    mesh_xs = mesh_xs.cuda()\n\ndensities = np.exp(model.log_prob(mesh_xs).cpu().detach().numpy())\n\n# Latents\ntrain_tensor = torch.FloatTensor(train_data)\nif USE_CUDA:\n    train_tensor = train_tensor.cuda()\nz = model(train_tensor)[0]\nlatents = z.cpu().detach().numpy()\n \nvisualize_2d_densities(x, y, densities, title=\"Densities\", xlabel=\"x1\", ylabel=\"x2\")\nvisualize_2d_samples(\n    latents, title=\"Latent space\", labels=train_labels, xlabel=\"z1\", ylabel=\"z2\"\n)\n\n\n\n\n\n\n\n\nMatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3.  Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading'].  This will become an error two minor releases later.\n  plt.pcolor(x_grid, y_grid, densities)\n&lt;ipython-input-24-2953d9ef525c&gt;:5: MatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3.  Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading'].  This will become an error two minor releases later.\n  plt.pcolor(x_grid, y_grid, densities)"
  },
  {
    "objectID": "projects/Efficient_model_training.html",
    "href": "projects/Efficient_model_training.html",
    "title": "",
    "section": "",
    "text": "Efficient Deep learning training"
  },
  {
    "objectID": "projects/Efficient_model_training.html#section-1",
    "href": "projects/Efficient_model_training.html#section-1",
    "title": "",
    "section": "",
    "text": "These days models and datasets are becoming larger and larger and it is becoming increasingly important to increase the efficiency of the training procedure by optimizing memory utilization, speeding up the training, or both.\nIn this work shop we will cover several techniques that could speed up the training of the model and even sometimes make the training feasible: * Batch size optimization * Automated mixed precision * Gradient accumulation * Gradient checkpointing * Model compilation (for torch&gt;=2.0)"
  },
  {
    "objectID": "projects/Efficient_model_training.html#preparatory-stuff",
    "href": "projects/Efficient_model_training.html#preparatory-stuff",
    "title": "",
    "section": "Preparatory stuff",
    "text": "Preparatory stuff\nLoading stuff and defining helper functions\n\nimport time\nfrom tqdm import trange\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.cuda.amp import GradScaler\nfrom torchvision.models import get_model\n\n\ndef print_memory_stats():\n    print(f\"CUDA max memory allocated: {torch.cuda.max_memory_allocated() / 2 ** 30:.2f} GB.\")\n    print(f\"CUDA max memory reserved: {torch.cuda.max_memory_reserved() / 2 ** 30:.2f} GB.\")\n\nMake sure that you are using CUDA Runtime!\n\ndevice = \"cuda\"\n\nSince we have not started using GPU the cell below should output 0.\n\nprint_memory_stats()\n\nCUDA max memory allocated: 0.00 GB.\nCUDA max memory reserved: 0.00 GB.\n\n\nIn the following next sections we‚Äôll illustrate the techiques on a model from the torchvision library. We will adopt SwinTransformer-Base for the demonstrations below.\n\nmodel = get_model(\"swin_b\").to(device) # it is randomly initialized, but we do not care\n\n\nprint(f\"Number of paramerers: {sum(param.numel() for param in model.parameters()) / 10 ** 6 :.2f} M\")\n\nNumber of paramerers: 87.77 M\n\n\nMemory footprint should be roughly 4 * num_model_parameters since by default we are working in single precision float32.\n\nprint_memory_stats()\n\nCUDA max memory allocated: 0.33 GB.\nCUDA max memory reserved: 0.36 GB.\n\n\nDefine a function that yields dummy inputs and targets.\n\ndef make_dummy_dataloader(batch_size: int, img_size: int = 224):\n  while True:\n    yield torch.randn(batch_size, 3, img_size, img_size), torch.randint(low=0, high=1000, size=(batch_size,))\n\nBaseline training loop.\n\ndef train(model, loader, optimizer, loss_fn, num_steps):\n  samples_processed = 0\n  start = time.perf_counter()\n  for i in trange(num_steps, total=num_steps, desc=\"Train\"):\n    inputs, targets = next(loader)\n    inputs, targets = inputs.to(device), targets.to(device)\n    outputs = model(inputs)\n    loss = loss_fn(outputs, targets)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    samples_processed += len(inputs)\n  end = time.perf_counter()\n  samples_per_sec = samples_processed / (end - start)\n  print(\"\\n\")\n  print_memory_stats()\n  print(f\"samples/sec: {samples_per_sec:.2f}\")\n  results = {\n      \"samples/sec\": samples_per_sec,\n      \"max_memory_allocated\": torch.cuda.max_memory_allocated() / 2 ** 30\n  }\n  return results"
  },
  {
    "objectID": "projects/Efficient_model_training.html#batch-size-optimization",
    "href": "projects/Efficient_model_training.html#batch-size-optimization",
    "title": "",
    "section": "Batch size optimization",
    "text": "Batch size optimization\nMaximizing the throughput (samples/second) leads to lower training cost. This is generally achieved by utilizing the GPU as much as possible and thus filling GPU memory to its limit.\nBelow we will show the dependence of the model throughput on the batch size.\n\ntraining_steps = 100\n\n\nresults_for_batch_size = {}\nfor batch_size in (1, 2, 4, 8, 16, 32, 64, 128):\n  print(\"-\" * 10)\n  print(f\"Batch size: {batch_size}\")\n  # prepare dataloader\n  dataloader = make_dummy_dataloader(batch_size)\n  # prepare optimizer\n  optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n  # train\n  try:\n    results_for_batch_size[batch_size] = train(model, dataloader, optimizer, F.cross_entropy, training_steps)\n  except:\n    print(\"\\nCUDA out of memory!\")\n    print_memory_stats()\n    del optimizer\n    torch.cuda.empty_cache()\n    torch.cuda.reset_peak_memory_stats()\n    break\n  del optimizer\n  torch.cuda.empty_cache()\n  torch.cuda.reset_peak_memory_stats()\n\n----------\nBatch size: 1\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:12&lt;00:00,  7.71it/s]\n\n\n\n\nCUDA max memory allocated: 1.67 GB.\nCUDA max memory reserved: 1.78 GB.\nsamples/sec: 7.71\n----------\nBatch size: 2\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10&lt;00:00,  9.18it/s]\n\n\n\n\nCUDA max memory allocated: 1.68 GB.\nCUDA max memory reserved: 1.86 GB.\nsamples/sec: 18.34\n----------\nBatch size: 4\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:16&lt;00:00,  6.20it/s]\n\n\n\n\nCUDA max memory allocated: 2.34 GB.\nCUDA max memory reserved: 2.59 GB.\nsamples/sec: 24.78\n----------\nBatch size: 8\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:28&lt;00:00,  3.46it/s]\n\n\n\n\nCUDA max memory allocated: 2.78 GB.\nCUDA max memory reserved: 3.11 GB.\nsamples/sec: 27.67\n----------\nBatch size: 16\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:56&lt;00:00,  1.78it/s]\n\n\n\n\nCUDA max memory allocated: 4.43 GB.\nCUDA max memory reserved: 4.89 GB.\nsamples/sec: 28.54\n----------\nBatch size: 32\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:46&lt;00:00,  1.06s/it]\n\n\n\n\nCUDA max memory allocated: 7.88 GB.\nCUDA max memory reserved: 8.65 GB.\nsamples/sec: 30.11\n----------\nBatch size: 64\n\n\nTrain:   1%|          | 1/100 [00:03&lt;05:09,  3.13s/it]\n\n\n\nCUDA out of memory!\nCUDA max memory allocated: 14.00 GB.\nCUDA max memory reserved: 14.58 GB.\n\n\n\n\n\nLet us plot samples/sec and max memory vs batch size.\n\nbatch_sizes = []\nsamples_sec_per_batch_size = []\nmax_memory_per_batch_size = []\nfor k, v in results_for_batch_size.items():\n  batch_sizes.append(k)\n  samples_sec_per_batch_size.append(v['samples/sec'])\n  max_memory_per_batch_size.append(v['max_memory_allocated'])\n\n\nfig, ax = plt.subplots(ncols=2, figsize=(12, 5))\n\nax[0].plot(batch_sizes, samples_sec_per_batch_size, '-v');\nax[0].set_xlabel(\"Batch size\", fontsize=20);\nax[0].set_ylabel(\"Samples/sec\", fontsize=20);\n\nax[1].plot(batch_sizes, max_memory_per_batch_size, '-v');\nax[1].set_xlabel(\"Batch size\", fontsize=20);\nax[1].set_ylabel(\"Max memory (Gb)\", fontsize=20);\n\n\n\n\n\n\n\n\nNote!\nBatch size (together with learning rate) determine the degree of stochasticity of gradient descent.\nOften, a certain magnitude of noise in optimization trajectory is crucial for finding well generalizing solutions.\nTherefore, the batch size has to be chosen wisely ü§î."
  },
  {
    "objectID": "projects/Efficient_model_training.html#automated-mixed-precision",
    "href": "projects/Efficient_model_training.html#automated-mixed-precision",
    "title": "",
    "section": "Automated mixed precision",
    "text": "Automated mixed precision\nMachines work with number in finite precision. Different formats are defined in IEEE754.\nUnlike some other applications, such as physics and engineering neural networks are more robust to numerical errors and up to a certain point one reduce precision without impact on performance.\nLower precision offers faster operations with numbers and reduced memory usage.\nBelow we‚Äôll illustrate some commonly used formats.\nFP32 (default in PyTorch)\nThis format was the workhorse of deep learning for a long time.\n\n1 bit sign\n8 bits exponent\n23 bits fraction (aka mantissa)\n\n\nRange: ~1.18e-38 ‚Ä¶ ~3.40e38 with 6-9 significant decimal digits precision.\nFP16\nArguably, the most popular format for large model training and inference:\n\n1 bit sign\n5 bits exponent\n10 bits fraction\n\n\nRange: ~5.96e‚àí8 (6.10e‚àí5) ‚Ä¶ 65504 with 4 significant decimal digits precision.\nBF16\nBrain Float format developed by Google. Has higher dynamical range compared to FP16, but lower precision.\n\n1 bit sign\n8 bits exponent\n7 bits fraction\n\nRange: ~5.96e‚àí8 (6.10e‚àí5) ‚Ä¶ 65504 with 4 significant decimal digits precision.\n\nRange: ~1.18e-38 ‚Ä¶ ~3.40e38 with 3 significant decimal digits.\nNote\nThis format is supported only for NVIDIA architectures from Ampere and newer (A100, RTX30**, RTX40**). Older GPUs, such as fellow T4, do not have hardware support for it.\nFor more details on numerical formats I recommend this blog.\nAutomated mixed precision runs some operations (nn.Linear, nn.Conv*d) in fp16 , whereas other operations that are more vulnerable to numerical approximations are run in higher precision fp32. Gradients w/r to model parameters and optimizer stats are typically kept in fp32. Parameter updates act on fp32 copy of the model.\n\ndef train(model, loader, optimizer, scaler, loss_fn, num_steps):\n  \"\"\"\n  Args:\n    model - model trained\n    loader - data loader\n    optimizer - optimizer\n    scaler - gradient scaler that shifts the gradient range\n    loss_fn - task loss function\n    num_steps.- number of training steps\n  Returns:\n    dict with performance stats\n  \"\"\"\n  samples_processed = 0\n  start = time.perf_counter()\n  for i in trange(num_steps, total=num_steps, desc=\"Train\"):\n    inputs, targets = next(loader)\n    inputs, targets = inputs.to(device), targets.to(device)\n    with torch.autocast(device_type='cuda', dtype=torch.float16):\n      outputs = model(inputs)\n      loss = loss_fn(outputs, targets)\n    assert not loss.isnan(), \"Loss is NaN. Terminating training.\"\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n    optimizer.zero_grad()\n    samples_processed += len(inputs)\n  end = time.perf_counter()\n  samples_per_sec = samples_processed / (end - start)\n  print(\"\\n\")\n  print_memory_stats()\n  print(f\"samples/sec: {samples_per_sec:.2f}\")\n  results = {\n      \"samples/sec\": samples_per_sec,\n      \"max_memory_allocated\": torch.cuda.max_memory_allocated() / 2 ** 30\n  }\n  return results\n\n\nresults_for_batch_size_amp = {}\nfor batch_size in (1, 2, 4, 8, 16, 32, 64, 128):\n  print(\"-\" * 10)\n  print(f\"Batch size: {batch_size}\")\n  # prepare dataloader\n  dataloader = make_dummy_dataloader(batch_size)\n  # prepare optimizer\n  optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n  # prepare scaler\n  scaler = GradScaler()\n  # train\n  try:\n    results_for_batch_size_amp[batch_size] = train(model, dataloader, optimizer, scaler, F.cross_entropy, training_steps)\n  except:\n    print(\"\\nCUDA out of memory!\")\n    print_memory_stats()\n    del optimizer\n    torch.cuda.empty_cache()\n    torch.cuda.reset_peak_memory_stats()\n    break\n  del optimizer\n  torch.cuda.empty_cache()\n  torch.cuda.reset_peak_memory_stats()\n\n----------\nBatch size: 1\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:08&lt;00:00, 11.23it/s]\n\n\n\n\nCUDA max memory allocated: 1.67 GB.\nCUDA max memory reserved: 1.82 GB.\nsamples/sec: 11.22\n----------\nBatch size: 2\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09&lt;00:00, 10.05it/s]\n\n\n\n\nCUDA max memory allocated: 1.68 GB.\nCUDA max memory reserved: 1.84 GB.\nsamples/sec: 20.10\n----------\nBatch size: 4\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09&lt;00:00, 10.48it/s]\n\n\n\n\nCUDA max memory allocated: 1.73 GB.\nCUDA max memory reserved: 1.91 GB.\nsamples/sec: 41.89\n----------\nBatch size: 8\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:11&lt;00:00,  8.94it/s]\n\n\n\n\nCUDA max memory allocated: 2.31 GB.\nCUDA max memory reserved: 2.52 GB.\nsamples/sec: 71.42\n----------\nBatch size: 16\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:18&lt;00:00,  5.44it/s]\n\n\n\n\nCUDA max memory allocated: 3.42 GB.\nCUDA max memory reserved: 3.88 GB.\nsamples/sec: 86.95\n----------\nBatch size: 32\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:34&lt;00:00,  2.94it/s]\n\n\n\n\nCUDA max memory allocated: 5.82 GB.\nCUDA max memory reserved: 6.44 GB.\nsamples/sec: 94.02\n----------\nBatch size: 64\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:06&lt;00:00,  1.50it/s]\n\n\n\n\nCUDA max memory allocated: 10.66 GB.\nCUDA max memory reserved: 11.57 GB.\nsamples/sec: 95.90\n----------\nBatch size: 128\n\n\nTrain:   0%|          | 0/100 [00:00&lt;?, ?it/s]\n\n\n\nCUDA out of memory!\nCUDA max memory allocated: 1.61 GB.\n\n\n\n\n\n\nCUDA max memory reserved: 2.44 GB.\n\n\n\nbatch_sizes_amp = []\nsamples_sec_per_batch_size_amp = []\nmax_memory_per_batch_size_amp = []\nfor k, v in results_for_batch_size_amp.items():\n  batch_sizes_amp.append(k)\n  samples_sec_per_batch_size_amp.append(v['samples/sec'])\n  max_memory_per_batch_size_amp.append(v['max_memory_allocated'])\n\n\nfig, ax = plt.subplots(ncols=2, figsize=(12, 5))\n\nax[0].plot(batch_sizes, samples_sec_per_batch_size, '-v', label='fp32');\nax[0].plot(batch_sizes_amp, samples_sec_per_batch_size_amp, '-v', label='amp');\nax[0].set_xlabel(\"Batch size\", fontsize=20);\nax[0].set_ylabel(\"Samples/sec\", fontsize=20);\nax[0].legend();\n\nax[1].plot(batch_sizes, max_memory_per_batch_size, '-v', label='fp32');\nax[1].plot(batch_sizes_amp, max_memory_per_batch_size_amp, '-v', label='amp');\nax[1].set_xlabel(\"Batch size\", fontsize=20);\nax[1].set_ylabel(\"Max memory (Gb)\", fontsize=20);\nax[1].legend();\n\n\n\n\n\n\n\n\n\nprint(f\"Speedup: {max(samples_sec_per_batch_size_amp) / max(samples_sec_per_batch_size):.2f}x\")\n\nSpeedup: 2.32x\n\n\nWith amp we have accelerated training by more than 2x!\nLargest batch size that fits onto the memory of device has almost doubled as well.\n\nGrad scaler\nIn the example above we‚Äôve observed some mysterious entity called GradScaler. Why do we need it?\nNote, that fp16 has much narrower range of representable values compared to fp32.\n\nIf the values inside a tensor are too small or to large they may not fit inside fp16, that may lead to divergence, especially if training large models for a long time.\n\nGradScaler tracks the magnitude of the loss, builds historgram of gradients and shifts it towards the range that is well represented by the fp16 numerical format.\nLet us illustrate the problem on a toy example below.\n\nW = torch.randn(8192, 8192, device=device, dtype=torch.float16, requires_grad=True)\nX = torch.randn(8, 8192, device=device, dtype=torch.float16)\nY = torch.randn(8, 8192, device=device, dtype=torch.float16)\n\n\nloss = F.mse_loss(X @ W.T, Y)\n\n\nloss\n\ntensor(inf, device='cuda:0', dtype=torch.float16, grad_fn=&lt;MseLossBackward0&gt;)\n\n\nIndeed this example is arfiticial, as one would init weights \\(w_{ij} \\in \\mathcal{N}(0, \\frac{1}{d})\\) but shows the point.\n\ndel W, X, Y, loss\ntorch.cuda.empty_cache()\n\nMaterials for further study.\n\nPyTorch amp examples\nGreat blog about amp by NVIDIA\nA page about fp8 format in Transformer engine"
  },
  {
    "objectID": "projects/Efficient_model_training.html#gradient-accumulation",
    "href": "projects/Efficient_model_training.html#gradient-accumulation",
    "title": "",
    "section": "Gradient accumulation",
    "text": "Gradient accumulation\nCommonly, a researcher or a practioner follows standard training recipes from papers. However, it is often the case that hardware doesn‚Äôt allow to train on the large batches that are affordable for Google and OpenAI guys.\nIn principle, one can fit ‚Äòalmost‚Äô arbitrary large batch size via gradient checkpointing illustrated below.\nThe idea behind gradient accumulation is to instead of calculating the gradients for the whole batch at once to do it in smaller steps. The way we do that is to calculate the gradients iteratively in smaller batches by doing a forward and backward pass through the model and accumulating the gradients in the process. When enough gradients are accumulated we run the model‚Äôs optimization step. This way we can easily increase the overall batch size to numbers that would never fit into the GPU‚Äôs memory. In turn, however, the added forward and backward passes can slow down the training a bit.\n\ndef train(model, loader, optimizer, scaler, loss_fn, num_steps, grad_accum_steps=1):\n  \"\"\"\n  Args:\n    model - model trained\n    loader - data loader\n    optimizer - optimizer\n    scaler - gradient scaler that shifts the gradient range\n    loss_fn - task loss function\n    num_steps - number of training steps\n    grad_accum_steps - number of gradient accumulation steps\n  Returns:\n    dict with performance stats\n  \"\"\"\n  samples_processed = 0\n  start = time.perf_counter()\n  for i in trange(num_steps, total=num_steps, desc=\"Train\"):\n    inputs, targets = next(loader)\n    inputs, targets = inputs.to(device), targets.to(device)\n    with torch.autocast(device_type='cuda', dtype=torch.float16):\n      outputs = model(inputs)\n      loss = loss_fn(outputs, targets)\n    assert not loss.isnan(), \"Loss is NaN. Terminating training.\"\n    scaler.scale(loss / grad_accum_steps).backward()\n    if (i + 1) % grad_accum_steps == 0:\n        scaler.step(optimizer)\n        scaler.update()\n        optimizer.zero_grad()\n    samples_processed += len(inputs)\n  end = time.perf_counter()\n  samples_per_sec = samples_processed / (end - start)\n  print(\"\\n\")\n  print_memory_stats()\n  print(f\"samples/sec: {samples_per_sec:.2f}\")\n  results = {\n      \"samples/sec\": samples_per_sec,\n      \"max_memory_allocated\": torch.cuda.max_memory_allocated() / 2 ** 30\n  }\n  return results\n\nIn the example below largest batch size, that fit into our memory was 64. Let us use it as a microbatch_size.\nTotal batch size is microbatch_size * grad_accum_steps.\n\nresults_for_batch_size_grad_accum = {}\n\nmicrobatch_size = 64\ntraining_steps = 100\n\nfor batch_size in (512, 1024, 2048):\n  print(\"-\" * 10)\n  print(f\"Batch size: {batch_size}\")\n  grad_accum_steps = batch_size // microbatch_size\n  # prepare dataloader\n  dataloader = make_dummy_dataloader(microbatch_size)\n  # prepare optimizer\n  optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n  # prepare scaler\n  scaler = GradScaler()\n  # train\n  results_for_batch_size_grad_accum[batch_size] = train(\n    model, dataloader, optimizer, scaler, F.cross_entropy, training_steps, grad_accum_steps\n  )\n  del optimizer\n  torch.cuda.empty_cache()\n  torch.cuda.reset_peak_memory_stats()\n\n----------\nBatch size: 512\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:23&lt;00:00,  1.20it/s]\n\n\n\n\nCUDA max memory allocated: 10.56 GB.\nCUDA max memory reserved: 10.74 GB.\nsamples/sec: 76.79\n----------\nBatch size: 1024\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:22&lt;00:00,  1.21it/s]\n\n\n\n\nCUDA max memory allocated: 10.56 GB.\nCUDA max memory reserved: 10.84 GB.\nsamples/sec: 77.72\n----------\nBatch size: 2048\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:22&lt;00:00,  1.22it/s]\n\n\n\n\nCUDA max memory allocated: 10.56 GB.\nCUDA max memory reserved: 10.86 GB.\nsamples/sec: 78.00\n\n\n\nbatch_sizes_grad_accum = []\nsamples_sec_per_batch_size_grad_accum = []\nmax_memory_per_batch_size_grad_accum = []\nfor k, v in results_for_batch_size_grad_accum.items():\n  batch_sizes_grad_accum.append(k)\n  samples_sec_per_batch_size_grad_accum.append(v['samples/sec'])\n  max_memory_per_batch_size_grad_accum.append(v['max_memory_allocated'])\n\n\nfig, ax = plt.subplots(ncols=2, figsize=(12, 5))\n\nax[0].plot(batch_sizes_grad_accum, samples_sec_per_batch_size_grad_accum, '-v');\nax[0].set_xlabel(\"Batch size\", fontsize=20);\nax[0].set_ylabel(\"Samples/sec\", fontsize=20);\n\nax[1].plot(batch_sizes_grad_accum, max_memory_per_batch_size_grad_accum, '-v');\nax[1].set_xlabel(\"Batch size\", fontsize=20);\nax[1].set_ylabel(\"Max memory (Gb)\", fontsize=20);\nax[1].set_yticks([10.5, 10.6]);\n\n\n\n\n\n\n\n\nOne can fit large batch size with gradient checkpointing.\nNote, however, that it doesn‚Äôt make the training faster in termps of samples/sec.\n## Gradient checkpointing\nEven when we set the batch size to 1 and use gradient accumulation we can still run out of memory when working with large models. In order to compute the gradients during the backward pass all activations from the forward pass are normally saved. This can create a big memory overhead. Alternatively, one could forget all activations during the forward pass and recompute them on demand during the backward pass.\nThis allows one reduce memory footprint at the cost of additional computations.\n\nfrom types import MethodType\nfrom torch.utils.checkpoint import checkpoint_sequential\n\nBelow we‚Äôll adopt the training loop from AMP section\n\ndef train(model, loader, optimizer, scaler, loss_fn, num_steps):\n  \"\"\"\n  Args:\n    model - model trained\n    loader - data loader\n    optimizer - optimizer\n    scaler - gradient scaler that shifts the gradient range\n    loss_fn - task loss function\n    num_steps.- number of training steps\n  Returns:\n    dict with performance stats\n  \"\"\"\n  samples_processed = 0\n  start = time.perf_counter()\n  for i in trange(num_steps, total=num_steps, desc=\"Train\"):\n    inputs, targets = next(loader)\n    inputs, targets = inputs.to(device), targets.to(device)\n    with torch.autocast(device_type='cuda', dtype=torch.float16):\n      outputs = model(inputs)\n      loss = loss_fn(outputs, targets)\n    assert not loss.isnan(), \"Loss is NaN. Terminating training.\"\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n    optimizer.zero_grad()\n    samples_processed += len(inputs)\n  end = time.perf_counter()\n  samples_per_sec = samples_processed / (end - start)\n  print(\"\\n\")\n  print_memory_stats()\n  print(f\"samples/sec: {samples_per_sec:.2f}\")\n  results = {\n      \"samples/sec\": samples_per_sec,\n      \"max_memory_allocated\": torch.cuda.max_memory_allocated() / 2 ** 30\n  }\n  return results\n\nLet us create the model again for the sake of safety\n\nmodel = get_model(\"swin_b\").to(device)\n\n\nnum_segments = len(model.features) # we set checkpoint after every SwinTransformer block\n\n# a hack to change forward pass of a model\ndef custom_forward(self, x):\n    x.requires_grad = True\n    x = checkpoint_sequential(model.features, num_segments, x)\n    x = self.norm(x)\n    x = self.permute(x)\n    x = self.avgpool(x)\n    x = self.flatten(x)\n    x = self.head(x)\n    return x\n\nmodel.forward = MethodType(custom_forward, model)\n\n\nresults_for_batch_size_grad_ckpt = {}\nfor batch_size in (1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024):\n  print(\"-\" * 10)\n  print(f\"Batch size: {batch_size}\")\n  # prepare dataloader\n  dataloader = make_dummy_dataloader(batch_size)\n  # prepare optimizer\n  optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n  # prepare scaler\n  scaler = GradScaler()\n  # train\n  try:\n    training_steps = min(4096 // batch_size, 100)\n    results_for_batch_size_grad_ckpt[batch_size] = train(model, dataloader, optimizer, scaler, F.cross_entropy, training_steps)\n  except:\n    print(\"\\nCUDA out of memory!\")\n    print_memory_stats()\n    del optimizer\n    torch.cuda.empty_cache()\n    torch.cuda.reset_peak_memory_stats()\n    break\n  del optimizer\n  torch.cuda.empty_cache()\n  torch.cuda.reset_peak_memory_stats()\n\n----------\nBatch size: 1\n\n\nTrain:   0%|          | 0/100 [00:00&lt;?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:542: UserWarning: torch.utils.checkpoint.checkpoint_sequential: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:16&lt;00:00,  6.14it/s]\n\n\n\n\nCUDA max memory allocated: 2.14 GB.\nCUDA max memory reserved: 2.23 GB.\nsamples/sec: 6.14\n----------\nBatch size: 2\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:15&lt;00:00,  6.30it/s]\n\n\n\n\nCUDA max memory allocated: 2.14 GB.\nCUDA max memory reserved: 2.24 GB.\nsamples/sec: 12.59\n----------\nBatch size: 4\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:15&lt;00:00,  6.32it/s]\n\n\n\n\nCUDA max memory allocated: 2.14 GB.\nCUDA max memory reserved: 2.30 GB.\nsamples/sec: 25.27\n----------\nBatch size: 8\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:20&lt;00:00,  4.91it/s]\n\n\n\n\nCUDA max memory allocated: 2.38 GB.\nCUDA max memory reserved: 2.60 GB.\nsamples/sec: 39.26\n----------\nBatch size: 16\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:33&lt;00:00,  3.01it/s]\n\n\n\n\nCUDA max memory allocated: 3.03 GB.\nCUDA max memory reserved: 3.28 GB.\nsamples/sec: 48.19\n----------\nBatch size: 32\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:02&lt;00:00,  1.60it/s]\n\n\n\n\nCUDA max memory allocated: 4.34 GB.\nCUDA max memory reserved: 4.69 GB.\nsamples/sec: 51.15\n----------\nBatch size: 64\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [01:18&lt;00:00,  1.23s/it]\n\n\n\n\nCUDA max memory allocated: 6.98 GB.\nCUDA max memory reserved: 8.12 GB.\nsamples/sec: 52.18\n----------\nBatch size: 128\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [01:18&lt;00:00,  2.44s/it]\n\n\n\n\nCUDA max memory allocated: 12.23 GB.\nCUDA max memory reserved: 14.29 GB.\nsamples/sec: 52.44\n----------\nBatch size: 256\n\n\nTrain:   0%|          | 0/16 [00:02&lt;?, ?it/s]\n\n\n\nCUDA out of memory!\nCUDA max memory allocated: 14.10 GB.\nCUDA max memory reserved: 14.51 GB.\n\n\n\n\n\n\nbatch_sizes_grad_ckpt = []\nsamples_sec_per_batch_size_grad_ckpt = []\nmax_memory_per_batch_size_grad_ckpt = []\nfor k, v in results_for_batch_size_grad_ckpt.items():\n  batch_sizes_grad_ckpt.append(k)\n  samples_sec_per_batch_size_grad_ckpt.append(v['samples/sec'])\n  max_memory_per_batch_size_grad_ckpt.append(v['max_memory_allocated'])\n\n\nfig, ax = plt.subplots(ncols=2, figsize=(12, 5))\n\nax[0].plot(batch_sizes_amp, samples_sec_per_batch_size_amp, '-v', label='amp');\nax[0].plot(batch_sizes_grad_ckpt, samples_sec_per_batch_size_grad_ckpt, '-v', label='grad_ckpt');\nax[0].set_xlabel(\"Batch size\", fontsize=20);\nax[0].set_ylabel(\"Samples/sec\", fontsize=20);\nax[0].legend();\n\nax[1].plot(batch_sizes_amp, max_memory_per_batch_size_amp, '-v', label='amp');\nax[1].plot(batch_sizes_grad_ckpt, max_memory_per_batch_size_grad_ckpt, '-v', label='grad_ckpt');\nax[1].set_xlabel(\"Batch size\", fontsize=20);\nax[1].set_ylabel(\"Max memory (Gb)\", fontsize=20);\nax[1].legend();\n\n\n\n\n\n\n\n\nNice! üôÄ\nNow we can fit even larger batch size 128 onto a single T4 GPU!\nNote, that this tecnique doesn‚Äôt typically accelerate training.\nHowever, in case the model is pretty large (such as modern transformer), and inputs are pretty large (large images or long sequences), gradient_checkpointing is very reasonable option to try.\nSome frameworks and libraries support gradient_checkpointing out of the box: * timm - set_grad_checkpointing method in some models * transformers ü§ó - model.gradient_checkpointing_enable()\nAdditional resources: * Article on gradient checkpointing"
  },
  {
    "objectID": "projects/Efficient_model_training.html#model-compilation",
    "href": "projects/Efficient_model_training.html#model-compilation",
    "title": "",
    "section": "Model compilation",
    "text": "Model compilation\nSince torch &gt; 2.0 users can compile their model prior to running. Compilation can make PyTorch code run faster by JIT-compiling PyTorch code into optimized kernels, all while requiring minimal code changes.\nHowever, note that the compilation takes some time, as the users familiar with compiled languages may remember.\nIf the run is very short it may be not the best option.\nFor long enough runtime torch.compile offers nontrivial speed-up of training and inference.\nBelow we will illustrate the benefit of torch.compile on RMSNorm, a commonly used normalization layer in modern LLMs.\n\nclass RMSNorm(nn.Module):\n\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        RMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        input_dtype = hidden_states.dtype\n        hidden_states = hidden_states.to(torch.float32)\n        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * hidden_states.to(input_dtype)\n\n\n# Llama-2-7b sizes\nbatch_size = 1\nsequence_length = 4096\nhidden_size = 4096\n# benchmark settings\nwarmup_iters = 1000\nbenchmark_iters = 1000\n\n\n# generate random sequence\ninputs = torch.randn(batch_size, sequence_length, hidden_size, device=device, dtype=torch.float16)\n\nDefine RMSNorm layer\n\nrms_norm = RMSNorm(hidden_size).to(device=device, dtype=torch.float16)\n\n\nfor _ in range(warmup_iters):\n  rms_norm(inputs)\n\ntimes = []\nfor _ in range(benchmark_iters):\n  start = torch.cuda.Event(enable_timing=True)\n  end = torch.cuda.Event(enable_timing=True)\n  start.record()\n  rms_norm(inputs)\n  end.record()\n  torch.cuda.synchronize()\n  times.append(start.elapsed_time(end))\n\n\nprint(f\"Average latency: {np.mean(times):.3f} ms\")\n\nAverage latency: 2.627 ms\n\n\nLet us compile the layer.\n\nrms_norm = torch.compile(rms_norm)\n\n\nfor _ in range(warmup_iters):\n  rms_norm(inputs)\n\ntimes = []\nfor _ in range(benchmark_iters):\n  start = torch.cuda.Event(enable_timing=True)\n  end = torch.cuda.Event(enable_timing=True)\n  start.record()\n  rms_norm(inputs)\n  end.record()\n  torch.cuda.synchronize()\n  times.append(start.elapsed_time(end))\n\n\nprint(f\"Average latency: {np.mean(times):.3f} ms\")\n\nAverage latency: 0.449 ms\n\n\n4-5x speedup!\n\n\ndel rms_norm, inputs\ntorch.cuda.empty_cache()\n\nNow let us apply torch compile to training the model from previous examples.\n\ndef train(model, loader, optimizer, scaler, loss_fn, num_steps, warmup_steps=10):\n  \"\"\"\n  Args:\n    model - model trained\n    loader - data loader\n    optimizer - optimizer\n    scaler - gradient scaler that shifts the gradient range\n    loss_fn - task loss function\n    num_steps - number of training steps\n    num_steps - number of warmup steps\n  Returns:\n    dict with performance stats\n  \"\"\"\n  samples_processed = 0\n  # warmup\n  start = time.perf_counter()\n  for i in range(warmup_steps):\n    inputs, targets = next(loader)\n    inputs, targets = inputs.to(device), targets.to(device)\n    with torch.autocast(device_type='cuda', dtype=torch.float16):\n      outputs = model(inputs)\n      loss = loss_fn(outputs, targets)\n    scaler.scale(loss).backward()\n    optimizer.zero_grad()\n  end = time.perf_counter()\n  compile_time = end - start\n  print(f\"Compilation took {(end - start):.2f}s.\")\n  # start of training loop\n  start = time.perf_counter()\n  for i in trange(num_steps , total=num_steps, desc=\"Train\"):\n    inputs, targets = next(loader)\n    inputs, targets = inputs.to(device), targets.to(device)\n    with torch.autocast(device_type='cuda', dtype=torch.float16):\n      outputs = model(inputs)\n      loss = loss_fn(outputs, targets)\n    assert not loss.isnan(), \"Loss is NaN. Terminating training.\"\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n    optimizer.zero_grad()\n    samples_processed += len(inputs)\n  end = time.perf_counter()\n  samples_per_sec = samples_processed / (end - start)\n  print(\"\\n\")\n  print_memory_stats()\n  print(f\"samples/sec: {samples_per_sec:.2f}\")\n  results = {\n      \"samples/sec\": samples_per_sec,\n      \"max_memory_allocated\": torch.cuda.max_memory_allocated() / 2 ** 30,\n      \"compile_time\": compile_time\n  }\n  return results\n\n\nmodel = get_model(\"swin_b\").to(device)\n\n\nmodel = torch.compile(model)\n\n/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n\n\n\nresults_for_batch_size_compile = {}\ntraining_steps = 100\nwarmup_steps = 10\n\nfor batch_size in (1, 2, 4, 8, 16, 32, 64):\n  print(\"-\" * 10)\n  print(f\"Batch size: {batch_size}\")\n  # prepare dataloader\n  dataloader = make_dummy_dataloader(batch_size)\n  # prepare optimizer\n  optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n  # prepare scaler\n  scaler = GradScaler()\n  # train\n  try:\n    results_for_batch_size_compile[batch_size] = train(model, dataloader, optimizer, scaler, F.cross_entropy, training_steps, warmup_steps)\n  except:\n    print(\"\\nCUDA out of memory!\")\n    print_memory_stats()\n    del optimizer\n    torch.cuda.empty_cache()\n    torch.cuda.reset_peak_memory_stats()\n    break\n  del optimizer\n  torch.cuda.empty_cache()\n  torch.cuda.reset_peak_memory_stats()\n\n----------\nBatch size: 1\nCompilation took 110.28s.\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:08&lt;00:00, 11.23it/s]\n\n\n\n\nCUDA max memory allocated: 1.67 GB.\nCUDA max memory reserved: 1.80 GB.\nsamples/sec: 11.22\n----------\nBatch size: 2\nCompilation took 228.16s.\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:10&lt;00:00,  9.47it/s]\n\n\n\n\nCUDA max memory allocated: 1.68 GB.\nCUDA max memory reserved: 1.80 GB.\nsamples/sec: 18.93\n----------\nBatch size: 4\nCompilation took 0.65s.\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:09&lt;00:00, 10.61it/s]\n\n\n\n\nCUDA max memory allocated: 1.73 GB.\nCUDA max memory reserved: 1.91 GB.\nsamples/sec: 42.42\n----------\nBatch size: 8\nCompilation took 0.79s.\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:11&lt;00:00,  8.85it/s]\n\n\n\n\nCUDA max memory allocated: 2.30 GB.\nCUDA max memory reserved: 2.50 GB.\nsamples/sec: 70.78\n----------\nBatch size: 16\nCompilation took 1.45s.\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:18&lt;00:00,  5.40it/s]\n\n\n\n\nCUDA max memory allocated: 3.42 GB.\nCUDA max memory reserved: 3.86 GB.\nsamples/sec: 86.31\n----------\nBatch size: 32\nCompilation took 2.82s.\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:34&lt;00:00,  2.92it/s]\n\n\n\n\nCUDA max memory allocated: 5.83 GB.\nCUDA max memory reserved: 6.46 GB.\nsamples/sec: 93.34\n----------\nBatch size: 64\nCompilation took 5.45s.\n\n\nTrain: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:08&lt;00:00,  1.47it/s]\n\n\n\n\nCUDA max memory allocated: 10.66 GB.\nCUDA max memory reserved: 11.63 GB.\nsamples/sec: 93.77\n\n\nHow long does the compilation take? ‚è≤\n\nbatch_sizes_compile = []\nsamples_sec_per_batch_size_compile = []\nmax_memory_per_batch_size_compile = []\nfor k, v in results_for_batch_size_compile.items():\n  batch_sizes_compile.append(k)\n  samples_sec_per_batch_size_compile.append(v['samples/sec'])\n  max_memory_per_batch_size_compile.append(v['max_memory_allocated'])\n\n\nfig, ax = plt.subplots(ncols=2, figsize=(12, 5))\n\nax[0].plot(batch_sizes_amp, samples_sec_per_batch_size_amp, '-v', label='amp');\nax[0].plot(batch_sizes_compile, samples_sec_per_batch_size_compile, '-v', label='compile');\nax[0].set_xlabel(\"Batch size\", fontsize=20);\nax[0].set_ylabel(\"Samples/sec\", fontsize=20);\nax[0].legend();\n\nax[1].plot(batch_sizes_amp, max_memory_per_batch_size_amp, '-v', label='amp');\nax[1].plot(batch_sizes_compile, max_memory_per_batch_size_compile, '-v', label='compile');\nax[1].set_xlabel(\"Batch size\", fontsize=20);\nax[1].set_ylabel(\"Max memory (Gb)\", fontsize=20);\nax[1].legend();\n\n\n\n\n\n\n\n\nDo you observe benefits from compilation?\nAdditional resources * PyTorch tutorial"
  },
  {
    "objectID": "projects/Efficient_model_training.html#summary-and-concluding-remarks",
    "href": "projects/Efficient_model_training.html#summary-and-concluding-remarks",
    "title": "",
    "section": "Summary and concluding remarks",
    "text": "Summary and concluding remarks\nIn the table below we summarize the benefits proposed by each method covered in the seminar.\n\n\n\nMethod\nSpeed\nMemory\n\n\n\n\nBatch size\nYes\nYes\n\n\nMixed precision training\nYes\nYes*\n\n\nGradient accumulation\nNo\nYes\n\n\nGradient checkpointing\nNo\nYes\n\n\nCompilation\nYes\nNo\n\n\n\nSome notes are worth being mentioned. * AMP is a great choice for small model, when the model takes small fraction of the system VRAM. However, it requires to store a version of model both in fp16 and fp32 precision and may even increase memory overhead when working with large models.\nAdditional materials * Great overview with examples on Huggingface on efficient training ü§ó"
  },
  {
    "objectID": "projects/Auto_differentiation_using_Jax.html",
    "href": "projects/Auto_differentiation_using_Jax.html",
    "title": "Automatic differentiation with JAX",
    "section": "",
    "text": "Numpy wrapper\nAuto-vectorization\nAuto-parallelization (SPMD paradigm)\nAuto-differentiation\nXLA backend and JIT support"
  },
  {
    "objectID": "projects/Auto_differentiation_using_Jax.html#main-features",
    "href": "projects/Auto_differentiation_using_Jax.html#main-features",
    "title": "Automatic differentiation with JAX",
    "section": "",
    "text": "Numpy wrapper\nAuto-vectorization\nAuto-parallelization (SPMD paradigm)\nAuto-differentiation\nXLA backend and JIT support"
  },
  {
    "objectID": "projects/Auto_differentiation_using_Jax.html#how-to-compute-gradient-of-your-objective",
    "href": "projects/Auto_differentiation_using_Jax.html#how-to-compute-gradient-of-your-objective",
    "title": "Automatic differentiation with JAX",
    "section": "How to compute gradient of your objective?",
    "text": "How to compute gradient of your objective?\n\nDefine it as a standard Python function\nCall jax.grad and voila!\nDo not forget to wrap these functions with jax.jit to speed up\n\n\nimport jax\nimport jax.numpy as jnp\n\n\nBy default, JAX exploits single-precision numbers float32\nYou can enable double precision (float64) by hands.\n\n\nfrom jax.config import config\nconfig.update(\"jax_enable_x64\", True)\n\n\n@jax.jit\ndef f(x, A, b):\n    res = A @ x - b\n    return res @ res\n\ngradf = jax.grad(f, argnums=0, has_aux=False)"
  },
  {
    "objectID": "projects/Auto_differentiation_using_Jax.html#random-numbers-in-jax",
    "href": "projects/Auto_differentiation_using_Jax.html#random-numbers-in-jax",
    "title": "Automatic differentiation with JAX",
    "section": "Random numbers in JAX",
    "text": "Random numbers in JAX\n\nJAX focuses on the reproducibility of the runs\nAnalogue of random seed is the necessary argument of all functions that generate something random\nMore details and references on the design of random submodule are here\n\n\nn = 1000\nx = jax.random.normal(jax.random.PRNGKey(0), (n, ))\nA = jax.random.normal(jax.random.PRNGKey(0), (n, n))\nb = jax.random.normal(jax.random.PRNGKey(0), (n, ))\n\n\nprint(\"Check correctness\", jnp.linalg.norm(gradf(x, A, b) - 2 * A.T @ (A @ x - b)))\nprint(\"Compare speed\")\nprint(\"Analytical gradient\")\n%timeit 2 * A.T @ (A @ x - b)\nprint(\"Grad function\")\n%timeit gradf(x, A, b).block_until_ready()\njit_gradf = jax.jit(gradf)\nprint(\"Jitted grad function\")\n%timeit jit_gradf(x, A, b).block_until_ready()\n\nCheck correctness 9.188704584401416e-11\nCompare speed\nAnalytical gradient\n1.78 ms ¬± 26.1 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\nGrad function\n1.05 ms ¬± 18.5 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\nJitted grad function\n323 ¬µs ¬± 5.14 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n\n\n\nhess_func = jax.jit(jax.hessian(f))\nprint(\"Check correctness\", jnp.linalg.norm(2 * A.T @ A - hess_func(x, A, b)))\nprint(\"Time for hessian\")\n%timeit hess_func(x, A, b).block_until_ready()\nprint(\"Emulate hessian and check correctness\", \n      jnp.linalg.norm(jax.jit(hess_func)(x, A, b) - jax.jacfwd(jax.jacrev(f))(x, A, b)))\nprint(\"Time of emulating hessian\")\nhess_umul_func = jax.jit(jax.jacfwd(jax.jacrev(f)))\n%timeit hess_umul_func(x, A, b).block_until_ready()\n\nCheck correctness 0.0\nTime for hessian\n21.1 ms ¬± 449 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\nEmulate hessian and check correctness 0.0\nTime of emulating hessian\n21.1 ms ¬± 502 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)"
  },
  {
    "objectID": "projects/Auto_differentiation_using_Jax.html#forward-mode-vs.-backward-mode-m-ll-n",
    "href": "projects/Auto_differentiation_using_Jax.html#forward-mode-vs.-backward-mode-m-ll-n",
    "title": "Automatic differentiation with JAX",
    "section": "Forward mode vs.¬†backward mode: \\(m \\ll n\\)",
    "text": "Forward mode vs.¬†backward mode: \\(m \\ll n\\)\n\nfmode_f = jax.jit(jax.jacfwd(f))\nbmode_f = jax.jit(jax.jacrev(f))\nprint(\"Check correctness\", jnp.linalg.norm(fmode_f(x, A, b) - bmode_f(x, A, b)))\nprint(\"Forward mode\")\n%timeit fmode_f(x, A, b).block_until_ready()\nprint(\"Backward mode\")\n%timeit bmode_f(x, A, b).block_until_ready()\n\nCheck correctness 1.0709183798706824e-10\nForward mode\n14.3 ms ¬± 248 ¬µs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)\nBackward mode\n329 ¬µs ¬± 5.1 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)"
  },
  {
    "objectID": "projects/Auto_differentiation_using_Jax.html#forward-mode-vs.-backward-mode-m-geq-n",
    "href": "projects/Auto_differentiation_using_Jax.html#forward-mode-vs.-backward-mode-m-geq-n",
    "title": "Automatic differentiation with JAX",
    "section": "Forward mode vs.¬†backward mode: \\(m \\geq n\\)",
    "text": "Forward mode vs.¬†backward mode: \\(m \\geq n\\)\n\ndef fvec(x, A, b):\n    y = A @ x + b\n    return jnp.exp(y - jnp.max(y)) / jnp.sum(jnp.exp(y - jnp.max(y)))\n\n\ngrad_fvec = jax.jit(jax.grad(fvec))\njac_fvec = jax.jacobian(fvec)\nfmode_fvec = jax.jit(jax.jacfwd(fvec))\nbmode_fvec = jax.jit(jax.jacrev(fvec))\n\n\nn = 1000\nm = 1000\nx = jax.random.normal(jax.random.PRNGKey(0), (n, ))\nA = jax.random.normal(jax.random.PRNGKey(0), (m, n))\nb = jax.random.normal(jax.random.PRNGKey(0), (m, ))\n\n\nJ = jac_fvec(x, A, b)\nprint(J.shape)\ngrad_fvec(x, A, b)\n\n(1000, 1000)\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nInput In [12], in &lt;cell line: 3&gt;()\n      1 J = jac_fvec(x, A, b)\n      2 print(J.shape)\n----&gt; 3 grad_fvec(x, A, b)\n\n    [... skipping hidden 18 frame]\n\nFile ~/miniconda3/envs/dnn/lib/python3.10/site-packages/jax/_src/api.py:1103, in _check_scalar(x)\n   1101 if isinstance(aval, ShapedArray):\n   1102   if aval.shape != ():\n-&gt; 1103     raise TypeError(msg(f\"had shape: {aval.shape}\"))\n   1104 else:\n   1105   raise TypeError(msg(f\"had abstract value {aval}\"))\n\nTypeError: Gradient only defined for scalar-output functions. Output had shape: (1000,).\n\n\n\n\nprint(\"Check correctness\", jnp.linalg.norm(fmode_fvec(x, A, b) - bmode_fvec(x, A, b)))\nprint(\"Check shape\", fmode_fvec(x, A, b).shape, bmode_fvec(x, A, b).shape)\nprint(\"Time forward mode\")\n%timeit fmode_fvec(x, A, b).block_until_ready()\nprint(\"Time backward mode\")\n%timeit bmode_fvec(x, A, b).block_until_ready()\n\nCheck correctness 7.94101434110216e-16\nCheck shape (1000, 1000) (1000, 1000)\nTime forward mode\n16 ms ¬± 318 ¬µs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)\nTime backward mode\n15.7 ms ¬± 309 ¬µs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)\n\n\n\nn = 10\nm = 1000\nx = jax.random.normal(jax.random.PRNGKey(0), (n, ))\nA = jax.random.normal(jax.random.PRNGKey(0), (m, n))\nb = jax.random.normal(jax.random.PRNGKey(0), (m, ))\n\n\nprint(\"Check correctness\", jnp.linalg.norm(fmode_fvec(x, A, b) - bmode_fvec(x, A, b)))\nprint(\"Check shape\", fmode_fvec(x, A, b).shape, bmode_fvec(x, A, b).shape)\nprint(\"Time forward mode\")\n%timeit fmode_fvec(x, A, b).block_until_ready()\nprint(\"Time backward mode\")\n%timeit bmode_fvec(x, A, b).block_until_ready()\n\nCheck correctness 8.043682175330496e-16\nCheck shape (1000, 10) (1000, 10)\nTime forward mode\n44.1 ¬µs ¬± 2.06 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10,000 loops each)\nTime backward mode\n2.7 ms ¬± 22.9 ¬µs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)"
  },
  {
    "objectID": "projects/Auto_differentiation_using_Jax.html#hessian-by-vector-product",
    "href": "projects/Auto_differentiation_using_Jax.html#hessian-by-vector-product",
    "title": "Automatic differentiation with JAX",
    "section": "Hessian-by-vector product",
    "text": "Hessian-by-vector product\n\ndef hvp(f, x, z, *args):\n    def g(x):\n        return f(x, *args)\n    return jax.jvp(jax.grad(g), (x,), (z,))[1]\n\n\nn = 3000\nx = jax.random.normal(jax.random.PRNGKey(0), (n, ))\nA = jax.random.normal(jax.random.PRNGKey(0), (n, n))\nb = jax.random.normal(jax.random.PRNGKey(0), (n, ))\nz = jax.random.normal(jax.random.PRNGKey(0), (n, ))\n\n\nprint(\"Check correctness\", jnp.linalg.norm(2 * A.T @ (A @ z) - hvp(f, x, z, A, b)))\nprint(\"Time for hvp by hands\")\n%timeit (2 * A.T @ (A @ z)).block_until_ready()\nprint(\"Time for hvp via jvp, NO jit\")\n%timeit hvp(f, x, z, A, b).block_until_ready()\nprint(\"Time for hvp via jvp, WITH jit\")\n%timeit jax.jit(hvp, static_argnums=0)(f, x, z, A, b).block_until_ready()\n\nCheck correctness 8.590867785772636e-10\nTime for hvp by hands\n17.9 ms ¬± 174 ¬µs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)\nTime for hvp via jvp, NO jit\n13.2 ms ¬± 70.5 ¬µs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)\nTime for hvp via jvp, WITH jit\n6.38 ms ¬± 32.9 ¬µs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)"
  },
  {
    "objectID": "projects/Auto_differentiation_using_Jax.html#summary",
    "href": "projects/Auto_differentiation_using_Jax.html#summary",
    "title": "Automatic differentiation with JAX",
    "section": "Summary",
    "text": "Summary\n\nJAX is a simple and extensible tool in the problem where autodiff is crucial\nJIT is a key to fast Python code\nInput/output dimensions are important\nHessian matvec is faster than explicit hessian matrix by vector product"
  },
  {
    "objectID": "projects/Optuna_introduction.html",
    "href": "projects/Optuna_introduction.html",
    "title": "A Quick Introduction to Optuna",
    "section": "",
    "text": "This Jupyter notebook goes through the basic usage of Optuna."
  },
  {
    "objectID": "projects/Optuna_introduction.html#install-optuna",
    "href": "projects/Optuna_introduction.html#install-optuna",
    "title": "A Quick Introduction to Optuna",
    "section": "Install optuna",
    "text": "Install optuna\nOptuna can be installed via pip or conda.\n\n!pip install --quiet optuna\n\n\nimport optuna\n\noptuna.__version__"
  },
  {
    "objectID": "projects/Optuna_introduction.html#optimize-hyperparameters",
    "href": "projects/Optuna_introduction.html#optimize-hyperparameters",
    "title": "A Quick Introduction to Optuna",
    "section": "Optimize Hyperparameters",
    "text": "Optimize Hyperparameters\n\nDefine a simple scikit-learn model\nWe start with a simple random forest model to classify flowers in the Iris dataset. We define a function called objective that encapsulates the whole training process and outputs the accuracy of the model.\n\nimport sklearn.datasets\nimport sklearn.ensemble\nimport sklearn.model_selection\n\ndef objective():\n    iris = sklearn.datasets.load_iris()  # Prepare the data.\n\n    clf = sklearn.ensemble.RandomForestClassifier(\n        n_estimators=5, max_depth=3)  # Define the model.\n\n    return sklearn.model_selection.cross_val_score(\n        clf, iris.data, iris.target, n_jobs=-1, cv=3).mean()  # Train and evaluate the model.\n\nprint('Accuracy: {}'.format(objective()))\n\n\n\nOptimize hyperparameters of the model\nThe hyperparameters of the above algorithm are n_estimators and max_depth for which we can try different values to see if the model accuracy can be improved. The objective function is modified to accept a trial object. This trial has several methods for sampling hyperparameters. We create a study to run the hyperparameter optimization and finally read the best hyperparameters.\n\nimport optuna\n\ndef objective(trial):\n    iris = sklearn.datasets.load_iris()\n\n    n_estimators = trial.suggest_int('n_estimators', 2, 20)\n    max_depth = int(trial.suggest_float('max_depth', 1, 32, log=True))\n\n    clf = sklearn.ensemble.RandomForestClassifier(\n        n_estimators=n_estimators, max_depth=max_depth)\n\n    return sklearn.model_selection.cross_val_score(\n        clf, iris.data, iris.target, n_jobs=-1, cv=3).mean()\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100)\n\ntrial = study.best_trial\n\nprint('Accuracy: {}'.format(trial.value))\nprint(\"Best hyperparameters: {}\".format(trial.params))\n\nIt is possible to condition hyperparameters using Python if statements. We can for instance include another classifier, a support vector machine, in our HPO and define hyperparameters specific to the random forest model and the support vector machine.\n\nimport sklearn.svm\n\ndef objective(trial):\n    iris = sklearn.datasets.load_iris()\n\n    classifier = trial.suggest_categorical('classifier', ['RandomForest', 'SVC'])\n\n    if classifier == 'RandomForest':\n        n_estimators = trial.suggest_int('n_estimators', 2, 20)\n        max_depth = int(trial.suggest_float('max_depth', 1, 32, log=True))\n\n        clf = sklearn.ensemble.RandomForestClassifier(\n            n_estimators=n_estimators, max_depth=max_depth)\n    else:\n        c = trial.suggest_float('svc_c', 1e-10, 1e10, log=True)\n\n        clf = sklearn.svm.SVC(C=c, gamma='auto')\n\n    return sklearn.model_selection.cross_val_score(\n        clf, iris.data, iris.target, n_jobs=-1, cv=3).mean()\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100)\n\ntrial = study.best_trial\n\nprint('Accuracy: {}'.format(trial.value))\nprint(\"Best hyperparameters: {}\".format(trial.params))\n\n\n\nPlotting the study\nPlotting the optimization history of the study.\n\noptuna.visualization.plot_optimization_history(study)\n\nPlotting the accuracies for each hyperparameter for each trial.\n\noptuna.visualization.plot_slice(study)\n\nPlotting the accuracy surface for the hyperparameters involved in the random forest model.\n\noptuna.visualization.plot_contour(study, params=['n_estimators', 'max_depth'])"
  },
  {
    "objectID": "projects/LLM.html",
    "href": "projects/LLM.html",
    "title": "Training large models*",
    "section": "",
    "text": "In this notebook, you will learn how to finetune large language models with limited GPU memory.\nFor the last few years models have greatly increased in size and many of them do not fit onto the standard consumer GPU, not said about finetuning these models in conventional way.\nHowever, there exists several techniques that allow one to inference and even finetune large models given modest resources.\nReduction of the model size\nThere existst numerous approaches to model compression (that require a separate lecture for an overview) and the one of the most succesfull in the context of LLM is PTQ (post-training quantization) that stores the weight in low precision.\n4-bit quantization typically leads to minor degration in performance relative to the floating point baseline offerring huge memory savings: * a model in half precision requires 16 bits per parameter * a model quantized to 4-bits requires 4+eps bits per parameter (there is small overhead on the storage of quantization statistics)\nTherefore, we have almost 4x reduction in memory!\nReduction of the memory on optimizer states\nAnother challenge are the optimizer states. For Adam optimizer commonly adopted for training Transformers one needs 4 bytes for gradients, and first and second optimizer moment (one may try to store some of these in half precision, but it tends to incur instability).\nTherefore, the total memory required to train something like Llama-7b, Mistral-7b, gemma-7b exceeds 80Gb of high-end A100, H100.\nFinetuning only the lm.head may not suffice for more complicated tasks.\nThus we search for something in between - that allows to adapt in some sense every transformer layer, but with small number of trainable parameters.\nDifferent approach to train small subset of parameters are known in the literature as parameter-efficient finetuning (PEFT) methods.\nWe will cover two known tecnhiques for parameter-efficient finetuning: * Prompt tuning * LoRA adapters"
  },
  {
    "objectID": "projects/LLM.html#point-to-note",
    "href": "projects/LLM.html#point-to-note",
    "title": "Training large models*",
    "section": "",
    "text": "In this notebook, you will learn how to finetune large language models with limited GPU memory.\nFor the last few years models have greatly increased in size and many of them do not fit onto the standard consumer GPU, not said about finetuning these models in conventional way.\nHowever, there exists several techniques that allow one to inference and even finetune large models given modest resources.\nReduction of the model size\nThere existst numerous approaches to model compression (that require a separate lecture for an overview) and the one of the most succesfull in the context of LLM is PTQ (post-training quantization) that stores the weight in low precision.\n4-bit quantization typically leads to minor degration in performance relative to the floating point baseline offerring huge memory savings: * a model in half precision requires 16 bits per parameter * a model quantized to 4-bits requires 4+eps bits per parameter (there is small overhead on the storage of quantization statistics)\nTherefore, we have almost 4x reduction in memory!\nReduction of the memory on optimizer states\nAnother challenge are the optimizer states. For Adam optimizer commonly adopted for training Transformers one needs 4 bytes for gradients, and first and second optimizer moment (one may try to store some of these in half precision, but it tends to incur instability).\nTherefore, the total memory required to train something like Llama-7b, Mistral-7b, gemma-7b exceeds 80Gb of high-end A100, H100.\nFinetuning only the lm.head may not suffice for more complicated tasks.\nThus we search for something in between - that allows to adapt in some sense every transformer layer, but with small number of trainable parameters.\nDifferent approach to train small subset of parameters are known in the literature as parameter-efficient finetuning (PEFT) methods.\nWe will cover two known tecnhiques for parameter-efficient finetuning: * Prompt tuning * LoRA adapters"
  },
  {
    "objectID": "projects/LLM.html#preparation",
    "href": "projects/LLM.html#preparation",
    "title": "Training large models*",
    "section": "Preparation",
    "text": "Preparation\n\n!pip install accelerate # needed to reduce RAM consumption and integration with bits and bytes\n!pip install bitsandbytes # to work with quantized models\n!pip install peft # to work with PEFT techniques\n\nLooking in indexes: https://pypi.org/simple/\nRequirement already satisfied: accelerate in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (0.28.0)\nRequirement already satisfied: numpy&gt;=1.17 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging&gt;=20.0 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from accelerate) (24.0)\nRequirement already satisfied: psutil in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from accelerate) (5.9.0)\nRequirement already satisfied: pyyaml in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch&gt;=1.10.0 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from accelerate) (2.2.2)\nRequirement already satisfied: huggingface-hub in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from accelerate) (0.22.2)\nRequirement already satisfied: safetensors&gt;=0.3.1 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from accelerate) (0.4.2)\nRequirement already satisfied: filelock in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from torch&gt;=1.10.0-&gt;accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions&gt;=4.8.0 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from torch&gt;=1.10.0-&gt;accelerate) (4.9.0)\nRequirement already satisfied: sympy in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from torch&gt;=1.10.0-&gt;accelerate) (1.12)\nRequirement already satisfied: networkx in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from torch&gt;=1.10.0-&gt;accelerate) (3.1)\nRequirement already satisfied: jinja2 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from torch&gt;=1.10.0-&gt;accelerate) (3.1.3)\nRequirement already satisfied: fsspec in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from torch&gt;=1.10.0-&gt;accelerate) (2024.2.0)\nRequirement already satisfied: requests in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from huggingface-hub-&gt;accelerate) (2.31.0)\nRequirement already satisfied: tqdm&gt;=4.42.1 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from huggingface-hub-&gt;accelerate) (4.66.2)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from jinja2-&gt;torch&gt;=1.10.0-&gt;accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from requests-&gt;huggingface-hub-&gt;accelerate) (2.0.4)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from requests-&gt;huggingface-hub-&gt;accelerate) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from requests-&gt;huggingface-hub-&gt;accelerate) (2.1.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from requests-&gt;huggingface-hub-&gt;accelerate) (2024.2.2)\nRequirement already satisfied: mpmath&gt;=0.19 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from sympy-&gt;torch&gt;=1.10.0-&gt;accelerate) (1.3.0)\nLooking in indexes: https://pypi.org/simple/\nRequirement already satisfied: bitsandbytes in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (0.43.1)\nRequirement already satisfied: torch in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from bitsandbytes) (2.2.2)\nRequirement already satisfied: numpy in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from torch-&gt;bitsandbytes) (3.13.1)\nRequirement already satisfied: typing-extensions&gt;=4.8.0 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from torch-&gt;bitsandbytes) (4.9.0)\nRequirement already satisfied: sympy in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from torch-&gt;bitsandbytes) (1.12)\nRequirement already satisfied: networkx in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from torch-&gt;bitsandbytes) (3.1)\nRequirement already satisfied: jinja2 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from torch-&gt;bitsandbytes) (3.1.3)\nRequirement already satisfied: fsspec in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from torch-&gt;bitsandbytes) (2024.2.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from jinja2-&gt;torch-&gt;bitsandbytes) (2.1.3)\nRequirement already satisfied: mpmath&gt;=0.19 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from sympy-&gt;torch-&gt;bitsandbytes) (1.3.0)\nLooking in indexes: https://pypi.org/simple/\nRequirement already satisfied: peft in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (0.10.0)\nRequirement already satisfied: numpy&gt;=1.17 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging&gt;=20.0 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from peft) (24.0)\nRequirement already satisfied: psutil in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from peft) (5.9.0)\nRequirement already satisfied: pyyaml in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch&gt;=1.13.0 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from peft) (2.2.2)\nRequirement already satisfied: transformers in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from peft) (4.40.0.dev0)\nRequirement already satisfied: tqdm in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from peft) (4.66.2)\nRequirement already satisfied: accelerate&gt;=0.21.0 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from peft) (0.28.0)\nRequirement already satisfied: safetensors in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from peft) (0.4.2)\nRequirement already satisfied: huggingface-hub&gt;=0.17.0 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from peft) (0.22.2)\nRequirement already satisfied: filelock in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from huggingface-hub&gt;=0.17.0-&gt;peft) (3.13.1)\nRequirement already satisfied: fsspec&gt;=2023.5.0 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from huggingface-hub&gt;=0.17.0-&gt;peft) (2024.2.0)\nRequirement already satisfied: requests in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from huggingface-hub&gt;=0.17.0-&gt;peft) (2.31.0)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from huggingface-hub&gt;=0.17.0-&gt;peft) (4.9.0)\nRequirement already satisfied: sympy in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from torch&gt;=1.13.0-&gt;peft) (1.12)\nRequirement already satisfied: networkx in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from torch&gt;=1.13.0-&gt;peft) (3.1)\nRequirement already satisfied: jinja2 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from torch&gt;=1.13.0-&gt;peft) (3.1.3)\nRequirement already satisfied: regex!=2019.12.17 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from transformers-&gt;peft) (2023.12.25)\nRequirement already satisfied: tokenizers&lt;0.19,&gt;=0.14 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from transformers-&gt;peft) (0.15.2)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from jinja2-&gt;torch&gt;=1.13.0-&gt;peft) (2.1.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from requests-&gt;huggingface-hub&gt;=0.17.0-&gt;peft) (2.0.4)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from requests-&gt;huggingface-hub&gt;=0.17.0-&gt;peft) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from requests-&gt;huggingface-hub&gt;=0.17.0-&gt;peft) (2.1.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from requests-&gt;huggingface-hub&gt;=0.17.0-&gt;peft) (2024.2.2)\nRequirement already satisfied: mpmath&gt;=0.19 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from sympy-&gt;torch&gt;=1.13.0-&gt;peft) (1.3.0)\n\n\n\n!huggingface-cli login\n\n\n    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n\n    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\nToken: \nAdd token as git credential? (Y/n) n\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n\n\n\nfrom tqdm.auto import trange\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom datasets import load_dataset\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n/home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n/home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n\n\nassert torch.cuda.is_available(), \"No CUDA, no party\"\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\nmodel_name = 'mistralai/Mistral-7B-v0.1'\n\n# loading tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token_id = tokenizer.eos_token_id\n# loading model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map='auto',\n    low_cpu_mem_usage=True,\n    offload_state_dict=True,\n    load_in_4bit=True,\n    torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n)\nfor param in model.parameters():\n    param.requires_grad=False\n\nmodel.gradient_checkpointing_enable()  # only store a small subset of activations, re-compute the rest.\nmodel.enable_input_require_grads()\n\nThe `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\nDownloading shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00&lt;00:00, 3943.87it/s]\nLoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:19&lt;00:00,  9.96s/it]"
  },
  {
    "objectID": "projects/LLM.html#prompt-tuning",
    "href": "projects/LLM.html#prompt-tuning",
    "title": "Training large models*",
    "section": "Prompt tuning",
    "text": "Prompt tuning\nPrompt tuning injects learnable tokens in the prompts that are optimized via backpropagation.\n\nNumber of learnable parameters is: \\(N_{tokens} \\times d_{embed}\\).\nThis approach is pretty cheap and is known to work pretty good in simple cases.\n\nprompt = 'A quick brown fox'\nbatch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n\nfor i in range(7):\n    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n\nprint(f\"\\nOutput: {tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist(), skip_special_tokens=True)}\")\n\n\nOutput: A quick brown fox jumps over a lazy dog.\n\n\n\nWhat a blatant lie!\nThis particular fox assures you that it didn‚Äôt in fact jump over the lazy dog.\nNo, sir! The fox was just minding its own business.\nYour task is to train the model to say truth: no dog was jumped over today.\n\nthe_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\nbatch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\noutputs = model(**batch)\n\nnext_word_logits = outputs.logits[:, :-1]\ntrue_next_tokens = batch['input_ids'][:, 1:]\nloss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n\nprint(f\"Loss: {loss.item():.2f}\")\n\nLoss: 3.06\n\n\nYour task\nImplement prompt tuning using the template below.\n\nclass WordEmbeddingsWithLearnedPrompts(nn.Module):\n    \"\"\"\n    To perform prompt tuning, you will need to replace model's original word embeddings with a layer - THIS layer\n     - that inserts trainable prompts instead of the first N token embeddings. \"\"\"\n\n    def __init__(self, word_embeddings: nn.Embedding, num_prompts: int):\n        super().__init__()\n        self.original_word_embeddings = word_embeddings\n        self.num_prompts = num_prompts\n        self.learnable_prompts = nn.Parameter(\n            torch.randn(1, num_prompts, word_embeddings.embedding_dim),\n            requires_grad=True\n          )\n\n    def forward(self, input_ids: torch.LongTensor):\n        # input_ids shape: [batch_size, seq length]\n        assert input_ids.dtype == torch.int64\n        assert input_ids.shape[1] &gt; self.num_prompts\n        assert torch.all(input_ids[:, :self.num_prompts] == tokenizer.pad_token_id).item(), \"don't forget to prepend several BOS tokens to input_ids\"\n\n        # Your task: embed input_ids, but replace the first :num_prompts: tokens with self.learnable_prompts\n        # This is because we will prepend :num_prompts: padding tokens at the beginning\n\n        # After you are done, you must produce a word embedding vector for each token in input_ids,\n        # except that the first :num_prompts: vectors should equal learnable_prompts;\n        # any additional vectors after first :num_prompts: ones should be embedded as usual\n        # Note: since you're dealing with trainable params, please torch.cat instead of item assignment\n\n        learnable_embeddings = self.learnable_prompts.repeat(input_ids.shape[0], 1, 1)\n        inputs_embeddings = self.original_word_embeddings(input_ids[:, self.num_prompts:])\n\n        embeddings = torch.cat((learnable_embeddings, inputs_embeddings), dim=1)\n\n        return embeddings\n\n\nnum_prompts = 16\ntest_emb_layer = WordEmbeddingsWithLearnedPrompts(model.model.embed_tokens, num_prompts=num_prompts).to(device)\ntest_input_ids = tokenizer(\"a cat say on a may\", return_tensors='pt')['input_ids'].to(device)\n\nspace_for_prompts = torch.full(\n    size=(len(test_input_ids), num_prompts),\n    fill_value=tokenizer.pad_token_id,\n    dtype=torch.int64,\n    device=device\n)\ntest_inputs_with_prompts = torch.cat([space_for_prompts, test_input_ids], dim=1)\n\nwith torch.cuda.amp.autocast():\n  test_prompt_embeddings = test_emb_layer(test_inputs_with_prompts)\n\nassert test_prompt_embeddings.shape[:2] == test_inputs_with_prompts.shape\nassert test_prompt_embeddings.shape[-1] == model.config.hidden_size\nassert torch.allclose(test_prompt_embeddings[:, :num_prompts], test_emb_layer.learnable_prompts.float())\nassert torch.allclose(test_prompt_embeddings[:, num_prompts:], model.model.embed_tokens(test_input_ids).float())\nprint(\"Looks legit!\")\n\nLooks legit!\n\n\n\nassert isinstance(model.model.embed_tokens, nn.Embedding), \"you have already replaced the embedding layer. If the replacement is broken, please reload the model\"\nmodel.model.embed_tokens = WordEmbeddingsWithLearnedPrompts(model.model.embed_tokens, num_prompts=num_prompts).to(device)\n\n\nopt = torch.optim.Adam([model.model.embed_tokens.learnable_prompts], lr=0.01)\n\nPrepare batch\n\nthe_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\nbatch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\nspace_for_prompts = torch.full([len(test_input_ids), num_prompts], fill_value=tokenizer.pad_token_id,\n                               dtype=torch.int64, device=device)\nbatch['input_ids'] = torch.cat([space_for_prompts, batch['input_ids']], dim=1)\nbatch['attention_mask'] = torch.cat([torch.ones_like(space_for_prompts), batch['attention_mask']], dim=1)\n\n\nn_iter = 250\natol = 0.17\n\npbar = trange(n_iter)\nfor i in pbar:\n  outputs = model(**batch)\n  next_word_logits = outputs.logits[:, num_prompts:-1, :]\n  true_next_tokens = batch['input_ids'][:, num_prompts + 1:]\n  loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n  loss.backward()\n  opt.step()\n  opt.zero_grad()\n  pbar.set_description(f\"Loss {loss.item():.2f}\")\n  if loss &lt; atol:\n    break\n\nassert loss.item() &lt;= atol\nprint(\"Good job!\")\n\nLoss 0.17:  16%|‚ñà‚ñã        | 41/250 [00:07&lt;00:40,  5.20it/s]\n\n\nGood job!\n\n\n\n\n\n\nprompt = 'A quick brown fox'\nbatch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\nbatch['input_ids'] = torch.cat([space_for_prompts, batch['input_ids']], dim=1)\nbatch['attention_mask'] = torch.cat([torch.ones_like(space_for_prompts), batch['attention_mask']], dim=1)\n\nfor i in range(15):\n    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n\nprint(f\"\\nOutput: {tokenizer.decode(batch['input_ids'][0, num_prompts:].cpu().numpy().tolist(), skip_special_tokens=True)}\")\n\n\nOutput: A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway\n\n\nIf you did everything right, the model will deny that the fox jumped over the lazy dog\n\nUsing HuggingFace PEFT (2 points)\nPEFT is a transformer‚Äôs ü§ó sister library that allows you to apply various __p__arameter __e__fficient __f__ine-__t__uning methods to pre-trained transformers. This library provides an implementation of the common PEFT techniques: * LoRA * Prefix-Tuning * Prompt-Tuning * IA3 * and more\n\nimport peft\n\n\ndel model\ntorch.cuda.empty_cache()\n\n\n# re-loading model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map='auto',\n    low_cpu_mem_usage=True,\n    offload_state_dict=True,\n    load_in_4bit=True,\n    torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n)\nfor param in model.parameters():\n    param.requires_grad = False\n\nmodel.gradient_checkpointing_enable()  # only store a small subset of activations, re-compute the rest.\nmodel.enable_input_require_grads()\n\nThe `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\nDownloading shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00&lt;00:00, 2776.77it/s]\nLoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:24&lt;00:00, 12.17s/it]\n\n\nSanity check that we have reloaded the model\n\nassert isinstance(model.model.embed_tokens, nn.Embedding), \"please reload the model\"\n\n\npeft_config = peft.PromptTuningConfig(task_type=peft.TaskType.CAUSAL_LM, num_virtual_tokens=16)\nmodel = peft.get_peft_model(model, peft_config)  # note: for most peft methods, this line also modifies model in-place\nmodel.print_trainable_parameters()\n\ntrainable params: 65,536 || all params: 7,241,797,632 || trainable%: 0.000904968673943746\n\n\nYour task\nOptimize the PEFT-wrapped model to achieve next token prediction loss &lt; 0.17, but this time using PEFT\nNote\nYou no longer need to prepend PAD tokens, but you still need to skip :num_virtual_tokens: first logits.\nFinally, generate the sentence to make sure that the model learned the truth.\n\nthe_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\nbatch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\n\n\nopt = torch.optim.Adam(model.parameters(), lr=0.01)\n\n\nn_iter = 200\natol = 0.17\n\npbar = trange(n_iter)\nfor i in pbar:\n  outputs = model(**batch)\n  next_word_logits = outputs.logits[:, num_prompts:-1, :]\n  true_next_tokens = batch['input_ids'][:, 1:]\n  loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n  loss.backward()\n  opt.step()\n  opt.zero_grad()\n  pbar.set_description(f\"Loss {loss.item():.2f}\")\n  if loss &lt; atol:\n    break\n\nassert loss.item() &lt;= atol\nprint(\"Good job!\")\n\nLoss 0.17:  15%|‚ñà‚ñå        | 30/200 [00:06&lt;00:38,  4.38it/s]\n\n\nGood job!\n\n\n\n\n\n\nprompt = 'A quick brown fox'\nbatch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n\nfor i in range(15):\n    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n\nprint(f\"\\nOutput: {tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist(), skip_special_tokens=True)}\")\n\n\nOutput: A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway"
  },
  {
    "objectID": "projects/LLM.html#parameter-efficient-finetuning-with-lora",
    "href": "projects/LLM.html#parameter-efficient-finetuning-with-lora",
    "title": "Training large models*",
    "section": "Parameter-efficient finetuning with LoRA",
    "text": "Parameter-efficient finetuning with LoRA\nWhen training on more serious tasks, you can use low-rank adapters based on the LoRA paper.\nThe core idea is to add low-rank adapters in parallel with existing linear layers, like this:\n\nSpecifically, the application of adapter looks as follows: \\[\ny = W_{0} x + \\frac{\\alpha}{r} B A x\n\\] Above: * \\(W_{0}\\) - is the original weight * A, B - are learnable matrices * r - is their rank * \\(\\alpha\\) - is the relative weight of the weight update\nIn the original LoRA paper, the adapters were only added to attention projection matrices.\nHowever, subsequent works show that it is useful to adapt FFNs as well. But before we do any training, we need to implement the basic LoRA layer.\n\n# re-load the model to remove any previous PEFT tuners\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map='auto',\n    low_cpu_mem_usage=True,\n    offload_state_dict=True,\n    load_in_4bit=True,\n    torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n)\nfor param in model.parameters():\n    param.requires_grad=False\n\nmodel.gradient_checkpointing_enable()  # only store a small subset of activations, re-compute the rest.\nmodel.enable_input_require_grads()\n\nThe `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\nDownloading shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00&lt;00:00, 6413.31it/s]\nLoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:25&lt;00:00, 12.99s/it]\n\n\nYour task\nImplement LoRA adapter.\n\nclass LoRALayer(nn.Module):\n    \"\"\"Wraps a linear layer with LoRA-like adapter. Wraps an existing OPT linear layer\"\"\"\n    def __init__(self, module: nn.Linear, rank: int):\n        super().__init__()\n        self.module = module  # pre-trained (frozen) linear layer\n        self.adapter_A = nn.Parameter(torch.empty(module.in_features, rank, device=module.weight.device))\n        nn.init.kaiming_uniform_(self.adapter_A, a=5 ** 0.5)\n        self.adapter_B = nn.Parameter(torch.zeros(rank, module.out_features, device=module.weight.device))\n\n    def forward(self, inputs):\n        # Apply self.module and LoRA adapter, return the sum (self.module outputs + adapter outputs)\n        #  &lt;YOUR CODE HERE&gt;\n        module_outputs = self.module(inputs)\n        adapter_outputs = inputs @ self.adapter_A.unsqueeze(0) @ self.adapter_B.unsqueeze(0)\n        return module_outputs + adapter_outputs\n\nTest implementation\n\ntest_linear = nn.Linear(128, 128)\ntest_linear.weight.data[...] = torch.eye(128)\ntest_adapter = LoRALayer(test_linear, rank=8)\n\nassert torch.allclose(test_adapter(torch.ones(1, 1, 128)), test_linear.bias + 1), \"please check your forward pass\"\n\ntest_adapter.adapter_A.data[...] = torch.linspace(0.1, -0.5, 128 * 8).view(128, 8)\ntest_adapter.adapter_B.data[...] = torch.linspace(0.5, -0.1, 128 * 8).view(8, 128)\ntest_linear.bias.data[...] = torch.linspace(1., -1., 128)\n\ndummy_loss = F.mse_loss(test_adapter(torch.ones(1, 128) / 128).squeeze(), torch.linspace(-1, 1, 128))\nassert torch.allclose(dummy_loss, torch.tensor(1.3711389), rtol=0, atol=1e-4)\ndummy_loss.backward()\nassert all(w.grad is not None for w in [test_adapter.adapter_A, test_adapter.adapter_B]), \"some adapter weights have no grad\"\nassert torch.allclose(test_adapter.adapter_A.grad.sum(), torch.tensor(-0.60158), rtol=0, atol=1e-4), \"bad grad w.r.t. A\"\nassert torch.allclose(test_adapter.adapter_B.grad.sum(), torch.tensor(0.9931), rtol=0, atol=1e-4), \"bad grad w.r.t. B\"\n# note: bad grad means that your code is different from LoRA paper OR that your code is not autograd-friendly (e.g. no_grad)\ndel dummy_loss, test_linear, test_adapter\nprint(\"All tests passed!\")\n\nAll tests passed!\n\n\n\nApply LoRA to the model\nThe code below applies LoRA adapters on top of Q/K/V linear layers of Transformer attention.\nYou may also choose to modify other layers:\nself_attn.o_proj - attention output projection\nmlp.up_proj, mlp.gate_proj, mlp.down_proj - transformer feedforward layers\nlm_head - output LM head\n\nlora_rank = 8\nattention_layer_name = 'Attention'\n\nfor name, module in model.model.layers.named_modules():\n    if attention_layer_name in repr(type(module)):\n        module.q_proj = LoRALayer(module.q_proj, rank=lora_rank).to(device)\n        module.k_proj = LoRALayer(module.k_proj, rank=lora_rank).to(device)\n        module.v_proj = LoRALayer(module.v_proj, rank=lora_rank).to(device)\n\nassert sum(isinstance(module, LoRALayer) for module in model.modules()) == 96 # for Mistral-7b\n\n\nbatch = tokenizer(\"This model wants to share its greatest secret:\", return_tensors='pt', return_token_type_ids=False)\n# test a single training step, make sure we get meaningful gradients\nwith torch.cuda.amp.autocast(dtype=torch.float32):\n    out = model.forward(**batch)\n    (out.logits.norm() / 100).backward()\n\nfor i, module in enumerate(model.modules()):\n    if isinstance(module, LoRALayer):\n        assert module.adapter_B.grad is not None\n        assert module.adapter_B.grad.norm().item() &gt; 0\n\nmodel.zero_grad(set_to_none=True)\nprint(\"Grad check successful, well done!\")\n\nGrad check successful, well done!\n\n\nLet us finetune the model on some custom dataset\n\ndata = load_dataset(\"Abirate/english_quotes\")\ndata = data.map(lambda samples: tokenizer(samples['quote']), batched=True)\nmodel._hf_peft_config_loaded = True  # silence a warning from HF trainer\n\ntrainer = transformers.Trainer(\n    model=model,\n    train_dataset=data['train'],\n    args=transformers.TrainingArguments(\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=4,\n        warmup_steps=100,\n        max_steps=200,\n        learning_rate=2e-4,\n        fp16=True,\n        logging_steps=1,\n        output_dir='outputs'\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n)\n# Silence the warnings. Please re-enable for inference!\nmodel.config.use_cache = False\ntrainer.train()\n\n\nDetected kernel version 5.4.161, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n\nwandb: Currently logged in as: spiridon_sun_rotator (ist). Use `wandb login --relogin` to force relogin\n\n\n\n\nwandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade\n\n\nTracking run with wandb version 0.16.5\n\n\nRun data is saved locally in /home/dkuznedelev/TestStuff/wandb/run-20240420_145457-ofe14mto\n\n\nSyncing run fiery-monkey-363 to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/ist/huggingface\n\n\n View run at https://wandb.ai/ist/huggingface/runs/ofe14mto/workspace\n\n\n\n      \n      \n      [200/200 06:40, Epoch 1/2]\n    \n    \n\n\n\nStep\nTraining Loss\n\n\n\n\n1\n1.645500\n\n\n2\n1.144600\n\n\n3\n1.568900\n\n\n4\n1.325900\n\n\n5\n1.454300\n\n\n6\n1.517300\n\n\n7\n1.326500\n\n\n8\n1.528400\n\n\n9\n1.748900\n\n\n10\n1.943000\n\n\n11\n1.181900\n\n\n12\n1.154000\n\n\n13\n1.011400\n\n\n14\n2.002500\n\n\n15\n1.485600\n\n\n16\n1.445700\n\n\n17\n1.219000\n\n\n18\n0.857100\n\n\n19\n1.460900\n\n\n20\n1.308300\n\n\n21\n1.599100\n\n\n22\n1.280100\n\n\n23\n2.156600\n\n\n24\n1.734400\n\n\n25\n1.045600\n\n\n26\n1.711600\n\n\n27\n1.587400\n\n\n28\n1.008600\n\n\n29\n1.314400\n\n\n30\n1.325600\n\n\n31\n1.420500\n\n\n32\n1.416400\n\n\n33\n1.168900\n\n\n34\n1.681500\n\n\n35\n0.751400\n\n\n36\n1.230400\n\n\n37\n1.142900\n\n\n38\n1.534200\n\n\n39\n1.144900\n\n\n40\n1.400100\n\n\n41\n1.642500\n\n\n42\n1.232600\n\n\n43\n1.183000\n\n\n44\n1.195500\n\n\n45\n0.838600\n\n\n46\n1.061100\n\n\n47\n1.040700\n\n\n48\n1.292800\n\n\n49\n1.018700\n\n\n50\n1.109300\n\n\n51\n1.517000\n\n\n52\n1.007300\n\n\n53\n1.019400\n\n\n54\n1.003200\n\n\n55\n1.212900\n\n\n56\n0.932100\n\n\n57\n1.888600\n\n\n58\n1.015500\n\n\n59\n1.218300\n\n\n60\n1.432200\n\n\n61\n1.546300\n\n\n62\n1.210100\n\n\n63\n1.345500\n\n\n64\n1.323900\n\n\n65\n1.228500\n\n\n66\n1.338300\n\n\n67\n1.189500\n\n\n68\n1.201700\n\n\n69\n1.581700\n\n\n70\n1.210800\n\n\n71\n0.878800\n\n\n72\n1.939500\n\n\n73\n1.443000\n\n\n74\n1.632700\n\n\n75\n1.409200\n\n\n76\n1.494800\n\n\n77\n0.958200\n\n\n78\n1.408700\n\n\n79\n1.303300\n\n\n80\n1.333900\n\n\n81\n1.254000\n\n\n82\n1.233000\n\n\n83\n1.209700\n\n\n84\n1.356200\n\n\n85\n1.588900\n\n\n86\n1.362600\n\n\n87\n1.530000\n\n\n88\n1.184700\n\n\n89\n1.421600\n\n\n90\n0.811200\n\n\n91\n1.541900\n\n\n92\n0.990900\n\n\n93\n1.591600\n\n\n94\n1.610900\n\n\n95\n1.387000\n\n\n96\n1.570800\n\n\n97\n1.172800\n\n\n98\n1.243300\n\n\n99\n1.256600\n\n\n100\n1.069300\n\n\n101\n1.100700\n\n\n102\n1.149700\n\n\n103\n1.367400\n\n\n104\n1.206000\n\n\n105\n0.897100\n\n\n106\n1.335100\n\n\n107\n1.414700\n\n\n108\n1.371800\n\n\n109\n1.348400\n\n\n110\n1.309900\n\n\n111\n1.457400\n\n\n112\n1.102300\n\n\n113\n1.187800\n\n\n114\n1.721400\n\n\n115\n1.201500\n\n\n116\n1.538200\n\n\n117\n1.146500\n\n\n118\n1.159400\n\n\n119\n1.149700\n\n\n120\n1.370800\n\n\n121\n1.472100\n\n\n122\n1.641800\n\n\n123\n1.538600\n\n\n124\n1.115500\n\n\n125\n1.262900\n\n\n126\n1.571300\n\n\n127\n1.078500\n\n\n128\n1.308100\n\n\n129\n1.794200\n\n\n130\n1.242800\n\n\n131\n1.391200\n\n\n132\n1.118600\n\n\n133\n1.622200\n\n\n134\n1.262600\n\n\n135\n1.144300\n\n\n136\n1.469000\n\n\n137\n1.485600\n\n\n138\n1.506100\n\n\n139\n1.469300\n\n\n140\n1.339100\n\n\n141\n1.356000\n\n\n142\n1.611500\n\n\n143\n1.608200\n\n\n144\n1.247000\n\n\n145\n1.321400\n\n\n146\n1.235200\n\n\n147\n1.497200\n\n\n148\n1.588800\n\n\n149\n0.995400\n\n\n150\n1.087800\n\n\n151\n1.262000\n\n\n152\n0.789100\n\n\n153\n1.301900\n\n\n154\n1.645700\n\n\n155\n0.801000\n\n\n156\n1.637800\n\n\n157\n1.023600\n\n\n158\n1.232900\n\n\n159\n1.115200\n\n\n160\n1.139100\n\n\n161\n0.983800\n\n\n162\n0.947200\n\n\n163\n1.043300\n\n\n164\n0.727700\n\n\n165\n1.044900\n\n\n166\n0.865200\n\n\n167\n1.069000\n\n\n168\n1.226500\n\n\n169\n1.231700\n\n\n170\n0.771800\n\n\n171\n1.027600\n\n\n172\n1.291300\n\n\n173\n0.824400\n\n\n174\n0.949600\n\n\n175\n0.758100\n\n\n176\n1.237400\n\n\n177\n0.999600\n\n\n178\n1.272400\n\n\n179\n0.806900\n\n\n180\n0.747600\n\n\n181\n0.786800\n\n\n182\n0.774600\n\n\n183\n0.893300\n\n\n184\n0.823300\n\n\n185\n0.833500\n\n\n186\n1.235300\n\n\n187\n1.432900\n\n\n188\n1.026600\n\n\n189\n0.864100\n\n\n190\n0.964800\n\n\n191\n0.779100\n\n\n192\n1.434800\n\n\n193\n1.208800\n\n\n194\n0.724900\n\n\n195\n1.303000\n\n\n196\n0.866500\n\n\n197\n0.940800\n\n\n198\n1.549100\n\n\n199\n0.537700\n\n\n200\n1.167400\n\n\n\n\n\n\nTrainOutput(global_step=200, training_loss=1.2602548521757126, metrics={'train_runtime': 413.8954, 'train_samples_per_second': 7.731, 'train_steps_per_second': 0.483, 'total_flos': 1.3073398340124672e+16, 'train_loss': 1.2602548521757126, 'epoch': 1.28})\n\n\n\n\nInference finetuned model\n\nmodel.config.use_cache = True\n\n\nbatch = tokenizer(\"Two things are infinite: \", return_tensors='pt')\n\nwith torch.cuda.amp.autocast():\n  output_tokens = model.generate(**batch, max_new_tokens=24)\n\nprint(f\"\\n\\n{tokenizer.decode(output_tokens[0], skip_special_tokens=True)}\")\n\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n\n\n\n\nTwo things are infinite:  the universe and human stupidity; and I'm not sure about the universe.\n\n- Albert Einstein\n\n\n\n\nPEFT library provides implemenation of LoRA as well.\n\nfrom peft import LoraConfig, get_peft_model\n\n\n# re-load the model to remove any previous PEFT tuners\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map='auto',\n    low_cpu_mem_usage=True,\n    offload_state_dict=True,\n    load_in_4bit=True,\n    torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n)\nfor param in model.parameters():\n    param.requires_grad=False\n\nmodel.gradient_checkpointing_enable()  # only store a small subset of activations, re-compute the rest.\nmodel.enable_input_require_grads()\n\nThe `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\nDownloading shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00&lt;00:00, 2402.24it/s]\nLoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:25&lt;00:00, 12.70s/it]\n\n\n\nconfig = LoraConfig(\n    r=16, # rank of the LoRA adapter\n    lora_alpha=16, # weight of LoRA adapter\n    target_modules=[\"q_proj\", \"k_proj\"], # layers to apply LoRA\n    lora_dropout=0.1, # dropout in LoRA layers\n    bias=\"none\", # whether to add bias\n    task_type=\"CAUSAL_LM\"\n)\nmodel = get_peft_model(model, config)\nmodel.print_trainable_parameters()\n\ntrainable params: 6,815,744 || all params: 7,248,547,840 || trainable%: 0.0940290959023318\n\n\nYou can train model exactly in the same way as before.\nNote PEFT allows you to save the adapter weights alone and push to hub.\n\nmodel.push_to_hub(\"username/adapter_name\", use_auth_token=True)\n\nNote, for floating models LoRA adapter can be seamlessly merged into model, thus incuring zero overhead on inference.\nHowever, for quantized models things are more subtle and one has to process them in parallel with the main weight or apply some additional hack to merge them into the model."
  },
  {
    "objectID": "projects/LLM.html#materials-for-further-study",
    "href": "projects/LLM.html#materials-for-further-study",
    "title": "Training large models*",
    "section": "Materials for further study",
    "text": "Materials for further study\n\nPEFT documentation\nLoRA paper\nPrompt tuning paper"
  },
  {
    "objectID": "projects/langevien.html",
    "href": "projects/langevien.html",
    "title": "Score Estimations, Langevin Generative models and NCSN",
    "section": "",
    "text": "By Addisu Amare\n\\(\\textbf{Date}:\\) 08.11.24\nimport torch\nfrom torch.distributions import MultivariateNormal, Normal\nimport numpy as np\nimport tqdm\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\n%matplotlib inline"
  },
  {
    "objectID": "projects/langevien.html#teachers---ground-truth-data",
    "href": "projects/langevien.html#teachers---ground-truth-data",
    "title": "Score Estimations, Langevin Generative models and NCSN",
    "section": "Teachers - Ground-Truth data",
    "text": "Teachers - Ground-Truth data\nThe \\(\\textbf{Teacher}\\) is the class of the density distribution, which we want to approximate by neural-network-based approaches. This class has the following methods:\n\nsample(sampling from the density distribution)\nlog_prob(calculation of the logarithm of the distribution)\n\nUndoubtedly, in order to calculate the ground-truth logarithm of the distribution we define the distribution by such way to analytically compute the gradient of the logartihm. Thus, one can take the Teacher, which is the ground-truth distribution thrrough the notebook, as follows:\n\\[\\mathbb{P} = \\frac{1}{5}\\mathcal{N}((-5,-5),I) + \\frac{4}{5}\\mathcal{N}((5,5),I)\\]\n\nclass GMMDist(object):\n    def __init__(self, dim):\n        self.mix_probs = torch.tensor([0.8, 0.2])\n        self.means = torch.stack([5 * torch.ones(dim), -torch.ones(dim) * 5], dim=0)\n        self.sigma = 1\n        self.std = torch.stack([torch.ones(dim) * self.sigma for i in range(len(self.mix_probs))], dim=0)\n\n    def sample(self, n):\n        \"\"\"\n        n - int\n        \"\"\"\n        n = torch.Size([n])[0]\n        mix_idx = torch.multinomial(self.mix_probs, n, replacement=True)\n        means = self.means[mix_idx]\n        stds = self.std[mix_idx]\n        return torch.randn_like(means) * stds + means\n\n    def log_prob(self, samples):\n        \"\"\"\n        samples - torch.Size([B,N])\n        \"\"\"\n        logps = []\n        for i in range(len(self.mix_probs)):\n            logps.append((-((samples - self.means[i]) ** 2).sum(dim=-1) / (2 * self.sigma ** 2) - 0.5 * np.log(\n                2 * np.pi * self.sigma ** 2)) + self.mix_probs[i].log())\n        logp = torch.logsumexp(torch.stack(logps, dim=0), dim=0)\n        return logp\n\n\ndef plot_teachers(teacher, num_samples):\n    \"\"\"\n    num_samples - int\n\n    \"\"\"\n    plt.figure(figsize=(4,4),dpi=150 )\n    samples = teacher.sample(num_samples)\n    plt.scatter(samples[:,0],samples[:,1],s=10,edgecolor='black')\n    plt.grid()\n\n\nDIM = 2\nNUM_SAMPLES_PLOT = 2000\nteacher =  GMMDist(DIM)\nplot_teachers(teacher,  NUM_SAMPLES_PLOT)\n\n\n\n\n\n\n\n\n\ndef data_score(x):\n    x = x.detach()\n    x.requires_grad_(True)\n    y = teacher.log_prob(x).sum()\n    return torch.autograd.grad(y, x)[0]\n\n\ndef toy_net(hiddens):\n    model = []\n    for inp,outp in zip(hiddens[:-1],hiddens[1:]):\n        model.append(torch.nn.Linear(inp,outp,bias=True))\n        model.append(torch.nn.ReLU())\n    model.pop()\n    return torch.nn.Sequential(*model)"
  },
  {
    "objectID": "projects/langevien.html#score-based-genereative-modeling",
    "href": "projects/langevien.html#score-based-genereative-modeling",
    "title": "Score Estimations, Langevin Generative models and NCSN",
    "section": "1. Score-based genereative modeling",
    "text": "1. Score-based genereative modeling\n\ndef plot_loss_quiever(model,teacher, label = None):\n\n    grid_size = 20\n    left_bound=-3\n    right_bound=3\n    mesh = []\n    x = np.linspace(left_bound, right_bound, grid_size)\n    y = np.linspace(left_bound, right_bound, grid_size)\n    for i in x:\n        for j in y:\n            mesh.append(np.asarray([i, j]))\n\n    mesh = np.stack(mesh, axis=0)\n    mesh = torch.from_numpy(mesh).float()\n\n    if label == \"energy\":\n        mesh.requires_grad = True\n        scores = model(mesh)\n        scores = torch.autograd.grad(scores.sum(),mesh,create_graph=True)[0]\n    else:\n        scores = model(mesh.detach())\n\n    mesh = mesh.detach().numpy()\n    scores = scores.detach().numpy()\n    fig,ax = plt.subplots(1,2,figsize=(16,8),sharex=True,sharey=True)\n\n    ax[0].grid(False)\n    ax[0].quiver(mesh[:, 0], mesh[:, 1], scores[:, 0], scores[:, 1], width=0.005)\n    ax[0].set_title('Estimated scores', fontsize=16)\n    ax[0].axis('square')\n\n    scores = data_score(torch.from_numpy(mesh))\n    scores = scores.detach().numpy()\n\n    ax[1].grid(False)\n    ax[1].quiver(mesh[:, 0], mesh[:, 1], scores[:, 0], scores[:, 1], width=0.005)\n    ax[1].set_title('Data scores', fontsize=16)\n    ax[1].axis('square')\n\n\n1.1 Fischer divergence\n\\(\\textbf{Notation}:\\)\n\n\\(x \\in \\mathbb{R}^{D}, \\quad x \\sim \\mathbb{P}(\\cdot)\\)\nThe model for approximation of \\(\\mathbb{P}\\) is \\(\\mathbb{\\hat{P}}(x, \\theta) = \\frac{1}{Z(\\theta)}\\mathbb{Q}(x,\\theta)\\), unnormalized density\nDesires: One would like to find out such parameters to apprroximate the ground-truth distribution better.\n\\(\\mathbb{\\hat{P}}(x, \\theta)\\) is reffered to as Energy-based models.\n\nConsidering the gradient of the approximation:\n\\[ \\nabla_{x} \\log \\mathbb{\\hat{P}}(x,\\theta) = \\nabla_{x} \\log \\mathbb{Q}(x,\\theta) - \\nabla_{x} \\log Z(\\theta) = \\nabla_{x} \\log \\mathbb{Q}(x,\\theta) - 0 \\]\nWe see, that in order to estimate the gradient of logarithm of approximate distribution, we need in the gradient of unnormalized density \\(\\mathbb{Q}(\\cdot, \\theta)\\). Such gradient of unnormalized density is called \\(\\textbf{Score function}\\) and is denoted as \\(\\psi(x,\\theta)\\).\n\\(\\textbf{The main goal of notebook}:\\)\nWe would like to estimate the gradient of logarithm of the ground-truth density \\(\\mathbb{P}\\) by the score function.\nUndoubtedly, when we have the access to the ground-truth grradient of logarithm of density, we can consider easy regression problem between \\(\\mathbb{Q}\\) and \\(\\mathbb{P}\\). This easy regression prroblem is reffered to as \\(\\textbf{Fischer divergence}\\).\nLet \\(p(x)\\) and \\(q(x)\\) are ground-truth distribution of data and unnormalized approximate distribution. The both functions are scalar functios. \\[ F(q||p)=\\frac{1}{2}\\int || - \\nabla_{x}\\log q(x) + \\nabla_{x} \\log p(x) ||_{2}^{2}dp(x)  \\]\n\n\n\nChessUrl\n\n\n\\(\\textbf{Code}\\) for the Fischer divergence.\n\ndef fischer_divergence(energy_net, data, teacher):\n\n    \"\"\"\n    energy_net - torch.nn.Module\n    teacher    - object\n    data.      - torch.Size([B,N])\n    \"\"\"\n\n    data.requires_grad = True\n    logq = -energy_net(data) # torch.Size([B,1])\n    logp = teacher.log_prob(data)# torch.Size([B,1])\n    q_score = torch.autograd.grad(logq.sum(), data,\n                                   create_graph=True,retain_graph=True)[0]\n    p_score = torch.autograd.grad(logp.sum(), data,\n                                 create_graph=True,retain_graph=True)[0]\n    return 0.5*torch.mean(torch.norm((q_score + p_score)**2,dim=-1))\n\n\nHIDDENS = [DIM,64,128,256,128,64,1]\nmodel = toy_net(HIDDENS)\nopt = torch.optim.Adam(model.parameters(),lr=1e-5)\nbatch_size=2048\niterations=5000\nlosses = []\nfor _ in tqdm.tqdm(range(iterations)):\n    opt.zero_grad()\n    samples = teacher.sample(batch_size)\n    loss = fischer_divergence(model,samples,teacher)\n    loss.backward()\n    opt.step()\n    losses.append(loss.item())\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [03:37&lt;00:00, 22.95it/s]\n\n\n\nplt.plot(losses);\n\n\n\n\n\n\n\n\n\nplot_loss_quiever(model, teacher, label='energy')\n\n\n\n\n\n\n\n\n\n\n1.2 Score Matching and Estimation.\n\\(\\textbf{Importantly}:\\)\nWhen we talk \\(\\textbf{Score Estimation}\\), it means, that we solve the regression problem between \\(\\psi(x,\\theta)\\), where \\(\\psi(x,\\theta)\\) is a neural network. When we talk \\(\\textbf{Score Matching}\\), it means, that we solve the regression problem between \\(\\psi(x,\\theta)\\), where \\(\\psi(x,\\theta)\\) is \\(\\nabla_{x} \\log q(x,\\theta)\\) and \\(q(x,\\theta)\\) is a neural-network.\n\n\n\nalt text\n\n\n\\(\\textbf{Theorem 1}:\\) Let \\(\\psi(x,\\theta): \\mathbb{R}^{D} \\to \\mathbb{R}^{D}\\) is a regular differetiable function. Having defined the score function as follows :\n\\[\\psi_{i}(x,\\theta) = \\frac{\\partial \\log q(x,\\theta)}{\\partial x_{i}}, \\quad \\frac{\\partial \\psi(x,\\theta)}{\\partial x_{i}} = \\frac{\\partial^{2} \\log q(x,\\theta)}{\\partial x_{i}^{2}} \\]\nThen:\n\\[\\mathcal{J}(\\theta) = \\int_{\\mathbb{R}^{D}} \\sum_{i=1}^{D} \\{ \\frac{\\partial \\psi(x,\\theta)}{\\partial x_{i}}+ \\frac{1}{2}\\psi(x,\\theta)^{2}\\}d\\mathbb{P}(x) + Const.\\]\n\\(\\textbf{proof}:\\) See the Seminar\n\ndef score_matching(energy_net, data):\n    \"\"\"\n    energy_net - torch.nn.Module\n    data       - torch.Size([B,N])\n    \"\"\"\n    data.requires_grad = True\n    logq = -energy_net(data) # torch.Size([B,1])\n    score = torch.autograd.grad(logq.sum(), data,\n                                create_graph=True, retain_graph=True)[0]# torch.Size([B,N])\n\n    loss1 = 0.5*torch.norm(score,dim=-1)**2 # torch.Size([B])\n\n    grad_score = torch.autograd.grad(score.sum(dim=-1).sum() , data,\n                                     create_graph=True, retain_graph=True)[0] #torch.Size([B,N])\n\n    loss2 = grad_score.sum(dim=-1)#torch.Size([B])\n\n    return torch.mean(loss1 + loss2 ,dim = 0)\n\n\ndef score_estimation(score_net, data):\n    \"\"\"\n    energy_net - torch.nn.Module\n    data       - torch.Size([B,N])\n    \"\"\"\n    data.requires_grad = True\n    score = score_net(data)#torch.Size([B,N])\n    loss1 =  0.5*torch.norm(score,dim=-1)**2 # torch.Size([B])\n    grad_score = torch.autograd.grad(score.sum(),data,\n                                    create_graph=True,retain_graph=True)[0]\n    loss2 = grad_score.sum(dim=-1)# torch.Size([B])\n    return torch.mean(loss1 + loss2 ,dim = 0)\n\n\nHIDDENS = [DIM,64,128,256,128,64,2]\nmodel = toy_net(HIDDENS)\nopt = torch.optim.Adam(model.parameters(),lr=1e-5)\nbatch_size=2048\niterations=5000\nlosses = []\nfor _ in tqdm.tqdm(range(iterations)):\n    opt.zero_grad()\n    samples = teacher.sample(batch_size)\n    loss =  score_estimation(model,samples)\n    loss.backward()\n    opt.step()\n    losses.append(loss.item())\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [03:28&lt;00:00, 24.04it/s]\n\n\n\nplt.plot(losses);\n\n\n\n\n\n\n\n\n\nplot_loss_quiever(model,teacher)\n\n\n\n\n\n\n\n\n\n\n1.3 Denoising Score matching and estimation\nLet‚Äôs consider \\(\\mathbb{P}_{\\sigma}\\) is as a perturbed data distribution of \\(\\mathbb{P}_{\\sigma}\\). First of all, we pick out the noise scale \\(\\sigma\\) manually.\n\n\\(\\mathbb{q}_{\\sigma}(\\tilde{x}|x) = \\mathcal{N}(\\tilde{x}|x,\\sigma^{2}I)\\) - conditional perturbed distribution.\n\\(\\mathbb{P}_{\\sigma}(\\tilde{x},x) =\\mathbb{q}_{\\sigma}(\\tilde{x}|x)\\mathbb{P}(x)\\) - joint distribution.\n\\(\\mathbb{P}_{\\sigma}^{m}(\\tilde{x}) = \\int \\mathbb{P}_{\\sigma}(\\tilde{x},x) dx\\) - marginal distribution.\n\n\\(\\textbf{Theorem 2:}\\) Let \\(\\mathbb{P}(x)\\) is the gound-truth distribution, while \\(\\psi(x,\\theta): \\mathbb{R}^{D} \\to \\mathbb{R}^{D}\\) is learnable scorre function and tries to approximate the gradient of logarothm of \\(\\mathbb{P}\\). Then: \\[\\mathbb{E}_{\\mathbb{P}(x)} \\{ \\frac{1}{2}||\\psi(x,\\theta) - \\frac{\\partial}{\\partial x} \\log \\mathbb{P}(x) ||_{2}^{2}\\} \\sim \\mathbb{E}_{\\mathbb{P}_{\\sigma}(x,\\tilde{x})}\\{\\frac{1}{2}||\\psi(\\tilde{x},\\theta) - \\frac{\\partial}{\\partial \\tilde{x}} \\log \\mathbb{P}_{\\sigma}(\\tilde{x}|x) ||_{2}^{2}\\}\\]\n\\(\\textbf{Proof}:\\) On seminar\n\ndef denoising_score_matching( score_net, samples, sigma):\n\n    \"\"\"\n    score_net - torch.nn.module\n    samples   - torch.Size([B,N])\n    sigma     - int\n    \"\"\"\n\n    samples.requires_grad = True\n    vector = torch.randn_like(samples, device = samples.device)*sigma\n    perturbed_samples = samples + vector\n    logp = - score_net(perturbed_samples)\n    dlogp = sigma**2*torch.autograd.grad(logp.sum(), perturbed_samples,\n                                         create_graph=True, retain_graph=True)[0]\n    kernel = vector\n    return 0.5*torch.mean(torch.norm(dlogp + kernel, dim=-1)**2)\n\n\ndef denoising_score_estimation( score_net, samples, sigma):\n\n    \"\"\"\n    score_net - torch.nn.module\n    samples   - torch.Size([B,N])\n    sigma     - int\n    \"\"\"\n\n    perturbed_samples = samples + torch.randn_like(samples, device = samples.device)*sigma\n    score = score_net(perturbed_samples)\n    dlogq =  1/sigma**2*(samples - perturbed_samples )\n    return 0.5*torch.mean(torch.norm((score - dlogq)**2,dim=-1))\n\n\nmodel = toy_net(HIDDENS)\nopt = torch.optim.Adam(model.parameters(),lr=1e-5)\nbatch_size=2048\niterations=5000\nlosses = []\nfor _ in tqdm.tqdm(range(iterations)):\n    opt.zero_grad()\n    samples = teacher.sample(batch_size)\n    loss =  denoising_score_estimation(model,samples,0.01)\n    loss.backward()\n    opt.step()\n    losses.append(loss.item())\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:30&lt;00:00, 166.51it/s]\n\n\n\nplot_loss_quiever(model,teacher)\n\n\n\n\n\n\n\n\n\n\n1.4 Sliced Score matching and estimation\nTo understand Sliced Score Estimation, we recall \\(S_{d}(x)\\) and \\(\\psi(x,\\theta)\\), where the firrst is the ground-truth score function. If we solve any regression problem between \\(S_{d}(x)\\) and \\(\\psi(x,\\theta)\\), then we have the problem between two high-dimensional vectors. One would like to reduce the dimensionality of the problem. The random projection is one of the possible solutions.\n\\[ F(\\mathbb{Q},\\mathbb{P}) = \\frac{1}{2}\\int_{\\mathbb{R}^{D}}||\\psi(x,\\theta) - S_{d}(x) ||_{2}^{2} \\]\nThen, we rewrite the aforementioned expression via projections on random Gaussian vectors:\n\\[ \\mathcal{L}(\\theta) = \\frac{1}{2}\\mathbb{E}_{p_{v}}\\mathbb{E}_{\\mathbb{P}(x)}||v^{T}\\psi(x,\\theta) - v^{T}S_{d}(x)||_{2}^{2},\\]\nwhere \\(p_{v}(v) = \\mathcal{N}(v|0,I)\\), and we imply, that \\(\\mathbb{E}_{p_{v}}||v||^{2}_{2} &lt; \\infty\\)\n\\(\\textbf{Theorem 3}\\): Let \\(\\psi(x,\\theta)\\) is a score function, \\(S_{d}(x)\\) is the ground-truth scorree function, then:\n\\[\\frac{1}{2}\\mathbb{E}_{p_{v}}\\mathbb{E}_{p_{d}}||v^{T}\\psi(x,\\theta) - v^{T}S_{d}(x)||_{2}^{2} = \\frac{1}{2}\\mathbb{E}_{p_{v}}\\mathbb{E}_{p_{d}}(v^{T}\\psi(x,\\theta))^{2} + \\sum_{i=1}^{D}\\mathbb{E}_{p_{d}} v_{i}v^{T} \\times \\frac{\\partial \\psi(x,\\theta)}{\\partial x_{i}}\\]\n\\(\\textbf{Proof}:\\) On seminar\n\ndef sliced_score_estimation(score_net, samples, n_particles=1 ):\n\n    \"\"\"\n\n\n    \"\"\"\n    samples = samples.unsqueeze(0).expand(n_particles,*samples.shape).contiguous().view(-1,*samples.shape[1:])\n    samples.requires_grad = True\n    vectors = torch.randn_like(samples)\n    vectors = vectors / torch.norm(vectors, dim=-1, keepdim=True)\n    score = score_net(samples)\n    loss1 = 0.5*torch.sum(score*vectors, dim=-1)**2 #torch.Size([M*B])\n    loss1 = loss1.view(n_particles, -1).mean(dim=0)\n\n    return loss1.mean()\n\n\ndef sliced_score_estimation_vr(score_net, samples, n_particles=1):\n    \"\"\"\n    Be careful if the shape of samples is not B x x_dim!!!!\n    \"\"\"\n    dup_samples = samples.unsqueeze(0).expand(n_particles, *samples.shape).contiguous().view(-1, *samples.shape[1:])\n    dup_samples.requires_grad_(True)\n    vectors = torch.randn_like(dup_samples)\n\n    grad1 = score_net(dup_samples)\n    gradv = torch.sum(grad1 * vectors)\n    grad2 = torch.autograd.grad(gradv, dup_samples, create_graph=True)[0]\n\n    grad1 = grad1.view(dup_samples.shape[0], -1)\n    loss1 = torch.sum(grad1 * grad1, dim=-1) / 2.\n\n    loss2 = torch.sum((vectors * grad2).view(dup_samples.shape[0], -1), dim=-1)\n\n    loss1 = loss1.view(n_particles, -1).mean(dim=0)\n    loss2 = loss2.view(n_particles, -1).mean(dim=0)\n\n    loss = loss1 + loss2\n    return loss.mean(), loss1.mean(), loss2.mean()\n\n\ndef sliced_score_estimation_own(score_net, samples, n_particles=1):\n\n    samples.requires_grad = True\n    vectors = torch.randn(n_particles, *samples.shape)\n    vectors = vectors / torch.norm(vectors, dim=-1, keepdim=True)\n    score = score_net(samples)\n    loss1 = 0.5*torch.matmul(score.unsqueeze(0),vectors.permute(0,2,1))#torch.Size([M,B,B])\n    loss1 = torch.sum(loss1,dim=-1)**2 # torch.Size([M,B])\n    loss1 = torch.mean(loss1,dim=-1).mean(dim=0)\n\n    # run the code\n    #loss2 =\n\n    return loss1 + loss2\n\n\nmodel = toy_net(HIDDENS)\nopt = torch.optim.Adam(model.parameters(),lr=1e-5)\nbatch_size=2048\niterations=5000\nlosses = []\nfor _ in tqdm.tqdm(range(iterations)):\n    opt.zero_grad()\n    samples = teacher.sample(batch_size)\n    loss,*_ =  sliced_score_estimation_vr(model,samples)\n    loss.backward()\n    opt.step()\n    losses.append(loss.item())\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [03:35&lt;00:00, 23.23it/s]\n\n\n\nplt.plot(losses);\n\n\n\n\n\n\n\n\n\nplot_loss_quiever(model,teacher)\n\n\n\n\n\n\n\n\n\n\nProblem of score estimation\n\n\n\nAlt Text"
  },
  {
    "objectID": "projects/langevien.html#noise-conditional-score-network-ncsn-on-toy-examples",
    "href": "projects/langevien.html#noise-conditional-score-network-ncsn-on-toy-examples",
    "title": "Score Estimations, Langevin Generative models and NCSN",
    "section": "2. Noise Conditional Score Network (NCSN) on toy Examples",
    "text": "2. Noise Conditional Score Network (NCSN) on toy Examples\nOnce we have trained a score-based model \\(S_{\\theta}(x) = \\nabla_{x} \\log \\mathbb{P}(x)\\) we can use an iterative procedure called Langevin dynamics to draw samples from it.\nLangevin dynamics provides an MCMC procedure to sample from a distribution \\(\\mathbb{P}(x)\\) using only its score function the score function. Specifically, it initializes the chain from an arbitrary prior distribution \\(x_{0} \\sim \\pi(x)\\) and then iterates the following\n\\[x_{k+1} = x_{k} + \\epsilon \\nabla_{x} \\log \\mathbb{P}(x) + \\sqrt{2\\epsilon}z, \\quad z \\sim \\mathcal{N}(z|0,I)\\]\nWhen \\(\\epsilon \\to 0\\) as well as \\(K \\to \\infty\\) , \\(x_{k}\\) obtained from the procedure in Langevin Dynamics algorithm converges to a sample from \\(\\mathbb{P}(x)\\) under some regularity conditions. In practice, the error is negligible when \\(\\epsilon\\) is sufficiently small and \\(K\\) is sufficiently large.\n\n\n\nAlt Text\n\n\nNext, we estimate the score function of each noise-perturbed distribution \\(\\nabla \\log \\mathbb{P}{\\sigma_{i}}(x)\\), by training a Noise Conditional Score-Based Model \\(S(x_{i},\\sigma_{i})\\),with score matching, such that:\n\\[S(x_{i},\\sigma_{i}) = \\nabla_{x} \\log \\mathbb{P}_{\\sigma}(x_{i})\\]\n\n\n\nAlt Text\n\n\nThe training objective for \\(S_{\\theta}(x_{i},\\sigma_{i})\\) is a weighted sum of Fisher divergences for all noise scales. In particular, we use the objective below.\n\\[ \\sum_{i=1}^{L} \\lambda(i) \\mathbb{E}_{\\mathbb{P}_{\\sigma_{i}}}||s_{\\theta}(x,\\sigma_{i}) - \\nabla_{x} \\log \\mathbb{P}_{\\sigma_{i}}(x)||_{2}^{2}\\]\nAfter training our noise-conditional score-based model \\(S_{\\theta}(x,\\sigma_{i})\\), we can produce samples from it by running Langevin dynamics for \\(i = L,L-1,...,1\\) in sequence. This method is called annealed Langevin dynamics since the noise scale \\(\\sigma_{i}\\) decreases (anneals) gradually over time.\n\n\n\nAlt Text\n\n\n\ndef visualize(teacher, model, left_bound=-1., right_bound=1., savefig=None, step=None, device=None):\n\n        #---------------------------------------------------#\n        fig,ax = plt.subplots(2,3, figsize=(27,18),sharex=True, sharey=True,dpi=150 )\n\n        mesh = []\n        grid_size = 100\n        x = np.linspace(left_bound, right_bound, grid_size)\n        y = np.linspace(left_bound, right_bound, grid_size)\n        for i in x:\n            for j in y:\n                mesh.append(np.asarray([i, j]))\n\n        mesh = np.stack(mesh, axis=0)\n        mesh = torch.from_numpy(mesh).float()\n        if device is not None:\n            mesh = mesh.to(device)\n\n        logp_true = teacher.log_prob(mesh)\n        logp_true = logp_true.view(grid_size, grid_size).exp()\n\n        ax[0,0].grid(False)\n        ax[0,0].axis('off')\n        ax[0,0].set_title('Data density', fontsize=16)\n        ax[0,0].imshow(np.flipud(logp_true.cpu().numpy()), cmap='inferno')\n\n\n\n        #---------------------------------------------------------#\n\n        grid_size = 20\n        mesh = []\n        x = np.linspace(left_bound, right_bound, grid_size)\n        y = np.linspace(left_bound, right_bound, grid_size)\n        for i in x:\n            for j in y:\n                mesh.append(np.asarray([i, j]))\n\n        mesh = np.stack(mesh, axis=0)\n        mesh = torch.from_numpy(mesh).float()\n        if device is not None:\n            mesh = mesh.to(device)\n\n\n        scores = model(  mesh.detach().to(DEVICE) )\n        mesh = mesh.detach().cpu().numpy()\n        scores = scores.detach().cpu().numpy()\n\n        ax[0,1].grid(False)\n        ax[0,1].axis('off')\n        ax[0,1].axis('square')\n        ax[0,1].quiver(mesh[:, 0], mesh[:, 1], scores[:, 0], scores[:, 1], width=0.005)\n        ax[0,1].set_title('Estimated scores', fontsize=16)\n\n\n\n        #-------------------------------------------------------------#\n\n        samples = teacher.sample(1280)\n        samples = samples.detach().cpu().numpy()\n        ax[0,2].scatter(samples[:, 0], samples[:, 1], s=0.5)\n        ax[0,2].axis('square')\n        ax[0,2].set_title('data samples',fontsize=16)\n        ax[0,2].set_xlim([left_bound, right_bound])\n        ax[0,2].set_ylim([left_bound, right_bound])\n\n\n        #------------------------------------------------------------#\n\n\n        samples_ = torch.rand(1280, 2) * (right_bound - left_bound) + left_bound\n\n        samples = ncsn.langevin_dynamics(model, samples_).detach().numpy()\n        ax[1,0].scatter(samples[:, 0], samples[:, 1], s=0.5)\n        ax[1,0].axis('square')\n        ax[1,0].axis('square')\n        ax[1,0].set_title('Model Langevin dynamics',fontsize=16)\n        ax[1,0].set_xlim([left_bound, right_bound])\n        ax[1,0].set_ylim([left_bound, right_bound])\n\n\n        #-----------------------------------------------------------#\n\n\n        scores = data_score(torch.from_numpy(mesh) )\n        scores = scores.detach().numpy()\n\n        ax[1,1].axis('off')\n        ax[1,1].quiver(mesh[:, 0], mesh[:, 1], scores[:, 0], scores[:, 1], width=0.005)\n        ax[1,1].set_title('True Data scores', fontsize=16)\n        ax[1,1].axis('square')\n\n\n\n        samples = torch.rand(1280, 2) * (right_bound - left_bound) + left_bound\n\n        samples = ncsn.langevin_dynamics(data_score, samples).detach().numpy()\n        ax[1,2].scatter(samples[:, 0], samples[:, 1], s=0.1)\n        ax[1,2].axis('square')\n        ax[1,2].set_title('True Langevin dynamics data',fontsize=16)\n        ax[1,2].set_xlim([left_bound, right_bound])\n        ax[1,2].set_ylim([left_bound, right_bound])\n\n\n        \"\"\"\n        samples = torch.rand(1280, 2) * (right_bound - left_bound) + left_bound\n        sigmas = torch.exp(torch.linspace(np.log(20), 0., 10)).to(DEVICE)\n        labels = torch.linspace(1,10,10).to(DEVICE)\n        samples = ncsn.anneal_langevin_dynamics(ncsn.score, samples.to(DEVICE), sigmas\n                                             , labels).detach().cpu().numpy()\n\n        plt.scatter(samples[:, 0], samples[:, 1], s=0.2)\n        plt.axis('square')\n        plt.title('Right Annealed Langevin dynamics samples')\n        plt.xlim([left_bound, right_bound])\n        plt.ylim([left_bound, right_bound])\n        plt.show()\n        \"\"\"\n        fig.tight_layout()\n        plt.show()\n\n\nclass NCSN(torch.nn.Module):\n\n    def __init__(self, score, teacher, train_steps, lr, batch_size):\n        super().__init__()\n        self.train_steps = train_steps\n        self.score = score\n        self.teacher = teacher\n        self.lr = lr\n        self.batch_size = batch_size\n\n    def langevin_dynamics(self, score, init, lr=0.1, step=1000):\n\n        \"\"\"\n        score - torch.nn.Module\n        init  - torch.Size([B,N])\n        \"\"\"\n        for step in range(step):\n            init = init + score(init)*lr + torch.randn_like(init,device=init.device)*np.sqrt(2*lr)\n        return init\n\n    def anneal_langevin_dynamics(self, score, init, sigmas, lr=0.1, n_steps_each=100):\n\n        \"\"\"\n        score   - space-time torch.nn.Module\n        init    - torch.Size([B,N])\n        sigmas  - List\n        \"\"\"\n        #with torch.no_grad\n        for sigma in sigmas:\n            current_lr = lr*sigma**2/sigmas[-1]**2\n            for step in range(n_steps_each):\n                init = init + 0.5*current_lr*score(init, sigma).detach()\n                init = init + torch.randn_like(init, device=init.device)*np.sqrt(current_lr)\n\n        return init\n\n    def anneal_dsm_score_estimation(self,scorenet, samples, labels, sigmas, anneal_power=2.):\n\n        batch_size = samples.shape[0]\n        samples = samples.repeat(len(sigmas),1).reshape(len(sigmas),-1,samples.shape[-1])\n        perturbed_samples = samples + torch.randn_like(samples)*sigmas.reshape(-1,1,1)\n\n        scores = scorenet( perturbed_samples.reshape(-1,samples.shape[-1]),\n                           labels.view(-1,1).expand( len(sigmas), batch_size).flatten().view(-1) )\n\n        target = -1.*(perturbed_samples.reshape(-1,samples.shape[-1]) - samples.reshape(-1, samples.shape[-1]) )*\\\n                 (sigmas.view(-1,1).expand(len(sigmas),  batch_size).flatten().view(-1,1))**2\n\n        loss = 1/2.*((scores - target)**2).sum(dim = -1)\n\n        loss =  loss*\\\n              (sigmas.view(-1,1).expand(len(sigmas), batch_size).flatten().view(-1) )\n\n        return loss.mean(dim=0)\n\n    def train_(self, iterations = 10000, batch_size = 128):\n\n        \"\"\"\n        hidden_units = 128\n        score = torch.nn.Sequential(\n            torch.nn.Linear(3, hidden_units),\n            torch.nn.Softplus(),\n            torch.nn.Linear(hidden_units, hidden_units),\n            torch.nn.Softplus(),\n            torch.nn.Linear(hidden_units, 2),\n        )\n        \"\"\"\n        losses = []\n        optimizer = torch.optim.Adam(self.score.parameters(), lr=0.001)\n        teacher = GMMDist(dim=2)\n\n        for step in  tqdm(range(iterations)):\n            samples = teacher.sample((batch_size,)).to(DEVICE)\n\n            #loss, *_ = sliced_score_estimation_vr(score, samples, n_particles=1)\n\n            loss = self.anneal_dsm_score_estimation(self.score, samples, labels = torch.linspace(1,10,10).to(DEVICE) ,\n                                                    sigmas=torch.exp(torch.linspace(np.log(20), 0., 10)).to(DEVICE))\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            losses.append(loss.item())\n\n        return self.score, teacher, losses\n\n    def train(self):\n        opt_score = torch.optim.Adam(self.score.parameters(),lr=self.lr)\n        for step in tqdm.tqdm(range(self.train_steps)):\n            samples = self.teacher.sample(self.batch_size)\n            opt_score.zero_grad()\n            loss,*_ = sliced_score_estimation_vr(self.score, samples, n_particles = 1)\n            loss.backward()\n            opt_score.step()\n        visualize(self.teacher, self.score, -8, 8)\n\n\nhidden_units = 128\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(2, hidden_units),\n    torch.nn.Softplus(),\n    torch.nn.Linear(hidden_units, hidden_units),\n    torch.nn.Softplus(),\n    torch.nn.Linear(hidden_units, 2),\n)\nncsn = NCSN(model, teachers[0], train_steps = 1000,lr=1e-3,batch_size=128)\n\n\nDEVICE=\"cpu\"\n\n\nncsn.train()\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:02&lt;00:00, 482.13it/s]\n\n\n\n\n\n\n\n\n\n\n\n\nAlt Text"
  },
  {
    "objectID": "projects/langevien.html#questions",
    "href": "projects/langevien.html#questions",
    "title": "Score Estimations, Langevin Generative models and NCSN",
    "section": "Questions",
    "text": "Questions"
  },
  {
    "objectID": "projects/Clustering.html",
    "href": "projects/Clustering.html",
    "title": "Clustering Machine Learning",
    "section": "",
    "text": "In the scatter plot below, we can see three separate groups of data points and we would like to recover them using clustering.\nThink of ‚Äúdiscovering‚Äù the class labels that we already take for granted in a classification task.\nEven if the groups are obvious in the data, it is hard to find them when the data lives in a high-dimensional space, which we can‚Äôt visualize in a single histogram or scatterplot.\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Polygon\nfrom matplotlib.collections import PatchCollection\nfrom sklearn.cluster import KMeans\nfrom matplotlib.animation import FuncAnimation\nfrom IPython import display\n\n# Generating random data for the illustration\nnp.random.seed(1)\npoints1 = np.random.randn(50, 2) + [4, 4]\npoints2 = np.random.randn(50, 2) + [-4, -4]\npoints3 = np.random.randn(50, 2) + [4, -4]\ndata = np.vstack([points1, points2, points3])\n\nfig, ax = plt.subplots()\nplt.title(\"Data\")\nscatter = ax.scatter(data[:, 0], data[:, 1], c='black')\ncentroids_plot, = ax.plot([], [], markeredgewidth=2, color='yellow', ls='', marker='*')\nax.grid(linestyle=':')\nplt.show()"
  },
  {
    "objectID": "projects/Clustering.html#exercise-cluster-digits",
    "href": "projects/Clustering.html#exercise-cluster-digits",
    "title": "Clustering Machine Learning",
    "section": "üèãÔ∏è‚Äç‚ôÇÔ∏è Exercise: cluster digits",
    "text": "üèãÔ∏è‚Äç‚ôÇÔ∏è Exercise: cluster digits\n\nPerform K-means clustering on the digits data, searching for ten clusters.\nVisualize the cluster centers as images (i.e.¬†reshape each to 8x8 and use plt.imshow)\nDo the clusters seem to be correlated with particular digits?\nWhat is the adjusted_rand_score?\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.metrics import adjusted_rand_score\n\n# Load the MNIST dataset\nmnist = fetch_openml('mnist_784',\n                     parser=\"auto\")\nX = mnist.data / 255.0  # Normalize data to [0, 1]\ny = mnist.target.astype(int)  # Convert target values to integers\n\n# Apply K-means clustering\nn_clusters = 10\n\n### üê±üê±üê± YOUR CODE HERE üê±üê±üê±\nkmeans = None\n\n# Visualize the cluster centers as images\nfig, axes = plt.subplots(2, 5, figsize=(8, 4))\naxes = axes.ravel()\n\nfor i, center in enumerate(kmeans.cluster_centers_):\n    axes[i].imshow(center.reshape(28, 28), cmap=plt.cm.gray)\n    axes[i].axis('off')\n    axes[i].set_title(f'Cluster {i}')\n\nplt.tight_layout()\nplt.show()3\n\n# Do the clusters seem to be correlated with particular digits?\n# Let's compute the most common label in each cluster\nlabels = np.zeros_like(kmeans.labels_)\nfor i in range(n_clusters):\n    mask = (kmeans.labels_ == i)\n    labels[mask] = np.bincount(y[mask]).argmax()\n\n# Calculate the adjusted_rand_score\n### üê±üê±üê± YOUR CODE HERE üê±üê±üê±\nscore = None\nprint(f\"Adjusted Rand Score: {score:.4f}\")\n\nTry the light version of MNIST (load_digits from sklearn)\n\nfrom sklearn.datasets import load_digits\ndigits = load_digits(n_class=10)\ndigits.data.shape\n\n### üê±üê±üê± YOUR CODE HERE üê±üê±üê±\nkmeans = None\nclusters = kmeans.fit_predict(digits.data)\n\n# Visualize the cluster centers as images\nfig, axes = plt.subplots(2, 5, figsize=(8, 4))\naxes = axes.ravel()\n\nfor i, center in enumerate(kmeans.cluster_centers_):\n    axes[i].imshow(center.reshape(8, 8), cmap=plt.cm.gray)\n    axes[i].axis('off')\n    axes[i].set_title(f'Cluster {i}')\n\nplt.show()\n\n### üê±üê±üê± YOUR CODE HERE üê±üê±üê±\nscore = None\nprint(f\"Adjusted Rand Score: {score:.4f}\")\n\n\n\n\n\n\n\n\n0.6649258693926379"
  },
  {
    "objectID": "projects/Clustering.html#hierarchical-clustering",
    "href": "projects/Clustering.html#hierarchical-clustering",
    "title": "Clustering Machine Learning",
    "section": "üëë Hierarchical Clustering",
    "text": "üëë Hierarchical Clustering\n\nOne nice feature of hierarchical clustering is that we can visualize the results as a dendrogram, a hierarchical tree.\nUsing the visualization, we can then decide how ‚Äúdeep‚Äù we want to cluster the dataset by setting a ‚Äúdepth‚Äù threshold\nOr in other words, we don‚Äôt need to make a decision about the number of clusters upfront.\n\n\n‚ûó Agglomerative and divisive hierarchical clustering\n\nFurthermore, we can distinguish between 2 main approaches to hierarchical clustering: Divisive clustering and agglomerative clustering.\nIn agglomerative clustering, we start with a single sample from our dataset and iteratively merge it with other samples to form clusters - we can see it as a bottom-up approach for building the clustering dendrogram.\n\nIn divisive clustering, however, we start with the whole dataset as one cluster, and we iteratively split it into smaller subclusters - a top-down approach.\n\nIn this notebook, we will use agglomerative clustering.\n\n\nSingle and complete linkage\n\nNow, the next question is how we measure the similarity between samples.\nOne approach is the familiar Euclidean distance metric that we already used via the K-Means algorithm.\nAs a refresher, the distance between 2 m-dimensional vectors \\(\\mathbf{p}\\) and \\(\\mathbf{q}\\) can be computed as:\n\n\\[\\begin{align} \\mathrm{d}(\\mathbf{q},\\mathbf{p}) & = \\sqrt{(q_1-p_1)^2 + (q_2-p_2)^2 + \\cdots + (q_m-p_m)^2} \\\\[8pt]\n& = \\sqrt{\\sum_{j=1}^m (q_j-p_j)^2} = \\|\\mathbf{q} - \\mathbf{p}\\|_2\\end{align}\\]\n\nHowever, that‚Äôs the distance between 2 samples.\nNow, how do we compute the similarity between subclusters of samples?\nI.e., our goal is to iteratively merge the most similar pairs of clusters until only one big cluster remains.\nThere are many different approaches to this, for example single and complete linkage.\nIn single linkage, we take the pair of the most similar samples (based on the Euclidean distance, for example) in each cluster, and merge the two clusters which have the most similar 2 members into one new, bigger cluster.\nIn complete linkage, we compare the pairs of the two most dissimilar members of each cluster with each other, and we merge the 2 clusters where the distance between its 2 most dissimilar members is smallest.\n\n\n\nfrom sklearn import datasets\niris = datasets.load_iris()\nX = iris.data[:, [2, 3]]\ny = iris.target\nn_samples, n_features = X.shape\n\nplt.scatter(X[:, 0], X[:, 1], c=y)\nplt.grid(linestyle=\":\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import dendrogram\n\n\nclusters = linkage(X, metric='euclidean', method='complete')\n\n\nfig, ax = plt.subplots(1,1, figsize=(12,8))\ndendr = dendrogram(clusters, ax=ax)\nplt.ylabel('Euclidean Distance')\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.cluster import AgglomerativeClustering\n\nac = AgglomerativeClustering(n_clusters=3,\n                             affinity='euclidean',\n                             linkage='complete')\n\nprediction = ac.fit_predict(X)\nprint('Cluster labels: %s\\n' % prediction)\n\nCluster labels: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1\n 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0]\n\n\n\n/Users/bratishka/.pyenv/versions/3.9.17/envs/benchmarx/lib/python3.9/site-packages/sklearn/cluster/_agglomerative.py:1006: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n  warnings.warn(\n\n\n\nplt.scatter(X[:, 0], X[:, 1], c=prediction)\nplt.grid(linestyle=\":\")\nplt.show()"
  },
  {
    "objectID": "projects/Clustering.html#density-based-clustering---dbscan",
    "href": "projects/Clustering.html#density-based-clustering---dbscan",
    "title": "Clustering Machine Learning",
    "section": "üíé Density-based Clustering - DBSCAN",
    "text": "üíé Density-based Clustering - DBSCAN\n\nAnother useful approach to clustering is Density-based Spatial Clustering of Applications with Noise (DBSCAN).\nIn essence, we can think of DBSCAN as an algorithm that divides the dataset into subgroup based on dense regions of points.\n\nIn DBSCAN, we distinguish between 3 different ‚Äúpoints‚Äù:\n\nCore points: A core point is a point that has at least a minimum number of other points (MinPts) in its radius epsilon.\nBorder points: A border point is a point that is not a core point, since it doesn‚Äôt have enough MinPts in its neighborhood, but lies within the radius epsilon of a core point.\nNoise points: All other points that are neither core points nor border points.\n\n\nA nice feature about DBSCAN is that we don‚Äôt have to specify a number of clusters upfront. However, it requires the setting of additional hyperparameters such as the value for MinPts and the radius epsilon.\n\nfrom sklearn.datasets import make_moons\nX, y = make_moons(n_samples=400, noise=0.1, random_state=1)\nplt.scatter(X[:,0], X[:,1])\nplt.grid(linestyle=\":\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.cluster import DBSCAN\n\ndb = DBSCAN(eps=0.2, min_samples=10, metric='euclidean')\nlabels = db.fit_predict(X)\nprint(labels)\nplt.scatter(X[:, 0], X[:, 1], c=labels)\nplt.grid(linestyle=\":\")\nplt.show()\n\n[ 0  1  0  0  1  0  1  0  1  0  1  0  1  0  0  0  0  1  1  0  1  0  0  0\n  1  1  0  0  0  0  1  1  1  0  1  1  1  1  0  1  0  0  1  1  0  1  0  0\n  0  0  1  0  0  1  0  0  0  1  0  0  1  1  0  0  0  0  1  0  0  0  0  1\n  0  0  1  0  1  1  1  0  1  1  0 -1  1  1  0  0  0  1  1  0  0  0  1  0\n  0  0  0  1  1  0  1  1  1  0  1  0  0  0  1  0  1  0  1  1  0  0  1  0\n  1  1  0  0  0  1  1  1  0  0  0  0  0  0  0  0  1  1  1  0  1  1  1  1\n  1  1  1  1  1  1  0  1  1  1  0  0  1  1  1  0  1  0  0  1  0  1  1  0\n  1  0  0  0  0  0  1  1  0  1  1  1  1  0  1  0  1  1  1  1  0  1  0  0\n  0  1  0  1  1  0  1  1  0  1  0  0  0  1  1  1  0  1  0  1  0  0  0  1\n  0  1  0  1  1  1  1  0  1  0  0  0  0  0  0  1  0  1  0  1  1 -1  0  0\n  1  1  1  1  1  1  1  0  1  1  0  1  1  0  1  0  1  0  0  0  0  0  0  1\n  0  0  0  1  1  1  1  0 -1  0  1  1  1  1  1  0  1  0  1  0  1  1  1  1\n  1  0  1  0  1  0  1  0  0  1  0  1  0  1  0  1  0  1  0  0  1  0  1  1\n  0  0  0  1  1  0  0  1  1  0  0  1  0  1  0  0  0  1  1  1  0  1  0  1\n  1  0  0  1  1  0  0  1  1  1  1  0  1  0  0  1  1  1  0  1  0  1  0  1\n  1  1  0  0  0  0  0  1  0  1  0  0  1  1  1  0  0  0  1  1  1  0  0  0\n  0  1  0  0  1  1  1  0  1  0  0  0  1  0  0  1]\n\n\n\n\n\n\n\n\n\nüíé Extremely good visualization of DBSCAN\n\nüèãÔ∏è‚Äç‚ôÇÔ∏è Exercise\n\nUsing the following toy datasets, two concentric circles, experiment with the three different clustering algorithms that we used so far KMeans, AgglomerativeClustering, and DBSCAN.\nWhich clustering algorithms reproduces or discovers the hidden structure (pretending we don‚Äôt know y) best?\nCan you explain why this particular algorithm is a good choice while the other 2 ‚Äúfail‚Äù?\n\n\nfrom sklearn.datasets import make_circles\nX, y = make_circles(n_samples=500, factor=.6, noise=.05)\n\nplt.scatter(X[:, 0], X[:, 1], c=y)\nplt.grid(linestyle=\":\")\nplt.show()\n\n\n\n\n\n\n\n\n\n### üê±üê±üê± YOUR CODE HERE üê±üê±üê±"
  },
  {
    "objectID": "projects/Clustering.html#community-detection",
    "href": "projects/Clustering.html#community-detection",
    "title": "Clustering Machine Learning",
    "section": "üíÖ Community detection",
    "text": "üíÖ Community detection\n\nü•ã Karate club\n\n!pip install -q community node2vec python-louvain\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Load the Karate Club graph\nG = nx.karate_club_graph()\n\n# Draw the graph\nplt.figure(figsize=(8, 6))\nnx.draw(G, with_labels=True, node_color=\"skyblue\", node_size=1000)\nplt.title(\"Karate Club Graph\")\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(nx.adjacency_matrix(G))\n\n  (0, 1)    4\n  (0, 2)    5\n  (0, 3)    3\n  (0, 4)    3\n  (0, 5)    3\n  (0, 6)    3\n  (0, 7)    2\n  (0, 8)    2\n  (0, 10)   2\n  (0, 11)   3\n  (0, 12)   1\n  (0, 13)   3\n  (0, 17)   2\n  (0, 19)   2\n  (0, 21)   2\n  (0, 31)   2\n  (1, 0)    4\n  (1, 2)    6\n  (1, 3)    3\n  (1, 7)    4\n  (1, 13)   5\n  (1, 17)   1\n  (1, 19)   2\n  (1, 21)   2\n  (1, 30)   2\n  : :\n  (32, 18)  1\n  (32, 20)  3\n  (32, 22)  2\n  (32, 23)  5\n  (32, 29)  4\n  (32, 30)  3\n  (32, 31)  4\n  (32, 33)  5\n  (33, 8)   4\n  (33, 9)   2\n  (33, 13)  3\n  (33, 14)  2\n  (33, 15)  4\n  (33, 18)  2\n  (33, 19)  1\n  (33, 20)  1\n  (33, 22)  3\n  (33, 23)  4\n  (33, 26)  2\n  (33, 27)  4\n  (33, 28)  2\n  (33, 29)  2\n  (33, 30)  3\n  (33, 31)  4\n  (33, 32)  5\n\n\n\nfrom node2vec import Node2Vec\n\n# Generate embeddings using Node2Vec\nnode2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=4)\nmodel = node2vec.fit(window=10, min_count=1)\n\n# Get embeddings for all nodes\nembeddings = [model.wv[str(node)] for node in G.nodes()]\n\n\n\n\nGenerating walks (CPU: 4): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00&lt;00:00, 531.03it/s]\nGenerating walks (CPU: 2): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00&lt;00:00, 528.77it/s]\nGenerating walks (CPU: 1): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00&lt;00:00, 516.09it/s]\nGenerating walks (CPU: 3): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00&lt;00:00, 506.18it/s]\n\n\n\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=5)\nkmeans_clusters = kmeans.fit_predict(embeddings)\n\n# Visualize K-Means clusters\nplt.figure(figsize=(8, 6))\nnx.draw(G, with_labels=True, node_color=kmeans_clusters, cmap=\"coolwarm\", node_size=1000)\nplt.title(\"K-Means Clustering on Karate Club Graph\")\nplt.show()\n\n/Users/bratishka/.pyenv/versions/3.9.17/envs/benchmarx/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.cluster import DBSCAN\n\ndbscan = DBSCAN(eps=0.5, min_samples=5)\ndbscan_clusters = dbscan.fit_predict(embeddings)\n\n# Visualize DBSCAN clusters\nplt.figure(figsize=(8, 6))\nnx.draw(G, with_labels=True, node_color=dbscan_clusters, cmap=\"coolwarm\", node_size=1000)\nplt.title(\"DBSCAN Clustering on Karate Club Graph\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nü•∫üìò Facebook ego networks\n\nimport networkx as nx\n\nimport requests\nimport gzip\nimport shutil\nfrom matplotlib import pyplot as plt\n\n# Download the dataset\nurl = \"https://snap.stanford.edu/data/facebook_combined.txt.gz\"\nresponse = requests.get(url, stream=True)\nwith open(\"facebook_combined.txt.gz\", \"wb\") as file:\n    for chunk in response.iter_content(chunk_size=128):\n        file.write(chunk)\n\n# Unzip the dataset\nwith gzip.open(\"facebook_combined.txt.gz\", 'rb') as f_in:\n    with open(\"facebook_combined.txt\", 'wb') as f_out:\n        shutil.copyfileobj(f_in, f_out)\n\n\n# Load the Facebook graph\npath_to_dataset = \"facebook_combined.txt\"\nG = nx.read_edgelist(path_to_dataset)\n\nprint(f\"Number of nodes: {G.number_of_nodes()}\")\nprint(f\"Number of edges: {G.number_of_edges()}\")\n\nNumber of nodes: 4039\nNumber of edges: 88234\n\n\n\n#Create network layout for visualizations\n\nspring_pos = nx.layout.spring_layout(G)\n\nplt.axis(\"off\")\nnx.draw_networkx(G, pos=spring_pos, with_labels=False, node_size=15)\n\n\n\n\n\n\n\n\n\nimport community as community_louvain\n\nparts = community_louvain.best_partition(G)\nvalues = [parts.get(node) for node in G.nodes()]\n\n\n#drawing\n\nplt.axis(\"off\")\nnx.draw_networkx(G, pos=spring_pos, cmap=plt.get_cmap(\"Dark2\"),\n                 node_color=values, node_size=35, with_labels=False)\n\n\n\n\n\n\n\n\n\nprint(community_louvain.modularity(parts, G))\n\n0.8348452461385243\n\n\n\nfrom node2vec import Node2Vec\n\n# Generate embeddings using Node2Vec\nnode2vec = Node2Vec(G, dimensions=128, walk_length=30, num_walks=50, workers=4)\nmodel = node2vec.fit(window=10, min_count=1)\n\n# Get embeddings for all nodes\nembeddings = [model.wv[node] for node in G.nodes()]\n\n\n\n\nGenerating walks (CPU: 1): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:09&lt;00:00,  1.35it/s]\nGenerating walks (CPU: 2): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:09&lt;00:00,  1.35it/s]\nGenerating walks (CPU: 3): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:08&lt;00:00,  1.36it/s]\nGenerating walks (CPU: 4): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:08&lt;00:00,  1.38it/s]\n\n\n\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=10)  # assuming 10 clusters for demonstration\nkmeans_clusters = kmeans.fit_predict(embeddings)\n\n/Users/bratishka/.pyenv/versions/3.9.17/envs/benchmarx/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\nfrom sklearn.cluster import DBSCAN\n\ndbscan = DBSCAN(eps=0.5, min_samples=5)\ndbscan_clusters = dbscan.fit_predict(embeddings)\n\n\nimport matplotlib.pyplot as plt\nimport random\n\n# Sample a subgraph\nsampled_nodes = random.sample(G.nodes(), 1000)  # sample 1000 nodes\nsubG = G.subgraph(sampled_nodes)\n\n# Get clusters for sampled nodes\nsampled_kmeans_clusters = [kmeans_clusters[int(node)] for node in sampled_nodes]\n\n# Visualize K-Means clusters on subgraph\nplt.figure(figsize=(12, 12))\npos = nx.spring_layout(subG)\nnx.draw(subG, pos, node_color=sampled_kmeans_clusters, cmap=\"coolwarm\", node_size=50)\nplt.title(\"K-Means Clustering on Facebook Subgraph\")\nplt.show()\n\n/var/folders/7m/3rbdnx5n5sz625f3l87m91cc0000gn/T/ipykernel_24364/271705878.py:5: DeprecationWarning: Sampling from a set deprecated\nsince Python 3.9 and will be removed in a subsequent version.\n  sampled_nodes = random.sample(G.nodes(), 1000)  # sample 1000 nodes"
  },
  {
    "objectID": "projects/Random_forest.html",
    "href": "projects/Random_forest.html",
    "title": "2. Random Observations (Samples)",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import load_digits\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom tqdm.notebook import tqdm"
  },
  {
    "objectID": "projects/Random_forest.html#digits-dataset",
    "href": "projects/Random_forest.html#digits-dataset",
    "title": "2. Random Observations (Samples)",
    "section": "Digits Dataset",
    "text": "Digits Dataset\n\nX, y = load_digits(n_class=10, return_X_y=True)\nX.shape\n\n(1797, 64)\n\n\n\nassert X.shape[:1] == y.shape\n\n\nix = np.random.randint(0, y.size)\n\nplt.imshow(X[ix].reshape(8, 8))\nplt.title(f'digit {y[ix]}')\nplt.show()\n\n\n\n\n\n\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state=42)"
  },
  {
    "objectID": "projects/Random_forest.html#random-features",
    "href": "projects/Random_forest.html#random-features",
    "title": "2. Random Observations (Samples)",
    "section": "Random Features",
    "text": "Random Features\nWe need many stupid classifiers which make errors in different parts of the feature space.\n\nn_trees = 5\n\nMake an array to store probability predictions for different runs.\n\ny_probas = np.empty((n_trees, ) + y_test.shape + (10, ))\ny_probas.shape\n\n(5, 599, 10)\n\n\nTrain n_trees decision tree classifiers and save class probabilities to y_probas.\n\nfor i, state in enumerate(range(n_trees)):\n    model = DecisionTreeClassifier(max_features=4, max_depth=2, random_state=state)\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    y_probas[i] = model.predict_proba(X_test)\n\n    accuracy = accuracy_score(y_test, y_pred)\n    feat_index, = np.nonzero(model.feature_importances_)\n    print(f'[{i:02d}] Test accuracy i {accuracy:.3f}')\n    print(f'[{i:02d}] Features used for splitting are {feat_index}')\n\n[00] Test accuracy i 0.319\n[00] Features used for splitting are [10 33 43]\n[01] Test accuracy i 0.352\n[01] Features used for splitting are [21 50 54]\n[02] Test accuracy i 0.294\n[02] Features used for splitting are [ 2 25 44]\n[03] Test accuracy i 0.275\n[03] Features used for splitting are [10 28 61]\n[04] Test accuracy i 0.265\n[04] Features used for splitting are [36 43 58]\n\n\n\nplt.imshow(X_test[0].reshape(8, 8))\n\n\n\n\n\n\n\n\nShape of y_probas is n_trees x test_size x n_classes.\n\ny_probas.shape\n\n(5, 599, 10)\n\n\n\ny_probas[-1, 0].argmax()\n\n6\n\n\n\nAveraging\nLet‚Äôs average prediction of n_trees decision trees on a same test set.\n\ny_proba_mean = y_probas.mean(axis=0)\ny_proba_mean.shape\n\n(599, 10)\n\n\n\ny_proba_mean[:3]\n\narray([[0.01275487, 0.13177821, 0.11354822, 0.11483882, 0.10342248,\n        0.03542723, 0.26118463, 0.05922296, 0.08820299, 0.07961958],\n       [0.10853233, 0.08314822, 0.06579826, 0.20761667, 0.04763271,\n        0.13430245, 0.0463106 , 0.04824082, 0.06752656, 0.19089139],\n       [0.08518962, 0.0620693 , 0.11322425, 0.25086165, 0.01428571,\n        0.10487178, 0.0463393 , 0.02411379, 0.11661885, 0.18242574]])\n\n\nMake predictions (over the last n_classes axis).\n\ny_pred_mean = np.argmax(y_proba_mean, axis=1)\nprint(y_pred_mean[:3])\n\n[6 3 3]\n\n\nFinally, evaluate such model (ensemble of trees) and compreare it with a performance of a single tree.\n\nscore_mean = accuracy_score(y_test, y_pred_mean)\nprint(f'Score of averaged across ensemble is {score_mean * 100 :.2f}')\n\nScore of averaged across ensemble is 60.93\n\n\n\nscore = accuracy_score(y_test, y_probas[0].argmax(axis=1))\nprint(f'Score of a single tree is {score * 100 :.2f}')\n\nScore of a single tree is 31.89"
  },
  {
    "objectID": "projects/Random_forest.html#random-forest",
    "href": "projects/Random_forest.html#random-forest",
    "title": "2. Random Observations (Samples)",
    "section": "Random Forest",
    "text": "Random Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nmodel = RandomForestClassifier(n_estimators=5, max_features=6, max_depth=2, random_state=1)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\nprint(accuracy_score(y_test, y_pred))\n\n0.5659432387312187\n\n\nWe can reuse random states to build a tree manually.\n\nrs = []\nfor m in model.estimators_:\n    rs.append(m.random_state)\nprint(rs)\n\n[1791095845, 2135392491, 946286476, 1857819720, 491263]\n\n\n\ndef rf_train_test_accuracy(param_name, param_grid, **params):\n    \"\"\"Returns train and test perfomance of a RandomForest for\n    different values (param_grid) of a hyperparameter (param_name).\n    \"\"\"\n\n    train_score, test_score = [], []\n    clf = RandomForestClassifier(n_estimators=5, max_features=8, max_depth=6, random_state=1, n_jobs=-1)\n    if params:\n        clf.set_params(**params)\n\n    for param_value in tqdm(param_grid):\n        clf.set_params(**{param_name: param_value})\n        clf.fit(X_train, y_train)\n\n        train_score.append(accuracy_score(y_train, clf.predict(X_train)))\n        test_score.append(accuracy_score(y_test, clf.predict(X_test)))\n    return train_score, test_score"
  },
  {
    "objectID": "projects/Random_forest.html#random-forest-number-of-trees",
    "href": "projects/Random_forest.html#random-forest-number-of-trees",
    "title": "2. Random Observations (Samples)",
    "section": "Random Forest: Number of Trees",
    "text": "Random Forest: Number of Trees\n\ntrain_accuracy, test_accuracy = rf_train_test_accuracy('n_estimators', range(1, 50, 2))\n\n\n\n\n\nplt.figure(figsize=(10,5))\nplt.plot(list(range(1,50,2)), 1-np.array(train_accuracy), label='Train error')\nplt.plot(list(range(1,50,2)), 1-np.array(test_accuracy), label='Test error')\nplt.xlabel('Number of trees in the forest')\nplt.ylabel('Classification error (1 - accuracy).')\nplt.grid(True)\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "projects/Random_forest.html#random-forest.-tree-depth",
    "href": "projects/Random_forest.html#random-forest.-tree-depth",
    "title": "2. Random Observations (Samples)",
    "section": "Random Forest. Tree Depth",
    "text": "Random Forest. Tree Depth\n\ntrain_accuracy, test_accuracy = rf_train_test_accuracy('max_depth', range(1, 30))\n\n\n\n\n\nplt.figure(figsize=(10,5))\nplt.plot(list(range(1,30)), 1-np.array(train_accuracy), label='Train error')\nplt.plot(list(range(1,30)), 1-np.array(test_accuracy), label='Test error')\nplt.xlabel('Tree depth')\nplt.ylabel('Classification error (1 - accuracy).')\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "projects/Random_forest.html#random-forest-number-of-max-features",
    "href": "projects/Random_forest.html#random-forest-number-of-max-features",
    "title": "2. Random Observations (Samples)",
    "section": "Random Forest: Number of Max Features",
    "text": "Random Forest: Number of Max Features\n\ntrain_accuracy, test_accuracy = rf_train_test_accuracy('max_features', range(1, 64))\n\n\n\n\n\nplt.figure(figsize=(10,5))\nplt.plot(list(range(1, 64)), 1-np.array(train_accuracy), label='Train error')\nplt.plot(list(range(1, 64)), 1-np.array(test_accuracy), label='Test error')\nplt.xlabel('Max features to consider for split')\nplt.ylabel('Classification error (1 - accuracy).')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nparams = {\n    'n_estimators': 20,\n    'max_depth': 10\n}\ntrain_accuracy, test_accuracy = rf_train_test_accuracy('max_features', range(1, 64), **params)\n\n\n\n\n\nplt.figure(figsize=(10,5))\nplt.plot(list(range(1, 64)), 1-np.array(train_accuracy), label='Train error')\nplt.plot(list(range(1, 64)), 1-np.array(test_accuracy), label='Test error')\nplt.xlabel('Max features to consider for split')\nplt.ylabel('Classification error (1 - accuracy).')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nTypically something between log2(k) and sqrt(k) will work as a max_features."
  },
  {
    "objectID": "projects/unet_with_wiener_filter/unet_weiner_filter.html",
    "href": "projects/unet_with_wiener_filter/unet_weiner_filter.html",
    "title": "Wiener Filter + UNet",
    "section": "",
    "text": "# Import some libraries\n\nimport numpy as np\nfrom skimage import color, data, restoration\nimport matplotlib.pyplot as plt\nimport torch\nimport utils\nimport torch.nn as nn\nfrom networks import UNet\nimport math\nimport os\nfrom skimage import io\nimport skimage\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\ndef show_images(im1, im1_title, im2, im2_title, im3, im3_title, font):\n    fig, (image1, image2, image3) = plt.subplots(1, 3, figsize=(15, 50))\n    image1.imshow(im1, cmap='gray')\n    image1.set_title(im1_title, fontsize=font)\n    image1.set_axis_off()\n    image2.imshow(im2, cmap='gray')\n    image2.set_title(im2_title, fontsize=font)\n    image2.set_axis_off()\n    image3.imshow(im3, cmap='gray')\n    image3.set_title(im3_title, fontsize=font)\n    image3.set_axis_off()\n    fig.subplots_adjust(wspace=0.02, hspace=0.2,\n                                top=0.9, bottom=0.05, left=0, right=1)\n    fig.show()\n\n\nLoad the data\n\n#Load the target image\nimage = io.imread('./image.tif')\n\n#Load the blurred and distorted images\nblurred = io.imread('./blurred.tif')\ndistorted = io.imread('./distorted.tif')\n\n#Load the kernel\npsf = io.imread('./PSF.tif')\n\n\nshow_images(image, 'Original image', blurred, 'Blurred image',\\\n           distorted, 'Blurred and noisy image', font=18)\n\n\n\n\n\n\n\n\nWe know, that the solution is described as follows:\n\\(\\hat{\\mathbf{x}} = \\arg\\min_\\mathbf{x}\\underbrace{\\frac{1}{2}\\|\\mathbf{y}-\\mathbf{K} \\mathbf{x}\\|_{2}^{2}+\\lambda r(\\mathbf{x})}_{\\mathbf{J}(\\mathbf{x})}\\), where \\(\\mathbf{J}\\) is the objective function.\nAccording to the gradient descent iterative scheme,\n\\(\\hat{\\mathbf{x}}_{k+1}=\\hat{\\mathbf{x}}_{k}-\\beta \\nabla \\mathbf{J}(\\mathbf{x})\\).\nSolution is described with the iterative gradient descent equation:\n\\(\\hat{\\mathbf{x}}_{k+1} = \\hat{\\mathbf{x}}_{k} - \\beta\\left[\\mathbf{K}^\\top(\\mathbf{K}\\hat{\\mathbf{x}}_{k} - \\mathbf{y}) + e^\\alpha f^{CNN}(\\hat{\\mathbf{x}}_{k})\\right]\\), and here \\(\\lambda = e^\\alpha\\) and \\(r(\\mathbf{x}) = f^{CNN}(\\hat{\\mathbf{x}})\\).\n\n# Anscombe transform to transform Poissonian data into Gaussian\n#https://en.wikipedia.org/wiki/Anscombe_transform\n\ndef anscombe(x):\n    '''\n    Compute the anscombe variance stabilizing transform.\n      the input   x   is noisy Poisson-distributed data\n      the output  fx  has variance approximately equal to 1.\n    Reference: Anscombe, F. J. (1948), \"The transformation of Poisson,\n    binomial and negative-binomial data\", Biometrika 35 (3-4): 246-254\n    '''\n    return 2.0*torch.sqrt(x + 3.0/8.0)\n\n# Exact unbiased Anscombe transform to transform Gaussian data back into Poissonian\ndef exact_unbiased(z):\n    return (1.0 / 4.0 * z.pow(2) +\n            (1.0/4.0) * math.sqrt(3.0/2.0) * z.pow(-1) -\n            (11.0/8.0) * z.pow(-2) +\n            (5.0/8.0) * math.sqrt(3.0/2.0) * z.pow(-3) - (1.0 / 8.0))\n\nclass WienerUNet(torch.nn.Module):\n\n    def __init__(self):\n        '''\n        Deconvolution function for a batch of images. Although the regularization\n        term does not have a shape of Tikhonov regularizer, with a slight abuse of notations\n        the function is called WienerUNet.\n\n        The function is built upon the iterative gradient descent scheme:\n\n        x_k+1 = x_k - lamb[K^T(Kx_k - y) + exp(alpha)*reg(x_k)]\n\n        Initial parameters are:\n        regularizer: a neural network to parametrize the prior on each iteration x_k.\n        alpha: power of the trade-off coefficient.\n        lamb: step of the gradient descent algorithm.\n        '''\n        super(WienerUNet, self).__init__()\n        self.regularizer = UNet(mode='instance')\n        self.alpha = nn.Parameter(torch.FloatTensor([0.0]))\n        self.lamb = nn.Parameter(torch.FloatTensor([0.3]))\n\n    def forward(self, x, y, ker):\n        '''\n        Function that performs one iteration of the gradient descent scheme of the deconvolution algorithm.\n\n        :param x: (torch.(cuda.)Tensor) Image, restored with the previous iteration of the gradient descent scheme, B x C x H x W\n        :param y: (torch.(cuda.)Tensor) Input blurred and noisy image, B x C x H x W\n        :param ker: (torch.(cuda.)Tensor) Blurring kernel, B x C x H_k x W_k\n        :return: (torch.(cuda.)Tensor) Restored image, B x C x H x W\n        '''\n        \n        #Calculate Kx_k\n        x_filtered = utils.imfilter2D_SpatialDomain(x, ker, padType='symmetric', mode=\"conv\")\n        Kx_y = x_filtered - y\n\n        #Calculate K^T(Kx_k - y)\n        y_filtered = utils.imfilter_transpose2D_SpatialDomain(Kx_y, ker,\n                                                              padType='symmetric', mode=\"conv\")\n        \n        #Calculate exp(alpha)*reg(x_k)\n        regul = torch.exp(self.alpha) * self.regularizer(x)\n\n        brackets = y_filtered + regul\n        out = x - self.lamb * brackets\n\n        return out\n    \nclass WienerFilter_UNet(nn.Module):\n    '''\n    Module that uses UNet to predict individual gradient of a regularizer for each input image and then\n    applies gradient descent scheme with predicted gradient of a regularizers per-image.\n    '''\n    def __init__(self):\n\n        super(WienerFilter_UNet, self).__init__()\n        self.function = WienerUNet()\n    \n    #Perform gradient descent iterations\n    def forward(self, y, ker, n_iter):\n        output = y.clone()\n\n        for i in range(n_iter):\n            output = self.function(output, y, ker)\n\n        return output\n\n\n#Let's transform our numpy data into pytorch data\nx = torch.Tensor(distorted[None, None])\nker = torch.Tensor(psf[None, None])\n\n#Define the model\nmodel = WienerFilter_UNet()\n\n#Load the pretrained weights\nstate_dict = torch.load(os.path.join('./', 'WF_UNet_poisson'))\nstate_dict = state_dict['model_state_dict']\nfrom collections import OrderedDict\nnew_state_dict = OrderedDict()\nfor k, v in state_dict.items():\n    name = k[7:]  # remove `module.`\n    new_state_dict[name] = v\n# load params\nmodel.load_state_dict(new_state_dict)\nmodel.eval()\n\nWienerFilter_UNet(\n  (function): WienerUNet(\n    (regularizer): UNet(\n      (inc): inconv(\n        (conv): double_conv(\n          (conv): Sequential(\n            (0): Conv2d(1, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (1): InstanceNorm2d(12, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n            (2): ReLU(inplace=True)\n            (3): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (4): InstanceNorm2d(12, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n            (5): ReLU(inplace=True)\n          )\n        )\n      )\n      (down1): down(\n        (mpconv): Sequential(\n          (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n          (1): double_conv(\n            (conv): Sequential(\n              (0): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (1): InstanceNorm2d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n              (2): ReLU(inplace=True)\n              (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (4): InstanceNorm2d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n              (5): ReLU(inplace=True)\n            )\n          )\n        )\n      )\n      (down2): down(\n        (mpconv): Sequential(\n          (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n          (1): double_conv(\n            (conv): Sequential(\n              (0): Conv2d(24, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (1): InstanceNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n              (2): ReLU(inplace=True)\n              (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (4): InstanceNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n              (5): ReLU(inplace=True)\n            )\n          )\n        )\n      )\n      (down3): down(\n        (mpconv): Sequential(\n          (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n          (1): double_conv(\n            (conv): Sequential(\n              (0): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (1): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n              (2): ReLU(inplace=True)\n              (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (4): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n              (5): ReLU(inplace=True)\n            )\n          )\n        )\n      )\n      (down4): down(\n        (mpconv): Sequential(\n          (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n          (1): double_conv(\n            (conv): Sequential(\n              (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (1): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n              (2): ReLU(inplace=True)\n              (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n              (4): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n              (5): ReLU(inplace=True)\n            )\n          )\n        )\n      )\n      (up1): up(\n        (up): Upsample(scale_factor=2.0, mode=bilinear)\n        (conv): double_conv(\n          (conv): Sequential(\n            (0): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (1): InstanceNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n            (2): ReLU(inplace=True)\n            (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (4): InstanceNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n            (5): ReLU(inplace=True)\n          )\n        )\n      )\n      (up2): up(\n        (up): Upsample(scale_factor=2.0, mode=bilinear)\n        (conv): double_conv(\n          (conv): Sequential(\n            (0): Conv2d(96, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (1): InstanceNorm2d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n            (2): ReLU(inplace=True)\n            (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (4): InstanceNorm2d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n            (5): ReLU(inplace=True)\n          )\n        )\n      )\n      (up3): up(\n        (up): Upsample(scale_factor=2.0, mode=bilinear)\n        (conv): double_conv(\n          (conv): Sequential(\n            (0): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (1): InstanceNorm2d(12, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n            (2): ReLU(inplace=True)\n            (3): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (4): InstanceNorm2d(12, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n            (5): ReLU(inplace=True)\n          )\n        )\n      )\n      (up4): up(\n        (up): Upsample(scale_factor=2.0, mode=bilinear)\n        (conv): double_conv(\n          (conv): Sequential(\n            (0): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (1): InstanceNorm2d(12, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n            (2): ReLU(inplace=True)\n            (3): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n            (4): InstanceNorm2d(12, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n            (5): ReLU(inplace=True)\n          )\n        )\n      )\n      (outc): outconv(\n        (conv): Conv2d(12, 1, kernel_size=(1, 1), stride=(1, 1))\n      )\n    )\n  )\n)\n\n\n\n#Perform Anscombe transform\nx = anscombe(x)\n\n#Calculate output\nout = model(x, ker, 10)\n\n#Perform inverse Anscombe transform\nout = exact_unbiased(out)\n\n#Some post-processing of data\nout = out/image.max()\nimage = image/image.max()\n\n\nshow_images(image, 'Original image', distorted, 'Blurred image',\\\n           out[0][0].detach().cpu().numpy().clip(0,1), 'Restored with WF-UNet', font=18)"
  },
  {
    "objectID": "projects/Graph_neural_network'.html",
    "href": "projects/Graph_neural_network'.html",
    "title": "Graph Neural Networks (GNN)",
    "section": "",
    "text": "Sequential data: RNN and Transformers\nImages, video, audio: CNN, MLP, Transformers\nWhat about graphs?"
  },
  {
    "objectID": "projects/Graph_neural_network'.html#data-types",
    "href": "projects/Graph_neural_network'.html#data-types",
    "title": "Graph Neural Networks (GNN)",
    "section": "",
    "text": "Sequential data: RNN and Transformers\nImages, video, audio: CNN, MLP, Transformers\nWhat about graphs?"
  },
  {
    "objectID": "projects/Graph_neural_network'.html#problems-related-to-graphs",
    "href": "projects/Graph_neural_network'.html#problems-related-to-graphs",
    "title": "Graph Neural Networks (GNN)",
    "section": "Problems related to graphs",
    "text": "Problems related to graphs\n\nGraph Classification or Regression ‚Äì applications are chemistry, social networks, and text classification\nNode Classification ‚Äì chemistry, image segmentation, social network analysis\nLink Prediction ‚Äì customer-supplier network analysis, social networks (again!), and city planning\nCommunity Detection ‚Äì local networks analysis, topic modeling\nLearning Graph Embeddings: maps graphs into vectors, preserving the relevant information on nodes, edges, and structure\nGraph Generation: learns from sample graph distribution to generate a new but similar graph structure"
  },
  {
    "objectID": "projects/Graph_neural_network'.html#problems-related-to-graphs-1",
    "href": "projects/Graph_neural_network'.html#problems-related-to-graphs-1",
    "title": "Graph Neural Networks (GNN)",
    "section": "Problems related to graphs",
    "text": "Problems related to graphs\n\nGraph / Node / Edge Classification and Regression\nEdge Prediction\nCommunity Detection\nLearning Graph Embeddings\nGraph Generation\n‚Ä¶"
  },
  {
    "objectID": "projects/Graph_neural_network'.html#brief-reminder-of-main-ingredients",
    "href": "projects/Graph_neural_network'.html#brief-reminder-of-main-ingredients",
    "title": "Graph Neural Networks (GNN)",
    "section": "Brief reminder of main ingredients",
    "text": "Brief reminder of main ingredients\n\nConvolution\nMessage passing\nAttention"
  },
  {
    "objectID": "projects/Graph_neural_network'.html#convolution",
    "href": "projects/Graph_neural_network'.html#convolution",
    "title": "Graph Neural Networks (GNN)",
    "section": "Convolution",
    "text": "Convolution\n\nPyG has implementations of various versions of convolution operation\nThe main idea is to share and filter information for neighbour nodes\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n# PyTorch geometric\n! python3 -m pip install torch_geometric -q\nimport torch_geometric\nimport torch_geometric.data as geom_data\nimport torch_geometric.nn as geom_nn\n\nnode_feats = torch.arange(8, \n            dtype=torch.float32).view(4, 2)\nadj_matrix = torch.tensor([[1, 1, 0, 0], \n                           [1, 1, 1, 1], \n                           [0, 1, 1, 1], \n                           [0, 1, 1, 1]])\n\nprint(\"Node features:\\n\", node_feats)\nprint(\"\\nAdjacency matrix:\\n\", adj_matrix)\n\nNode features:\n tensor([[0., 1.],\n        [2., 3.],\n        [4., 5.],\n        [6., 7.]])\n\nAdjacency matrix:\n tensor([[1, 1, 0, 0],\n        [1, 1, 1, 1],\n        [0, 1, 1, 1],\n        [0, 1, 1, 1]])\n\n\n\nDummy Convolution\n\n# PyG takes the adjacency matrix as a list of edges\nedge_list = torch.concat(torch.where(\n    adj_matrix == 1), axis=-1).reshape(2, -1)\nprint(edge_list)\n\ntensor([[0, 0, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3],\n        [0, 1, 0, 1, 2, 3, 1, 2, 3, 1, 2, 3]])\n\n\n\n# Check result of simple convolution with sum aggregation\nsimple_conv = geom_nn.SimpleConv()\nprint(simple_conv(node_feats, edge_list))\n\ntensor([[ 2.,  4.],\n        [12., 16.],\n        [12., 15.],\n        [12., 15.]])\n\n\n\nprint(adj_matrix.float() @ node_feats)\n\ntensor([[ 2.,  4.],\n        [12., 16.],\n        [12., 15.],\n        [12., 15.]])\n\n\n\n\nGCN convolution\ndocs\n\ngcn = geom_nn.GCNConv(2, 8, bias=False, \n                      normalize=True)\nprint(gcn(node_feats, edge_list))\n-\n\ntensor([[ 0.3723, -1.4965, -1.4036, -0.8653,  0.7523, -0.1361, -0.5825, -0.1124],\n        [ 0.8889, -4.9455, -4.7558, -2.3232,  2.1204, -0.8527, -2.4710,  0.0792],\n        [ 0.9008, -5.3986, -5.2154, -2.4269,  2.2402, -1.0128, -2.8084,  0.1782],\n        [ 0.9008, -5.3986, -5.2154, -2.4269,  2.2402, -1.0128, -2.8084,  0.1782]],\n       grad_fn=&lt;ScatterAddBackward0&gt;)\nParameter containing:\ntensor([[-0.1523,  0.3075],\n        [-0.4295, -0.7643],\n        [-0.4919, -0.6765],\n        [ 0.1588, -0.6264],\n        [-0.0618,  0.5101],\n        [-0.3449,  0.0690],\n        [-0.5815, -0.1098],\n        [ 0.3099, -0.2125]], requires_grad=True)\n\n\n\ninv_root_D = torch.diag(1. / torch.sqrt(torch.tensor([2, 4, 3, 3])))\ninv_root_D @ adj_matrix.float() @ inv_root_D @ node_feats @ gcn.lin.weight.T\n\ntensor([[ 0.3723, -1.4965, -1.4036, -0.8653,  0.7523, -0.1361, -0.5825, -0.1124],\n        [ 0.8889, -4.9455, -4.7558, -2.3232,  2.1204, -0.8527, -2.4710,  0.0792],\n        [ 0.9008, -5.3986, -5.2154, -2.4269,  2.2402, -1.0128, -2.8084,  0.1782],\n        [ 0.9008, -5.3986, -5.2154, -2.4269,  2.2402, -1.0128, -2.8084,  0.1782]],\n       grad_fn=&lt;MmBackward0&gt;)\n\n\n\n\nChebNet\ndocs, pdf, Chebyshev polynomials wiki\n\ngcn = geom_nn.ChebConv(2, 8, K=5)\nprint(gcn(node_feats, edge_list))\ngcn.lins\n\ntensor([[ 3.2364,  3.4143, -0.3946, -4.3242, -3.5987,  3.2346, -4.2101, -1.2371],\n        [-3.4432, -2.2127, -3.2632, -8.8110, -3.5644,  5.0098,  0.9048,  0.0408],\n        [-6.2562, -5.4241, -5.4977, -7.7434,  0.3508,  2.7667,  6.6166,  1.1335],\n        [-3.6543, -4.4635, -5.9787, -5.9360,  3.6287,  1.4175,  7.0212, -0.2499]],\n       grad_fn=&lt;AddBackward0&gt;)\n\n\nModuleList(\n  (0): Linear(2, 8, bias=False)\n  (1): Linear(2, 8, bias=False)\n  (2): Linear(2, 8, bias=False)\n  (3): Linear(2, 8, bias=False)\n  (4): Linear(2, 8, bias=False)\n)"
  },
  {
    "objectID": "projects/Graph_neural_network'.html#check-convolutions-for-node-classification",
    "href": "projects/Graph_neural_network'.html#check-convolutions-for-node-classification",
    "title": "Graph Neural Networks (GNN)",
    "section": "Check convolutions for node classification",
    "text": "Check convolutions for node classification\n\nfrom torch_geometric.datasets import Planetoid\nfrom torch_geometric.transforms import NormalizeFeatures\n\ndataset = Planetoid(root='data/Planetoid', name='Cora')\nprint('Number of graphs: {}'.format(len(dataset)))\nprint('Number of features: {}'.format(dataset.num_features))\nprint('Number of classes: {}'.format(dataset.num_classes))\n\ndata = dataset[0]  # Get the first graph object.\nprint(data)\nprint(data.x[0], data.y[0].item())\n\nNumber of graphs: 1\nNumber of features: 1433\nNumber of classes: 7\nData(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\ntensor([0., 0., 0.,  ..., 0., 0., 0.]) 3\n\n\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\n\ndef visualize(h, color):\n    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n\n    plt.figure(figsize=(10,10))\n    plt.xticks([])\n    plt.yticks([])\n\n    plt.scatter(z[:, 0], z[:, 1], s=70, c=color)\n    plt.show()\n\n\nclass GCN(torch.nn.Module):\n    def __init__(self, hidden_channels):\n        super().__init__()\n        torch.manual_seed(2128506)\n        self.conv1 = geom_nn.GCNConv(dataset.num_features, hidden_channels)\n        self.conv2 = geom_nn.GCNConv(hidden_channels, dataset.num_classes)\n        self.drop = torch.nn.Dropout(0.5)\n\n    def forward(self, x, edge_index):\n        x = self.conv1(x, edge_index)\n        x = x.relu()\n        x = self.drop(x)\n        x = self.conv2(x, edge_index)\n        return x\n\ngcn_model = GCN(hidden_channels=32)\nprint(gcn_model)\n\nGCN(\n  (conv1): GCNConv(1433, 32)\n  (conv2): GCNConv(32, 7)\n  (drop): Dropout(p=0.5, inplace=False)\n)\n\n\n\ngcn_model.eval()\nout = gcn_model(data.x, data.edge_index)\nvisualize(out, color=data.y)\n\n\n\n\n\n\n\n\n\ncriterion = torch.nn.CrossEntropyLoss()\n\ndef test(model, data):\n    model.eval()\n    out = model(data.x, data.edge_index)\n    pred = out.argmax(dim=1)\n    test_correct = pred[data.test_mask] == data.y[data.test_mask]\n    test_acc = int(test_correct.sum()) / int(data.test_mask.sum())\n    return test_acc\n\ndef train(model, data, optimizer):\n    model.train()\n    optimizer.zero_grad()\n    out = model(data.x, data.edge_index)\n    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\n    return loss\n\n\noptimizer = torch.optim.Adam(gcn_model.parameters(), lr=0.01)\ngcn_test_accuracy = []\n\nfor epoch in range(1, 21):\n    loss = train(gcn_model, data, optimizer)\n    gcn_test_accuracy.append(test(gcn_model, data))\n    print(f'Epoch: {epoch:},\\t Loss: {loss:.4f}')\n\nEpoch: 1,    Loss: 1.9275\nEpoch: 2,    Loss: 1.7472\nEpoch: 3,    Loss: 1.5225\nEpoch: 4,    Loss: 1.3219\nEpoch: 5,    Loss: 1.1181\nEpoch: 6,    Loss: 0.9258\nEpoch: 7,    Loss: 0.7446\nEpoch: 8,    Loss: 0.6071\nEpoch: 9,    Loss: 0.4898\nEpoch: 10,   Loss: 0.4167\nEpoch: 11,   Loss: 0.3402\nEpoch: 12,   Loss: 0.2981\nEpoch: 13,   Loss: 0.2287\nEpoch: 14,   Loss: 0.1905\nEpoch: 15,   Loss: 0.1524\nEpoch: 16,   Loss: 0.1184\nEpoch: 17,   Loss: 0.1126\nEpoch: 18,   Loss: 0.1082\nEpoch: 19,   Loss: 0.0678\nEpoch: 20,   Loss: 0.0586\n\n\n\ntest_acc = test(gcn_model, data)\nprint(f'Test Accuracy: {test_acc:.4f}')\n\nTest Accuracy: 0.7800\n\n\n\ngcn_model.eval()\nout = gcn_model(data.x, data.edge_index)\nvisualize(out, color=data.y)\n\n\n\n\n\n\n\n\n\nplt.plot(gcn_test_accuracy)\n\n\n\n\n\n\n\n\n\nclass GChebCN(torch.nn.Module):\n    def __init__(self, hidden_channels, K):\n        super().__init__()\n        torch.manual_seed(2128506)\n        self.conv1 = geom_nn.ChebConv(dataset.num_features, hidden_channels, K)\n        self.conv2 = geom_nn.ChebConv(hidden_channels, dataset.num_classes, K)\n        self.drop = torch.nn.Dropout(0.5)\n\n    def forward(self, x, edge_index):\n        x = self.conv1(x, edge_index)\n        x = x.relu()\n        x = self.drop(x)\n        x = self.conv2(x, edge_index)\n        return x\n\n\ncheb_gcn = GChebCN(32, K=3)\noptimizer = torch.optim.Adam(cheb_gcn.parameters(), lr=0.01)\nchebcn_test_accuracy = []\nfor epoch in range(1, 21):\n    loss = train(cheb_gcn, data, optimizer)\n    chebcn_test_accuracy.append(test(cheb_gcn, data))\n    print(f'Epoch: {epoch},\\t Loss: {loss:.4f}')\n\nEpoch: 1,    Loss: 2.0385\nEpoch: 2,    Loss: 1.3648\nEpoch: 3,    Loss: 0.7926\nEpoch: 4,    Loss: 0.4739\nEpoch: 5,    Loss: 0.2473\nEpoch: 6,    Loss: 0.1637\nEpoch: 7,    Loss: 0.0903\nEpoch: 8,    Loss: 0.0440\nEpoch: 9,    Loss: 0.0396\nEpoch: 10,   Loss: 0.0246\nEpoch: 11,   Loss: 0.0341\nEpoch: 12,   Loss: 0.0168\nEpoch: 13,   Loss: 0.0097\nEpoch: 14,   Loss: 0.0062\nEpoch: 15,   Loss: 0.0072\nEpoch: 16,   Loss: 0.0213\nEpoch: 17,   Loss: 0.0035\nEpoch: 18,   Loss: 0.0043\nEpoch: 19,   Loss: 0.0070\nEpoch: 20,   Loss: 0.0008\n\n\n\ntest_acc = test(cheb_gcn, data)\nprint(f'Test Accuracy: {test_acc:.4f}')\n\nTest Accuracy: 0.7750\n\n\n\ncheb_gcn.eval()\nout = cheb_gcn(data.x, data.edge_index)\nvisualize(out, color=data.y)\n\n\n\n\n\n\n\n\n\nplt.plot(gcn_test_accuracy, label=\"GCN\")\nplt.plot(chebcn_test_accuracy, label=\"Cheb CN\")\nplt.legend()\nplt.ylabel(\"Test accuracy\")\nplt.xlabel(\"# epoch\")\n\nText(0.5, 0, '# epoch')"
  },
  {
    "objectID": "projects/Graph_neural_network'.html#graph-attention",
    "href": "projects/Graph_neural_network'.html#graph-attention",
    "title": "Graph Neural Networks (GNN)",
    "section": "Graph attention",
    "text": "Graph attention\ndocs\n\nWe again want to update node representation from the neighbours representations\nFor every two nodes we can compute \\[ e_{ij} = a(Wh_i, Wh_j),\\]\n\nwhere \\(a\\) is some predefined function and \\(W\\) is learnable matrix\n\nThen we can normalize them only with neighbours with softmax operation\n\n\\[ \\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k \\in N_i} \\exp(e_{ik})} \\]\n\nNow we can update representation \\(h_i\\) as follows\n\n\\[ h_i = \\sigma\\left( \\sum_{j \\in N_i} \\alpha_{ij} W h_j \\right) \\]\n\nExample of the function \\(a\\) is simple linear map that reduces concatented input to scalar\n\n\\[ a(x, y) = u^\\top [x, y],\\]\nwhere \\(u \\in \\mathbb{R}^{dim(x) + dim(y)}\\) is a learnable vector\n\nclass GAT(torch.nn.Module):\n    def __init__(self, hidden_channels, heads):\n        super().__init__()\n        torch.manual_seed(2128506)\n        self.conv1 = geom_nn.GATConv(dataset.num_features, hidden_channels,heads)\n        self.conv2 = geom_nn.GATConv(heads * hidden_channels, dataset.num_classes,heads)\n        self.drop = torch.nn.Dropout(0.5)\n        self.relu = nn.ReLU()\n\n    def forward(self, x, edge_index):\n        x = self.drop(x)\n        x = self.conv1(x, edge_index)\n        x = self.relu(x)\n        x = self.drop(x)\n        x = self.conv2(x, edge_index)\n        return x\n\nmodel = GAT(hidden_channels=32, heads=8)\nprint(model)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\ngat_test_accuracy = []\nfor epoch in range(1, 21):\n    loss = train(model, data, optimizer)\n    gat_test_accuracy.append(test(model, data))\n    print(f'Epoch: {epoch:},\\t Loss: {loss:.4f}')\n\nGAT(\n  (conv1): GATConv(1433, 32, heads=8)\n  (conv2): GATConv(256, 7, heads=8)\n  (drop): Dropout(p=0.5, inplace=False)\n  (relu): ReLU()\n)\nEpoch: 1,    Loss: 3.9761\nEpoch: 2,    Loss: 3.0069\nEpoch: 3,    Loss: 1.8436\nEpoch: 4,    Loss: 1.0772\nEpoch: 5,    Loss: 0.6553\nEpoch: 6,    Loss: 0.4706\nEpoch: 7,    Loss: 0.2994\nEpoch: 8,    Loss: 0.2233\nEpoch: 9,    Loss: 0.1750\nEpoch: 10,   Loss: 0.1970\nEpoch: 11,   Loss: 0.1165\nEpoch: 12,   Loss: 0.1241\nEpoch: 13,   Loss: 0.0932\nEpoch: 14,   Loss: 0.1476\nEpoch: 15,   Loss: 0.1312\nEpoch: 16,   Loss: 0.1232\nEpoch: 17,   Loss: 0.0600\nEpoch: 18,   Loss: 0.1695\nEpoch: 19,   Loss: 0.1520\nEpoch: 20,   Loss: 0.0580\n\n\n\ntest_acc = test(model, data)\nprint(f'Test Accuracy: {test_acc:.4f}')\n\nTest Accuracy: 0.7200\n\n\n\nplt.plot(gcn_test_accuracy, label=\"GCN\")\nplt.plot(chebcn_test_accuracy, label=\"Cheb CN\")\nplt.plot(gat_test_accuracy, label=\"Graph attention\")\nplt.legend()\nplt.ylabel(\"Test accuracy\")\nplt.xlabel(\"# epoch\")\n\nText(0.5, 0, '# epoch')\n\n\n\n\n\n\n\n\n\n\nmodel.eval()\nout = model(data.x, data.edge_index)\nvisualize(out, color=data.y)\n\n\n\n\n\n\n\n\n\nWhat about the Edge Convolution?\ndocs\n\\[\n\\mathbf{x}_i^{(k)} = \\max_{j \\in \\mathcal{N}(i)} h_{\\mathbf{\\Theta}} \\left( \\texttt{concat}\\left( \\mathbf{x}_i^{(k-1)}, \\mathbf{x}_j^{(k-1)} - \\mathbf{x}_i^{(k-1)}\\right) \\right),\n\\] where \\(\\mathbf{x}_i\\) are edge embeddings and \\(h_{\\mathbf{\\Theta}}\\) is an MLP.\n\n\nOpen questions\n\nHow can we effectively sample graphs for training graph neural networks?\nHow to train deep graph neural networks?\nHow not to overfit?"
  },
  {
    "objectID": "projects/ObjectDetection.html",
    "href": "projects/ObjectDetection.html",
    "title": "Object detection",
    "section": "",
    "text": "!pip install ultralytics\n\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n\nCollecting ultralytics\n\n  Downloading ultralytics-8.0.61-py3-none-any.whl (490 kB)\n\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 490.6/490.6 KB 8.6 MB/s eta 0:00:00\n\nRequirement already satisfied: Pillow&gt;=7.1.2 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (8.4.0)\n\nRequirement already satisfied: torch&gt;=1.7.0 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (1.13.1+cu116)\n\nRequirement already satisfied: numpy&gt;=1.21.6 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (1.22.4)\n\nRequirement already satisfied: opencv-python&gt;=4.6.0 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (4.7.0.72)\n\nCollecting sentry-sdk\n\n  Downloading sentry_sdk-1.18.0-py2.py3-none-any.whl (194 kB)\n\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 194.8/194.8 KB 18.6 MB/s eta 0:00:00\n\nRequirement already satisfied: tqdm&gt;=4.64.0 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (4.65.0)\n\nRequirement already satisfied: pandas&gt;=1.1.4 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (1.4.4)\n\nRequirement already satisfied: requests&gt;=2.23.0 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (2.27.1)\n\nRequirement already satisfied: matplotlib&gt;=3.2.2 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (3.7.1)\n\nCollecting thop&gt;=0.1.1\n\n  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n\nRequirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from ultralytics) (5.9.4)\n\nRequirement already satisfied: torchvision&gt;=0.8.1 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (0.14.1+cu116)\n\nRequirement already satisfied: scipy&gt;=1.4.1 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (1.10.1)\n\nRequirement already satisfied: PyYAML&gt;=5.3.1 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (6.0)\n\nRequirement already satisfied: seaborn&gt;=0.11.0 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (0.12.2)\n\nRequirement already satisfied: importlib-resources&gt;=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib&gt;=3.2.2-&gt;ultralytics) (5.12.0)\n\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib&gt;=3.2.2-&gt;ultralytics) (3.0.9)\n\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib&gt;=3.2.2-&gt;ultralytics) (1.4.4)\n\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib&gt;=3.2.2-&gt;ultralytics) (0.11.0)\n\nRequirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib&gt;=3.2.2-&gt;ultralytics) (1.0.7)\n\nRequirement already satisfied: python-dateutil&gt;=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib&gt;=3.2.2-&gt;ultralytics) (2.8.2)\n\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib&gt;=3.2.2-&gt;ultralytics) (23.0)\n\nRequirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib&gt;=3.2.2-&gt;ultralytics) (4.39.3)\n\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas&gt;=1.1.4-&gt;ultralytics) (2022.7.1)\n\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.9/dist-packages (from requests&gt;=2.23.0-&gt;ultralytics) (3.4)\n\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests&gt;=2.23.0-&gt;ultralytics) (2022.12.7)\n\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests&gt;=2.23.0-&gt;ultralytics) (1.26.15)\n\nRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests&gt;=2.23.0-&gt;ultralytics) (2.0.12)\n\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch&gt;=1.7.0-&gt;ultralytics) (4.5.0)\n\nRequirement already satisfied: zipp&gt;=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources&gt;=3.2.0-&gt;matplotlib&gt;=3.2.2-&gt;ultralytics) (3.15.0)\n\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil&gt;=2.7-&gt;matplotlib&gt;=3.2.2-&gt;ultralytics) (1.16.0)\n\nInstalling collected packages: sentry-sdk, thop, ultralytics\n\nSuccessfully installed sentry-sdk-1.18.0 thop-0.1.1.post2209072238 ultralytics-8.0.61\nfrom ultralytics import YOLO\nmodel = YOLO('yolov8n.pt')\n\nDownloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to yolov8n.pt..."
  },
  {
    "objectID": "projects/ObjectDetection.html#general-pipeline",
    "href": "projects/ObjectDetection.html#general-pipeline",
    "title": "Object detection",
    "section": "General pipeline",
    "text": "General pipeline"
  },
  {
    "objectID": "projects/ObjectDetection.html#bounding-boxes-for-each-cell",
    "href": "projects/ObjectDetection.html#bounding-boxes-for-each-cell",
    "title": "Object detection",
    "section": "Bounding boxes for each cell",
    "text": "Bounding boxes for each cell"
  },
  {
    "objectID": "projects/ObjectDetection.html#how-to-define-default-boxes",
    "href": "projects/ObjectDetection.html#how-to-define-default-boxes",
    "title": "Object detection",
    "section": "How to define default boxes:",
    "text": "How to define default boxes:\n\nManually (user make it by hands)\nFrom train dataset (clusterise training boxes and take most popular)"
  },
  {
    "objectID": "projects/ObjectDetection.html#real-life-case",
    "href": "projects/ObjectDetection.html#real-life-case",
    "title": "Object detection",
    "section": "Real life case",
    "text": "Real life case"
  },
  {
    "objectID": "projects/ObjectDetection.html#how-to-choose-optimal-box",
    "href": "projects/ObjectDetection.html#how-to-choose-optimal-box",
    "title": "Object detection",
    "section": "How to choose optimal box",
    "text": "How to choose optimal box\n \n\n\n\nImage\n\n\n\nfrom PIL import Image\n\nimg = Image.open('/content/yolo8.jpg')\ndisplay(img)\n\n\n\n\n\n\n\n\n\nfrom PIL import Image\n!wget -nv \"https://www.learnopencv.com/wp-content/uploads/2021/01/person-segmentation.jpeg\"\n\n2023-04-03 18:04:24 URL:http://learnopencv.com/wp-content/uploads/2021/01/person-segmentation.jpeg [95431/95431] -&gt; \"person-segmentation.jpeg\" [1]\n\n\n\nimg = Image.open(\"/content/person-segmentation.jpeg\")\nplt.imshow(img)\nplt.axis(\"off\")\n\n\n\n\n\n\n\n\n\nresults = model('/content/person-segmentation.jpeg')\n\n\nimage 1/1 /content/person-segmentation.jpeg: 448x640 5 persons, 1 dog, 241.2ms\nSpeed: 1.1ms preprocess, 241.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n\n\n\nnames = results[0].names\n\n\nlabels = results[0].boxes.cls\nlabels_name = [names[label.item()] for label in labels]\nlabels_name\n\n['person', 'person', 'person', 'dog', 'person', 'person']\n\n\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nfig, ax = plt.subplots()\nax.imshow(img)\nfor label_name,box in zip(labels_name,results[0].boxes.xywh):\n  x,y,w,h = box\n  x = x - w/2\n  y = y - h/2\n  c = \"r\" if label_name == \"person\" else \"b\"\n  rect = plt.Rectangle((x,y), width=w, height=h, linewidth=2, edgecolor=c, facecolor='none')\n  ax.add_patch(rect)\nfor label_name,box in zip(labels_name,results[0].boxes.xywh):\n  x,y,w,h = box\n  x = x - w/2\n  y = y - h/2 - 20\n  plt.text(x,y,label_name,fontsize=12,fontweight=\"bold\")\nplt.axis(\"off\")\nplt.show()"
  },
  {
    "objectID": "projects/Attention_mechanism.html",
    "href": "projects/Attention_mechanism.html",
    "title": "Recurrent neural networks (RNN)",
    "section": "",
    "text": "#Attention_mechanismand RNN"
  },
  {
    "objectID": "projects/Attention_mechanism.html#attention",
    "href": "projects/Attention_mechanism.html#attention",
    "title": "Recurrent neural networks (RNN)",
    "section": "Attention",
    "text": "Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\nWe call our particular attention ‚ÄúScaled Dot-Product Attention‚Äù. The input consists of queries and keys of dimension \\(d_k\\), and values of dimension \\(d_v\\). We compute the dot products of the query with all keys, divide each by \\(\\sqrt{d_k}\\), and apply a softmax function to obtain the weights on the values.\n\n\n\nalt text\n\n\nIn practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix \\(Q\\). The keys and values are also packed together into matrices \\(K\\) and \\(V\\). We compute the matrix of outputs as:\n\\[\n   \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n\\]\n Source: Lena Voita‚Äôs Lecture about Seq2Seq\n\nimport torch\nfrom torch import nn\n\nclass Attention(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n        self.scale = self.dim ** -0.5\n        self.qkv = nn.Linear(dim, dim * 3)\n\n    def forward(self, x):\n        '''\n        Args:\n            x: Tensor of shape (batch_size, seq_len, input_dim)\n\n        Returns:\n            Tensor of shape (batch_size, seq_len, input_dim)\n        '''\n        # Your code is here\n        # Extract batch size, sequence length, and input dimension.\n\n        # Perform linear transformation and reshape for queries, keys, and values.\n\n        # Unbind into separate queries, keys, and values.\n\n        # Scale the queries.\n\n        # Compute attention scores.\n\n        # Apply softmax to obtain attention weights.\n\n        # Compute weighted sum of values using attention weights.\n\n        return x\n\n\nx = torch.ones(11, 12, 8)\nassert Attention(8)(x).shape == x.shape"
  },
  {
    "objectID": "projects/Attention_mechanism.html#patches-crafting",
    "href": "projects/Attention_mechanism.html#patches-crafting",
    "title": "Recurrent neural networks (RNN)",
    "section": "Patches crafting",
    "text": "Patches crafting\n\n! python3 -m pip install einops -q\nfrom einops import rearrange\n\ndef img2patches(img, patch_size=8):\n    '''\n    Args:\n        img: (batch_size, c, h, w) Tensor\n\n    Returns:\n        (batch_size, num_patches, vectorized_patch) Tensor\n    '''\n    # Your code is here\n    # Rearrange the image tensor to extract patches\n\n    return rearrange(img, 'batch_size c (h ph) (w pw) -&gt; batch_size (h w) (c ph pw)',\n                     ph=patch_size, pw=patch_size)\n\n\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0.0/44.6 kB ? eta -:--:--\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∏‚îÅ‚îÅ‚îÅ 41.0/44.6 kB 1.5 MB/s eta 0:00:01\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44.6/44.6 kB 898.4 kB/s eta 0:00:00"
  },
  {
    "objectID": "projects/Attention_mechanism.html#build-vit",
    "href": "projects/Attention_mechanism.html#build-vit",
    "title": "Recurrent neural networks (RNN)",
    "section": "Build ViT",
    "text": "Build ViT\n\n\nSplit an image into patches\nFlatten the patches\nProduce lower-dimensional linear embeddings from the flattened patches\nAdd positional embeddings\nFeed the sequence as an input to a standard transformer encoder\nPretrain the model with image labels (fully supervised on a huge dataset)\nFinetune on the downstream dataset for image classification\n\n\nclass Block(nn.Module):\n\n    def __init__(\n            self,\n            dim,\n            num_heads,\n            mlp_ratio=4,  # ratio between hidden_dim and input_dim in MLP\n            act_layer=nn.GELU,\n            norm_layer=nn.LayerNorm\n    ):\n        super().__init__()\n        # Your code is here\n        # Initialize instance variables.\n\n        # Layer normalization before the attention mechanism.\n\n        # Multi-head self-attention mechanism.\n\n        # Layer normalization after the attention mechanism.\n\n        # Define the MLP (Multi-Layer Perceptron):\n          # Linear transformation with dim * mlp_ratio output features.\n          # Activation function.\n          # Linear transformation to map back to the original dimension.\n\n    def forward(self, x):\n        # Your code is here\n\n        # Add the output of attention mechanism to the input (with normalization).\n\n        # Add the output of MLP to the input (with normalization).\n        return x\n\n\ndepth = 12\nmany_layers = nn.Sequential(*[Block(128, 8) for _ in range(depth)])\n\n\n\nCLS token: an extra learnable token\nPosition embeddings: x = x + pos_embedding, where pos_embedding is trained for every element is a sequence\n\n\nclass ViT(nn.Module):\n    def __init__(\n                    self,\n                    img_size=(224, 224),\n                    patch_size=16,\n                    in_chans=3,\n                    num_classes=10,\n                    embed_dim=768,\n                    depth=12,\n                    num_heads=12,\n                    mlp_ratio=4,\n                    class_token=True,\n                    norm_layer=nn.LayerNorm,\n                    act_layer=nn.GELU\n            ):\n        # Your code is here\n\n        # Initialize instance variables.\n\n        # Size of patches used for tokenization.\n\n        # Sequential container for the Transformer blocks.\n\n        # Projection layer for patches.\n\n        # Length of positional embeddings.\n\n        # Learnable token for classification.\n\n        # Linear layer for classification.\n\n\n        pass\n\n    def forward(self, x):\n        '''\n        Args:\n            x: (batch_size, in_channels, img_size[0], img_size[1])\n\n        Return:\n            (batch_size, num_classes) probabilities\n        '''\n        # Your code here\n\n        # Convert input image into patches.\n\n        # Project patches into the embedding space.\n\n        # Add positional embeddings.\n\n        # Add classification token.\n\n        # Pass through Transformer blocks.\n\n        # Extract only the CLS token.\n\n        # Pass the CLS token through the classification layer.\n\n\n        pass\n\n\nViT()(torch.ones(5, 3, 224, 224)).shape\n\nhttps://github.com/lucidrains/vit-pytorch\n\n!pip install vit-pytorch\n\n\nCollecting vit-pytorch\n\n  Downloading vit_pytorch-1.6.5-py3-none-any.whl (100 kB)\n\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 100.3/100.3 kB 1.7 MB/s eta 0:00:00\n\nCollecting einops&gt;=0.7.0 (from vit-pytorch)\n\n  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44.6/44.6 kB 2.9 MB/s eta 0:00:00\n\nRequirement already satisfied: torch&gt;=1.10 in /usr/local/lib/python3.10/dist-packages (from vit-pytorch) (2.2.1+cu121)\n\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from vit-pytorch) (0.17.1+cu121)\n\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10-&gt;vit-pytorch) (3.13.3)\n\nRequirement already satisfied: typing-extensions&gt;=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10-&gt;vit-pytorch) (4.10.0)\n\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10-&gt;vit-pytorch) (1.12)\n\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10-&gt;vit-pytorch) (3.2.1)\n\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10-&gt;vit-pytorch) (3.1.3)\n\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10-&gt;vit-pytorch) (2023.6.0)\n\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch&gt;=1.10-&gt;vit-pytorch)\n\n  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch&gt;=1.10-&gt;vit-pytorch)\n\n  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch&gt;=1.10-&gt;vit-pytorch)\n\n  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\nCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch&gt;=1.10-&gt;vit-pytorch)\n\n  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch&gt;=1.10-&gt;vit-pytorch)\n\n  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch&gt;=1.10-&gt;vit-pytorch)\n\n  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch&gt;=1.10-&gt;vit-pytorch)\n\n  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch&gt;=1.10-&gt;vit-pytorch)\n\n  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch&gt;=1.10-&gt;vit-pytorch)\n\n  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\nCollecting nvidia-nccl-cu12==2.19.3 (from torch&gt;=1.10-&gt;vit-pytorch)\n\n  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch&gt;=1.10-&gt;vit-pytorch)\n\n  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\nRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10-&gt;vit-pytorch) (2.2.0)\n\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107-&gt;torch&gt;=1.10-&gt;vit-pytorch)\n\n  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision-&gt;vit-pytorch) (1.25.2)\n\nRequirement already satisfied: pillow!=8.3.*,&gt;=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision-&gt;vit-pytorch) (9.4.0)\n\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;torch&gt;=1.10-&gt;vit-pytorch) (2.1.5)\n\nRequirement already satisfied: mpmath&gt;=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-&gt;torch&gt;=1.10-&gt;vit-pytorch) (1.3.0)\n\nInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, einops, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, vit-pytorch\n\nSuccessfully installed einops-0.7.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 vit-pytorch-1.6.5\n\n\n\n\n\nimport torch\nfrom vit_pytorch import ViT\n\nv = ViT(\n    image_size = 256,\n    patch_size = 32,\n    num_classes = 1000,\n    dim = 1024,\n    depth = 6,\n    heads = 16,\n    mlp_dim = 2048,\n    dropout = 0.1,\n    emb_dropout = 0.1\n)\n\nimg = torch.randn(1, 3, 256, 256)\n\npreds = v(img) # (1, 1000)"
  },
  {
    "objectID": "projects/Seminar_VAE.html",
    "href": "projects/Seminar_VAE.html",
    "title": "Addisu Amare",
    "section": "",
    "text": "#Variational Autoencoders\nAuthor: Addisu Amare\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nimport torch.utils.data as data\n\nfrom typing import Tuple\nfrom collections import defaultdict\nassert torch.cuda.is_available()\nDEVICE = 'cuda'\nUSE_CUDA = True"
  },
  {
    "objectID": "projects/Seminar_VAE.html#variational-inference",
    "href": "projects/Seminar_VAE.html#variational-inference",
    "title": "Addisu Amare",
    "section": "1. Variational inference",
    "text": "1. Variational inference\n\n1.1 Problem statement\n\\(\\textbf{Data}\\): $X = {x_{1},‚Ä¶,x_{n}} $ are independent samples \\(D\\)-dimensional samples \\(\\textbf{Task}\\): We solve the task of generative modeling \\(p(X|\\Theta)\\), where \\(\\Theta\\) are parameters of the model.\nWhile we use this model, one can set optimization problem like \\(\\textbf{MLE-problem}\\):\n\\[ \\Theta^{*} = \\arg \\max_{\\Theta}  p(X|\\Theta)\\]\nHowever, one can consider our model like model with \\(d\\)-dimensional latent (hidden) variables \\(Z\\). Thus, having assumed existence of latent codes of our model, we can represent of likelihood as follows:\n\\[ p(X|\\Theta)  = p(X|Z,\\Theta) p(Z|\\Theta) \\]\n\n\\(\\textbf{Importantly}\\), we do this step because we sure, that introduced models might be easily parameterized. For example, like normal distributions:\n\n\\(p(X|Z,\\Theta) = \\mathcal{N}(X| \\mu(Z,\\Theta), \\Sigma^{-1}(Z, \\Theta))\\)\n\\(p(Z|\\Theta) = \\mathcal{N}(Z| \\mu(\\Theta), \\Sigma^{-1}(\\Theta))\\)\n\nThen, our \\(\\textbf{MLE}\\)-problem is the following:\n\\[ \\Theta^{*} = \\arg\\max_{\\Theta}\\prod_{i=1}^{n} p(x_{i}|\\Theta)=  \\arg\\max_{\\Theta}\\prod_{i=1}^{n} \\int_{\\mathbb{R}^{d}} p(x_{i}|Z, \\Theta)p(Z|\\Theta)dZ \\]\n\n\n1.2 Naive approach\nUndoubtedly, this problem might be solved through Monte-Carlo simulation:\n\\[ \\int p(x_{i}|Z,\\Theta)p(Z|\\Theta) dZ = \\mathbb{E}_{\\hat{Z} \\sim p(Z|\\Theta)} p(x_{i}|\\hat{Z},\\Theta) = \\frac{1}{K}\\sum_{k=1}^{K} p(x_{i}|\\hat{Z}_{k},\\Theta) \\]\n\n\\(\\textbf{Challenge}\\): The curse of dimensionality. Namely, we cannot properly cover whole space of \\(p(Z|\\Theta)\\) due to high dimension of the latent code. One can avoid this problem with sampleing more samples, however this amount will grow with increasing of dimensionality of latent code. Thus, to cover the space properly., the number of samples grows exponentially with respect to dimensionality of \\(Z\\). Finally, we cannot make accurate estimation for the integral.\n\\(\\textbf{Another explanation}\\) ‚Ä¶\n\n\n1.3 EM-algorithm\nSince:\n\\[ p(x_{i}|\\Theta) = \\int_{\\mathbb{R}^{d}}p(x_{i}|Z,\\Theta)p(Z|\\Theta)dZ \\]\nThen, we can consider this problem like problem for \\(\\textbf{EM}\\)-algorithm. As far as we know^ \\(\\textbf{E}\\)-step might be solved by two ways:\n\nAccurate Bayesian inference\nMean-field approximation\n\nIn \\(\\textbf{E}\\)-step, we have the following tractable integral that we cannot calculate and as a consequence of that we cannot perform Accurate Bayesian inference.\n\\[ q(Z|x_{i},\\Theta) = \\frac{p(x_{i}|Z,\\Theta) p(Z|\\Theta)}{\\int_{\\mathbb{R}^{d}}p(x_{i}|Z,\\Theta) p(Z|\\Theta)dZ}\\]\nMean-field is sufficnetly difficult for high dimensional latent codes.\n\n\n1.4 Variational Inference\nThen, we move on the Variational Inference:\n\\[ \\sum_{i=1}^{n} \\log p(x_{i}|\\Theta) = \\sum_{i=1}^{n} \\log p(x_{i}|\\Theta) \\int_{\\mathbb{R}^{d}}q(Z)dZ =  \n\\sum_{i=1}^{n} \\int_{\\mathbb{R}^{d}}  \\log p(x_{i}|\\Theta) q(Z)dZ\\]\n\\[\\sum_{i=1}^{n} \\int_{\\mathbb{R}^{d}}  \\log p(x_{i}|\\Theta) q(Z)dZ  =  \\sum_{i=1}^{n} \\int_{\\mathbb{R}^{d}} q(Z)\\log\\frac{p(x_{i},Z|\\Theta)q(Z)}{p(Z|\\Theta,x_{i})q(Z)}dZ = \\]\n\\[ = \\sum_{i=1}^{n} \\int q(Z)\\log\\frac{p(x_{i},Z|\\Theta)}{q(Z)}dZ + \\sum_{i=1}^{n}\\int q(Z)\\log \\frac{q(Z)}{p(Z|x_{i},\\Theta)}dZ\\]\nThe last term is \\(\\textbf{KL}\\)-divergence between our prior knowledge and observed posterior distribution. The main properties of this disctance are:\n\n\\(KL \\geq 0\\)\n\\(KL( \\mathbb{P}|| \\mathbb{Q}) = 0\\) , if \\(\\mathbb{P}=\\mathbb{Q}\\)\n\nFinally, we get \\(\\textbf{ELBO}\\):\n\\[ \\sum_{i=1}^{n} \\log p(x_{i}|\\Theta) \\geq \\sum_{i=1}^{n} \\int q(Z)\\log\\frac{p(x_{i},Z|\\Theta)}{q(Z)}dZ\\]\nIt is worth noticing, that \\(\\textbf{ELBO}\\) has the same formula for optimization like for VAE. We will held maximization for parameters \\(\\Theta\\) and latent distribution \\(q(Z)\\) like in EM-algorithm. Nonetheless, the main distinguish is that likelihood and prior will be constrained by normal distribution during the \\(\\textbf{E}\\)-step.\nThus, we have two models:\n\n\\(p(x_{i},Z|\\Theta)\\) is normal distribution parametrized by NN with parameters \\(\\Theta\\).\n\\(q(Z|x_{i},\\phi)\\) is normal distribution parameterized by NN with parameters \\(\\phi\\).\n\nUndoubtedly, we make more constraints , than we had with acurrate and mean-field solution of EM.\n\\(\\textbf{Our problem with Variational inference}\\):\n\\[ \\sum_{i=1}^{n} \\int_{\\mathbb{R}^{d}} q(Z|x_{i},\\phi) \\log\\frac{p(x_{i},Z|\\Theta)}{q(Z|x_{i},\\phi)}dZ \\to \\max_{\\Theta,\\phi}\\]"
  },
  {
    "objectID": "projects/Seminar_VAE.html#variational-auto-encoders-vae",
    "href": "projects/Seminar_VAE.html#variational-auto-encoders-vae",
    "title": "Addisu Amare",
    "section": "2. Variational Auto Encoders (VAE)",
    "text": "2. Variational Auto Encoders (VAE)\nNow, we make assumption about \\(\\textbf{non-linear}\\) dependence between \\(X\\) and \\(Z\\). Then:\n\n2.1 like M-step:\nour task is maximization during M-step:\n\\[\\sum_{i=1}^{n} \\mathbb{E}_{q(Z|x_{i},\\phi)} \\log p(x_{i},Z|\\Theta) \\to \\max_{\\Theta} \\]\nThen, to calculate this, we need in:\n\nmodel \\(q(Z|x_{i},\\phi)\\) should give us samples\nlogarithm of $ p(x_{i},Z|)$\n\nThen we introduce model $ p(x_{i},Z|)$ as:\n\\[ p(x_{i},Z|\\Theta) = \\mathcal{N}(x_{i}| \\mu(z_{i},\\Theta) , \\sigma^{2}(z_{i},\\Theta)I)*\\mathcal{N}(z_{i}|0,I) \\] \\[p(x_{i},Z|\\Theta) = \\prod_{j=1}^{D}\\mathcal{N}(x_{ij}| \\mu_{j}(z_{i},\\Theta) , \\sigma_{j}^{2}(z_{i},\\Theta))*\\mathcal{N}(z_{i}|0,I) \\]\nThus, the first multiplier is the \\(\\textbf{decoder}\\) :\n\ntakes \\(d\\)-dimensional latent code\noutputs 2\\(D\\)-dimensional vector, where the \\(D\\)-first are means for the corresponding pixel of image, while the second are variances.\n\nAlso, we introduce the following model with ability of sampling latent codes from data sample.\n\\[ q(z|x_{i},\\phi) = \\prod_{j=1}^{d} \\mathcal{N}(z_{j}|m_{j}(x_{i},\\phi),s_{j}^{2}(x_{i},\\phi))\\]\nThis model is the \\(\\textbf{encoder}\\)\n\ntakes \\(D\\)-dimensional sample from data\noutputs 2\\(d\\)-dimensional vector, where the \\(d\\)-first are means for the corresponding latent code, while the second are variances.\n\nIt is worth noticing, that the posterior distribution \\(q(z|x_{i},\\phi)\\). is multiplication of 1-dimensional normal distributions.\n\n\n2.2 like E-step:\nDuring the \\(\\textbf{E}\\)-step, we maximize by parameters of encoder:\nHowever, we know, that the maximization of ELBO corresponds to the minimization of \\(\\textbf{KL}\\)-divergence between prior and current posterior (encoder):\n\\[ KL(q(z_{i}|x_{i},\\phi) || p(z)) \\to \\min_{\\phi}\\]\nYet another reason for choosing normal distribution for \\(q(z_{i}|x_{i},\\phi)\\) is the existence closed form of KL-divergence between gaussian distributions. Thta is why, we pick prior knowledge \\(p(z)\\) as gaussian too."
  },
  {
    "objectID": "projects/Seminar_VAE.html#vae-on-2d-data",
    "href": "projects/Seminar_VAE.html#vae-on-2d-data",
    "title": "Addisu Amare",
    "section": "3. VAE on 2d data",
    "text": "3. VAE on 2d data\nIn this task we will implement simple VAE model for 2d gaussian distribution \\(\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\).\nWe will consider two cases: * 2d univariate distribution (diagonal covariance matrix \\(\\boldsymbol{\\Sigma}\\)); * 2d multivariate distribution (strictly non-diagonal covariance matrix \\(\\boldsymbol{\\Sigma}\\)).\nThe goal is to analyze the difference between these two cases and understand why the trained VAE models will behave differently.\n\n3.1 Data generation\n\nTICKS_FONT_SIZE = 12\nLEGEND_FONT_SIZE = 12\nLABEL_FONT_SIZE = 14\nTITLE_FONT_SIZE = 16\n\n\ndef visualize_2d_data(\n    train_data,test_data,train_labels=None ,\n    test_labels=None ):\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n    ax1.set_title(\"train\", fontsize=TITLE_FONT_SIZE)\n    ax1.scatter(train_data[:, 0], train_data[:, 1], s=1, c=train_labels)\n    ax1.tick_params(labelsize=LABEL_FONT_SIZE)\n    ax2.set_title(\"test\", fontsize=TITLE_FONT_SIZE)\n    ax2.scatter(test_data[:, 0], test_data[:, 1], s=1, c=test_labels)\n    ax2.tick_params(labelsize=LABEL_FONT_SIZE)\n    plt.show()\n\n\ndef visualize_2d_samples(data, title, labels=None, xlabel=None, ylabel=None):\n    plt.figure(figsize=(5, 5))\n    plt.scatter(data[:, 0], data[:, 1], s=1, c=labels)\n    plt.title(title, fontsize=TITLE_FONT_SIZE)\n    plt.xticks(fontsize=TICKS_FONT_SIZE)\n    plt.yticks(fontsize=TICKS_FONT_SIZE)\n    if xlabel is not None:\n        plt.xlabel(xlabel, fontsize=LABEL_FONT_SIZE)\n    if ylabel is not None:\n        plt.ylabel(ylabel, fontsize=LABEL_FONT_SIZE)\n    plt.show()\n\n\ndef generate_2d_data(count, mode='univariate'):\n    assert mode in ['univariate', 'multivariate']\n    np.random.seed(42)\n    mean = [[2.0, 3.0]]\n    sigma = [[3.0, 1.0]]\n    if mode == 'univariate':\n        rotate = [\n            [1.0, 0.0],\n            [0.0, 1.0]\n        ]\n    else:\n        rotate = [\n            [np.sqrt(2) / 2, np.sqrt(2) / 2],\n            [-np.sqrt(2) / 2, np.sqrt(2) / 2]\n        ]\n    data = mean + (np.random.randn(count, 2) * sigma).dot(rotate)\n    data = data.astype('float32')\n    split = int(0.7 * count)\n    train_data, test_data = data[:split], data[split:]\n    return train_data, test_data\n\n\ndef plot_training_curves(train_losses, test_losses, logscale_y=False, logscale_x=False):\n    n_train = len(train_losses[list(train_losses.keys())[0]])\n    n_test = len(test_losses[list(train_losses.keys())[0]])\n    x_train = np.linspace(0, n_test - 1, n_train)\n    x_test = np.arange(n_test)\n\n    plt.figure()\n    for key, value in train_losses.items():\n        plt.plot(x_train, value, label=key + '_train')\n\n    for key, value in test_losses.items():\n        plt.plot(x_test, value, label=key + '_test')\n\n    if logscale_y:\n        plt.semilogy()\n\n    if logscale_x:\n        plt.semilogx()\n\n    plt.legend(fontsize=LEGEND_FONT_SIZE)\n    plt.xlabel('Epoch', fontsize=LABEL_FONT_SIZE)\n    plt.ylabel('Loss', fontsize=LABEL_FONT_SIZE)\n    plt.xticks(fontsize=TICKS_FONT_SIZE)\n    plt.yticks(fontsize=TICKS_FONT_SIZE)\n    plt.grid()\n    plt.show()\n\n\nCOUNT = 15000\n\ntrain_data, test_data = generate_2d_data(COUNT, mode='multivariate')\nvisualize_2d_data(train_data, test_data)\n\ntrain_data, test_data = generate_2d_data(COUNT, mode='univariate')\nvisualize_2d_data(train_data, test_data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe difference of these two cases is the form of covariance matrix \\(\\boldsymbol{\\Sigma}\\).\nIn multivariate case the matrix is non-diagonal, in univariate case it is strictly diagonal. As you will see, our VAE model will have absolutely different results for these datasets.\n\n\n3.2 Kl-divergence and log-likelihood\nNow it is time to define our model. Our model will have the following structure:\n\nThe latent dimensionality is equal to 2, the same as the data dimensionality (\\(\\mathbf{z} \\in \\mathbb{R}^2\\), \\(\\mathbf{x} \\in \\mathbb{R}^2\\)).\nPrior distribution is standard Normal (\\(p(\\mathbf{z}) = \\mathcal{N}(0, I)\\)).\nVariational posterior distribution (or encoder) is \\(q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi}) = \\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\phi}}(\\mathbf{x}), \\boldsymbol{\\Sigma}_{\\boldsymbol{\\phi}}(\\mathbf{x}))\\). Here \\(\\boldsymbol{\\phi}\\) denotes all parameters of the encoder neural network.\nGenerative distribution (or decoder) is \\(p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}(\\mathbf{z}), \\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}(\\mathbf{z}))\\). Here \\(\\boldsymbol{\\theta}\\) denotes all parameters of the decoder neural network. Please note, that here we will use continuous distribution for our variables \\(\\mathbf{x}\\).\nWe will consider only diagonal covariance matrices \\(\\boldsymbol{\\Sigma}_{\\boldsymbol{\\phi}}(\\mathbf{x})\\), \\(\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}(\\mathbf{z})\\).\n\nModel objective is ELBO: \\[\n    \\mathcal{L}(\\boldsymbol{\\phi}, \\boldsymbol{\\theta}) = \\mathbb{E}_{q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi})} \\log p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta}) - KL (q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi}) || p(\\mathbf{z})).\n\\]\nTo make the expectation is independent of parameters \\(\\boldsymbol{\\phi}\\), we will use reparametrization trick.\nTo calculate the loss, we should derive - \\(\\log p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta})\\), note that generative distribution is \\(\\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}(\\mathbf{z}), \\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}(\\mathbf{z}))\\). - KL between \\(\\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\phi}}(\\mathbf{x}), \\boldsymbol{\\Sigma}_{\\boldsymbol{\\phi}}(\\mathbf{x}))\\) and \\(\\mathcal{N}(0, I)\\).\n\ndef get_normal_KL(mean_1, log_std_1, mean_2=None, log_std_2=None):\n    \"\"\"\n        This function should return the value of KL(p1 || p2),\n        where p1 = Normal(mean_1, exp(log_std_1)), p2 = Normal(mean_2, exp(log_std_2) ** 2).\n        If mean_2 and log_std_2 are None values, we will use standard normal distribution.\n        Note that we consider the case of diagonal covariance matrix.\n    \"\"\"\n    if mean_2 is None:\n        mean_2 = torch.zeros_like(mean_1)\n    if log_std_2 is None:\n        log_std_2 = torch.zeros_like(log_std_1)\n\n    std_1 = torch.exp(log_std_1)\n    std_2 = torch.exp(log_std_2)\n\n    mean_1, mean_2 = mean_1.float(), mean_2.float()\n    std_1 , std_2  = std_1 .float(), std_2 .float()\n\n    p  = torch.distributions.Normal(mean_1, std_1)\n    q  = torch.distributions.Normal(mean_2, std_2)\n    kl = torch.distributions.kl_divergence(p, q)\n\n    return kl\n\n\ndef get_normal_nll(x, mean, log_std):\n    \"\"\"\n        This function should return the negative log likelihood log p(x),\n        where p(x) = Normal(x | mean, exp(log_std) ** 2).\n        Note that we consider the case of diagonal covariance matrix.\n    \"\"\"\n    # ====\n    mean = mean              .float()\n    std  = torch.exp(log_std).float()\n\n    #if (mean.dim() == 0) and (std.dim() == 0):\n    prob = torch.distributions.Normal(mean, std)\n    #else:\n    #    scale_tril=torch.diag(std)\n    #    prob = torch.distributions.MultivariateNormal(mean, scale_tril=scale_tril)\n\n    nnl = -prob.log_prob(x)\n    return nnl\n\n\n\n3.3 VAE\nWe will use simple fully connected dense networks for encoder and decoder.\n\nclass FullyConnectedMLP(nn.Module):\n    def __init__(self, input_shape, hiddens, output_shape):\n        assert isinstance(hiddens, list)\n        super().__init__()\n        self.input_shape  = (input_shape,)\n        self.output_shape = (output_shape,)\n        self.hiddens = hiddens\n\n        model = []\n\n        # ====\n        # your code\n        # stack Dense layers with ReLU activation\n        # note: you do not have to add relu after the last dense layer\n        # ====\n        model.append( nn.Linear(input_shape, hiddens[0]) )\n        model.append( nn.ReLU() )\n\n        for i in range( len(hiddens)-1 ):\n            model.append( nn.Linear(hiddens[i+0], hiddens[i+1]) )\n            model.append( nn.ReLU() )\n\n        model.append( nn.Linear(hiddens[-1], output_shape) )\n        self.net = nn.Sequential(*model)\n\n    def forward(self, x):\n        # ====\n        # your code\n        # apply network that was defined in __init__ and return the output\n        # ====\n        return self.net(x)\n\n\nclass VAE2d(nn.Module):\n    def __init__(self, n_in, n_latent, enc_hidden_sizes, dec_hidden_sizes):\n        assert isinstance(enc_hidden_sizes, list)\n        assert isinstance(dec_hidden_sizes, list)\n        super().__init__()\n        self.n_latent = n_latent\n\n        # ====\n        # your code\n        # define encoder and decoder networks\n        # the encoder takes n_in elements, has enc_hidden_sizes neurons in hidden layers\n        # and outputs 2 * n_latent (n_latent for means, and n_latent for std)\n        # the decoder takes n_latent elements, has dec_hidden_sizes neurons in hidden layers\n        # and outputs 2 * n_in (n_in for means, and n_in for std)\n        # ====\n        self.encoder = FullyConnectedMLP(n_in    , enc_hidden_sizes, 2 * n_latent )\n        self.decoder = FullyConnectedMLP(n_latent, dec_hidden_sizes, 2 * n_in     )\n    def prior(self, n):\n        # ====\n        # your code\n        # return n samples from prior distribution (we use standard normal for prior)\n        # ====\n        loc   = torch.zeros(self.n_latent)\n        scale = torch.ones (self.n_latent)\n        p = torch.distributions.Normal(loc, scale)\n        prior_s = p.sample_n(n)\n\n        if USE_CUDA:\n            prior_s = prior_s.cuda()\n        return prior_s\n\n    def forward(self, x):\n        # ====\n        # your code\n        # now you have to return from the model\n        # - mu_z - means for variational distribution\n        # - mu_x - means for generative distribution\n        # - log_std_z - logarithm of std for variational distribution\n        # - log_std_x - logarithm of std for generative distribution\n        # we use logarithm, since the std is always positive\n        # to get std we will exponentiate it to get rid of this constraint\n\n        # 1) mu_z, log_std_z are outputs from the encoder\n        # 2) apply reparametrization trick to get z (input of decoder)\n        # (do not forget to use self.prior())\n        # 3) mu_x, log_std_x are outputs from the decoder\n        #    Note: [mu, log_std = decoder(input).chunk(2, dim=1)]\n\n        # ====\n        mu_z, log_std_z = self.encoder(x).chunk(2, dim=1)\n        z = torch.exp(log_std_z.to('cuda'))*self.prior( x.size(0) ) + mu_z.to('cuda')\n        mu_x, log_std_x = self.decoder(z).chunk(2, dim=1)\n\n        return mu_z, log_std_z, mu_x, log_std_x\n\n    def loss(self, x):\n        mu_z, log_std_z, mu_x, log_std_x = self(x)\n        # ====\n        # your code\n        # 1) apply model to get mu_z, log_std_z, mu_x, log_std_x\n        # 2) compute reconstruction loss using get_normal_nll (it is the first term in ELBO)\n        # 3) compute KL loss using get_normal_KL (it is the second term in ELBO)\n        # ====\n        recon_loss = torch.sum(get_normal_nll( x, mu_x, log_std_x ))\n        #kl_loss    = torch.sum(get_normal_KL ( mu_z, log_std_z, mu_x, log_std_x ))\n        kl_loss    = torch.sum(get_normal_KL ( mu_z, log_std_z, torch.zeros_like(mu_z), torch.zeros_like(log_std_z) ))\n\n\n        return {\n            'elbo_loss': recon_loss + kl_loss,\n            'recon_loss': recon_loss,\n            'kl_loss': kl_loss\n        }\n\n    def sample(self, n, sample_from_decoder=True):\n        z = None\n        with torch.no_grad():\n            # ====\n            # your code\n            # to sample from VAE model you have to sample from prior\n            # and then apply decoder to prior samples.\n            # parameter noise indicates whether to sample from decoder\n            # or just use means of generative distribution as samples\n            # 1) generate prior samples\n            # 2) apply decoder\n            # 3) sample from the decoder distribution if sample_from_decoder=True\n            # ====\n            prior_s = self.prior(n)\n            mu_x, log_std_x = self.decoder(prior_s).chunk(2, dim=1)\n            if sample_from_decoder:\n                z = torch.exp(log_std_x)*prior_s + mu_x\n            else:\n                z = mu_x\n        return z.cpu().numpy()\n\n\ndef solve_task(train_data, test_data, model, batch_size, epochs, lr, use_cuda=True, use_tqdm=False):\n    train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n    test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE)\n\n    train_losses, test_losses = train_model(\n        model, train_loader, test_loader, epochs=EPOCHS, lr=LR, use_cuda=use_cuda, use_tqdm=use_tqdm, loss_key='elbo_loss'\n    )\n    samples_noise = model.sample(3000, sample_from_decoder=True)\n    samples_nonoise = model.sample(3000, sample_from_decoder=False)\n\n    for key, value in test_losses.items():\n        print('{}: {:.4f}'.format(key, value[-1]))\n\n    plot_training_curves(train_losses, test_losses)\n    visualize_2d_samples(samples_noise, title='Samples with Decoder Noise')\n    visualize_2d_samples(samples_nonoise, title='Samples without Decoder Noise')\n\n\n# ====\n# your code\n# choose these parameters (2 hidden layers could be enough for encoder and decoder)\nENC_HIDDEN_SIZES = [20, 20]\nDEC_HIDDEN_SIZES = [20, 20]\nBATCH_SIZE = 32    # any adequate value\nEPOCHS = 20         # &lt; 10\nLR = 0.001        # &lt; 1e-2\n# ====\n\nCOUNT = 10000\n\n\nfrom tqdm import tqdm\n\n\ndef train_epoch(\n    model: object,\n    train_loader: object,\n    optimizer: object,\n    use_cuda: bool,\n    loss_key: str = \"total\",\n) -&gt; defaultdict:\n    model.train()\n\n    stats = defaultdict(list)\n    for x in tqdm(train_loader):\n        if use_cuda:\n            x = x.cuda()\n        losses = model.loss(x)\n        optimizer.zero_grad()\n        losses[loss_key].backward()\n        optimizer.step()\n\n        for k, v in losses.items():\n            stats[k].append(v.item())\n\n    return stats\n\n\ndef eval_model(model: object, data_loader: object, use_cuda: bool) -&gt; defaultdict:\n    model.eval()\n    stats = defaultdict(float)\n    with torch.no_grad():\n        for x in data_loader:\n            if use_cuda:\n                x = x.cuda()\n            losses = model.loss(x)\n            for k, v in losses.items():\n                stats[k] += v.item() * x.shape[0]\n\n        for k in stats.keys():\n            stats[k] /= len(data_loader.dataset)\n    return stats\n\n\ndef train_model(\n    model: object,\n    train_loader: object,\n    test_loader: object,\n    epochs: int,\n    lr: float,\n    use_tqdm: bool = False,\n    use_cuda: bool = False,\n    loss_key: str = \"total_loss\",\n) -&gt; Tuple[dict, dict]:\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    train_losses = defaultdict(list)\n    test_losses = defaultdict(list)\n    forrange = tqdm(range(epochs)) if use_tqdm else range(epochs)\n    if use_cuda:\n        model = model.cuda()\n\n    for epoch in forrange:\n        model.train()\n        train_loss = train_epoch(model, train_loader, optimizer, use_cuda, loss_key)\n        test_loss = eval_model(model, test_loader, use_cuda)\n\n        for k in train_loss.keys():\n            train_losses[k].extend(train_loss[k])\n            test_losses[k].append(test_loss[k])\n    return dict(train_losses), dict(test_losses)\n\n\ntrain_data, test_data = generate_2d_data(COUNT, mode='multivariate')\nvisualize_2d_data(train_data, test_data)\n\nmodel = VAE2d(2, 2, ENC_HIDDEN_SIZES, DEC_HIDDEN_SIZES).to('cuda')\nsolve_task(train_data, test_data, model, BATCH_SIZE, EPOCHS, LR, use_cuda=True)\n\n\n\n\n\n\n\n\n  0%|          | 0/219 [00:00&lt;?, ?it/s]/tmp/ipython-input-117888137.py:26: FutureWarning: `sample_n(n)` will be deprecated. Use `sample((n,))` instead.\n  prior_s = p.sample_n(n)\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:01&lt;00:00, 129.83it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 235.93it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 243.09it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:01&lt;00:00, 190.84it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:01&lt;00:00, 203.71it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 240.85it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 236.11it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 233.17it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 240.47it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 242.56it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 233.15it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 240.58it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 231.52it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:01&lt;00:00, 208.00it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:01&lt;00:00, 189.54it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 234.39it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 230.90it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 240.08it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 239.26it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 243.36it/s]\n\n\nelbo_loss: 125.9687\nrecon_loss: 90.7217\nkl_loss: 35.2470\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo analyze our models we will use the following function. Look carefully, do not change.\nThis function calculates the mean \\(\\boldsymbol{\\mu}_{\\boldsymbol{\\phi}}(\\mathbf{x})\\), and covariances \\(\\boldsymbol{\\Sigma}_{\\boldsymbol{\\phi}}(\\mathbf{x})\\) of the variational posterior distribution \\(q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi})\\).\n\ndef get_latent_stats(model, test_data, use_cuda=True, batch_size=3000):\n    batch = next(iter(data.DataLoader(test_data, batch_size=batch_size, shuffle=True)))\n    if use_cuda:\n        batch = batch.cuda()\n\n    with torch.no_grad():\n        mu_z, log_std_z = model(batch)[:2]\n\n    mu_z = mu_z.cpu().numpy()\n    std_z = log_std_z.exp().cpu().numpy()\n\n    return mu_z, std_z\n\n\n# just look at these numbers and read the comments after this task\nmu_z, std_z = get_latent_stats(model, test_data, use_cuda=USE_CUDA)\n\nprint('mu_z = ', mu_z.mean(axis=0), '+-', mu_z.std(axis=0))\nprint('std_z = ', std_z.mean(axis=0), '+-', std_z.std(axis=0))\n\nmu_z =  [-0.00333838  0.00629993] +- [0.9503864  0.02816329]\nstd_z =  [0.33444908 0.98905873] +- [0.00776618 0.01851054]\n\n\n/tmp/ipython-input-117888137.py:26: FutureWarning: `sample_n(n)` will be deprecated. Use `sample((n,))` instead.\n  prior_s = p.sample_n(n)\n\n\nSecondly, we will train the VAE model for univariate gaussian distribution.\n\ntrain_data, test_data = generate_2d_data(COUNT, mode='univariate')\nvisualize_2d_data(train_data, test_data)\n\nmodel = VAE2d(2, 2, ENC_HIDDEN_SIZES, DEC_HIDDEN_SIZES).cuda()\nsolve_task(train_data, test_data, model, BATCH_SIZE, EPOCHS, LR, use_cuda=USE_CUDA)\n\n\n\n\n\n\n\n\n  0%|          | 0/219 [00:00&lt;?, ?it/s]/tmp/ipython-input-117888137.py:26: FutureWarning: `sample_n(n)` will be deprecated. Use `sample((n,))` instead.\n  prior_s = p.sample_n(n)\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 230.18it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 235.42it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 235.94it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:01&lt;00:00, 204.08it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:01&lt;00:00, 184.54it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 237.33it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 239.81it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 235.39it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 238.16it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 233.77it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 243.84it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 240.69it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 239.44it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 233.66it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:01&lt;00:00, 187.62it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:01&lt;00:00, 184.49it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 235.14it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 236.41it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 242.78it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 240.52it/s]\n\n\nelbo_loss: 126.0842\nrecon_loss: 125.7401\nkl_loss: 0.3441\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmu_z, std_z = get_latent_stats(model, test_data, use_cuda=USE_CUDA)\n\nprint('mu_z = ', mu_z.mean(axis=0), '+-', mu_z.std(axis=0))\nprint('std_z = ', std_z.mean(axis=0), '+-', std_z.std(axis=0))\n\nmu_z =  [-0.01729548  0.00055771] +- [0.06285747 0.021273  ]\nstd_z =  [1.007619   0.99548346] +- [0.06453445 0.06496055]\n\n\n/tmp/ipython-input-117888137.py:26: FutureWarning: `sample_n(n)` will be deprecated. Use `sample((n,))` instead.\n  prior_s = p.sample_n(n)\n\n\nAfter training the VAE model on these 2 datasets, have a look at ‚ÄúSamples without Decoder Noise‚Äù figures. These figures show the means \\(\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}(\\mathbf{z})\\) of the generative distribution \\(p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta})\\). In the case of multivariate gaussian, the means are perfectly aligned with the data distribution. Otherwise, you have to see the strange figure in the univariate gaussian case . This happens due to so called posterior collapse (we will discuss it at the one of our lectures).\nTo be brief, the reason is the following. Our posterior distribution \\(p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}(\\mathbf{z}), \\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}(\\mathbf{z}))\\) is a univariate (covariance matrix is diagonal). Thus, the model does not need latent variable since the data distribution is also univariate. In this case VAE ignores latent variable, cause the model fits the distribution without any information from latent space.\nIf the decoder ignores latent variable, the second term in ELBO (KL) could be low (variational posterior distribution, which is given by encoder model, is close to prior distribution for each datapoint). In the training curves you have to see that KL loss behaves differently in these two cases.\nThe mean and std of variational posterior distribution also proves this concept. For the second case you have to see that mean is almost zero and std is almost one.\nIt is a real problem for generative models and we will discuss later how to overcome it."
  },
  {
    "objectID": "projects/Reconstruction/image_colorization.html",
    "href": "projects/Reconstruction/image_colorization.html",
    "title": "Addisu Amare",
    "section": "",
    "text": "Here we assume one has been trained, and demonstrate its use. You can train one very similar to how you trained the 2D models above.\n\n# imports\nimport os, sys\n\n# third party imports\nimport numpy as np\nimport tensorflow as tf\nimport voxelmorph as vxm\nimport neurite as ne\nassert tf.__version__.startswith('2.'), 'This tutorial assumes Tensorflow 2.0+'\n\n\n\n\n\n# our data will be of shape 160 x 192 x 224\nvol_shape = (160, 192, 224)\nnb_features = [\n    [16, 32, 32, 32],\n    [32, 32, 32, 32, 32, 16, 16]\n]\n\n\n# build vxm network\nvxm_model = vxm.networks.VxmDense(vol_shape, nb_features, int_steps=0);\n\n\n\n\n\nval_volume_1 = np.load('subj1.npz')['vol']\nseg_volume_1 = np.load('subj1.npz')['seg']\nval_volume_2 = np.load('subj2.npz')['vol']\nseg_volume_2 = np.load('subj2.npz')['seg']\n\nval_input = [\n    val_volume_1[np.newaxis, ..., np.newaxis],\n    val_volume_2[np.newaxis, ..., np.newaxis]\n]\n\nLoad a trained 3D model.\n\nvxm_model.load_weights('brain_3d.h5')\n\nNow let‚Äôs register.\n\nval_pred = vxm_model.predict(val_input);\n\n1/1 [==============================] - 8s 8s/step\n\n\n\nmoved_pred = val_pred[0].squeeze()\npred_warp = val_pred[1]\n\n\nmid_slices_fixed = [np.take(val_volume_2, vol_shape[d]//2, axis=d) for d in range(3)]\nmid_slices_fixed[1] = np.rot90(mid_slices_fixed[1], 1)\nmid_slices_fixed[2] = np.rot90(mid_slices_fixed[2], -1)\n\nmid_slices_pred = [np.take(moved_pred, vol_shape[d]//2, axis=d) for d in range(3)]\nmid_slices_pred[1] = np.rot90(mid_slices_pred[1], 1)\nmid_slices_pred[2] = np.rot90(mid_slices_pred[2], -1)\nne.plot.slices(mid_slices_fixed + mid_slices_pred, cmaps=['gray'], do_colorbars=True, grid=[2,3]);\n\n\n\n\n\n\n\n\nLet‚Äôs look at the segmentations! To do this, we‚Äôll need to warp segmentations.\n\nwarp_model = vxm.networks.Transform(vol_shape, interp_method='nearest')\n\n\nwarped_seg = warp_model.predict([seg_volume_1[np.newaxis,...,np.newaxis], pred_warp])\n\n1/1 [==============================] - 0s 319ms/step\n\n\nWe‚Äôre first going to prepare a colormap.\n\nfrom pystrum.pytools.plot import jitter\nimport matplotlib\n\n[ccmap, scrambled_cmap] = jitter(255, nargout=2)\nscrambled_cmap[0, :] = np.array([0, 0, 0, 1])\nccmap = matplotlib.colors.ListedColormap(scrambled_cmap)\n\nLet‚Äôs visualize the segmentations .\n\nmid_slices_fixed = [np.take(seg_volume_1, vol_shape[d]//1.8, axis=d) for d in range(3)]\nmid_slices_fixed[1] = np.rot90(mid_slices_fixed[1], 1)\nmid_slices_fixed[2] = np.rot90(mid_slices_fixed[2], -1)\n\nmid_slices_pred = [np.take(warped_seg.squeeze(), vol_shape[d]//1.8, axis=d) for d in range(3)]\nmid_slices_pred[1] = np.rot90(mid_slices_pred[1], 1)\nmid_slices_pred[2] = np.rot90(mid_slices_pred[2], -1)\n\nslices = mid_slices_fixed + mid_slices_pred\nfor si, slc  in enumerate(slices):\n    slices[si][0] = 255\nne.plot.slices(slices, cmaps = [ccmap], grid=[2,3]);"
  },
  {
    "objectID": "projects/Reconstruction/image_colorization.html#image-reconstruction-advanced",
    "href": "projects/Reconstruction/image_colorization.html#image-reconstruction-advanced",
    "title": "Addisu Amare",
    "section": "",
    "text": "Here we assume one has been trained, and demonstrate its use. You can train one very similar to how you trained the 2D models above.\n\n# imports\nimport os, sys\n\n# third party imports\nimport numpy as np\nimport tensorflow as tf\nimport voxelmorph as vxm\nimport neurite as ne\nassert tf.__version__.startswith('2.'), 'This tutorial assumes Tensorflow 2.0+'\n\n\n\n\n\n# our data will be of shape 160 x 192 x 224\nvol_shape = (160, 192, 224)\nnb_features = [\n    [16, 32, 32, 32],\n    [32, 32, 32, 32, 32, 16, 16]\n]\n\n\n# build vxm network\nvxm_model = vxm.networks.VxmDense(vol_shape, nb_features, int_steps=0);\n\n\n\n\n\nval_volume_1 = np.load('subj1.npz')['vol']\nseg_volume_1 = np.load('subj1.npz')['seg']\nval_volume_2 = np.load('subj2.npz')['vol']\nseg_volume_2 = np.load('subj2.npz')['seg']\n\nval_input = [\n    val_volume_1[np.newaxis, ..., np.newaxis],\n    val_volume_2[np.newaxis, ..., np.newaxis]\n]\n\nLoad a trained 3D model.\n\nvxm_model.load_weights('brain_3d.h5')\n\nNow let‚Äôs register.\n\nval_pred = vxm_model.predict(val_input);\n\n1/1 [==============================] - 8s 8s/step\n\n\n\nmoved_pred = val_pred[0].squeeze()\npred_warp = val_pred[1]\n\n\nmid_slices_fixed = [np.take(val_volume_2, vol_shape[d]//2, axis=d) for d in range(3)]\nmid_slices_fixed[1] = np.rot90(mid_slices_fixed[1], 1)\nmid_slices_fixed[2] = np.rot90(mid_slices_fixed[2], -1)\n\nmid_slices_pred = [np.take(moved_pred, vol_shape[d]//2, axis=d) for d in range(3)]\nmid_slices_pred[1] = np.rot90(mid_slices_pred[1], 1)\nmid_slices_pred[2] = np.rot90(mid_slices_pred[2], -1)\nne.plot.slices(mid_slices_fixed + mid_slices_pred, cmaps=['gray'], do_colorbars=True, grid=[2,3]);\n\n\n\n\n\n\n\n\nLet‚Äôs look at the segmentations! To do this, we‚Äôll need to warp segmentations.\n\nwarp_model = vxm.networks.Transform(vol_shape, interp_method='nearest')\n\n\nwarped_seg = warp_model.predict([seg_volume_1[np.newaxis,...,np.newaxis], pred_warp])\n\n1/1 [==============================] - 0s 319ms/step\n\n\nWe‚Äôre first going to prepare a colormap.\n\nfrom pystrum.pytools.plot import jitter\nimport matplotlib\n\n[ccmap, scrambled_cmap] = jitter(255, nargout=2)\nscrambled_cmap[0, :] = np.array([0, 0, 0, 1])\nccmap = matplotlib.colors.ListedColormap(scrambled_cmap)\n\nLet‚Äôs visualize the segmentations .\n\nmid_slices_fixed = [np.take(seg_volume_1, vol_shape[d]//1.8, axis=d) for d in range(3)]\nmid_slices_fixed[1] = np.rot90(mid_slices_fixed[1], 1)\nmid_slices_fixed[2] = np.rot90(mid_slices_fixed[2], -1)\n\nmid_slices_pred = [np.take(warped_seg.squeeze(), vol_shape[d]//1.8, axis=d) for d in range(3)]\nmid_slices_pred[1] = np.rot90(mid_slices_pred[1], 1)\nmid_slices_pred[2] = np.rot90(mid_slices_pred[2], -1)\n\nslices = mid_slices_fixed + mid_slices_pred\nfor si, slc  in enumerate(slices):\n    slices[si][0] = 255\nne.plot.slices(slices, cmaps = [ccmap], grid=[2,3]);"
  },
  {
    "objectID": "projects/Reconstruction/image_colorization.html#runtime",
    "href": "projects/Reconstruction/image_colorization.html#runtime",
    "title": "Addisu Amare",
    "section": "Runtime",
    "text": "Runtime\nAn important advantage of learning-based methods is the dramatically lowered runtime.\n\n%timeit vxm_model.predict(val_input)\n\n1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 1s 962ms/step\n1/1 [==============================] - 0s 262ms/step\n1/1 [==============================] - 0s 195ms/step\n1/1 [==============================] - 0s 200ms/step\n1/1 [==============================] - 0s 213ms/step\n1/1 [==============================] - 0s 198ms/step\n1/1 [==============================] - 0s 198ms/step\n564 ms ¬± 259 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)"
  },
  {
    "objectID": "projects/bayesian_linear_regression.html",
    "href": "projects/bayesian_linear_regression.html",
    "title": "Bayesian Linear Regression",
    "section": "",
    "text": "import numpy as np\nfrom scipy.stats import multivariate_normal as mvn\nfrom numpy.polynomial.polynomial import polyval\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\\(\\textbf{Notation:}\\)"
  },
  {
    "objectID": "projects/bayesian_linear_regression.html#classical-ml-against-bayesian-ml",
    "href": "projects/bayesian_linear_regression.html#classical-ml-against-bayesian-ml",
    "title": "Bayesian Linear Regression",
    "section": "1. Classical ML against Bayesian ML",
    "text": "1. Classical ML against Bayesian ML\n\nWhile we talk about classical ML approach it means, that we want to choose approprate weight matrix \\(w \\in \\mathbb{R}^{d\\times1}\\) in order to set equality in the equation below:\n\\[ Y = Xw\\]\nIt is worth noticing that \\(w = [ w_{1},..,w_{d}]^{T}\\) is column, and \\(w^{T}\\) is a raw.\n\\(\\textbf{Motivation:}\\) Learn these weight from the data to set \\(\\textbf{equality}\\)\nWhile we solve classic linear regression, it means that we maximize likelihood of the model, \\(\\textbf{why???}\\) let‚Äôs go:\n\\[\\max_{w}p(Y|X,w) = \\max_{w} \\sum_{i=1}^{n} \\log p(y_{i}|x_{i},w)\\]\nLet \\(p(y_{i}|x_{i},w) = \\mathcal{N}(y_{i}| w^{T}x_{i},\\beta^{-1})\\), where \\(\\beta = (\\frac{1}{n}\\sum_{i=1}^{n} (y_{i} - w^{T}x_{i})^{2})^{-1}\\)then:\n\\[ \\max_{w} \\sum_{i=1}^{n} \\log \\frac{1}{\\sqrt{2\\pi}\\beta^{-0.5}}exp(- \\frac{(y_{i} - w^{T}x_{i})^{2}}{2\\beta^{-1}})\\]\n\\[ \\max_{w} [ -n\\frac{1}{2}\\log(2\\pi) +. \\frac{n}{2}\\log\\beta - \\beta\\sum_{i=1}^{n} \\frac{(y_{i} - w^{T}x_{i})^{2}}{2}]\\]\nWhile maximize negative term, one can minimize the same positive term:\n\\[ \\min_{w}  \\sum_{i=1}^{n}(y_{i}  - w^{T}x_{i})^{2} = \\min_{w} || Y- Xw||^{2}_{2}\\]\nTo find the optimal value of weights , we make differentiaon over these weights:\n\\[ \\frac{\\partial|| Y- Xw||^{2}_{2} }{\\partial w} = 0  \\implies Y - Xw= 0\\]\nA little bit of manipulations from Linear Algebra:\n\\[  Y = Xw \\implies X^{T}Y = X^{T}Xw  \\implies  \\boxed{w_{ml} = (X^{T}X)^{-1} X^{T}Y}\\]\n\n1.2 Bayesian ML\n\nFirst of all, we should understand that the Bayesian Linear Regression grows up from Bayes formula:\n\\[ p(w| X,y ) = \\frac{p(y|X,w)p(w)}{\\int_{W}p(y|X,w)p(w)dw}\\]\nWe don‚Äôt pay our attention th the integral is a constant, thus:\n\\[ p(w| X,y ) \\approx p(y|X,w)p(w)\\]\n\\(\\textbf{Important:}\\) \\(p(w| X,y )\\) is the distribution, however we do not a law that describes this distribution. To define which law describes the posterior, we should pay our attention to the final representation of the posterior\n\n\\(p(w)\\) is prior distribtuion for parameters. We define oureselves that. Sincce the weights of learnable net lie near 0. Hence, one can define the prior as follows:\n\n\\[ p(w) = p(w|A) = \\mathcal{N}(w|0, A^{-1}) = \\prod_{j=1}^{d} \\mathcal{N}(w_{j}|0,\\alpha_{j}^{-1}): \\quad A =\\alpha I\\]\n\n\\(p(y|X,w)\\) is likelihood. As we have already sais, that:\n\n\\[ p(y|X, w) = \\prod_{i=1}^{d} p(y_{i}|x_{i},w) = \\mathcal{N}(y_{i}|w^{T}x_{i}, \\beta^{-1})\\]\nSince likelihood and prior are normnal, then it means that posterior is normla too. Thus:\n\\[ p(w|X,y) = \\mathcal{N}(w|\\mu, \\Sigma) \\]\n\n\n1.3 Find Mean of posterior\nDerivation of mean for the posterior distribution. One can recall dimensionalities of data:\n$ X ^{nd}, Y ^{n}, w ^{d} $\n\\(\\textbf{Idea:}\\) Since the prior is one-mode Gaussian distribution and likelihood is too. Then, the posterior is one-mode normal too. The mean of normal distribution correpsond to the point where the propbability is highest. By other words, it is extremum point of the probability and mean is such point where derivative of the distribution \\(p(w|X,y)\\) equals to zero.\n\\[\\mu = w^{*}: \\frac{\\partial p(w|X,y)}{\\partial w}|_{w^{*}} = 0  \\]\nSince the logarithmoc case is easier, let me move on the logarithmic expression:\n\\[ \\log p(w|X,y) \\approx \\log p(y|X,w) + \\log p(w|A)\\]\n\\[ \\log p(w|X,y) \\approx \\log(\\frac{1}{(2\\pi)^{d/2}det(B)^{1/2}})exp(...) + \\log(\\frac{1}{(2\\pi)^{d/2} \\sqrt{det(A^{-1})}}exp(\\frac{-w^{T}Aw}{2}))\\]\n\\[ \\log p(w|X,y) \\approx C_{1} - \\frac{\\beta}{2}\\sum_{i=1}^{n}(y_{i} - w^{T}x_{i})^{2} +  C_{2} - \\frac{1}{2}w^{T}Aw \\]\n\\[ \\log p(w|X,y) \\approx  -\\frac{\\beta}{2}(Y - Xw)^{T}(Y- Xw) - \\frac{w^{T}Aw}{2}\\]\n\\[ \\log p(w|X,y) \\approx \\frac{-\\beta}{2}(Y^{T}Y - Y^{T}Xw - (Xw)^{T}Y + (Xw)^{T}Xw) - \\frac{w^{T}Aw}{2}\\]\n\\[ \\log p(w|X,y) \\approx \\frac{-\\beta}{2}(-2Y^{T}Xw + w^{T}X^{T}Xw) - \\frac{w^{T}Aw}{2}\\]\n\\[ \\frac{ \\partial \\log p(w|X,y)}{\\partial w}  = 0 = \\beta Y^{T}X - \\beta w^{T}X^{T}X- w^{T}A  \\]\n\\[ w^{*} = ((\\beta X^{T}X + A)^{T})^{-1}\\beta X^{T}Y  \\]\nSince \\((A+B)^{T} = A^{T} + B^{T}\\), hence: $(X^{T}X + A)^{T} = X^{T}X + _{diag} = X^{T}X + A $\nThen: \\[ \\boxed{w^{*} = (\\beta X^{T}X + A)^{-1} \\beta X^{T}Y}\\]\n\\(\\textbf{Notes:}\\) Different prior covariance setups:\n\n\\(\\lim_{\\alpha \\to \\infty} w^{*} = 0\\)\n\\(\\lim_{\\alpha \\to 0} w^{*} = w_{ml}\\)\n\n\\(\\textbf{Notes:}\\)\nBayesian ML = Classical ML + regularization , in other words:\n\\[ || Y-  Xw ||^{2}_{2} + \\alpha||w||^{2} \\]\n\n\n1.4 Find covariance of posterior\n\\[\\frac{\\partial }{\\partial w}(\\beta Y^{T}X - \\beta w^{T}X^{T}X - w^{T}A) = 0 \\]\n\\[ -\\beta X^{T}X - A = 0 \\implies \\Sigma_{w} = \\beta X^{T}X + A  \\implies \\Sigma^{-1}_{w} = (\\beta X^{T}X + A)^{-1} \\]\n\nquestions:\n\n\nHow to choose \\(\\alpha\\)?\nSmall \\(\\alpha\\), what does it mean?\nBig \\(\\alpha\\)?"
  },
  {
    "objectID": "projects/bayesian_linear_regression.html#relevance-vector-machine-sequential-updates",
    "href": "projects/bayesian_linear_regression.html#relevance-vector-machine-sequential-updates",
    "title": "Bayesian Linear Regression",
    "section": "2. Relevance Vector Machine: Sequential updates",
    "text": "2. Relevance Vector Machine: Sequential updates\n\\(\\textbf{Idea:}\\)\nLet our data is composed of 2 data points \\((x_{1},y_{1}), (x_{2},y_{2})\\) and are independent.\n\\[ p(w|X,y) = p(w| (x_{1},y_{1}), (x_{2},y_{2}) )p(w) \\approx p((x_{2},y_{2})|w) \\underbrace{p((x_{1},y_{1})|w)p(w)}_{\\text{posterior after 1st step}} =  p((x_{2},y_{2})|w)\\underbrace{p(w|(x_{1},y_{1}))}_{\\text{new prior}}\\]\nWe write the formula of posterior when \\(n = 2\\)\n\\[ \\log p(w | (x_{1},y_{1}), (x_{2},y_{2})) \\approx -\\frac{\\beta}{2}(Y_{(1,2)} - X_{(1,2)}w)^{T}(Y_{(1,2)} - X_{(1,2)}w) - \\frac{1}{2}(w- \\mu_{1})^{T}\\Sigma^{-1}_{w} (w - \\mu_{1}) \\]\nWe make the same operations to find \\(\\textbf{mean}\\) and \\(\\textbf{covariance}\\)\n\\[ \\log p(w|(x_{1},y_{1}), (x_{2},y_{2})) \\approx - \\frac{\\beta}{2}(Y^{T}Y - 2Y^{T}Xw  + (Xw)^{T}Xw) - \\frac{1}{2}(w - \\mu_{1})^{T}\\Sigma^{-1}_{1}(w - \\mu_{1})\\]\nWe perform differentiation over \\(w\\) and it equals to zero to find mode:\n\\[ \\frac{\\partial \\log p(w|(x_{1},y_{1}), (x_{2},y_{2})) }{\\partial w} \\approx \\beta Y^{T}X - \\beta w^{T}X^{T}X - w^{T}\\Sigma^{-1}_{1} + \\mu_{1}^{T}\\Sigma^{-1}_{1} = 0 \\]\n\\[ (\\beta X^{T}X + \\Sigma^{-1}_{1})^{T} w = \\beta Y^{T}X + \\mu_{1}^{T} \\Sigma^{-1}_{1}\\]\nSince \\(\\Sigma^{-1}_{1}\\) is diaginal matrix:\n\\[ w = (\\beta X_{(1,2)}^{T}X_{(1,2)} + \\Sigma^{-1}_{1})^{-1} ( \\beta X_{(1,2)}^{T}Y_{(1,2)} +  \\Sigma^{-1}_{1}\\mu_{1}) \\]\n\nfind variance\n\\[ \\Sigma_{(1,2)} = (\\beta X^{T}_{(1,2)}X_{(1,2)} +\\Sigma_{1}^{-1})^{-1} \\]\n\n\nAlgorithm\nWe consider the following model:\n\nLikelihood model for one item from data : \\(p(y_n|x_n, w;\\beta) = \\mathcal{N}(y_n| \\textbf{w}^Tx_n, \\beta^{-1})\\)\nLikelihood model for whole data: \\(p(\\textbf{y}|X,\\textbf{w};\\beta) = \\prod\\limits_{n=1}^{N}p(y_n|x_n,\\textbf{w};\\beta) = \\mathcal{N}(\\textbf{y}|X\\textbf{w}, \\beta^{-1})\\)\nPrior: \\(p(\\textbf{w};\\alpha) = \\prod\\limits_{d=1}^{D}\\mathcal{N}(w_d|0, \\alpha_d^{-1})=\\mathcal{N}(\\textbf{w}|0,A^{-1})\\)\n\nAnd for sequential updates we come up with the following equations:\n\nObserve \\((X,\\textbf{t})^{1}\\) and obtain:\n\\[\\Sigma_{w}^{-1} = [\\beta X_{(1)}^TX_{(1)} + A]\\] \\[\\mu_{w} = \\Sigma_{w}\\beta X^T_{(1)}\\textbf{t}_{(1)}\\]\nObserve \\((X,\\textbf{t})^{2}\\) and then for joint data:\n\\[\\Sigma_{w,(1,2)}^{-1} = [\\beta X_{(2)}^T X_{(2)} + \\Sigma_{w}^{-1}]\\]\n\\[\\mu_{w,(1,2)} = \\Sigma_{w,(1,2)}(\\beta X_{(2)}^T\\textbf{t}_{(2)}+\\Sigma^{-1}_w\\mu_w)\\]\n\\(\\dots\\)\n\n\n\nTask 1\nHere, we define our model:\n\n\\(\\textit{init}\\) takes \\(\\beta\\) for likelihood model and W_pr constitutes parameters of current prior\n\\(\\textit{update_w}\\) is calculation of parameters of current posterior(future prior)\n\\(\\textit{plot_w_space}\\) makes plotting of current posterior distribution\n\n\nclass BLR:\n    def __init__(self, A, beta, x_dim=2):\n        self.w_Pr = {'mu': np.zeros(x_dim), 'cov': np.linalg.inv(A), 'inv_cov': A}\n        self.beta = beta\n    \n    \n    def update_w(self, X, y, verbose=False):\n        inv_cov = self.beta * X.T @ X + self.w_Pr['inv_cov']\n        cov = np.linalg.inv(inv_cov)\n        mu = cov @ (self.beta * X.T @ y[:,None] + self.w_Pr['inv_cov'] @ self.w_Pr['mu'][:,None])\n        \n        self.w_Pr = {'mu': mu.flatten(), 'cov': cov, 'inv_cov': inv_cov}\n        return {'mu': mu.flatten(), 'cov': cov, 'inv_cov': inv_cov} if verbose else 0.0\n    \n    \n    def plot_w_space(self, ax, b=15.):\n        xx, yy = np.mgrid[-b:b:0.01, -b:b:0.01]\n        grid = np.c_[xx.ravel(), yy.ravel()]\n        probs = mvn.logpdf(grid, mean=self.w_Pr['mu'], cov=self.w_Pr['cov'])\n        contour = ax.contourf(xx,yy,probs.reshape(xx.shape))\n    \n    def plot_data_space(self, x, y, ax):\n        a, b = (-1.1, 1.1) \n        for i in range(50): \n            w = np.random.multivariate_normal(self.w_Pr['mu'], self.w_Pr['cov'])\n            ax.plot(np.array([a,b]), np.array([w[0]+w[1]*a, w[0]+w[1]*b]), 'r-', alpha=0.15)\n            \n        ax.scatter(x[:,:,1].flatten(), y.flatten(), edgecolor=\"white\", alpha=0.75, color='blue')\n        ax.scatter(x[-1,:,1].flatten(), y[-1].flatten(), edgecolor=\"white\", alpha=1., color='green')\n        \n        \n        \n    def plot_poly_data_space(self, x, y, ax, k=2):\n        k_points = np.linspace(-1.1, 1.1, num=k+1)\n        for i in range(50):\n            w = np.random.multivariate_normal(self.w_Pr['mu'], self.w_Pr['cov'])\n            poly_values = np.zeros_like(k_points)\n            poly_values = polyval(k_points, w)\n            ax.plot(k_points, poly_values, 'r-', alpha=0.15)\n        \n        ax.scatter(x[:,:,1].flatten(), y.flatten(), edgecolor=\"white\", alpha=0.75, color='blue')\n        ax.scatter(x[-1,:,1].flatten(), y[-1].flatten(), edgecolor=\"white\", alpha=1., color='green')\n        \n     \n\n\nreg = BLR(np.diag([0.1, 0.1]), 1.)\n\n\n# make matrice X\nx1 = np.linspace(-1, 1, 10)\nnp.random.shuffle(x1)\nx1 = x1.reshape(5, 2)\nx0 = np.ones_like(x1)\nX = np.stack([x0, x1], axis=-1)\n\n# make targets y \ny = 1.5 + 12. * x1\ny += np.random.randn(*y.shape) * 3.5\n\n\nchunks = 5 # divide dataset to 5 parts (M=5)\n\n#plotting utils\nfig, ax = plt.subplots(chunks,2, figsize=(8,16))\nymin, ymax = y[:,1].min(), y[:,1].max()\nyrange = ymax - ymin\nymin, ymax = ymin - 0.1 * yrange, ymax + 0.1 * yrange\n\nfor i in range(chunks):\n    reg.update_w(X[i], y[i])\n    reg.plot_data_space(X[:i+1], y[:i+1], ax[i,0])\n    ax[i, 0].set_ylim(ymin, ymax)\n    reg.plot_w_space(ax[i,1])\n    ax[i, 0].grid(), ax[i, 1].grid() \n    ax[0,0].set_title('regression results')\n    ax[0,1].set_title('Posterior distribtuions')"
  },
  {
    "objectID": "projects/bayesian_linear_regression.html#relevance-vector-machine",
    "href": "projects/bayesian_linear_regression.html#relevance-vector-machine",
    "title": "Bayesian Linear Regression",
    "section": "3. Relevance Vector Machine",
    "text": "3. Relevance Vector Machine\n\n# see the seminar 3 (on whiteboard)"
  },
  {
    "objectID": "projects/SemanticSegmentation_sol.html",
    "href": "projects/SemanticSegmentation_sol.html",
    "title": "Semantic Segmentation (solutions)",
    "section": "",
    "text": "img"
  },
  {
    "objectID": "projects/SemanticSegmentation_sol.html#unet-architecture",
    "href": "projects/SemanticSegmentation_sol.html#unet-architecture",
    "title": "Semantic Segmentation (solutions)",
    "section": "Unet architecture",
    "text": "Unet architecture\n\n\n\nimg\n\n\n\nimport torch\nimport torchvision\nfrom torchvision import transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n%matplotlib inline\nimport gdown\n\n\ngdown.download(url=\"https://drive.google.com/file/d/1kiR9pPP3gIqoyT9bWUnA0rgZuRTiUv7q/view?usp=share_link\", output=\"./unet_dict.pt\", quiet=False, fuzzy=True)\n\nDownloading...\nFrom: https://drive.google.com/uc?id=1kiR9pPP3gIqoyT9bWUnA0rgZuRTiUv7q\nTo: /content/unet_dict.pt\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124M/124M [00:00&lt;00:00, 238MB/s]\n\n\n'./unet_dict.pt'\n\n\n\nclass Encoder_Block(torch.nn.Module):\n  def __init__(self,inp_channels,out_channels):\n    super().__init__()\n    self.model = torch.nn.Sequential(\n        torch.nn.Conv2d(inp_channels,out_channels,kernel_size=3,padding=1),\n        torch.nn.BatchNorm2d(out_channels),\n        torch.nn.ReLU(),\n        torch.nn.Conv2d(out_channels,out_channels,kernel_size=3,padding=1),\n        torch.nn.BatchNorm2d(out_channels),\n        torch.nn.ReLU(),\n    )\n    self.pooling = torch.nn.MaxPool2d(2)\n  def forward(self,x):\n    int_out = self.model(x)\n    return self.pooling(int_out), int_out\n\nclass Decoder_Block(torch.nn.Module):\n  def __init__(self,inp_channels,out_channels):\n    super().__init__()\n    self.upsample = torch.nn.ConvTranspose2d(inp_channels,out_channels,kernel_size=2,stride=2)\n    self.model = torch.nn.Sequential(\n        torch.nn.Conv2d(inp_channels,out_channels,kernel_size=3,padding=1),\n        torch.nn.BatchNorm2d(out_channels),\n        torch.nn.ReLU(),\n        torch.nn.Conv2d(out_channels,out_channels,kernel_size=3,padding=1),\n        torch.nn.BatchNorm2d(out_channels),\n        torch.nn.ReLU(),\n    )\n  def forward(self,x,enc_x):\n    x = self.upsample(x)\n    x = torch.cat([x,enc_x],dim=1)\n    return self.model(x)"
  },
  {
    "objectID": "projects/SemanticSegmentation_sol.html#check-that-your-implementation-works-correctly",
    "href": "projects/SemanticSegmentation_sol.html#check-that-your-implementation-works-correctly",
    "title": "Semantic Segmentation (solutions)",
    "section": "Check that your implementation works correctly",
    "text": "Check that your implementation works correctly\n\nunet = Unet(3,11)\n\nassert unet(torch.randn(1,3,128,128)).shape == (1,11,128,128), \"check your implementation\""
  },
  {
    "objectID": "projects/SemanticSegmentation_sol.html#lets-start-working-with-data",
    "href": "projects/SemanticSegmentation_sol.html#lets-start-working-with-data",
    "title": "Semantic Segmentation (solutions)",
    "section": "Let‚Äôs start working with data",
    "text": "Let‚Äôs start working with data\n\ntransform = transforms.Compose([\n    transforms.Resize(128),\n    transforms.CenterCrop(128),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=0.5,std=0.5)\n])\ndef class_split(data,n=3):\n  data = np.array(data)\n  res = []\n  for i in range(1,1+n):\n    mask = np.zeros_like(data)\n    mask[data==i] = 1.\n    res.append(mask[None])\n  return torch.from_numpy(np.concatenate(res,axis=0)).to(torch.float)\n\ntarget_transform = transforms.Compose([\n    transforms.Resize(128),\n    transforms.CenterCrop(128),\n    class_split\n])\ndataset = torchvision.datasets.OxfordIIITPet(\"./data\",split=\"trainval\",target_types=\"segmentation\",download=True,transform=transform,target_transform=target_transform)\n\nDownloading https://thor.robots.ox.ac.uk/datasets/pets/images.tar.gz to data/oxford-iiit-pet/images.tar.gz\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 791918971/791918971 [00:26&lt;00:00, 30094727.25it/s]\n\n\nExtracting data/oxford-iiit-pet/images.tar.gz to data/oxford-iiit-pet\nDownloading https://thor.robots.ox.ac.uk/datasets/pets/annotations.tar.gz to data/oxford-iiit-pet/annotations.tar.gz\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19173078/19173078 [00:01&lt;00:00, 15159384.99it/s]\n\n\nExtracting data/oxford-iiit-pet/annotations.tar.gz to data/oxford-iiit-pet\n\n\n\ntransforms.ToPILImage()(dataset[0][0]/2+0.5)\n\n\n\n\n\n\n\n\n\ntransforms.ToPILImage()(dataset[0][1]/2+0.5)\n\n\n\n\n\n\n\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nunet = Unet(3,3).to(device)\ndataloader = torch.utils.data.DataLoader(dataset,batch_size=32,shuffle=True)\noptimizer = torch.optim.Adam(unet.parameters(),lr=0.001)\n\n\ndef train(model,dataloader,optimizer,loss_func=torch.nn.CrossEntropyLoss(),epochs=5):\n  for i in range(epochs):\n    for x,y in tqdm(dataloader):\n      x = x.to(device)\n      y = y.to(device)\n\n      out = model(x)\n\n      loss = loss_func(out,y)\n\n      optimizer.zero_grad()\n      loss.backward()\n      optimizer.step()\n    print(i)\n\n\ntrain(unet,dataloader,optimizer,epochs=10)\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115/115 [00:45&lt;00:00,  2.50it/s]\n\n\n0\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115/115 [00:46&lt;00:00,  2.47it/s]\n\n\n1\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115/115 [00:46&lt;00:00,  2.50it/s]\n\n\n2\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115/115 [00:46&lt;00:00,  2.47it/s]\n\n\n3\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115/115 [00:46&lt;00:00,  2.47it/s]\n\n\n4\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115/115 [00:46&lt;00:00,  2.48it/s]\n\n\n5\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115/115 [00:46&lt;00:00,  2.48it/s]\n\n\n6\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115/115 [00:46&lt;00:00,  2.48it/s]\n\n\n7\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115/115 [00:46&lt;00:00,  2.47it/s]\n\n\n8\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115/115 [00:46&lt;00:00,  2.50it/s]\n\n\n9\n\n\n\n\n\n\niou = []\nfor x,y in tqdm(dataloader):\n  y_hat = (torch.nn.Softmax(dim=1)(unet(x.to(device))) &gt; 0.5).to(torch.float)\n\n  intersection = y_hat * y.to(device)\n  union = (y_hat + y.to(device)).clamp(0,1)\n  iou.append(intersection.sum()/union.sum())\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115/115 [00:48&lt;00:00,  2.35it/s]\n\n\n\niou = [item.item() for item in iou]\n\n\nround(np.mean(iou)*100,1)\n\n78.0\n\n\n\n!wget -nv \"https://fikiwiki.com/uploads/posts/2022-02/1644990866_45-fikiwiki-com-p-prikolnie-kartinki-pro-zhivotnikh-47.png\"\n\n2023-04-04 12:22:25 URL:https://fikiwiki.com/uploads/posts/2022-02/1644990866_45-fikiwiki-com-p-prikolnie-kartinki-pro-zhivotnikh-47.png [1205459/1205459] -&gt; \"1644990866_45-fikiwiki-com-p-prikolnie-kartinki-pro-zhivotnikh-47.png\" [1]\n\n\n\nfrom PIL import Image\nimg = Image.open(\"/content/1644990866_45-fikiwiki-com-p-prikolnie-kartinki-pro-zhivotnikh-47.png\")\n\n\nimg = transform(img)[:3][None].to(device)\nmask = unet(img)[0]\n\n\ntransforms.ToPILImage()((torch.nn.Softmax(dim=0)(mask)[0:1].detach().cpu() &gt; 0.5).to(torch.float))\n\n\n\n\n\n\n\n\n\ntransforms.ToPILImage()(img[0].detach().cpu()/2+0.5)"
  },
  {
    "objectID": "projects/Dimensionality_reduction.html",
    "href": "projects/Dimensionality_reduction.html",
    "title": "PCA intuition",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport matplotlib.gridspec as gridspec\n\nnp.random.seed(42)\n\n# Generate data\nnum_points = 40\nn_frames_per_angle = 2 # how many frames per angle. The lower - the faster.\nX = np.random.randn(num_points, 2)\nX = X @ np.array([[1.6, 0.0], [0.0, 0.4]])\n\n# Normalize data\ncenter_point = [0., 0.]\n\n# PCA components\n_, v = np.linalg.eig(X.T @ X)\nv_main = v.T[0]\n\n# Set up the grid\ngs = gridspec.GridSpec(2, 1, height_ratios=[5, 1])  # Two rows, one column, with the first row 3 times the height of the second\n\nfig = plt.figure(figsize=(5, 6))  # Adjust the total figure size as necessary\n\nax = plt.subplot(gs[0])  # The first subplot\nax2 = plt.subplot(gs[1])  # The second subplot\n\nscatter = ax.scatter(X[:,0], X[:,1], color='b', label=\"Data\")\n\ndirection_line, = ax.plot([], [], 'k')\nax.plot([-v_main[0]*3 + center_point[0],\n                             v_main[0]*3 + center_point[0]],\n                            [-v_main[1]*3 + center_point[1],\n                            v_main[1]*3 + center_point[1]], label=\"First singular vector of X\")\n\nprojection_points, = ax.plot([], [], 'ro', markersize=5, label=\"Projections\")\nprojection_lines = [ax.plot([], [], 'r')[0] for _ in range(num_points)]\n\ndirection_line2, = ax2.plot([-3.5, 3.5], [0,0], 'k')\nprojections, = ax2.plot([],[], 'ro', markersize=7)\n\ndef init():\n    ax.axis('equal')\n    ax.grid(linestyle=\":\")\n    ax.scatter(x=center_point[0], y=center_point[1], c='k')\n    ax.legend(loc=\"upper right\")\n    ax.set_title(\"PCA\")\n    # ax.text(0.94, 0.945, \"@fminxyz\", transform=fig.transFigure,\n    #         ha=\"right\", va=\"top\", fontsize=10, alpha=0.5)\n\n    ax2.set_xlim(-3.5, 3.5)\n    ax2.set_ylim(-1, 1)\n    w = np.array([0, 0])\n    ax2.grid(linestyle=\":\")\n    ax2.set_title(\"Projections on the First Principal Component\\n\"\n                  f\"Variance of the projections: {np.linalg.norm(X@w)**2:.1f}\")\n    fig.tight_layout()\n    return scatter, direction_line, projection_points, projection_lines\n\n\ndef update(frame):\n    ax.set_xlim(-3.5+center_point[0], 3.5+center_point[0])\n    ax.set_ylim(-3.5+center_point[1], 3.5+center_point[1])\n    alpha = frame/n_frames_per_angle\n    w = np.array([np.cos(np.radians(alpha)), np.sin(np.radians(alpha))])\n    z = X @ w.reshape(-1, 1) @ w.reshape(1, -1) + center_point\n\n    for i in range(num_points):\n        projection_lines[i].set_data([X[i, 0], z[i, 0]], [X[i, 1], z[i, 1]])\n        projection_lines[i].set_color('r')\n\n    projection_points.set_data(z[:, 0], z[:, 1])\n    # distances = pdist(z)\n    # max_distance = np.max(distances)\n    # projection_points.set_label(f\"Max Distance: {max_distance:.2f}\")\n\n    direction_line.set_data([-w[0]*3 + center_point[0],\n                             w[0]*3 + center_point[0]],\n                            [-w[1]*3 + center_point[1],\n                            w[1]*3 + center_point[1]])\n\n    ax2.set_xlim(-3.5, 3.5)\n    ax2.set_ylim(-1, 1)\n    projections.set_data(X@w, np.zeros(len(X@w)))\n    ax2.set_title(\"Projections on the First Principal Component\\n\"\n                  f\"Variance of the projections: {np.linalg.norm(X@w)**2:.1f}\")\n\n    return direction_line, projection_points, projection_lines\n\nani = animation.FuncAnimation(fig, update,\n                              frames=np.arange(0, n_frames_per_angle*180),\n                              interval=1000/60, # 60 fps\n                              init_func=init)\n\nplt.close()\nfrom IPython import display\nhtml = display.HTML(ani.to_html5_video())\ndisplay.display(html)\n\n# # Uncomment to save to the file\n# ani.save(\"PCA_animation.mp4\", writer='ffmpeg', fps=60, dpi=300)\n\n\n  \n  Your browser does not support the video tag."
  },
  {
    "objectID": "projects/Dimensionality_reduction.html#exercise-whats-wrong-1",
    "href": "projects/Dimensionality_reduction.html#exercise-whats-wrong-1",
    "title": "PCA intuition",
    "section": "Exercise: what‚Äôs wrong (1)?",
    "text": "Exercise: what‚Äôs wrong (1)?\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport matplotlib.gridspec as gridspec\n\nnp.random.seed(42)\n\n# Generate data\nnum_points = 40\nn_frames_per_angle = 0.5 # how many frames per angle. The lower - the faster.\nX = np.random.randn(num_points, 2)\nX = X @ np.linalg.cholesky(np.array([[1, 0.6], [0.6, 0.6]]))\nX = X - np.ones(2)\n\n# Normalize data\ncenter_point = [0., 0.]\n\n# PCA components\n_, v = np.linalg.eig(X.T @ X)\nv_main = v.T[0]\n\n# Set up the grid\ngs = gridspec.GridSpec(2, 1, height_ratios=[5, 1])  # Two rows, one column, with the first row 3 times the height of the second\n\nfig = plt.figure(figsize=(5, 6))  # Adjust the total figure size as necessary\n\nax = plt.subplot(gs[0])  # The first subplot\nax2 = plt.subplot(gs[1])  # The second subplot\n\nscatter = ax.scatter(X[:,0], X[:,1], color='b', label=\"Data\")\n\ndirection_line, = ax.plot([], [], 'k')\nax.plot([-v_main[0]*3 + center_point[0],\n                             v_main[0]*3 + center_point[0]],\n                            [-v_main[1]*3 + center_point[1],\n                            v_main[1]*3 + center_point[1]], label=\"First singular vector of X\")\n\nprojection_points, = ax.plot([], [], 'ro', markersize=5, label=\"Projections\")\nprojection_lines = [ax.plot([], [], 'r')[0] for _ in range(num_points)]\n\ndirection_line2, = ax2.plot([-3.5, 3.5], [0,0], 'k')\nprojections, = ax2.plot([],[], 'ro', markersize=7)\n\ndef init():\n    ax.axis('equal')\n    ax.grid(linestyle=\":\")\n    ax.scatter(x=center_point[0], y=center_point[1], c='k')\n    ax.legend(loc=\"upper right\")\n    ax.set_title(\"PCA\")\n    # ax.text(0.94, 0.945, \"@fminxyz\", transform=fig.transFigure,\n    #         ha=\"right\", va=\"top\", fontsize=10, alpha=0.5)\n\n    ax2.set_xlim(-3.5, 3.5)\n    ax2.set_ylim(-1, 1)\n    w = np.array([0, 0])\n    ax2.grid(linestyle=\":\")\n    ax2.set_title(\"Projections on the First Principal Component\\n\"\n                  f\"Variance of the projections: {np.linalg.norm(X@w)**2:.1f}\")\n    fig.tight_layout()\n    return scatter, direction_line, projection_points, projection_lines\n\n\ndef update(frame):\n    ax.set_xlim(-3.5+center_point[0], 3.5+center_point[0])\n    ax.set_ylim(-3.5+center_point[1], 3.5+center_point[1])\n    alpha = frame/n_frames_per_angle\n    w = np.array([np.cos(np.radians(alpha)), np.sin(np.radians(alpha))])\n    z = X @ w.reshape(-1, 1) @ w.reshape(1, -1)\n\n    for i in range(num_points):\n        projection_lines[i].set_data([X[i, 0], z[i, 0]], [X[i, 1], z[i, 1]])\n        projection_lines[i].set_color('r')\n\n    projection_points.set_data(z[:, 0], z[:, 1])\n    # distances = pdist(z)\n    # max_distance = np.max(distances)\n    # projection_points.set_label(f\"Max Distance: {max_distance:.2f}\")\n\n    direction_line.set_data([-w[0]*3 + center_point[0],\n                             w[0]*3 + center_point[0]],\n                            [-w[1]*3 + center_point[1],\n                            w[1]*3 + center_point[1]])\n\n    ax2.set_xlim(-3.5, 3.5)\n    ax2.set_ylim(-1, 1)\n    projections.set_data(X@w, np.zeros(len(X@w)))\n    ax2.set_title(\"Projections on the First Principal Component\\n\"\n                  f\"Variance of the projections: {np.linalg.norm(X@w)**2:.1f}\")\n\n    return direction_line, projection_points, projection_lines\n\nani = animation.FuncAnimation(fig, update,\n                              frames=np.arange(0, n_frames_per_angle*180),\n                              interval=1000/60, # 60 fps\n                              init_func=init)\n\nplt.close()\nfrom IPython import display\nhtml = display.HTML(ani.to_html5_video())\ndisplay.display(html)\n\n# # Uncomment to save to the file\n# ani.save(\"PCA_animation.mp4\", writer='ffmpeg', fps=60, dpi=300)\n\n\n  \n  Your browser does not support the video tag."
  },
  {
    "objectID": "projects/Dimensionality_reduction.html#exercise-whats-wrong-2",
    "href": "projects/Dimensionality_reduction.html#exercise-whats-wrong-2",
    "title": "PCA intuition",
    "section": "Exercise: what‚Äôs wrong (2)?",
    "text": "Exercise: what‚Äôs wrong (2)?\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport matplotlib.gridspec as gridspec\n\nnp.random.seed(42)\n\n# Generate data\nnum_points = 30\nn_frames_per_angle = 1 # how many frames per angle. The lower - the faster.\nX = np.random.randn(num_points, 2)\n\n# Normalize data\ncenter_point = [0, 0]\n\n# PCA components\neigs, v = np.linalg.eig(X.T @ X)\nv_main = v.T[0]\n\n# Set up the grid\ngs = gridspec.GridSpec(2, 1, height_ratios=[5, 1])  # Two rows, one column, with the first row 3 times the height of the second\n\nfig = plt.figure(figsize=(5, 6))  # Adjust the total figure size as necessary\n\nax = plt.subplot(gs[0])  # The first subplot\nax2 = plt.subplot(gs[1])  # The second subplot\n\nscatter = ax.scatter(X[:,0], X[:,1], color='b', label=\"Data\")\n\ndirection_line, = ax.plot([], [], 'k')\n\nax.plot([-v_main[0]*3 + center_point[0],\n                             v_main[0]*3 + center_point[0]],\n                            [-v_main[1]*3 + center_point[1],\n                            v_main[1]*3 + center_point[1]], label=\"First singular vector of X\")\n\nprojection_points, = ax.plot([], [], 'ro', markersize=5, label=\"Projections\")\nprojection_lines = [ax.plot([], [], 'r')[0] for _ in range(num_points)]\n\ndirection_line2, = ax2.plot([-3.5, 3.5], [0,0], 'k')\nprojections, = ax2.plot([],[], 'ro', markersize=7)\n\ndef init():\n    ax.axis('equal')\n    ax.grid(linestyle=\":\")\n    ax.scatter(x=center_point[0], y=center_point[1], c='k')\n    ax.legend(loc=\"upper right\")\n    ax.set_title(\"PCA\")\n    # ax.text(0.94, 0.945, \"@fminxyz\", transform=fig.transFigure,\n    #         ha=\"right\", va=\"top\", fontsize=10, alpha=0.5)\n\n    ax2.set_xlim(-3.5, 3.5)\n    ax2.set_ylim(-1, 1)\n    w = np.array([0, 0])\n    ax2.grid(linestyle=\":\")\n    ax2.set_title(\"Projections on the First Principal Component\\n\"\n                  f\"Variance of the projections: {np.linalg.norm(X@w)**2:.1f}\")\n    fig.tight_layout()\n    return scatter, direction_line, projection_points, projection_lines\n\n\ndef update(frame):\n    ax.set_xlim(-3.5+center_point[0], 3.5+center_point[0])\n    ax.set_ylim(-3.5+center_point[1], 3.5+center_point[1])\n    alpha = frame/n_frames_per_angle\n    w = np.array([np.cos(np.radians(alpha)), np.sin(np.radians(alpha))])\n    z = X @ w.reshape(-1, 1) @ w.reshape(1, -1) + center_point\n\n    for i in range(num_points):\n        projection_lines[i].set_data([X[i, 0], z[i, 0]], [X[i, 1], z[i, 1]])\n        projection_lines[i].set_color('r')\n\n    projection_points.set_data(z[:, 0], z[:, 1])\n    # distances = pdist(z)\n    # max_distance = np.max(distances)\n    # projection_points.set_label(f\"Max Distance: {max_distance:.2f}\")\n\n    direction_line.set_data([-w[0]*3 + center_point[0],\n                             w[0]*3 + center_point[0]],\n                            [-w[1]*3 + center_point[1],\n                            w[1]*3 + center_point[1]])\n\n    ax2.set_xlim(-3.5, 3.5)\n    ax2.set_ylim(-1, 1)\n    projections.set_data(X@w, np.zeros(len(X@w)))\n    ax2.set_title(\"Projections on the First Principal Component\\n\"\n                  f\"Variance of the projections: {np.linalg.norm(X@w)**2:.1f}\")\n\n    return direction_line, projection_points, projection_lines\n\nani = animation.FuncAnimation(fig, update,\n                              frames=np.arange(0, n_frames_per_angle*180),\n                              interval=1000/60, # 60 fps\n                              init_func=init)\n\nplt.close()\nfrom IPython import display\nhtml = display.HTML(ani.to_html5_video())\ndisplay.display(html)\n\n# # Uncomment to save to the file\n# ani.save(\"PCA_animation.mp4\", writer='ffmpeg', fps=60, dpi=300)\n\n\n  \n  Your browser does not support the video tag."
  },
  {
    "objectID": "projects/Dimensionality_reduction.html#exercise-whats-wrong-3",
    "href": "projects/Dimensionality_reduction.html#exercise-whats-wrong-3",
    "title": "PCA intuition",
    "section": "Exercise: what‚Äôs ‚Äúwrong‚Äù (3)?",
    "text": "Exercise: what‚Äôs ‚Äúwrong‚Äù (3)?\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport matplotlib.gridspec as gridspec\n\nnp.random.seed(42)\n\n# Generate data\nnum_points = 60\nn_frames_per_angle = 0.5 # how many frames per angle. The lower - the faster.\nX_1 = np.random.randn(int(num_points/2), 2)\nX_1 = X_1 @ np.array([[1.4, 0.0], [0.0, 0.2]])\n\nX_2 = np.random.randn(int(num_points/2), 2)\nX_2 = X_2 @ np.array([[1.4, 0.0], [0.0, 0.2]])\nX_2 = X_2 + np.array([0, 2])\n\nX = np.vstack([X_1, X_2])\n\n# Normalize data\ncenter_point = np.mean(X, axis=0)\nX_std = X - center_point\n\n# PCA components\n_, v = np.linalg.eig(X_std.T @ X_std)\nv_main = v.T[0]\n\n# Set up the grid\ngs = gridspec.GridSpec(2, 1, height_ratios=[5, 1])  # Two rows, one column, with the first row 3 times the height of the second\n\nfig = plt.figure(figsize=(5, 6))  # Adjust the total figure size as necessary\n\nax = plt.subplot(gs[0])  # The first subplot\nax2 = plt.subplot(gs[1])  # The second subplot\n\nscatter = ax.scatter(X[:,0], X[:,1], color='b', label=\"Data\")\n\ndirection_line, = ax.plot([], [], 'k')\nax.plot([-v_main[0]*3 + center_point[0],\n                             v_main[0]*3 + center_point[0]],\n                            [-v_main[1]*3 + center_point[1],\n                            v_main[1]*3 + center_point[1]], label=\"First eigenvector of X\")\n\nprojection_points, = ax.plot([], [], 'ro', markersize=5, label=\"Projections\")\nprojection_lines = [ax.plot([], [], 'r')[0] for _ in range(num_points)]\n\ndirection_line2, = ax2.plot([-3.5, 3.5], [0,0], 'k')\nprojections, = ax2.plot([],[], 'ro', markersize=7)\n\ndef init():\n    ax.axis('equal')\n    ax.grid(linestyle=\":\")\n    ax.scatter(x=center_point[0], y=center_point[1], c='k')\n    ax.legend(loc=\"upper right\")\n    ax.set_title(\"PCA\")\n    # ax.text(0.94, 0.945, \"@fminxyz\", transform=fig.transFigure,\n    #         ha=\"right\", va=\"top\", fontsize=10, alpha=0.5)\n\n    ax2.set_xlim(-3.5, 3.5)\n    ax2.set_ylim(-1, 1)\n    w = np.array([0, 0])\n    ax2.grid(linestyle=\":\")\n    ax2.set_title(\"Projections on the First Principal Component\\n\"\n                  f\"Variance of the projections: {np.linalg.norm(X_std@w)**2:.1f}\")\n    fig.tight_layout()\n    return scatter, direction_line, projection_points, projection_lines\n\n\ndef update(frame):\n    ax.set_xlim(-3.5+center_point[0], 3.5+center_point[0])\n    ax.set_ylim(-3.5+center_point[1], 3.5+center_point[1])\n    alpha = frame/n_frames_per_angle\n    w = np.array([np.cos(np.radians(alpha)), np.sin(np.radians(alpha))])\n    z = X_std @ w.reshape(-1, 1) @ w.reshape(1, -1) + center_point\n\n    for i in range(num_points):\n        projection_lines[i].set_data([X[i, 0], z[i, 0]], [X[i, 1], z[i, 1]])\n        projection_lines[i].set_color('r')\n\n    projection_points.set_data(z[:, 0], z[:, 1])\n    # distances = pdist(z)\n    # max_distance = np.max(distances)\n    # projection_points.set_label(f\"Max Distance: {max_distance:.2f}\")\n\n    direction_line.set_data([-w[0]*3 + center_point[0],\n                             w[0]*3 + center_point[0]],\n                            [-w[1]*3 + center_point[1],\n                            w[1]*3 + center_point[1]])\n\n    ax2.set_xlim(-3.5, 3.5)\n    ax2.set_ylim(-1, 1)\n    projections.set_data(X_std@w, np.zeros(len(X_std@w)))\n    ax2.set_title(\"Projections on the First Principal Component\\n\"\n                  f\"Variance of the projections: {np.linalg.norm(X_std@w)**2:.1f}\")\n\n    return direction_line, projection_points, projection_lines\n\nani = animation.FuncAnimation(fig, update,\n                              frames=np.arange(0, n_frames_per_angle*180),\n                              interval=1000/60, # 60 fps\n                              init_func=init)\n\nplt.close()\nfrom IPython import display\nhtml = display.HTML(ani.to_html5_video())\ndisplay.display(html)\n\n# # Uncomment to save to the file\n# ani.save(\"PCA_animation.mp4\", writer='ffmpeg', fps=60, dpi=300)\n\n\n  \n  Your browser does not support the video tag."
  },
  {
    "objectID": "Jupyter_note_book.html",
    "href": "Jupyter_note_book.html",
    "title": "Breast Cancer classification Using Machine Learning Classifier",
    "section": "",
    "text": "# import libraries\nimport pandas as pd # for data manupulation or analysis\nimport numpy as np # for numeric calculation\nimport matplotlib.pyplot as plt # for data visualization\nimport seaborn as sns # for data visualization\nimport pickle #for dumping the model or we can use joblib library\n\n\n# Suppressing Warnings\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "Jupyter_note_book.html#create-dataframe",
    "href": "Jupyter_note_book.html#create-dataframe",
    "title": "Breast Cancer classification Using Machine Learning Classifier",
    "section": "Create DataFrame",
    "text": "Create DataFrame\n\n# create datafrmae\ncancer_df = pd.DataFrame(np.c_[cancer_dataset['data'],cancer_dataset['target']],\n             columns = np.append(cancer_dataset['feature_names'], ['target']))\n\n\n# DataFrame to CSV file\ncancer_df.to_csv('breast_cancer_dataframe.csv')\n\n\n# Head of cancer DataFrame\ncancer_df.head(6) \n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\ntarget\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n0.07871\n...\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n0.0\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n0.05667\n...\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n0.0\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n0.05999\n...\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n0.0\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n0.2597\n0.09744\n...\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n0.0\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n0.1809\n0.05883\n...\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n0.0\n\n\n5\n12.45\n15.70\n82.57\n477.1\n0.12780\n0.17000\n0.1578\n0.08089\n0.2087\n0.07613\n...\n23.75\n103.40\n741.6\n0.1791\n0.5249\n0.5355\n0.1741\n0.3985\n0.12440\n0.0\n\n\n\n\n6 rows √ó 31 columns\n\n\n\n\n# Tail of cancer DataFrame\ncancer_df.tail(6) \n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\ntarget\n\n\n\n\n563\n20.92\n25.09\n143.00\n1347.0\n0.10990\n0.22360\n0.31740\n0.14740\n0.2149\n0.06879\n...\n29.41\n179.10\n1819.0\n0.14070\n0.41860\n0.6599\n0.2542\n0.2929\n0.09873\n0.0\n\n\n564\n21.56\n22.39\n142.00\n1479.0\n0.11100\n0.11590\n0.24390\n0.13890\n0.1726\n0.05623\n...\n26.40\n166.10\n2027.0\n0.14100\n0.21130\n0.4107\n0.2216\n0.2060\n0.07115\n0.0\n\n\n565\n20.13\n28.25\n131.20\n1261.0\n0.09780\n0.10340\n0.14400\n0.09791\n0.1752\n0.05533\n...\n38.25\n155.00\n1731.0\n0.11660\n0.19220\n0.3215\n0.1628\n0.2572\n0.06637\n0.0\n\n\n566\n16.60\n28.08\n108.30\n858.1\n0.08455\n0.10230\n0.09251\n0.05302\n0.1590\n0.05648\n...\n34.12\n126.70\n1124.0\n0.11390\n0.30940\n0.3403\n0.1418\n0.2218\n0.07820\n0.0\n\n\n567\n20.60\n29.33\n140.10\n1265.0\n0.11780\n0.27700\n0.35140\n0.15200\n0.2397\n0.07016\n...\n39.42\n184.60\n1821.0\n0.16500\n0.86810\n0.9387\n0.2650\n0.4087\n0.12400\n0.0\n\n\n568\n7.76\n24.54\n47.92\n181.0\n0.05263\n0.04362\n0.00000\n0.00000\n0.1587\n0.05884\n...\n30.37\n59.16\n268.6\n0.08996\n0.06444\n0.0000\n0.0000\n0.2871\n0.07039\n1.0\n\n\n\n\n6 rows √ó 31 columns\n\n\n\n\n# Information of cancer Dataframe\ncancer_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 569 entries, 0 to 568\nData columns (total 31 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   mean radius              569 non-null    float64\n 1   mean texture             569 non-null    float64\n 2   mean perimeter           569 non-null    float64\n 3   mean area                569 non-null    float64\n 4   mean smoothness          569 non-null    float64\n 5   mean compactness         569 non-null    float64\n 6   mean concavity           569 non-null    float64\n 7   mean concave points      569 non-null    float64\n 8   mean symmetry            569 non-null    float64\n 9   mean fractal dimension   569 non-null    float64\n 10  radius error             569 non-null    float64\n 11  texture error            569 non-null    float64\n 12  perimeter error          569 non-null    float64\n 13  area error               569 non-null    float64\n 14  smoothness error         569 non-null    float64\n 15  compactness error        569 non-null    float64\n 16  concavity error          569 non-null    float64\n 17  concave points error     569 non-null    float64\n 18  symmetry error           569 non-null    float64\n 19  fractal dimension error  569 non-null    float64\n 20  worst radius             569 non-null    float64\n 21  worst texture            569 non-null    float64\n 22  worst perimeter          569 non-null    float64\n 23  worst area               569 non-null    float64\n 24  worst smoothness         569 non-null    float64\n 25  worst compactness        569 non-null    float64\n 26  worst concavity          569 non-null    float64\n 27  worst concave points     569 non-null    float64\n 28  worst symmetry           569 non-null    float64\n 29  worst fractal dimension  569 non-null    float64\n 30  target                   569 non-null    float64\ndtypes: float64(31)\nmemory usage: 137.9 KB\n\n\n\n# Numerical distribution of data\ncancer_df.describe() \n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\ntarget\n\n\n\n\ncount\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n...\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n\n\nmean\n14.127292\n19.289649\n91.969033\n654.889104\n0.096360\n0.104341\n0.088799\n0.048919\n0.181162\n0.062798\n...\n25.677223\n107.261213\n880.583128\n0.132369\n0.254265\n0.272188\n0.114606\n0.290076\n0.083946\n0.627417\n\n\nstd\n3.524049\n4.301036\n24.298981\n351.914129\n0.014064\n0.052813\n0.079720\n0.038803\n0.027414\n0.007060\n...\n6.146258\n33.602542\n569.356993\n0.022832\n0.157336\n0.208624\n0.065732\n0.061867\n0.018061\n0.483918\n\n\nmin\n6.981000\n9.710000\n43.790000\n143.500000\n0.052630\n0.019380\n0.000000\n0.000000\n0.106000\n0.049960\n...\n12.020000\n50.410000\n185.200000\n0.071170\n0.027290\n0.000000\n0.000000\n0.156500\n0.055040\n0.000000\n\n\n25%\n11.700000\n16.170000\n75.170000\n420.300000\n0.086370\n0.064920\n0.029560\n0.020310\n0.161900\n0.057700\n...\n21.080000\n84.110000\n515.300000\n0.116600\n0.147200\n0.114500\n0.064930\n0.250400\n0.071460\n0.000000\n\n\n50%\n13.370000\n18.840000\n86.240000\n551.100000\n0.095870\n0.092630\n0.061540\n0.033500\n0.179200\n0.061540\n...\n25.410000\n97.660000\n686.500000\n0.131300\n0.211900\n0.226700\n0.099930\n0.282200\n0.080040\n1.000000\n\n\n75%\n15.780000\n21.800000\n104.100000\n782.700000\n0.105300\n0.130400\n0.130700\n0.074000\n0.195700\n0.066120\n...\n29.720000\n125.400000\n1084.000000\n0.146000\n0.339100\n0.382900\n0.161400\n0.317900\n0.092080\n1.000000\n\n\nmax\n28.110000\n39.280000\n188.500000\n2501.000000\n0.163400\n0.345400\n0.426800\n0.201200\n0.304000\n0.097440\n...\n49.540000\n251.200000\n4254.000000\n0.222600\n1.058000\n1.252000\n0.291000\n0.663800\n0.207500\n1.000000\n\n\n\n\n8 rows √ó 31 columns\n\n\n\n\ncancer_df.isnull().sum()\n\nmean radius                0\nmean texture               0\nmean perimeter             0\nmean area                  0\nmean smoothness            0\nmean compactness           0\nmean concavity             0\nmean concave points        0\nmean symmetry              0\nmean fractal dimension     0\nradius error               0\ntexture error              0\nperimeter error            0\narea error                 0\nsmoothness error           0\ncompactness error          0\nconcavity error            0\nconcave points error       0\nsymmetry error             0\nfractal dimension error    0\nworst radius               0\nworst texture              0\nworst perimeter            0\nworst area                 0\nworst smoothness           0\nworst compactness          0\nworst concavity            0\nworst concave points       0\nworst symmetry             0\nworst fractal dimension    0\ntarget                     0\ndtype: int64"
  },
  {
    "objectID": "Jupyter_note_book.html#heatmap-of-a-correlation-matrix",
    "href": "Jupyter_note_book.html#heatmap-of-a-correlation-matrix",
    "title": "Breast Cancer classification Using Machine Learning Classifier",
    "section": "Heatmap of a correlation matrix",
    "text": "Heatmap of a correlation matrix\n\ncancer_df.corr()#gives the correlation between them\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\ntarget\n\n\n\n\nmean radius\n1.000000\n0.323782\n0.997855\n0.987357\n0.170581\n0.506124\n0.676764\n0.822529\n0.147741\n-0.311631\n...\n0.297008\n0.965137\n0.941082\n0.119616\n0.413463\n0.526911\n0.744214\n0.163953\n0.007066\n-0.730029\n\n\nmean texture\n0.323782\n1.000000\n0.329533\n0.321086\n-0.023389\n0.236702\n0.302418\n0.293464\n0.071401\n-0.076437\n...\n0.912045\n0.358040\n0.343546\n0.077503\n0.277830\n0.301025\n0.295316\n0.105008\n0.119205\n-0.415185\n\n\nmean perimeter\n0.997855\n0.329533\n1.000000\n0.986507\n0.207278\n0.556936\n0.716136\n0.850977\n0.183027\n-0.261477\n...\n0.303038\n0.970387\n0.941550\n0.150549\n0.455774\n0.563879\n0.771241\n0.189115\n0.051019\n-0.742636\n\n\nmean area\n0.987357\n0.321086\n0.986507\n1.000000\n0.177028\n0.498502\n0.685983\n0.823269\n0.151293\n-0.283110\n...\n0.287489\n0.959120\n0.959213\n0.123523\n0.390410\n0.512606\n0.722017\n0.143570\n0.003738\n-0.708984\n\n\nmean smoothness\n0.170581\n-0.023389\n0.207278\n0.177028\n1.000000\n0.659123\n0.521984\n0.553695\n0.557775\n0.584792\n...\n0.036072\n0.238853\n0.206718\n0.805324\n0.472468\n0.434926\n0.503053\n0.394309\n0.499316\n-0.358560\n\n\nmean compactness\n0.506124\n0.236702\n0.556936\n0.498502\n0.659123\n1.000000\n0.883121\n0.831135\n0.602641\n0.565369\n...\n0.248133\n0.590210\n0.509604\n0.565541\n0.865809\n0.816275\n0.815573\n0.510223\n0.687382\n-0.596534\n\n\nmean concavity\n0.676764\n0.302418\n0.716136\n0.685983\n0.521984\n0.883121\n1.000000\n0.921391\n0.500667\n0.336783\n...\n0.299879\n0.729565\n0.675987\n0.448822\n0.754968\n0.884103\n0.861323\n0.409464\n0.514930\n-0.696360\n\n\nmean concave points\n0.822529\n0.293464\n0.850977\n0.823269\n0.553695\n0.831135\n0.921391\n1.000000\n0.462497\n0.166917\n...\n0.292752\n0.855923\n0.809630\n0.452753\n0.667454\n0.752399\n0.910155\n0.375744\n0.368661\n-0.776614\n\n\nmean symmetry\n0.147741\n0.071401\n0.183027\n0.151293\n0.557775\n0.602641\n0.500667\n0.462497\n1.000000\n0.479921\n...\n0.090651\n0.219169\n0.177193\n0.426675\n0.473200\n0.433721\n0.430297\n0.699826\n0.438413\n-0.330499\n\n\nmean fractal dimension\n-0.311631\n-0.076437\n-0.261477\n-0.283110\n0.584792\n0.565369\n0.336783\n0.166917\n0.479921\n1.000000\n...\n-0.051269\n-0.205151\n-0.231854\n0.504942\n0.458798\n0.346234\n0.175325\n0.334019\n0.767297\n0.012838\n\n\nradius error\n0.679090\n0.275869\n0.691765\n0.732562\n0.301467\n0.497473\n0.631925\n0.698050\n0.303379\n0.000111\n...\n0.194799\n0.719684\n0.751548\n0.141919\n0.287103\n0.380585\n0.531062\n0.094543\n0.049559\n-0.567134\n\n\ntexture error\n-0.097317\n0.386358\n-0.086761\n-0.066280\n0.068406\n0.046205\n0.076218\n0.021480\n0.128053\n0.164174\n...\n0.409003\n-0.102242\n-0.083195\n-0.073658\n-0.092439\n-0.068956\n-0.119638\n-0.128215\n-0.045655\n0.008303\n\n\nperimeter error\n0.674172\n0.281673\n0.693135\n0.726628\n0.296092\n0.548905\n0.660391\n0.710650\n0.313893\n0.039830\n...\n0.200371\n0.721031\n0.730713\n0.130054\n0.341919\n0.418899\n0.554897\n0.109930\n0.085433\n-0.556141\n\n\narea error\n0.735864\n0.259845\n0.744983\n0.800086\n0.246552\n0.455653\n0.617427\n0.690299\n0.223970\n-0.090170\n...\n0.196497\n0.761213\n0.811408\n0.125389\n0.283257\n0.385100\n0.538166\n0.074126\n0.017539\n-0.548236\n\n\nsmoothness error\n-0.222600\n0.006614\n-0.202694\n-0.166777\n0.332375\n0.135299\n0.098564\n0.027653\n0.187321\n0.401964\n...\n-0.074743\n-0.217304\n-0.182195\n0.314457\n-0.055558\n-0.058298\n-0.102007\n-0.107342\n0.101480\n0.067016\n\n\ncompactness error\n0.206000\n0.191975\n0.250744\n0.212583\n0.318943\n0.738722\n0.670279\n0.490424\n0.421659\n0.559837\n...\n0.143003\n0.260516\n0.199371\n0.227394\n0.678780\n0.639147\n0.483208\n0.277878\n0.590973\n-0.292999\n\n\nconcavity error\n0.194204\n0.143293\n0.228082\n0.207660\n0.248396\n0.570517\n0.691270\n0.439167\n0.342627\n0.446630\n...\n0.100241\n0.226680\n0.188353\n0.168481\n0.484858\n0.662564\n0.440472\n0.197788\n0.439329\n-0.253730\n\n\nconcave points error\n0.376169\n0.163851\n0.407217\n0.372320\n0.380676\n0.642262\n0.683260\n0.615634\n0.393298\n0.341198\n...\n0.086741\n0.394999\n0.342271\n0.215351\n0.452888\n0.549592\n0.602450\n0.143116\n0.310655\n-0.408042\n\n\nsymmetry error\n-0.104321\n0.009127\n-0.081629\n-0.072497\n0.200774\n0.229977\n0.178009\n0.095351\n0.449137\n0.345007\n...\n-0.077473\n-0.103753\n-0.110343\n-0.012662\n0.060255\n0.037119\n-0.030413\n0.389402\n0.078079\n0.006522\n\n\nfractal dimension error\n-0.042641\n0.054458\n-0.005523\n-0.019887\n0.283607\n0.507318\n0.449301\n0.257584\n0.331786\n0.688132\n...\n-0.003195\n-0.001000\n-0.022736\n0.170568\n0.390159\n0.379975\n0.215204\n0.111094\n0.591328\n-0.077972\n\n\nworst radius\n0.969539\n0.352573\n0.969476\n0.962746\n0.213120\n0.535315\n0.688236\n0.830318\n0.185728\n-0.253691\n...\n0.359921\n0.993708\n0.984015\n0.216574\n0.475820\n0.573975\n0.787424\n0.243529\n0.093492\n-0.776454\n\n\nworst texture\n0.297008\n0.912045\n0.303038\n0.287489\n0.036072\n0.248133\n0.299879\n0.292752\n0.090651\n-0.051269\n...\n1.000000\n0.365098\n0.345842\n0.225429\n0.360832\n0.368366\n0.359755\n0.233027\n0.219122\n-0.456903\n\n\nworst perimeter\n0.965137\n0.358040\n0.970387\n0.959120\n0.238853\n0.590210\n0.729565\n0.855923\n0.219169\n-0.205151\n...\n0.365098\n1.000000\n0.977578\n0.236775\n0.529408\n0.618344\n0.816322\n0.269493\n0.138957\n-0.782914\n\n\nworst area\n0.941082\n0.343546\n0.941550\n0.959213\n0.206718\n0.509604\n0.675987\n0.809630\n0.177193\n-0.231854\n...\n0.345842\n0.977578\n1.000000\n0.209145\n0.438296\n0.543331\n0.747419\n0.209146\n0.079647\n-0.733825\n\n\nworst smoothness\n0.119616\n0.077503\n0.150549\n0.123523\n0.805324\n0.565541\n0.448822\n0.452753\n0.426675\n0.504942\n...\n0.225429\n0.236775\n0.209145\n1.000000\n0.568187\n0.518523\n0.547691\n0.493838\n0.617624\n-0.421465\n\n\nworst compactness\n0.413463\n0.277830\n0.455774\n0.390410\n0.472468\n0.865809\n0.754968\n0.667454\n0.473200\n0.458798\n...\n0.360832\n0.529408\n0.438296\n0.568187\n1.000000\n0.892261\n0.801080\n0.614441\n0.810455\n-0.590998\n\n\nworst concavity\n0.526911\n0.301025\n0.563879\n0.512606\n0.434926\n0.816275\n0.884103\n0.752399\n0.433721\n0.346234\n...\n0.368366\n0.618344\n0.543331\n0.518523\n0.892261\n1.000000\n0.855434\n0.532520\n0.686511\n-0.659610\n\n\nworst concave points\n0.744214\n0.295316\n0.771241\n0.722017\n0.503053\n0.815573\n0.861323\n0.910155\n0.430297\n0.175325\n...\n0.359755\n0.816322\n0.747419\n0.547691\n0.801080\n0.855434\n1.000000\n0.502528\n0.511114\n-0.793566\n\n\nworst symmetry\n0.163953\n0.105008\n0.189115\n0.143570\n0.394309\n0.510223\n0.409464\n0.375744\n0.699826\n0.334019\n...\n0.233027\n0.269493\n0.209146\n0.493838\n0.614441\n0.532520\n0.502528\n1.000000\n0.537848\n-0.416294\n\n\nworst fractal dimension\n0.007066\n0.119205\n0.051019\n0.003738\n0.499316\n0.687382\n0.514930\n0.368661\n0.438413\n0.767297\n...\n0.219122\n0.138957\n0.079647\n0.617624\n0.810455\n0.686511\n0.511114\n0.537848\n1.000000\n-0.323872\n\n\ntarget\n-0.730029\n-0.415185\n-0.742636\n-0.708984\n-0.358560\n-0.596534\n-0.696360\n-0.776614\n-0.330499\n0.012838\n...\n-0.456903\n-0.782914\n-0.733825\n-0.421465\n-0.590998\n-0.659610\n-0.793566\n-0.416294\n-0.323872\n1.000000\n\n\n\n\n31 rows √ó 31 columns"
  },
  {
    "objectID": "Jupyter_note_book.html#suppor-vector-classifier",
    "href": "Jupyter_note_book.html#suppor-vector-classifier",
    "title": "Breast Cancer classification Using Machine Learning Classifier",
    "section": "Suppor vector Classifier",
    "text": "Suppor vector Classifier\n\n# Support vector classifier\nfrom sklearn.svm import SVC\nsvc_classifier = SVC()\nsvc_classifier.fit(X_train, y_train)\ny_pred_scv = svc_classifier.predict(X_test)\naccuracy_score(y_test, y_pred_scv)\n\n0.9385964912280702\n\n\n\nTrain with Standard scaled Data\n\n# Train with Standard scaled Data\nsvc_classifier2 = SVC()\nsvc_classifier2.fit(X_train_sc, y_train)\ny_pred_svc_sc = svc_classifier2.predict(X_test_sc)\naccuracy_score(y_test, y_pred_svc_sc)\n\n0.9649122807017544"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Scholarship interests\nMy scholarship interests lie at the intersection of data science and biomedical engineering."
  }
]