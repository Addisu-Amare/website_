[
  {
    "objectID": "Hobbies.html",
    "href": "Hobbies.html",
    "title": "Addisu Amare",
    "section": "",
    "text": "Hobby\nCategory\nFrequency\nEquipment Needed\n\n\n\n\nCooking\nCreative\nDaily\nKitchen utensils, ingredients\n\n\nReading Novels\nIntellectual\nDaily\nBooks/e-reader\n\n\nFootball\nOutdoor Sport\nWeekly\nCleats, ball, sports gear\n\n\nVolleyball\nTeam Sport\nBi-weekly\nKnee pads, athletic shoes\n\n\n\n\n\n\nCooking - Creative expression and practical life skill\nReading - Mental escape and knowledge expansion\n\nFootball - Physical fitness and teamwork\nVolleyball - Social interaction and coordination"
  },
  {
    "objectID": "Hobbies.html#my-hobbies-at-a-glance",
    "href": "Hobbies.html#my-hobbies-at-a-glance",
    "title": "Addisu Amare",
    "section": "",
    "text": "Hobby\nCategory\nFrequency\nEquipment Needed\n\n\n\n\nCooking\nCreative\nDaily\nKitchen utensils, ingredients\n\n\nReading Novels\nIntellectual\nDaily\nBooks/e-reader\n\n\nFootball\nOutdoor Sport\nWeekly\nCleats, ball, sports gear\n\n\nVolleyball\nTeam Sport\nBi-weekly\nKnee pads, athletic shoes\n\n\n\n\n\n\nCooking - Creative expression and practical life skill\nReading - Mental escape and knowledge expansion\n\nFootball - Physical fitness and teamwork\nVolleyball - Social interaction and coordination"
  },
  {
    "objectID": "documents.html",
    "href": "documents.html",
    "title": "Articles & Projects",
    "section": "",
    "text": "Artificial Intelligence applications in medical imaging analysis.\n\n\nView PDF Download PDF\n\n\n\n\n\n\nFederated learning approaches with explainable AI for medical imaging.\n\n\nView PDF Download PDF\n\n\n\n\n\n\n\n\nAcademic projects and research from undergraduate studies.\n\n\nView PDF Download PDF\n\n\n\n\n\n\nComputer vision and AI-based fire detection system.\n\n\nView PDF Download PDF\n\n\n\n\n\n\n\nOption 1: View in Browser - Click ‚ÄúView PDF‚Äù to open directly in your browser\n\n\nOption 2: Download - Click ‚ÄúDownload PDF‚Äù to save to your device\n\n\nOption 3: GitHub View - All files are accessible via GitHub repository\n\n\n\nNote: All PDFs are hosted on GitHub. For best viewing experience, use Chrome, Firefox, or Edge browsers."
  },
  {
    "objectID": "documents.html#articles",
    "href": "documents.html#articles",
    "title": "Articles & Projects",
    "section": "",
    "text": "Artificial Intelligence applications in medical imaging analysis.\n\n\nView PDF Download PDF\n\n\n\n\n\n\nFederated learning approaches with explainable AI for medical imaging.\n\n\nView PDF Download PDF\n\n\n\n\n\n\n\n\nAcademic projects and research from undergraduate studies.\n\n\nView PDF Download PDF\n\n\n\n\n\n\nComputer vision and AI-based fire detection system.\n\n\nView PDF Download PDF\n\n\n\n\n\n\n\nOption 1: View in Browser - Click ‚ÄúView PDF‚Äù to open directly in your browser\n\n\nOption 2: Download - Click ‚ÄúDownload PDF‚Äù to save to your device\n\n\nOption 3: GitHub View - All files are accessible via GitHub repository\n\n\n\nNote: All PDFs are hosted on GitHub. For best viewing experience, use Chrome, Firefox, or Edge browsers."
  },
  {
    "objectID": "documents.html#projects",
    "href": "documents.html#projects",
    "title": "Articles & Projects",
    "section": "",
    "text": "Academic projects and research from undergraduate studies.\n\n\nView PDF Download PDF\n\n\n\n\n\n\nComputer vision and AI-based fire detection system.\n\n\nView PDF Download PDF\n\n\n\n\n\n\n\nOption 1: View in Browser - Click ‚ÄúView PDF‚Äù to open directly in your browser\n\n\nOption 2: Download - Click ‚ÄúDownload PDF‚Äù to save to your device\n\n\nOption 3: GitHub View - All files are accessible via GitHub repository\n\n\n\nNote: All PDFs are hosted on GitHub. For best viewing experience, use Chrome, Firefox, or Edge browsers."
  },
  {
    "objectID": "documents.html#how-to-view",
    "href": "documents.html#how-to-view",
    "title": "Articles & Projects",
    "section": "",
    "text": "Option 1: View in Browser - Click ‚ÄúView PDF‚Äù to open directly in your browser\n\n\nOption 2: Download - Click ‚ÄúDownload PDF‚Äù to save to your device\n\n\nOption 3: GitHub View - All files are accessible via GitHub repository\n\n\n\nNote: All PDFs are hosted on GitHub. For best viewing experience, use Chrome, Firefox, or Edge browsers."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Introduction: The AI Revolution in Healthcare",
    "section": "",
    "text": "The AI Revolution in Healthcare\n\n\n\nThe Future of Medical Imaging\nMedical imaging is evolving. Artificial intelligence is no longer a simple tool; in fact, it has become part and parcel of diagnostic workflows. There is a growing need for intelligent processing of medical images, which represent approximately 90% of all healthcare data.\n\n\nThe Problem\nRadiologists: Are experiencing a 30-40% increase in workload each year\nImaging technology: High-resolution, 3D and 4D images are becoming more common\nDiagnostic Accuracy: Error rates due to human interpretation can be as high as 30% in some instances\n\n\nHow AI solves these problems\nTwo significant advances have been made with respect to using AI in medical imaging - Convolutional Neural Networks (CNN) have become the standard when it comes to the majority of medical imaging techniques and algorithms.\n\n\n\nüéØ 90%\n\n\nHealthcare data is medical images\n\n\n\n\n‚è±Ô∏è 70%\n\n\nReduction in screening time\n\n\n\n\nüí∞ $4.5B\n\n\nMarket size by 2028\n\n\n\n\nüìà 34%\n\n\nAnnual growth rate\n\n\n\n\n\nWhat You‚Äôll Learn in This Series\n-This comprehensive series explores:\n-CNN applications across various medical specialties\nViT breakthroughs in complex imaging tasks\nReal-world implementations and case studies\nPerformance comparisons and benchmarks\nFuture trends and ethical considerations\nNavigation Guide Next: CNN Overview\nOr jump to: Cancer Detection | Neurology | Ophthalmology"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Below is the course I have taught at the University of Skoltech,Russia during independent period.\n\n\n\nI30202437 Python for Data Science Pseudo-code. Program design and structure. Flow control. Iteration. Lists (arrays). Functions. File I/O. Classes, objects, methods, and libraries."
  },
  {
    "objectID": "teaching.html#courses-taught",
    "href": "teaching.html#courses-taught",
    "title": "Teaching",
    "section": "",
    "text": "Below is the course I have taught at the University of Skoltech,Russia during independent period.\n\n\n\nI30202437 Python for Data Science Pseudo-code. Program design and structure. Flow control. Iteration. Lists (arrays). Functions. File I/O. Classes, objects, methods, and libraries."
  },
  {
    "objectID": "teaching.html#teaching-philosophy",
    "href": "teaching.html#teaching-philosophy",
    "title": "Teaching",
    "section": "Teaching Philosophy",
    "text": "Teaching Philosophy\nI‚Äôve learned that great teaching isn‚Äôt just about delivering content‚Äîit‚Äôs about creating the right conditions for learning to happen. My approach rests on three core pillars: sparking genuine motivation, building a supportive community, and staying flexibly responsive to my students‚Äô needs. When I get these right, I see curiosity catch fire, collaboration flourish, and obstacles to learning start to fall away"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Addisu A. Zena, Btech",
    "section": "",
    "text": "Bio\nAddisu is a technology and biomedical enthusiast who holds a Bachelor‚Äôs degree in Electronics & Communication Engineering (ECE) and Biomedical Engineering from Vellore Institute of Technology (Class of 2022).\nWith a deep interest in programming, data science, and human anatomy and physiology, he continuously works on projects at the intersection of engineering, machine learning, and biomedical research. He is always expanding his skill set through hands-on learning and new technologies.\n###Skills: - Python | MATLAB | LabVIEW\n- Data Science | Machine Learning\n\n\nContact\n\nemail: 0941813057estifanos@gmail.com\nLinkedIn: profile -tiktok:@tech_guy47"
  },
  {
    "objectID": "projects/Seminar_VAE.html",
    "href": "projects/Seminar_VAE.html",
    "title": "Addisu Amare",
    "section": "",
    "text": "#Variational Autoencoders\nAuthor: Addisu Amare\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nimport torch.utils.data as data\n\nfrom typing import Tuple\nfrom collections import defaultdict\nassert torch.cuda.is_available()\nDEVICE = 'cuda'\nUSE_CUDA = True"
  },
  {
    "objectID": "projects/Seminar_VAE.html#variational-inference",
    "href": "projects/Seminar_VAE.html#variational-inference",
    "title": "Addisu Amare",
    "section": "1. Variational inference",
    "text": "1. Variational inference\n\n1.1 Problem statement\n\\(\\textbf{Data}\\): $X = {x_{1},‚Ä¶,x_{n}} $ are independent samples \\(D\\)-dimensional samples \\(\\textbf{Task}\\): We solve the task of generative modeling \\(p(X|\\Theta)\\), where \\(\\Theta\\) are parameters of the model.\nWhile we use this model, one can set optimization problem like \\(\\textbf{MLE-problem}\\):\n\\[ \\Theta^{*} = \\arg \\max_{\\Theta}  p(X|\\Theta)\\]\nHowever, one can consider our model like model with \\(d\\)-dimensional latent (hidden) variables \\(Z\\). Thus, having assumed existence of latent codes of our model, we can represent of likelihood as follows:\n\\[ p(X|\\Theta)  = p(X|Z,\\Theta) p(Z|\\Theta) \\]\n\n\\(\\textbf{Importantly}\\), we do this step because we sure, that introduced models might be easily parameterized. For example, like normal distributions:\n\n\\(p(X|Z,\\Theta) = \\mathcal{N}(X| \\mu(Z,\\Theta), \\Sigma^{-1}(Z, \\Theta))\\)\n\\(p(Z|\\Theta) = \\mathcal{N}(Z| \\mu(\\Theta), \\Sigma^{-1}(\\Theta))\\)\n\nThen, our \\(\\textbf{MLE}\\)-problem is the following:\n\\[ \\Theta^{*} = \\arg\\max_{\\Theta}\\prod_{i=1}^{n} p(x_{i}|\\Theta)=  \\arg\\max_{\\Theta}\\prod_{i=1}^{n} \\int_{\\mathbb{R}^{d}} p(x_{i}|Z, \\Theta)p(Z|\\Theta)dZ \\]\n\n\n1.2 Naive approach\nUndoubtedly, this problem might be solved through Monte-Carlo simulation:\n\\[ \\int p(x_{i}|Z,\\Theta)p(Z|\\Theta) dZ = \\mathbb{E}_{\\hat{Z} \\sim p(Z|\\Theta)} p(x_{i}|\\hat{Z},\\Theta) = \\frac{1}{K}\\sum_{k=1}^{K} p(x_{i}|\\hat{Z}_{k},\\Theta) \\]\n\n\\(\\textbf{Challenge}\\): The curse of dimensionality. Namely, we cannot properly cover whole space of \\(p(Z|\\Theta)\\) due to high dimension of the latent code. One can avoid this problem with sampleing more samples, however this amount will grow with increasing of dimensionality of latent code. Thus, to cover the space properly., the number of samples grows exponentially with respect to dimensionality of \\(Z\\). Finally, we cannot make accurate estimation for the integral.\n\\(\\textbf{Another explanation}\\) ‚Ä¶\n\n\n1.3 EM-algorithm\nSince:\n\\[ p(x_{i}|\\Theta) = \\int_{\\mathbb{R}^{d}}p(x_{i}|Z,\\Theta)p(Z|\\Theta)dZ \\]\nThen, we can consider this problem like problem for \\(\\textbf{EM}\\)-algorithm. As far as we know^ \\(\\textbf{E}\\)-step might be solved by two ways:\n\nAccurate Bayesian inference\nMean-field approximation\n\nIn \\(\\textbf{E}\\)-step, we have the following tractable integral that we cannot calculate and as a consequence of that we cannot perform Accurate Bayesian inference.\n\\[ q(Z|x_{i},\\Theta) = \\frac{p(x_{i}|Z,\\Theta) p(Z|\\Theta)}{\\int_{\\mathbb{R}^{d}}p(x_{i}|Z,\\Theta) p(Z|\\Theta)dZ}\\]\nMean-field is sufficnetly difficult for high dimensional latent codes.\n\n\n1.4 Variational Inference\nThen, we move on the Variational Inference:\n\\[ \\sum_{i=1}^{n} \\log p(x_{i}|\\Theta) = \\sum_{i=1}^{n} \\log p(x_{i}|\\Theta) \\int_{\\mathbb{R}^{d}}q(Z)dZ =  \n\\sum_{i=1}^{n} \\int_{\\mathbb{R}^{d}}  \\log p(x_{i}|\\Theta) q(Z)dZ\\]\n\\[\\sum_{i=1}^{n} \\int_{\\mathbb{R}^{d}}  \\log p(x_{i}|\\Theta) q(Z)dZ  =  \\sum_{i=1}^{n} \\int_{\\mathbb{R}^{d}} q(Z)\\log\\frac{p(x_{i},Z|\\Theta)q(Z)}{p(Z|\\Theta,x_{i})q(Z)}dZ = \\]\n\\[ = \\sum_{i=1}^{n} \\int q(Z)\\log\\frac{p(x_{i},Z|\\Theta)}{q(Z)}dZ + \\sum_{i=1}^{n}\\int q(Z)\\log \\frac{q(Z)}{p(Z|x_{i},\\Theta)}dZ\\]\nThe last term is \\(\\textbf{KL}\\)-divergence between our prior knowledge and observed posterior distribution. The main properties of this disctance are:\n\n\\(KL \\geq 0\\)\n\\(KL( \\mathbb{P}|| \\mathbb{Q}) = 0\\) , if \\(\\mathbb{P}=\\mathbb{Q}\\)\n\nFinally, we get \\(\\textbf{ELBO}\\):\n\\[ \\sum_{i=1}^{n} \\log p(x_{i}|\\Theta) \\geq \\sum_{i=1}^{n} \\int q(Z)\\log\\frac{p(x_{i},Z|\\Theta)}{q(Z)}dZ\\]\nIt is worth noticing, that \\(\\textbf{ELBO}\\) has the same formula for optimization like for VAE. We will held maximization for parameters \\(\\Theta\\) and latent distribution \\(q(Z)\\) like in EM-algorithm. Nonetheless, the main distinguish is that likelihood and prior will be constrained by normal distribution during the \\(\\textbf{E}\\)-step.\nThus, we have two models:\n\n\\(p(x_{i},Z|\\Theta)\\) is normal distribution parametrized by NN with parameters \\(\\Theta\\).\n\\(q(Z|x_{i},\\phi)\\) is normal distribution parameterized by NN with parameters \\(\\phi\\).\n\nUndoubtedly, we make more constraints , than we had with acurrate and mean-field solution of EM.\n\\(\\textbf{Our problem with Variational inference}\\):\n\\[ \\sum_{i=1}^{n} \\int_{\\mathbb{R}^{d}} q(Z|x_{i},\\phi) \\log\\frac{p(x_{i},Z|\\Theta)}{q(Z|x_{i},\\phi)}dZ \\to \\max_{\\Theta,\\phi}\\]"
  },
  {
    "objectID": "projects/Seminar_VAE.html#variational-auto-encoders-vae",
    "href": "projects/Seminar_VAE.html#variational-auto-encoders-vae",
    "title": "Addisu Amare",
    "section": "2. Variational Auto Encoders (VAE)",
    "text": "2. Variational Auto Encoders (VAE)\nNow, we make assumption about \\(\\textbf{non-linear}\\) dependence between \\(X\\) and \\(Z\\). Then:\n\n2.1 like M-step:\nour task is maximization during M-step:\n\\[\\sum_{i=1}^{n} \\mathbb{E}_{q(Z|x_{i},\\phi)} \\log p(x_{i},Z|\\Theta) \\to \\max_{\\Theta} \\]\nThen, to calculate this, we need in:\n\nmodel \\(q(Z|x_{i},\\phi)\\) should give us samples\nlogarithm of $ p(x_{i},Z|)$\n\nThen we introduce model $ p(x_{i},Z|)$ as:\n\\[ p(x_{i},Z|\\Theta) = \\mathcal{N}(x_{i}| \\mu(z_{i},\\Theta) , \\sigma^{2}(z_{i},\\Theta)I)*\\mathcal{N}(z_{i}|0,I) \\] \\[p(x_{i},Z|\\Theta) = \\prod_{j=1}^{D}\\mathcal{N}(x_{ij}| \\mu_{j}(z_{i},\\Theta) , \\sigma_{j}^{2}(z_{i},\\Theta))*\\mathcal{N}(z_{i}|0,I) \\]\nThus, the first multiplier is the \\(\\textbf{decoder}\\) :\n\ntakes \\(d\\)-dimensional latent code\noutputs 2\\(D\\)-dimensional vector, where the \\(D\\)-first are means for the corresponding pixel of image, while the second are variances.\n\nAlso, we introduce the following model with ability of sampling latent codes from data sample.\n\\[ q(z|x_{i},\\phi) = \\prod_{j=1}^{d} \\mathcal{N}(z_{j}|m_{j}(x_{i},\\phi),s_{j}^{2}(x_{i},\\phi))\\]\nThis model is the \\(\\textbf{encoder}\\)\n\ntakes \\(D\\)-dimensional sample from data\noutputs 2\\(d\\)-dimensional vector, where the \\(d\\)-first are means for the corresponding latent code, while the second are variances.\n\nIt is worth noticing, that the posterior distribution \\(q(z|x_{i},\\phi)\\). is multiplication of 1-dimensional normal distributions.\n\n\n2.2 like E-step:\nDuring the \\(\\textbf{E}\\)-step, we maximize by parameters of encoder:\nHowever, we know, that the maximization of ELBO corresponds to the minimization of \\(\\textbf{KL}\\)-divergence between prior and current posterior (encoder):\n\\[ KL(q(z_{i}|x_{i},\\phi) || p(z)) \\to \\min_{\\phi}\\]\nYet another reason for choosing normal distribution for \\(q(z_{i}|x_{i},\\phi)\\) is the existence closed form of KL-divergence between gaussian distributions. Thta is why, we pick prior knowledge \\(p(z)\\) as gaussian too."
  },
  {
    "objectID": "projects/Seminar_VAE.html#vae-on-2d-data",
    "href": "projects/Seminar_VAE.html#vae-on-2d-data",
    "title": "Addisu Amare",
    "section": "3. VAE on 2d data",
    "text": "3. VAE on 2d data\nIn this task we will implement simple VAE model for 2d gaussian distribution \\(\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\).\nWe will consider two cases: * 2d univariate distribution (diagonal covariance matrix \\(\\boldsymbol{\\Sigma}\\)); * 2d multivariate distribution (strictly non-diagonal covariance matrix \\(\\boldsymbol{\\Sigma}\\)).\nThe goal is to analyze the difference between these two cases and understand why the trained VAE models will behave differently.\n\n3.1 Data generation\n\nTICKS_FONT_SIZE = 12\nLEGEND_FONT_SIZE = 12\nLABEL_FONT_SIZE = 14\nTITLE_FONT_SIZE = 16\n\n\ndef visualize_2d_data(\n    train_data,test_data,train_labels=None ,\n    test_labels=None ):\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n    ax1.set_title(\"train\", fontsize=TITLE_FONT_SIZE)\n    ax1.scatter(train_data[:, 0], train_data[:, 1], s=1, c=train_labels)\n    ax1.tick_params(labelsize=LABEL_FONT_SIZE)\n    ax2.set_title(\"test\", fontsize=TITLE_FONT_SIZE)\n    ax2.scatter(test_data[:, 0], test_data[:, 1], s=1, c=test_labels)\n    ax2.tick_params(labelsize=LABEL_FONT_SIZE)\n    plt.show()\n\n\ndef visualize_2d_samples(data, title, labels=None, xlabel=None, ylabel=None):\n    plt.figure(figsize=(5, 5))\n    plt.scatter(data[:, 0], data[:, 1], s=1, c=labels)\n    plt.title(title, fontsize=TITLE_FONT_SIZE)\n    plt.xticks(fontsize=TICKS_FONT_SIZE)\n    plt.yticks(fontsize=TICKS_FONT_SIZE)\n    if xlabel is not None:\n        plt.xlabel(xlabel, fontsize=LABEL_FONT_SIZE)\n    if ylabel is not None:\n        plt.ylabel(ylabel, fontsize=LABEL_FONT_SIZE)\n    plt.show()\n\n\ndef generate_2d_data(count, mode='univariate'):\n    assert mode in ['univariate', 'multivariate']\n    np.random.seed(42)\n    mean = [[2.0, 3.0]]\n    sigma = [[3.0, 1.0]]\n    if mode == 'univariate':\n        rotate = [\n            [1.0, 0.0],\n            [0.0, 1.0]\n        ]\n    else:\n        rotate = [\n            [np.sqrt(2) / 2, np.sqrt(2) / 2],\n            [-np.sqrt(2) / 2, np.sqrt(2) / 2]\n        ]\n    data = mean + (np.random.randn(count, 2) * sigma).dot(rotate)\n    data = data.astype('float32')\n    split = int(0.7 * count)\n    train_data, test_data = data[:split], data[split:]\n    return train_data, test_data\n\n\ndef plot_training_curves(train_losses, test_losses, logscale_y=False, logscale_x=False):\n    n_train = len(train_losses[list(train_losses.keys())[0]])\n    n_test = len(test_losses[list(train_losses.keys())[0]])\n    x_train = np.linspace(0, n_test - 1, n_train)\n    x_test = np.arange(n_test)\n\n    plt.figure()\n    for key, value in train_losses.items():\n        plt.plot(x_train, value, label=key + '_train')\n\n    for key, value in test_losses.items():\n        plt.plot(x_test, value, label=key + '_test')\n\n    if logscale_y:\n        plt.semilogy()\n\n    if logscale_x:\n        plt.semilogx()\n\n    plt.legend(fontsize=LEGEND_FONT_SIZE)\n    plt.xlabel('Epoch', fontsize=LABEL_FONT_SIZE)\n    plt.ylabel('Loss', fontsize=LABEL_FONT_SIZE)\n    plt.xticks(fontsize=TICKS_FONT_SIZE)\n    plt.yticks(fontsize=TICKS_FONT_SIZE)\n    plt.grid()\n    plt.show()\n\n\nCOUNT = 15000\n\ntrain_data, test_data = generate_2d_data(COUNT, mode='multivariate')\nvisualize_2d_data(train_data, test_data)\n\ntrain_data, test_data = generate_2d_data(COUNT, mode='univariate')\nvisualize_2d_data(train_data, test_data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe difference of these two cases is the form of covariance matrix \\(\\boldsymbol{\\Sigma}\\).\nIn multivariate case the matrix is non-diagonal, in univariate case it is strictly diagonal. As you will see, our VAE model will have absolutely different results for these datasets.\n\n\n3.2 Kl-divergence and log-likelihood\nNow it is time to define our model. Our model will have the following structure:\n\nThe latent dimensionality is equal to 2, the same as the data dimensionality (\\(\\mathbf{z} \\in \\mathbb{R}^2\\), \\(\\mathbf{x} \\in \\mathbb{R}^2\\)).\nPrior distribution is standard Normal (\\(p(\\mathbf{z}) = \\mathcal{N}(0, I)\\)).\nVariational posterior distribution (or encoder) is \\(q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi}) = \\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\phi}}(\\mathbf{x}), \\boldsymbol{\\Sigma}_{\\boldsymbol{\\phi}}(\\mathbf{x}))\\). Here \\(\\boldsymbol{\\phi}\\) denotes all parameters of the encoder neural network.\nGenerative distribution (or decoder) is \\(p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}(\\mathbf{z}), \\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}(\\mathbf{z}))\\). Here \\(\\boldsymbol{\\theta}\\) denotes all parameters of the decoder neural network. Please note, that here we will use continuous distribution for our variables \\(\\mathbf{x}\\).\nWe will consider only diagonal covariance matrices \\(\\boldsymbol{\\Sigma}_{\\boldsymbol{\\phi}}(\\mathbf{x})\\), \\(\\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}(\\mathbf{z})\\).\n\nModel objective is ELBO: \\[\n    \\mathcal{L}(\\boldsymbol{\\phi}, \\boldsymbol{\\theta}) = \\mathbb{E}_{q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi})} \\log p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta}) - KL (q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi}) || p(\\mathbf{z})).\n\\]\nTo make the expectation is independent of parameters \\(\\boldsymbol{\\phi}\\), we will use reparametrization trick.\nTo calculate the loss, we should derive - \\(\\log p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta})\\), note that generative distribution is \\(\\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}(\\mathbf{z}), \\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}(\\mathbf{z}))\\). - KL between \\(\\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\phi}}(\\mathbf{x}), \\boldsymbol{\\Sigma}_{\\boldsymbol{\\phi}}(\\mathbf{x}))\\) and \\(\\mathcal{N}(0, I)\\).\n\ndef get_normal_KL(mean_1, log_std_1, mean_2=None, log_std_2=None):\n    \"\"\"\n        This function should return the value of KL(p1 || p2),\n        where p1 = Normal(mean_1, exp(log_std_1)), p2 = Normal(mean_2, exp(log_std_2) ** 2).\n        If mean_2 and log_std_2 are None values, we will use standard normal distribution.\n        Note that we consider the case of diagonal covariance matrix.\n    \"\"\"\n    if mean_2 is None:\n        mean_2 = torch.zeros_like(mean_1)\n    if log_std_2 is None:\n        log_std_2 = torch.zeros_like(log_std_1)\n\n    std_1 = torch.exp(log_std_1)\n    std_2 = torch.exp(log_std_2)\n\n    mean_1, mean_2 = mean_1.float(), mean_2.float()\n    std_1 , std_2  = std_1 .float(), std_2 .float()\n\n    p  = torch.distributions.Normal(mean_1, std_1)\n    q  = torch.distributions.Normal(mean_2, std_2)\n    kl = torch.distributions.kl_divergence(p, q)\n\n    return kl\n\n\ndef get_normal_nll(x, mean, log_std):\n    \"\"\"\n        This function should return the negative log likelihood log p(x),\n        where p(x) = Normal(x | mean, exp(log_std) ** 2).\n        Note that we consider the case of diagonal covariance matrix.\n    \"\"\"\n    # ====\n    mean = mean              .float()\n    std  = torch.exp(log_std).float()\n\n    #if (mean.dim() == 0) and (std.dim() == 0):\n    prob = torch.distributions.Normal(mean, std)\n    #else:\n    #    scale_tril=torch.diag(std)\n    #    prob = torch.distributions.MultivariateNormal(mean, scale_tril=scale_tril)\n\n    nnl = -prob.log_prob(x)\n    return nnl\n\n\n\n3.3 VAE\nWe will use simple fully connected dense networks for encoder and decoder.\n\nclass FullyConnectedMLP(nn.Module):\n    def __init__(self, input_shape, hiddens, output_shape):\n        assert isinstance(hiddens, list)\n        super().__init__()\n        self.input_shape  = (input_shape,)\n        self.output_shape = (output_shape,)\n        self.hiddens = hiddens\n\n        model = []\n\n        # ====\n        # your code\n        # stack Dense layers with ReLU activation\n        # note: you do not have to add relu after the last dense layer\n        # ====\n        model.append( nn.Linear(input_shape, hiddens[0]) )\n        model.append( nn.ReLU() )\n\n        for i in range( len(hiddens)-1 ):\n            model.append( nn.Linear(hiddens[i+0], hiddens[i+1]) )\n            model.append( nn.ReLU() )\n\n        model.append( nn.Linear(hiddens[-1], output_shape) )\n        self.net = nn.Sequential(*model)\n\n    def forward(self, x):\n        # ====\n        # your code\n        # apply network that was defined in __init__ and return the output\n        # ====\n        return self.net(x)\n\n\nclass VAE2d(nn.Module):\n    def __init__(self, n_in, n_latent, enc_hidden_sizes, dec_hidden_sizes):\n        assert isinstance(enc_hidden_sizes, list)\n        assert isinstance(dec_hidden_sizes, list)\n        super().__init__()\n        self.n_latent = n_latent\n\n        # ====\n        # your code\n        # define encoder and decoder networks\n        # the encoder takes n_in elements, has enc_hidden_sizes neurons in hidden layers\n        # and outputs 2 * n_latent (n_latent for means, and n_latent for std)\n        # the decoder takes n_latent elements, has dec_hidden_sizes neurons in hidden layers\n        # and outputs 2 * n_in (n_in for means, and n_in for std)\n        # ====\n        self.encoder = FullyConnectedMLP(n_in    , enc_hidden_sizes, 2 * n_latent )\n        self.decoder = FullyConnectedMLP(n_latent, dec_hidden_sizes, 2 * n_in     )\n    def prior(self, n):\n        # ====\n        # your code\n        # return n samples from prior distribution (we use standard normal for prior)\n        # ====\n        loc   = torch.zeros(self.n_latent)\n        scale = torch.ones (self.n_latent)\n        p = torch.distributions.Normal(loc, scale)\n        prior_s = p.sample_n(n)\n\n        if USE_CUDA:\n            prior_s = prior_s.cuda()\n        return prior_s\n\n    def forward(self, x):\n        # ====\n        # your code\n        # now you have to return from the model\n        # - mu_z - means for variational distribution\n        # - mu_x - means for generative distribution\n        # - log_std_z - logarithm of std for variational distribution\n        # - log_std_x - logarithm of std for generative distribution\n        # we use logarithm, since the std is always positive\n        # to get std we will exponentiate it to get rid of this constraint\n\n        # 1) mu_z, log_std_z are outputs from the encoder\n        # 2) apply reparametrization trick to get z (input of decoder)\n        # (do not forget to use self.prior())\n        # 3) mu_x, log_std_x are outputs from the decoder\n        #    Note: [mu, log_std = decoder(input).chunk(2, dim=1)]\n\n        # ====\n        mu_z, log_std_z = self.encoder(x).chunk(2, dim=1)\n        z = torch.exp(log_std_z.to('cuda'))*self.prior( x.size(0) ) + mu_z.to('cuda')\n        mu_x, log_std_x = self.decoder(z).chunk(2, dim=1)\n\n        return mu_z, log_std_z, mu_x, log_std_x\n\n    def loss(self, x):\n        mu_z, log_std_z, mu_x, log_std_x = self(x)\n        # ====\n        # your code\n        # 1) apply model to get mu_z, log_std_z, mu_x, log_std_x\n        # 2) compute reconstruction loss using get_normal_nll (it is the first term in ELBO)\n        # 3) compute KL loss using get_normal_KL (it is the second term in ELBO)\n        # ====\n        recon_loss = torch.sum(get_normal_nll( x, mu_x, log_std_x ))\n        #kl_loss    = torch.sum(get_normal_KL ( mu_z, log_std_z, mu_x, log_std_x ))\n        kl_loss    = torch.sum(get_normal_KL ( mu_z, log_std_z, torch.zeros_like(mu_z), torch.zeros_like(log_std_z) ))\n\n\n        return {\n            'elbo_loss': recon_loss + kl_loss,\n            'recon_loss': recon_loss,\n            'kl_loss': kl_loss\n        }\n\n    def sample(self, n, sample_from_decoder=True):\n        z = None\n        with torch.no_grad():\n            # ====\n            # your code\n            # to sample from VAE model you have to sample from prior\n            # and then apply decoder to prior samples.\n            # parameter noise indicates whether to sample from decoder\n            # or just use means of generative distribution as samples\n            # 1) generate prior samples\n            # 2) apply decoder\n            # 3) sample from the decoder distribution if sample_from_decoder=True\n            # ====\n            prior_s = self.prior(n)\n            mu_x, log_std_x = self.decoder(prior_s).chunk(2, dim=1)\n            if sample_from_decoder:\n                z = torch.exp(log_std_x)*prior_s + mu_x\n            else:\n                z = mu_x\n        return z.cpu().numpy()\n\n\ndef solve_task(train_data, test_data, model, batch_size, epochs, lr, use_cuda=True, use_tqdm=False):\n    train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n    test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE)\n\n    train_losses, test_losses = train_model(\n        model, train_loader, test_loader, epochs=EPOCHS, lr=LR, use_cuda=use_cuda, use_tqdm=use_tqdm, loss_key='elbo_loss'\n    )\n    samples_noise = model.sample(3000, sample_from_decoder=True)\n    samples_nonoise = model.sample(3000, sample_from_decoder=False)\n\n    for key, value in test_losses.items():\n        print('{}: {:.4f}'.format(key, value[-1]))\n\n    plot_training_curves(train_losses, test_losses)\n    visualize_2d_samples(samples_noise, title='Samples with Decoder Noise')\n    visualize_2d_samples(samples_nonoise, title='Samples without Decoder Noise')\n\n\n# ====\n# your code\n# choose these parameters (2 hidden layers could be enough for encoder and decoder)\nENC_HIDDEN_SIZES = [20, 20]\nDEC_HIDDEN_SIZES = [20, 20]\nBATCH_SIZE = 32    # any adequate value\nEPOCHS = 20         # &lt; 10\nLR = 0.001        # &lt; 1e-2\n# ====\n\nCOUNT = 10000\n\n\nfrom tqdm import tqdm\n\n\ndef train_epoch(\n    model: object,\n    train_loader: object,\n    optimizer: object,\n    use_cuda: bool,\n    loss_key: str = \"total\",\n) -&gt; defaultdict:\n    model.train()\n\n    stats = defaultdict(list)\n    for x in tqdm(train_loader):\n        if use_cuda:\n            x = x.cuda()\n        losses = model.loss(x)\n        optimizer.zero_grad()\n        losses[loss_key].backward()\n        optimizer.step()\n\n        for k, v in losses.items():\n            stats[k].append(v.item())\n\n    return stats\n\n\ndef eval_model(model: object, data_loader: object, use_cuda: bool) -&gt; defaultdict:\n    model.eval()\n    stats = defaultdict(float)\n    with torch.no_grad():\n        for x in data_loader:\n            if use_cuda:\n                x = x.cuda()\n            losses = model.loss(x)\n            for k, v in losses.items():\n                stats[k] += v.item() * x.shape[0]\n\n        for k in stats.keys():\n            stats[k] /= len(data_loader.dataset)\n    return stats\n\n\ndef train_model(\n    model: object,\n    train_loader: object,\n    test_loader: object,\n    epochs: int,\n    lr: float,\n    use_tqdm: bool = False,\n    use_cuda: bool = False,\n    loss_key: str = \"total_loss\",\n) -&gt; Tuple[dict, dict]:\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    train_losses = defaultdict(list)\n    test_losses = defaultdict(list)\n    forrange = tqdm(range(epochs)) if use_tqdm else range(epochs)\n    if use_cuda:\n        model = model.cuda()\n\n    for epoch in forrange:\n        model.train()\n        train_loss = train_epoch(model, train_loader, optimizer, use_cuda, loss_key)\n        test_loss = eval_model(model, test_loader, use_cuda)\n\n        for k in train_loss.keys():\n            train_losses[k].extend(train_loss[k])\n            test_losses[k].append(test_loss[k])\n    return dict(train_losses), dict(test_losses)\n\n\ntrain_data, test_data = generate_2d_data(COUNT, mode='multivariate')\nvisualize_2d_data(train_data, test_data)\n\nmodel = VAE2d(2, 2, ENC_HIDDEN_SIZES, DEC_HIDDEN_SIZES).to('cuda')\nsolve_task(train_data, test_data, model, BATCH_SIZE, EPOCHS, LR, use_cuda=True)\n\n\n\n\n\n\n\n\n  0%|          | 0/219 [00:00&lt;?, ?it/s]/tmp/ipython-input-117888137.py:26: FutureWarning: `sample_n(n)` will be deprecated. Use `sample((n,))` instead.\n  prior_s = p.sample_n(n)\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:01&lt;00:00, 129.83it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 235.93it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 243.09it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:01&lt;00:00, 190.84it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:01&lt;00:00, 203.71it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 240.85it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 236.11it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 233.17it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 240.47it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 242.56it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 233.15it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 240.58it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 231.52it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:01&lt;00:00, 208.00it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:01&lt;00:00, 189.54it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 234.39it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 230.90it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 240.08it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 239.26it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 243.36it/s]\n\n\nelbo_loss: 125.9687\nrecon_loss: 90.7217\nkl_loss: 35.2470\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo analyze our models we will use the following function. Look carefully, do not change.\nThis function calculates the mean \\(\\boldsymbol{\\mu}_{\\boldsymbol{\\phi}}(\\mathbf{x})\\), and covariances \\(\\boldsymbol{\\Sigma}_{\\boldsymbol{\\phi}}(\\mathbf{x})\\) of the variational posterior distribution \\(q(\\mathbf{z} | \\mathbf{x}, \\boldsymbol{\\phi})\\).\n\ndef get_latent_stats(model, test_data, use_cuda=True, batch_size=3000):\n    batch = next(iter(data.DataLoader(test_data, batch_size=batch_size, shuffle=True)))\n    if use_cuda:\n        batch = batch.cuda()\n\n    with torch.no_grad():\n        mu_z, log_std_z = model(batch)[:2]\n\n    mu_z = mu_z.cpu().numpy()\n    std_z = log_std_z.exp().cpu().numpy()\n\n    return mu_z, std_z\n\n\n# just look at these numbers and read the comments after this task\nmu_z, std_z = get_latent_stats(model, test_data, use_cuda=USE_CUDA)\n\nprint('mu_z = ', mu_z.mean(axis=0), '+-', mu_z.std(axis=0))\nprint('std_z = ', std_z.mean(axis=0), '+-', std_z.std(axis=0))\n\nmu_z =  [-0.00333838  0.00629993] +- [0.9503864  0.02816329]\nstd_z =  [0.33444908 0.98905873] +- [0.00776618 0.01851054]\n\n\n/tmp/ipython-input-117888137.py:26: FutureWarning: `sample_n(n)` will be deprecated. Use `sample((n,))` instead.\n  prior_s = p.sample_n(n)\n\n\nSecondly, we will train the VAE model for univariate gaussian distribution.\n\ntrain_data, test_data = generate_2d_data(COUNT, mode='univariate')\nvisualize_2d_data(train_data, test_data)\n\nmodel = VAE2d(2, 2, ENC_HIDDEN_SIZES, DEC_HIDDEN_SIZES).cuda()\nsolve_task(train_data, test_data, model, BATCH_SIZE, EPOCHS, LR, use_cuda=USE_CUDA)\n\n\n\n\n\n\n\n\n  0%|          | 0/219 [00:00&lt;?, ?it/s]/tmp/ipython-input-117888137.py:26: FutureWarning: `sample_n(n)` will be deprecated. Use `sample((n,))` instead.\n  prior_s = p.sample_n(n)\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 230.18it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 235.42it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 235.94it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:01&lt;00:00, 204.08it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:01&lt;00:00, 184.54it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 237.33it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 239.81it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 235.39it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 238.16it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 233.77it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 243.84it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 240.69it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 239.44it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 233.66it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:01&lt;00:00, 187.62it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:01&lt;00:00, 184.49it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 235.14it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 236.41it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 242.78it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219/219 [00:00&lt;00:00, 240.52it/s]\n\n\nelbo_loss: 126.0842\nrecon_loss: 125.7401\nkl_loss: 0.3441\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmu_z, std_z = get_latent_stats(model, test_data, use_cuda=USE_CUDA)\n\nprint('mu_z = ', mu_z.mean(axis=0), '+-', mu_z.std(axis=0))\nprint('std_z = ', std_z.mean(axis=0), '+-', std_z.std(axis=0))\n\nmu_z =  [-0.01729548  0.00055771] +- [0.06285747 0.021273  ]\nstd_z =  [1.007619   0.99548346] +- [0.06453445 0.06496055]\n\n\n/tmp/ipython-input-117888137.py:26: FutureWarning: `sample_n(n)` will be deprecated. Use `sample((n,))` instead.\n  prior_s = p.sample_n(n)\n\n\nAfter training the VAE model on these 2 datasets, have a look at ‚ÄúSamples without Decoder Noise‚Äù figures. These figures show the means \\(\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}(\\mathbf{z})\\) of the generative distribution \\(p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta})\\). In the case of multivariate gaussian, the means are perfectly aligned with the data distribution. Otherwise, you have to see the strange figure in the univariate gaussian case . This happens due to so called posterior collapse (we will discuss it at the one of our lectures).\nTo be brief, the reason is the following. Our posterior distribution \\(p(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}(\\mathbf{z}), \\boldsymbol{\\Sigma}_{\\boldsymbol{\\theta}}(\\mathbf{z}))\\) is a univariate (covariance matrix is diagonal). Thus, the model does not need latent variable since the data distribution is also univariate. In this case VAE ignores latent variable, cause the model fits the distribution without any information from latent space.\nIf the decoder ignores latent variable, the second term in ELBO (KL) could be low (variational posterior distribution, which is given by encoder model, is close to prior distribution for each datapoint). In the training curves you have to see that KL loss behaves differently in these two cases.\nThe mean and std of variational posterior distribution also proves this concept. For the second case you have to see that mean is almost zero and std is almost one.\nIt is a real problem for generative models and we will discuss later how to overcome it."
  },
  {
    "objectID": "projects/DDPM_eng.html",
    "href": "projects/DDPM_eng.html",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "",
    "text": "Author: Addisu Amare"
  },
  {
    "objectID": "projects/DDPM_eng.html#another-view-to-vae-models",
    "href": "projects/DDPM_eng.html#another-view-to-vae-models",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "1. Another view to VAE models",
    "text": "1. Another view to VAE models\nQuestions for the audience\n\nWhat are discriminative models?\nWhat are generative models?\nWhat task do generative models solve?\n\nHaving defined the main task of generative models, it is logical to ask yourself the question - ‚ÄúHow do we actually evaluate the probability of objects from the training sample?‚Äù\n\n\n\ntitle\n\n\n\nThe principle of maximum likelihood:\n\nWhen we want to train a generative model \\(p(x|\\theta)\\), then the first attempt to do this is the basis for solving the so-called \\(\\textbf{MLE-problem}\\) or the problem of maximizing the likelihood of the model by selecting its parameters:\n\\[\\theta = \\arg\\max_{\\theta} \\log p(X|\\theta) = \\{ X = \\{ x_{i}\\}_{i=1}^{n}\\}= \\arg\\max_{\\theta} \\sum_{i=1}^{n} \\log p(x_{i}|\\theta)\\]\nQuestions for the audience Why are we not satisfied with this approach?\n\nLatent space:\n\nBefore we move on to another way of estimating the likelihood of data, we will recall the essence of latent representations. The main intuition of which is shown in the picture below:\n\n\n\ntitle\n\n\n\nModels of the variation encoder:\n\nAnother attempt to assess the likelihood is to introduce latent variables and consider the VAE model. When we are dealing with a variational autoencoder, we do not have access to an honest value of the logarithm of likelihood, so we optimize the corresponding lower bound, which we call as \\(\\textbf{ELBO}\\)\n\\[ \\log p(x|\\theta) \\geq \\mathcal{L}(\\theta,q) = \\int_{Z}q(z|x, \\phi)\\log\\frac{p(x,z|\\theta)}{q(z|x,\\phi)} dz\\]\n\nExpansion of the latent space: more is better than one\n\nWe know that the variational autoencoder has exactly one latent space. That is, we entered the latent space with an encoder and \\(\\bf{immediately}\\) leave it with the help of a decoder, while we do not explore the latent space in any way.\nHowever, let‚Äôs try to expand the number of latent spaces by introducing \\(T\\) consecutive latent spaces with corresponding decompositions of \\(f_{i}\\). That is, we use the encoder to move into the latent space and walk through the latent space for a certain number of steps\n\n\n\ntitle"
  },
  {
    "objectID": "projects/DDPM_eng.html#hierarchical-vae-models",
    "href": "projects/DDPM_eng.html#hierarchical-vae-models",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "2. Hierarchical VAE models",
    "text": "2. Hierarchical VAE models\nFor the simplicity, let me denote the follwoing:\n\nx = \\(x_{0}\\)\n\\(z_{1} = x_{1}\\)\n\\(z_{2} = x_{2}\\)\n\\(z_{T} = x_{T}\\)\n\nAfter introduction more convinient notation for \\(T\\) dimensional vectors, we should define transfromation functions between \\(f_{i}\\) between latent statements. Undoubtedly, one can consider neural networks for this purpose , however, do not worth to complicate our life\n\\(\\textbf{Assumption :}\\) Let these functions \\(f_{i}\\) are \\(\\textbf{not-learnable}\\) certain transfromations. Since we would like to add some stochasticity to the framework, one can consider fixed distributions as such transformations:\n\\[ q(x_{t}|x_{t_1}) = \\mathcal{N}(x_{t}| x_{t-1}, \\beta I) \\]\nNow, our method looks like \\(\\textbf{Brownian motion}\\) or \\(\\textbf{Random movements}\\).\n\n\n\ntitle\n\n\nHowever, if we will learn means of such transformations (normal distributions) , then we obtain method that is referred to as \\(\\textbf{Hierarchical VAE}\\). Please, see this \\(\\href{https://jmtomczak.github.io/blog/9/9_hierarchical_lvm_p1.html}{blog}\\) for best understanding of this concept\nThus, thanks to this copncept, we realize that one can whole latent spaces \\(Z = \\{ x_{1},...,x_{T}\\}\\) and then the corresponding \\(\\textbf{ELBO}\\) formula is:\n\\[ \\log p(x|\\theta) \\geq \\mathcal{L}(\\theta,q) = \\int_{Z}q(x_{1},...,x_{T}|x_{0}, \\phi)\\log\\frac{p(x_{0},x_{1},..,x_{T}|\\theta)}{q(x_{1},...,x_{T}|x_{0},\\phi)} dx_{1:T}\\]"
  },
  {
    "objectID": "projects/DDPM_eng.html#gaussian-diffusion-processes",
    "href": "projects/DDPM_eng.html#gaussian-diffusion-processes",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "3. Gaussian diffusion processes",
    "text": "3. Gaussian diffusion processes"
  },
  {
    "objectID": "projects/DDPM_eng.html#what-is-the-main-motivation-to-consider-many-steps-in-latent-space",
    "href": "projects/DDPM_eng.html#what-is-the-main-motivation-to-consider-many-steps-in-latent-space",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "1. What is the main motivation to consider many steps in latent space?",
    "text": "1. What is the main motivation to consider many steps in latent space?\n - answer is here"
  },
  {
    "objectID": "projects/DDPM_eng.html#what-components-do-lie-at-the-heart-of-the-loss-function-of-vae",
    "href": "projects/DDPM_eng.html#what-components-do-lie-at-the-heart-of-the-loss-function-of-vae",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "2. What components do lie at the heart of the loss function of VAE?",
    "text": "2. What components do lie at the heart of the loss function of VAE?\n - answer is here\n\n\n\ntitle\n\n\nBecause the quality of generation of the hierarchical model of the variation encoder is better than the usual one.\nA question to the audience\n\nthen what conclusion does the number of latent representations suggest?\n\nThen we realize that the more latent spaces we consider, the better and better the quality of the model.\nThen the question arises - ‚ÄúBut if you take a lot of such latent representations, what will happen?‚Äù\nAnd answering this question, let‚Äôs slightly correct our transitional density of the hierarchical model of the variation encoder:\n\\[ q(x_t | x_{t-1}) = \\mathcal{N}(x_t | x_{t-1}, \\beta I) \\to q(x_t | x_{t-1}) = \\mathcal{N}(x_t | \\sqrt{1-\\beta}x_{t-1}, \\beta I)\\]\nThen it turns out that such a transient density determines the Markov process (where the present \\(x_{t}\\) depends only on the past \\(x_{t-1}\\)):\n\\[ x_t = \\sqrt{1 - \\beta} x_{t-1} + \\sqrt{\\beta} \\epsilon \\]\nwhere $ $ is usually a standard normal random variable\nThus, our transition process between latent spaces is a Markov process, and here‚Äôs what‚Äôs interesting to say about it:\n\n\n\ntitle\n\n\n\nTheorem 1:\n\nGiven:\n\n$ x_0 (x) $\n$(0,1) $\n\n\nThen applying the Markov chain to an arbitrary distribution of \\(\\pi(x)\\)infinitely many times, we get \\(\\mathcal{N}(0,I)\\). Thus, \\(\\mathcal{N}(0,I)\\) is a stationary distribution of the chain. That is, the following condition will be fulfilled $ p_(x) = (0, I) = q(x |x‚Äô) p_(x‚Äô) dx‚Äô $\nIf we denote $ t ={s=1}^{t} (1 - _s) $. Then we can express the sample of the process at any point in time using:\n\n\\[ x_t = \\sqrt{\\overline{\\alpha}_t} x_0 + \\sqrt{1 - \\overline{\\alpha}_t} \\epsilon \\]\n\\[ q(x_t | x_0) = \\mathcal{N}(x_t | \\sqrt{\\overline{\\alpha}_t} x_0, (1 - \\overline{\\alpha}_t) I) \\]\nThis means that we can select any \\(x_t\\) using only \\(x_0\\).\n\n\n\nChessUrl\n\n\n\\(\\textbf{The essence of theorem 1}\\)\n\nThe Markov process that we are considering will transform any data distribution into a normal standard distribution in an infinitely long time\nThe process is the so-called \\(\\textbf{free simulation}\\), that is, taking any point \\(x_{0}\\) and any point in time \\(t\\), you can instantly find \\(x_{t}\\)\n\nWe have proved the existence of a stochastic transformation from data to noise.\nRemember that the diffusion process does not depend on the initial density of \\(\\pi(x)\\)(complex) and the only requirement is access to a sample from it. The main idea of diffusion models is to use any data distribution of our choice as a complex initial density and gradually noise them. Thus, we understand the direct diffusion process as:\n\\[ x_{0} \\sim p_{data}(x) \\implies \\mathcal{F}(x_{0}) = x_{T} : x_{T} \\sim \\mathcal{N}(0,I) \\]\nThe idea: We have an equation for the direct noise reduction process that looks like this:\n\\[ dx_t = \\sqrt{1 - \\beta_t} x_{t-1} + \\sqrt{\\beta} \\epsilon \\]\nThanks to this equation, you can construct \\(\\color{red}{\\textbf{!! Untrained !!}}\\) trajectory from data to noise.\nIf we are dealing with an ordinary differential equation (ODE), then we can run this ODE in reverse time and get trajectories from noise to data. However, our process is not defined by an ODE, but by some kind of complex Stochastic diff equation. And this means that I would like to learn how to unfold such random equations in time.\nMotivation for learning diffusion models\n\nThere is a process from data to noise\nI want to expand the process\nThe detailed process goes from noise to data\n\nAlso, one can compare architectures and concepts of another generative models:\n\nVAE\nFLOW-based models\nDiffusion models\nGAN\n\n\n\n\nChessUrl"
  },
  {
    "objectID": "projects/DDPM_eng.html#reverse-process",
    "href": "projects/DDPM_eng.html#reverse-process",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "4. Reverse process",
    "text": "4. Reverse process\n\\(\\textbf{Theoretical statetment:}\\) If forward process is represented as set of Gaussian condtional transfromtions \\(q(x_{t}|x_{t-1})\\), then the reverse process will be the same, but with unknown parameters \\(p{x_{t-1}|x_{t}}\\)\nThus, we have 2 joint distributions of latent codes:\nBy the property of Markovian chains, one can represent forward process as:\n\n\\(q(x_{1},...,x_{T}|x_{0}) = q( x_{T} | x_{T-1},x_{0}) q(x_{T-1} | x_{T-2},x_{0})...q(x_{2} | x_{1},x_{0})\\)\n\nBy the property of Markovian chains, one can represent reversed process as:\n\n\\(p(x_{1},...,x_{T}) = p(x_{T-1}|x_{T})p(x_{T-1}|x_{T})....p(x_{1}|x_{2})\\)\n\nNow, we pay our attention to the loss function:\n\\[\\int_{x_{1:T}}q(x_{1},...,x_{T}|x_{0})\\log\\frac{p(x_{0},x_{1},....,x_{T})}{q(x_{1},....,x_{T}|x_{0})}dx_{1: T}\\]\nThus, one can represent this loss as corresponding KL-divergenges:\n\\[ \\int_{x_{1:T}} q(x_{1},...,x_{T}|x_{0})\\log p(x_{0}|x_{1})dx_{1:T} + \\int_{x_{1:T}}q(x_{1},...,x_{T}|x_{0})\\log\\frac{p(x_{1},....,x_{T})}{q(x_{1},....,x_{T}|x_{0})}dx_{1: T} \\]\nNow , we should take into account the second term, that it is similar to minimization of KL-divergences, however, we chains in opposite directions. Then, one can represent forward Markov chains transformation probabilities in opposite direction via \\(\\textbf{Bayes Theorem}\\)\n\\(\\textbf{My desire:}\\) i would like to represent the forward markov chain as:\n\\(q(x_{1},...,x_{T}|x_{0}) = q(x_{T}|x_{0})q(x_{T-1}|x_{T},x_{0})q(x_{T-2}|x_{T-1},x_{0})...q(x_{1}|x_{2},x_{0})\\)\n\\(\\textbf{Bayes theorem:}\\)\n\\[ q(x_{t-1}|x_{t},x_{0}) = \\frac{q(x_{t}|x_{t-1},x_{0})q(x_{t-1}|x_{0})}{q(x_{t}|x_{0})}\\]\nClarifications:\n\nI have Markovian process,hence, there is no dependences on $x_{0} q(x_{t}|x_{t-1},x_{0}) q(x_{t}|x_{t-1}) $\nAll distributions are gaussian \\(\\implies\\) one can perform accurate bayesiam inference\n\nAt home, you can substitute corresponding probabilities and get formula for posterior:\n\\[q(x_{t-1}|x_{t},x_{0}) = \\mathcal{N}(x_{t-1}| \\hat{\\mu}_{t}(x_{t},x_{0}),\\hat{\\beta_{t}}I)\\]\n\nMean:\n\n\\[\\hat{\\mu}_{t}(x_{t},x_{0})= \\frac{\\sqrt{\\overline{\\alpha}_{t-1}}}{1 - \\overline{\\alpha_{t}}}\\beta x_{0} +\\frac{\\sqrt{1 - \\beta}(1 - \\overline{\\alpha_{t}} )}{1 - \\overline{\\alpha_{t}}}x_{t}  \\]\n\nVariance:\n\n\\[\\hat{\\beta_{t}} = \\beta_{t}(1 - \\overline{\\alpha_{t-1}}) \\frac{1}{1 - \\overline{\\alpha}_{t}}\\]"
  },
  {
    "objectID": "projects/DDPM_eng.html#the-loss-deriavation",
    "href": "projects/DDPM_eng.html#the-loss-deriavation",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "5. The loss deriavation",
    "text": "5. The loss deriavation\nNow, we return to ELBO:\n\\[ \\int q(x_{1}|x_{0}) \\log p(x_{0}|x_{1})dx_{1} + \\int q(x_{T}|x_{0})...q(x_{1}|x_{2},x_{0})\\log \\frac{p(x_{1}|x_{2})}{q(x_{1}|x_{2},x_{0})}\n\\frac{p(x_{T-1}|x_{T})}{q(x_{T-1}|x_{T},x_{0})}\\frac{....}{....} \\frac{p(x_{T})}{q(x_{T}|x_{0})}\\]\nThen:\n\\[ - \\sum_{t=1}^{T} \\mathbb{E}_{x_{1},..,x_{T}} KL(q(x_{t-1}|x_{t},x_{0}) || p(x_{t-1}|x_{t}))) - KL(q(x_{T}|x_{0}||p(x_{T})) + \\int q(x_{1}|x_{0}) \\log p(x_{0}|x_{1})dx_{1}\\]\n\nIf \\(x_{1} \\approx x_{0}\\), then const\nAs for certain KL in terminate state ?\n\n\\(\\textbf{Idea}\\): Minimization of KL divergences\n\nfor simplicity: \\(p_{\\theta}(x_{t-1}|x_{t}) = \\mathcal{N}(\\mu_{\\theta}(x_{t},t),\\hat{\\beta_{t}}I)\\)\nMean matching:\n\n\\[ KL(q(x_{t-1}|x_{t},x_{0})|| p_{\\theta}(x_{t-1}|x_{t})) = \\mathbb{E}_{x_{0},x_{1},...,x_{T},t}\\frac{1}{2\\hat{\\beta_{t}}}|| \\hat{\\mu}_{t}(x_{t},x_{0}) - \\mu_{\\theta}(x_{t},t)||_{2}^{2}\\]\n\n\n\nChessUrl"
  },
  {
    "objectID": "projects/DDPM_eng.html#re-parameterization-like-simplification",
    "href": "projects/DDPM_eng.html#re-parameterization-like-simplification",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "6. Re-parameterization like simplification",
    "text": "6. Re-parameterization like simplification\nWe would like to approximate \\(\\mu_{\\theta}\\) by \\(\\hat{\\mu_{t}}\\), but there is the problem! \\(\\hat{\\mu_{t}}(x_{0})\\), while trainable mean does not depend on \\(x_{0}\\). As a consequence of that, we cannot converge to 0 this problem:\n\\(\\textbf{Re-parametrization:}\\)\n\\[\\hat{\\mu}_{t}(x_{t},x_{0})= \\frac{\\sqrt{\\overline{\\alpha}_{t-1}}}{1 - \\overline{\\alpha_{t}}}\\beta x_{0} +\\frac{\\sqrt{1 - \\beta}(1 - \\overline{\\alpha_{t}} )}{1 - \\overline{\\alpha_{t}}}x_{t}  \\]\n\\[ \\mu_{theta}(x_{t},t) = \\frac{\\sqrt{\\overline{\\alpha}_{t-1}}}{1 - \\overline{\\alpha_{t}}}\\beta \\color{red}{x_{\\theta}(x_{t},t)} +\\frac{\\sqrt{1 - \\beta}(1 - \\overline{\\alpha_{t}} )}{1 - \\overline{\\alpha_{t}}}x_{t} \\]\n\nRed term is as estimatiomn for \\(x_{0}\\)\n\nThen:\n\\[\\frac{1}{2\\hat{\\beta}_{t}}||\\hat{\\mu_{t}}(x_{t},x_{0}) - \\mu_{\\theta}(x_{t},t)||_{2}^{2}\n= \\frac{1}{2\\hat{\\beta}_{t}} || \\frac{\\sqrt{\\overline{\\alpha}_{t-1}}}{1-\\overline{\\alpha_{t}}}\\beta (x_{0} - x_{\\theta}(x_{t},t))||^{2}_{2}\\]\n\\(\\textbf{Final reparamterization:}\\)\n\\[ x_{t} = \\sqrt{\\overline{\\alpha}_{t}}x_{0} + \\sqrt{1 - \\overline{\\alpha}_{t}}\\hat{\\epsilon} \\]\nThen, one can express:\n\\[x_{0} = \\frac{1}{\\sqrt{\\overline{\\alpha_{t}}}}(x_{t} - \\sqrt{1 - \\overline{\\alpha}_{t}}\\hat{\\epsilon} ) \\]\nThe, one can reparametrize predicted value as:\n\\[x_{\\theta} = \\frac{1}{\\sqrt{\\overline{\\alpha_{t}}}}(x_{t} - \\sqrt{1 - \\overline{\\alpha}_{t}} \\epsilon_{\\theta}(x_{t},t) \\]\nThus, the final loss function:\n\\[ \\mathcal{L}(\\theta) = \\sum_{i=1}^{n}\\sum_{t=2}^{T} \\frac{\\beta^{2}}{2\\hat{\\beta}_{t}(1-\\beta)(1- \\overline{\\alpha}_{t})} ||  \\frac{x^{i}_{t} - \\sqrt{\\overline{\\alpha}}_{t}x_{0}^{i}}{\\sqrt{1-\\overline{\\alpha}}_{t}} - \\epsilon_{\\theta}(x^{i}_{t},t) ||\\]"
  },
  {
    "objectID": "projects/DDPM_eng.html#coding-part",
    "href": "projects/DDPM_eng.html#coding-part",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "7. Coding part",
    "text": "7. Coding part\n\nimport torch\nfrom torch.nn import init\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import TensorDataset\nfrom sklearn.datasets import make_moons\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n7.1 models\nIt is importantly to point out that our neural network will predict \\(\\textbf{noise}\\).\nOur model is neural network, that has 2 inputs:\n\n\\(x_{t}\\) data\n\\(t\\) time like condition for more accurate prediction noise of \\(x_{t}\\)\n\nIt is worth to notice that we have the same model for each step , we do not train new model for each new step. \\(\\color{red}{One\\quad model\\quad for\\quad all\\quad steps!}\\)\nAlso, You should understand that \\(t\\) is like a value condition and in order to estimate his influence to corresponding noise as a condition , it would be great to create embeddings of the time by corresponding network below.\n\nclass SinusoidalEmbedding(nn.Module):\n    def __init__(self, size: int, scale: float = 1.0):\n        super().__init__()\n        self.size = size\n        self.scale = scale\n\n    def forward(self, x: torch.Tensor):\n        x = x * self.scale\n        half_size = self.size // 2\n        emb = torch.log(torch.Tensor([10000.0])) / (half_size - 1)\n        emb = torch.exp(-emb * torch.arange(half_size))\n        emb = x.unsqueeze(-1) * emb.unsqueeze(0)\n        emb = torch.cat((torch.sin(emb), torch.cos(emb)), dim=-1)\n        return emb\n\n    def __len__(self):\n        return self.size\n\n\nclass PositionalEmbedding(nn.Module):\n    def __init__(self, size: int, type: str, **kwargs):\n        super().__init__()\n\n        self.layer = SinusoidalEmbedding(size, **kwargs)\n\n    def forward(self, x: torch.Tensor):\n        return self.layer(x)\n\n\nclass Block(nn.Module):\n    def __init__(self, size: int):\n        super().__init__()\n\n        self.ff = nn.Linear(size, size)\n        self.act = nn.GELU()\n\n    def forward(self, x: torch.Tensor):\n        return x + self.act(self.ff(x))\n\n\nclass MLP(nn.Module):\n    def __init__(self, hidden_size: int = 128, hidden_layers: int = 3, emb_size: int = 128,\n                 time_emb: str = \"sinusoidal\", input_emb: str = \"sinusoidal\"):\n        super().__init__()\n\n        self.time_mlp = PositionalEmbedding(emb_size, time_emb)\n        self.input_mlp1 = PositionalEmbedding(emb_size, input_emb, scale=25.0)\n        self.input_mlp2 = PositionalEmbedding(emb_size, input_emb, scale=25.0)\n\n        concat_size = len(self.time_mlp.layer) + \\\n            len(self.input_mlp1.layer) + len(self.input_mlp2.layer)\n        layers = [nn.Linear(concat_size, hidden_size), nn.GELU()]\n\n        for _ in range(hidden_layers):\n            layers.append(Block(hidden_size))\n\n        layers.append(nn.Linear(hidden_size, 2))\n        self.joint_mlp = nn.Sequential(*layers)\n\n    def forward(self, x, t):\n        x1_emb = self.input_mlp1(x[:, 0])\n        x2_emb = self.input_mlp2(x[:, 1])\n        t_emb = self.time_mlp(t)\n        x = torch.cat((x1_emb, x2_emb, t_emb), dim=-1)\n        x = self.joint_mlp(x)\n        return x\n\n\n\n7.2 Data\nWe use simple two moons dataset as data.\n\ndef moons_dataset(n=8000):\n    X, _ = make_moons(n_samples=n, random_state=42, noise=0.03)\n    X[:, 0] = (X[:, 0] + 0.3) * 2 - 1\n    X[:, 1] = (X[:, 1] + 0.3) * 3 - 1\n    return TensorDataset(torch.from_numpy(X.astype(np.float32)))\n\n\n\n7.3 Gaussian Diffusion\nThis calss is composed of whole formulas from senimar , please familiari`e you with these formulas accurately.\n\nreconstruct x0:\n\n\\[x_{0} = \\frac{1}{\\sqrt{\\overline{\\alpha_{t}}}}(x_{t} - \\sqrt{1 - \\overline{\\alpha}_{t}}\\hat{\\epsilon} ) \\]\n\nq posterior:\n\n\\[\\hat{\\mu}_{t}(x_{t},x_{0})= \\frac{\\sqrt{\\overline{\\alpha}_{t-1}}}{1 - \\overline{\\alpha_{t}}}\\beta x_{0} +\\frac{\\sqrt{1 - \\beta}(1 - \\overline{\\alpha_{t}} )}{1 - \\overline{\\alpha_{t}}}x_{t}  \\]\n\nget_variance:\n\n\\[\\hat{\\beta_{t}} = \\beta_{t}(1 - \\overline{\\alpha_{t-1}}) \\frac{1}{1 - \\overline{\\alpha}_{t}}\\]\n\nstep:\n\n\nThe sampling moments of time to get \\(x_{t}\\)\nMake the prediction \\(x_{0}\\)\nMake the prediction for \\(x_{t-1}\\)\nCalculate noise\nMake one step of the backward process\n\n\nadd noise:\n\n\nThe expression noise through \\(x_{0}\\) and \\(x_{t}\\)\n\\(\\hat{\\epsilon} =  \\sqrt{\\overline{\\alpha_{t}}}\\frac{x_{0}}{\\sqrt{1 -\\overline{\\alpha_{t}}  }} - x_{t}\\frac{1}{\\sqrt{1- \\overline{\\alpha_{t}}}}\\)\n\n\nclass NoiseScheduler():\n    def __init__(self,\n                 num_timesteps=1000,\n                 beta_start=0.0001,\n                 beta_end=0.02,\n                 beta_schedule=\"linear\"):\n\n        self.num_timesteps = num_timesteps\n        if beta_schedule == \"linear\":\n            self.betas = torch.linspace(\n                beta_start, beta_end, num_timesteps, dtype=torch.float32)\n        elif beta_schedule == \"quadratic\":\n            self.betas = torch.linspace(\n                beta_start ** 0.5, beta_end ** 0.5, num_timesteps, dtype=torch.float32) ** 2\n\n        self.alphas = 1.0 - self.betas\n        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n        self.alphas_cumprod_prev = F.pad(\n            self.alphas_cumprod[:-1], (1, 0), value=1.)\n\n        # required for self.add_noise\n        self.sqrt_alphas_cumprod = self.alphas_cumprod ** 0.5\n        self.sqrt_one_minus_alphas_cumprod = (1 - self.alphas_cumprod) ** 0.5\n\n        # required for reconstruct_x0\n        self.sqrt_inv_alphas_cumprod = torch.sqrt(1 / self.alphas_cumprod)\n        self.sqrt_inv_alphas_cumprod_minus_one = torch.sqrt(\n            1 / self.alphas_cumprod - 1)\n\n        # required for q_posterior\n        self.posterior_mean_coef1 = self.betas * torch.sqrt(self.alphas_cumprod_prev) / (1. - self.alphas_cumprod)\n        self.posterior_mean_coef2 = (1. - self.alphas_cumprod_prev) * torch.sqrt(self.alphas) / (1. - self.alphas_cumprod)\n\n\n    def reconstruct_x0(self, x_t, t, noise):\n        s1 = self.sqrt_inv_alphas_cumprod[t]\n        s2 = self.sqrt_inv_alphas_cumprod_minus_one[t]\n        s1 = s1.reshape(-1, 1)\n        s2 = s2.reshape(-1, 1)\n        return s1 * x_t - s2 * noise\n\n    def q_posterior(self, x_0, x_t, t):\n        s1 = self.posterior_mean_coef1[t]\n        s2 = self.posterior_mean_coef2[t]\n        s1 = s1.reshape(-1, 1)\n        s2 = s2.reshape(-1, 1)\n        mu = s1 * x_0 + s2 * x_t\n        return mu\n\n    def get_variance(self, t):\n        if t == 0:\n            return 0\n\n        variance = self.betas[t] * (1. - self.alphas_cumprod_prev[t]) / (1. - self.alphas_cumprod[t])\n        variance = variance.clip(1e-20)\n        return variance\n\n    def step(self, model_output, timestep, sample):\n        t = timestep\n        pred_original_sample = self.reconstruct_x0(sample, t, model_output)\n        pred_prev_sample = self.q_posterior(pred_original_sample, sample, t)\n\n        variance = 0\n        if t &gt; 0:\n            noise = torch.randn_like(model_output)\n            variance = (self.get_variance(t) ** 0.5) * noise\n\n        pred_prev_sample = pred_prev_sample + variance\n\n        return pred_prev_sample\n\n    def add_noise(self, x_start, x_noise, timesteps):\n        s1 = self.sqrt_alphas_cumprod[timesteps]\n        s2 = self.sqrt_one_minus_alphas_cumprod[timesteps]\n\n        s1 = s1.reshape(-1, 1)\n        s2 = s2.reshape(-1, 1)\n\n        return s1 * x_start + s2 * x_noise\n\n    def __len__(self):\n        return self.num_timesteps\n\n\n\n7.4 Training\nWe choose hyper parameters for method and run it\n\nNUM_SAMPLES_DATA = 10_000\nBATCH_SIZE=128\n\nHIDDEN_SIZE = 128\nHIDDEN_LAYERS=3\nEMBEDDING_SIZE = 128\nTIME_EMBEDDING=\"sinusoidal\"\nINPUT_EMEDDING=\"sinusoidal\"\n\nNUM_TIMESTEPS = 50\nBETA_SCHEDULE = 'linear'\nLR = 5e-4\n\nNUM_EPOCHS=200\n\n\ndataset = moons_dataset(NUM_SAMPLES_DATA)\ndataloader = torch.utils.data.DataLoader(\n        dataset, batch_size=BATCH_SIZE , shuffle=True, drop_last=True)\n\nmodel = MLP(\n        hidden_size=HIDDEN_SIZE,\n        hidden_layers=HIDDEN_LAYERS,\n        emb_size=EMBEDDING_SIZE,\n        time_emb=TIME_EMBEDDING,\n        input_emb=INPUT_EMEDDING)\n\nnoise_scheduler = NoiseScheduler(\n        num_timesteps=NUM_TIMESTEPS,\n        beta_schedule=BETA_SCHEDULE)\n\noptimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr=LR,\n    )\n\n\nglobal_step = 0\nframes = []\nlosses = []\n\nfor epoch in tqdm(range(NUM_EPOCHS)):\n\n    model.train()\n\n\n    for step, batch in enumerate(dataloader):\n        batch = batch[0]\n        noise = torch.randn(batch.shape)\n        timesteps = torch.randint(a\n            0, noise_scheduler.num_timesteps, (batch.shape[0],)\n        ).long()\n\n        noisy = noise_scheduler.add_noise(batch, noise, timesteps)\n        noise_pred = model(noisy, timesteps)\n        loss = F.mse_loss(noise_pred, noise)\n        loss.backward(loss)\n\n        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        optimizer.zero_grad()\n        losses.append(loss.detach().item())\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:07&lt;00:00,  1.56it/s]\n\n\n\nmodel.eval()\nsample = torch.randn(1024, 2) # sampling from noise\ntimesteps = list(range(len(noise_scheduler)))[::-1]\nfor i, t in enumerate(tqdm(timesteps)):\n    t = torch.from_numpy(np.repeat(t,  1024)).long()\n    with torch.no_grad():\n        residual = model(sample, t)\n    sample = noise_scheduler.step(residual, t[0], sample)\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00&lt;00:00, 104.94it/s]\n\n\n\nplt.scatter(sample[:,0],sample[:,1], edgecolor='black', label=\"generated data\")\nplt.grid();\nplt.legend();\n\n\n\n\n\n\n\n\n\nmodel.eval()\ntraj = []\nsample = torch.randn(1024, 2) # sampling from noise\ntimesteps = list(range(len(noise_scheduler)))[::-1]\nfor i, t in enumerate(tqdm(timesteps)):\n    t = torch.from_numpy(np.repeat(t,  1024)).long()\n    with torch.no_grad():\n        residual = model(sample, t)\n    sample = noise_scheduler.step(residual, t[0], sample)\n    if t[0].item() % 5 == 0:\n        traj.append(sample.cpu())\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00&lt;00:00, 118.63it/s]\n\n\n\nfig,ax = plt.subplots(1,10,figsize=(40,4),dpi=200)\nfor idx in range(10):\n    ax[idx].scatter(traj[9-idx][:,0],traj[9-idx][:,1],edgecolor='black')\n    ax[idx].set_xticks([]);ax[idx].set_yticks([])\n    ax[idx].grid();\nfig.tight_layout(pad=0.01)"
  },
  {
    "objectID": "projects/langevien.html",
    "href": "projects/langevien.html",
    "title": "Score Estimations, Langevin Generative models and NCSN",
    "section": "",
    "text": "By Addisu Amare\n\\(\\textbf{Date}:\\) 08.11.24\nimport torch\nfrom torch.distributions import MultivariateNormal, Normal\nimport numpy as np\nimport tqdm\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\n%matplotlib inline"
  },
  {
    "objectID": "projects/langevien.html#teachers---ground-truth-data",
    "href": "projects/langevien.html#teachers---ground-truth-data",
    "title": "Score Estimations, Langevin Generative models and NCSN",
    "section": "Teachers - Ground-Truth data",
    "text": "Teachers - Ground-Truth data\nThe \\(\\textbf{Teacher}\\) is the class of the density distribution, which we want to approximate by neural-network-based approaches. This class has the following methods:\n\nsample(sampling from the density distribution)\nlog_prob(calculation of the logarithm of the distribution)\n\nUndoubtedly, in order to calculate the ground-truth logarithm of the distribution we define the distribution by such way to analytically compute the gradient of the logartihm. Thus, one can take the Teacher, which is the ground-truth distribution thrrough the notebook, as follows:\n\\[\\mathbb{P} = \\frac{1}{5}\\mathcal{N}((-5,-5),I) + \\frac{4}{5}\\mathcal{N}((5,5),I)\\]\n\nclass GMMDist(object):\n    def __init__(self, dim):\n        self.mix_probs = torch.tensor([0.8, 0.2])\n        self.means = torch.stack([5 * torch.ones(dim), -torch.ones(dim) * 5], dim=0)\n        self.sigma = 1\n        self.std = torch.stack([torch.ones(dim) * self.sigma for i in range(len(self.mix_probs))], dim=0)\n\n    def sample(self, n):\n        \"\"\"\n        n - int\n        \"\"\"\n        n = torch.Size([n])[0]\n        mix_idx = torch.multinomial(self.mix_probs, n, replacement=True)\n        means = self.means[mix_idx]\n        stds = self.std[mix_idx]\n        return torch.randn_like(means) * stds + means\n\n    def log_prob(self, samples):\n        \"\"\"\n        samples - torch.Size([B,N])\n        \"\"\"\n        logps = []\n        for i in range(len(self.mix_probs)):\n            logps.append((-((samples - self.means[i]) ** 2).sum(dim=-1) / (2 * self.sigma ** 2) - 0.5 * np.log(\n                2 * np.pi * self.sigma ** 2)) + self.mix_probs[i].log())\n        logp = torch.logsumexp(torch.stack(logps, dim=0), dim=0)\n        return logp\n\n\ndef plot_teachers(teacher, num_samples):\n    \"\"\"\n    num_samples - int\n\n    \"\"\"\n    plt.figure(figsize=(4,4),dpi=150 )\n    samples = teacher.sample(num_samples)\n    plt.scatter(samples[:,0],samples[:,1],s=10,edgecolor='black')\n    plt.grid()\n\n\nDIM = 2\nNUM_SAMPLES_PLOT = 2000\nteacher =  GMMDist(DIM)\nplot_teachers(teacher,  NUM_SAMPLES_PLOT)\n\n\n\n\n\n\n\n\n\ndef data_score(x):\n    x = x.detach()\n    x.requires_grad_(True)\n    y = teacher.log_prob(x).sum()\n    return torch.autograd.grad(y, x)[0]\n\n\ndef toy_net(hiddens):\n    model = []\n    for inp,outp in zip(hiddens[:-1],hiddens[1:]):\n        model.append(torch.nn.Linear(inp,outp,bias=True))\n        model.append(torch.nn.ReLU())\n    model.pop()\n    return torch.nn.Sequential(*model)"
  },
  {
    "objectID": "projects/langevien.html#score-based-genereative-modeling",
    "href": "projects/langevien.html#score-based-genereative-modeling",
    "title": "Score Estimations, Langevin Generative models and NCSN",
    "section": "1. Score-based genereative modeling",
    "text": "1. Score-based genereative modeling\n\ndef plot_loss_quiever(model,teacher, label = None):\n\n    grid_size = 20\n    left_bound=-3\n    right_bound=3\n    mesh = []\n    x = np.linspace(left_bound, right_bound, grid_size)\n    y = np.linspace(left_bound, right_bound, grid_size)\n    for i in x:\n        for j in y:\n            mesh.append(np.asarray([i, j]))\n\n    mesh = np.stack(mesh, axis=0)\n    mesh = torch.from_numpy(mesh).float()\n\n    if label == \"energy\":\n        mesh.requires_grad = True\n        scores = model(mesh)\n        scores = torch.autograd.grad(scores.sum(),mesh,create_graph=True)[0]\n    else:\n        scores = model(mesh.detach())\n\n    mesh = mesh.detach().numpy()\n    scores = scores.detach().numpy()\n    fig,ax = plt.subplots(1,2,figsize=(16,8),sharex=True,sharey=True)\n\n    ax[0].grid(False)\n    ax[0].quiver(mesh[:, 0], mesh[:, 1], scores[:, 0], scores[:, 1], width=0.005)\n    ax[0].set_title('Estimated scores', fontsize=16)\n    ax[0].axis('square')\n\n    scores = data_score(torch.from_numpy(mesh))\n    scores = scores.detach().numpy()\n\n    ax[1].grid(False)\n    ax[1].quiver(mesh[:, 0], mesh[:, 1], scores[:, 0], scores[:, 1], width=0.005)\n    ax[1].set_title('Data scores', fontsize=16)\n    ax[1].axis('square')\n\n\n1.1 Fischer divergence\n\\(\\textbf{Notation}:\\)\n\n\\(x \\in \\mathbb{R}^{D}, \\quad x \\sim \\mathbb{P}(\\cdot)\\)\nThe model for approximation of \\(\\mathbb{P}\\) is \\(\\mathbb{\\hat{P}}(x, \\theta) = \\frac{1}{Z(\\theta)}\\mathbb{Q}(x,\\theta)\\), unnormalized density\nDesires: One would like to find out such parameters to apprroximate the ground-truth distribution better.\n\\(\\mathbb{\\hat{P}}(x, \\theta)\\) is reffered to as Energy-based models.\n\nConsidering the gradient of the approximation:\n\\[ \\nabla_{x} \\log \\mathbb{\\hat{P}}(x,\\theta) = \\nabla_{x} \\log \\mathbb{Q}(x,\\theta) - \\nabla_{x} \\log Z(\\theta) = \\nabla_{x} \\log \\mathbb{Q}(x,\\theta) - 0 \\]\nWe see, that in order to estimate the gradient of logarithm of approximate distribution, we need in the gradient of unnormalized density \\(\\mathbb{Q}(\\cdot, \\theta)\\). Such gradient of unnormalized density is called \\(\\textbf{Score function}\\) and is denoted as \\(\\psi(x,\\theta)\\).\n\\(\\textbf{The main goal of notebook}:\\)\nWe would like to estimate the gradient of logarithm of the ground-truth density \\(\\mathbb{P}\\) by the score function.\nUndoubtedly, when we have the access to the ground-truth grradient of logarithm of density, we can consider easy regression problem between \\(\\mathbb{Q}\\) and \\(\\mathbb{P}\\). This easy regression prroblem is reffered to as \\(\\textbf{Fischer divergence}\\).\nLet \\(p(x)\\) and \\(q(x)\\) are ground-truth distribution of data and unnormalized approximate distribution. The both functions are scalar functios. \\[ F(q||p)=\\frac{1}{2}\\int || - \\nabla_{x}\\log q(x) + \\nabla_{x} \\log p(x) ||_{2}^{2}dp(x)  \\]\n\n\n\nChessUrl\n\n\n\\(\\textbf{Code}\\) for the Fischer divergence.\n\ndef fischer_divergence(energy_net, data, teacher):\n\n    \"\"\"\n    energy_net - torch.nn.Module\n    teacher    - object\n    data.      - torch.Size([B,N])\n    \"\"\"\n\n    data.requires_grad = True\n    logq = -energy_net(data) # torch.Size([B,1])\n    logp = teacher.log_prob(data)# torch.Size([B,1])\n    q_score = torch.autograd.grad(logq.sum(), data,\n                                   create_graph=True,retain_graph=True)[0]\n    p_score = torch.autograd.grad(logp.sum(), data,\n                                 create_graph=True,retain_graph=True)[0]\n    return 0.5*torch.mean(torch.norm((q_score + p_score)**2,dim=-1))\n\n\nHIDDENS = [DIM,64,128,256,128,64,1]\nmodel = toy_net(HIDDENS)\nopt = torch.optim.Adam(model.parameters(),lr=1e-5)\nbatch_size=2048\niterations=5000\nlosses = []\nfor _ in tqdm.tqdm(range(iterations)):\n    opt.zero_grad()\n    samples = teacher.sample(batch_size)\n    loss = fischer_divergence(model,samples,teacher)\n    loss.backward()\n    opt.step()\n    losses.append(loss.item())\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [03:37&lt;00:00, 22.95it/s]\n\n\n\nplt.plot(losses);\n\n\n\n\n\n\n\n\n\nplot_loss_quiever(model, teacher, label='energy')\n\n\n\n\n\n\n\n\n\n\n1.2 Score Matching and Estimation.\n\\(\\textbf{Importantly}:\\)\nWhen we talk \\(\\textbf{Score Estimation}\\), it means, that we solve the regression problem between \\(\\psi(x,\\theta)\\), where \\(\\psi(x,\\theta)\\) is a neural network. When we talk \\(\\textbf{Score Matching}\\), it means, that we solve the regression problem between \\(\\psi(x,\\theta)\\), where \\(\\psi(x,\\theta)\\) is \\(\\nabla_{x} \\log q(x,\\theta)\\) and \\(q(x,\\theta)\\) is a neural-network.\n\n\n\nalt text\n\n\n\\(\\textbf{Theorem 1}:\\) Let \\(\\psi(x,\\theta): \\mathbb{R}^{D} \\to \\mathbb{R}^{D}\\) is a regular differetiable function. Having defined the score function as follows :\n\\[\\psi_{i}(x,\\theta) = \\frac{\\partial \\log q(x,\\theta)}{\\partial x_{i}}, \\quad \\frac{\\partial \\psi(x,\\theta)}{\\partial x_{i}} = \\frac{\\partial^{2} \\log q(x,\\theta)}{\\partial x_{i}^{2}} \\]\nThen:\n\\[\\mathcal{J}(\\theta) = \\int_{\\mathbb{R}^{D}} \\sum_{i=1}^{D} \\{ \\frac{\\partial \\psi(x,\\theta)}{\\partial x_{i}}+ \\frac{1}{2}\\psi(x,\\theta)^{2}\\}d\\mathbb{P}(x) + Const.\\]\n\\(\\textbf{proof}:\\) See the Seminar\n\ndef score_matching(energy_net, data):\n    \"\"\"\n    energy_net - torch.nn.Module\n    data       - torch.Size([B,N])\n    \"\"\"\n    data.requires_grad = True\n    logq = -energy_net(data) # torch.Size([B,1])\n    score = torch.autograd.grad(logq.sum(), data,\n                                create_graph=True, retain_graph=True)[0]# torch.Size([B,N])\n\n    loss1 = 0.5*torch.norm(score,dim=-1)**2 # torch.Size([B])\n\n    grad_score = torch.autograd.grad(score.sum(dim=-1).sum() , data,\n                                     create_graph=True, retain_graph=True)[0] #torch.Size([B,N])\n\n    loss2 = grad_score.sum(dim=-1)#torch.Size([B])\n\n    return torch.mean(loss1 + loss2 ,dim = 0)\n\n\ndef score_estimation(score_net, data):\n    \"\"\"\n    energy_net - torch.nn.Module\n    data       - torch.Size([B,N])\n    \"\"\"\n    data.requires_grad = True\n    score = score_net(data)#torch.Size([B,N])\n    loss1 =  0.5*torch.norm(score,dim=-1)**2 # torch.Size([B])\n    grad_score = torch.autograd.grad(score.sum(),data,\n                                    create_graph=True,retain_graph=True)[0]\n    loss2 = grad_score.sum(dim=-1)# torch.Size([B])\n    return torch.mean(loss1 + loss2 ,dim = 0)\n\n\nHIDDENS = [DIM,64,128,256,128,64,2]\nmodel = toy_net(HIDDENS)\nopt = torch.optim.Adam(model.parameters(),lr=1e-5)\nbatch_size=2048\niterations=5000\nlosses = []\nfor _ in tqdm.tqdm(range(iterations)):\n    opt.zero_grad()\n    samples = teacher.sample(batch_size)\n    loss =  score_estimation(model,samples)\n    loss.backward()\n    opt.step()\n    losses.append(loss.item())\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [03:28&lt;00:00, 24.04it/s]\n\n\n\nplt.plot(losses);\n\n\n\n\n\n\n\n\n\nplot_loss_quiever(model,teacher)\n\n\n\n\n\n\n\n\n\n\n1.3 Denoising Score matching and estimation\nLet‚Äôs consider \\(\\mathbb{P}_{\\sigma}\\) is as a perturbed data distribution of \\(\\mathbb{P}_{\\sigma}\\). First of all, we pick out the noise scale \\(\\sigma\\) manually.\n\n\\(\\mathbb{q}_{\\sigma}(\\tilde{x}|x) = \\mathcal{N}(\\tilde{x}|x,\\sigma^{2}I)\\) - conditional perturbed distribution.\n\\(\\mathbb{P}_{\\sigma}(\\tilde{x},x) =\\mathbb{q}_{\\sigma}(\\tilde{x}|x)\\mathbb{P}(x)\\) - joint distribution.\n\\(\\mathbb{P}_{\\sigma}^{m}(\\tilde{x}) = \\int \\mathbb{P}_{\\sigma}(\\tilde{x},x) dx\\) - marginal distribution.\n\n\\(\\textbf{Theorem 2:}\\) Let \\(\\mathbb{P}(x)\\) is the gound-truth distribution, while \\(\\psi(x,\\theta): \\mathbb{R}^{D} \\to \\mathbb{R}^{D}\\) is learnable scorre function and tries to approximate the gradient of logarothm of \\(\\mathbb{P}\\). Then: \\[\\mathbb{E}_{\\mathbb{P}(x)} \\{ \\frac{1}{2}||\\psi(x,\\theta) - \\frac{\\partial}{\\partial x} \\log \\mathbb{P}(x) ||_{2}^{2}\\} \\sim \\mathbb{E}_{\\mathbb{P}_{\\sigma}(x,\\tilde{x})}\\{\\frac{1}{2}||\\psi(\\tilde{x},\\theta) - \\frac{\\partial}{\\partial \\tilde{x}} \\log \\mathbb{P}_{\\sigma}(\\tilde{x}|x) ||_{2}^{2}\\}\\]\n\\(\\textbf{Proof}:\\) On seminar\n\ndef denoising_score_matching( score_net, samples, sigma):\n\n    \"\"\"\n    score_net - torch.nn.module\n    samples   - torch.Size([B,N])\n    sigma     - int\n    \"\"\"\n\n    samples.requires_grad = True\n    vector = torch.randn_like(samples, device = samples.device)*sigma\n    perturbed_samples = samples + vector\n    logp = - score_net(perturbed_samples)\n    dlogp = sigma**2*torch.autograd.grad(logp.sum(), perturbed_samples,\n                                         create_graph=True, retain_graph=True)[0]\n    kernel = vector\n    return 0.5*torch.mean(torch.norm(dlogp + kernel, dim=-1)**2)\n\n\ndef denoising_score_estimation( score_net, samples, sigma):\n\n    \"\"\"\n    score_net - torch.nn.module\n    samples   - torch.Size([B,N])\n    sigma     - int\n    \"\"\"\n\n    perturbed_samples = samples + torch.randn_like(samples, device = samples.device)*sigma\n    score = score_net(perturbed_samples)\n    dlogq =  1/sigma**2*(samples - perturbed_samples )\n    return 0.5*torch.mean(torch.norm((score - dlogq)**2,dim=-1))\n\n\nmodel = toy_net(HIDDENS)\nopt = torch.optim.Adam(model.parameters(),lr=1e-5)\nbatch_size=2048\niterations=5000\nlosses = []\nfor _ in tqdm.tqdm(range(iterations)):\n    opt.zero_grad()\n    samples = teacher.sample(batch_size)\n    loss =  denoising_score_estimation(model,samples,0.01)\n    loss.backward()\n    opt.step()\n    losses.append(loss.item())\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:30&lt;00:00, 166.51it/s]\n\n\n\nplot_loss_quiever(model,teacher)\n\n\n\n\n\n\n\n\n\n\n1.4 Sliced Score matching and estimation\nTo understand Sliced Score Estimation, we recall \\(S_{d}(x)\\) and \\(\\psi(x,\\theta)\\), where the firrst is the ground-truth score function. If we solve any regression problem between \\(S_{d}(x)\\) and \\(\\psi(x,\\theta)\\), then we have the problem between two high-dimensional vectors. One would like to reduce the dimensionality of the problem. The random projection is one of the possible solutions.\n\\[ F(\\mathbb{Q},\\mathbb{P}) = \\frac{1}{2}\\int_{\\mathbb{R}^{D}}||\\psi(x,\\theta) - S_{d}(x) ||_{2}^{2} \\]\nThen, we rewrite the aforementioned expression via projections on random Gaussian vectors:\n\\[ \\mathcal{L}(\\theta) = \\frac{1}{2}\\mathbb{E}_{p_{v}}\\mathbb{E}_{\\mathbb{P}(x)}||v^{T}\\psi(x,\\theta) - v^{T}S_{d}(x)||_{2}^{2},\\]\nwhere \\(p_{v}(v) = \\mathcal{N}(v|0,I)\\), and we imply, that \\(\\mathbb{E}_{p_{v}}||v||^{2}_{2} &lt; \\infty\\)\n\\(\\textbf{Theorem 3}\\): Let \\(\\psi(x,\\theta)\\) is a score function, \\(S_{d}(x)\\) is the ground-truth scorree function, then:\n\\[\\frac{1}{2}\\mathbb{E}_{p_{v}}\\mathbb{E}_{p_{d}}||v^{T}\\psi(x,\\theta) - v^{T}S_{d}(x)||_{2}^{2} = \\frac{1}{2}\\mathbb{E}_{p_{v}}\\mathbb{E}_{p_{d}}(v^{T}\\psi(x,\\theta))^{2} + \\sum_{i=1}^{D}\\mathbb{E}_{p_{d}} v_{i}v^{T} \\times \\frac{\\partial \\psi(x,\\theta)}{\\partial x_{i}}\\]\n\\(\\textbf{Proof}:\\) On seminar\n\ndef sliced_score_estimation(score_net, samples, n_particles=1 ):\n\n    \"\"\"\n\n\n    \"\"\"\n    samples = samples.unsqueeze(0).expand(n_particles,*samples.shape).contiguous().view(-1,*samples.shape[1:])\n    samples.requires_grad = True\n    vectors = torch.randn_like(samples)\n    vectors = vectors / torch.norm(vectors, dim=-1, keepdim=True)\n    score = score_net(samples)\n    loss1 = 0.5*torch.sum(score*vectors, dim=-1)**2 #torch.Size([M*B])\n    loss1 = loss1.view(n_particles, -1).mean(dim=0)\n\n    return loss1.mean()\n\n\ndef sliced_score_estimation_vr(score_net, samples, n_particles=1):\n    \"\"\"\n    Be careful if the shape of samples is not B x x_dim!!!!\n    \"\"\"\n    dup_samples = samples.unsqueeze(0).expand(n_particles, *samples.shape).contiguous().view(-1, *samples.shape[1:])\n    dup_samples.requires_grad_(True)\n    vectors = torch.randn_like(dup_samples)\n\n    grad1 = score_net(dup_samples)\n    gradv = torch.sum(grad1 * vectors)\n    grad2 = torch.autograd.grad(gradv, dup_samples, create_graph=True)[0]\n\n    grad1 = grad1.view(dup_samples.shape[0], -1)\n    loss1 = torch.sum(grad1 * grad1, dim=-1) / 2.\n\n    loss2 = torch.sum((vectors * grad2).view(dup_samples.shape[0], -1), dim=-1)\n\n    loss1 = loss1.view(n_particles, -1).mean(dim=0)\n    loss2 = loss2.view(n_particles, -1).mean(dim=0)\n\n    loss = loss1 + loss2\n    return loss.mean(), loss1.mean(), loss2.mean()\n\n\ndef sliced_score_estimation_own(score_net, samples, n_particles=1):\n\n    samples.requires_grad = True\n    vectors = torch.randn(n_particles, *samples.shape)\n    vectors = vectors / torch.norm(vectors, dim=-1, keepdim=True)\n    score = score_net(samples)\n    loss1 = 0.5*torch.matmul(score.unsqueeze(0),vectors.permute(0,2,1))#torch.Size([M,B,B])\n    loss1 = torch.sum(loss1,dim=-1)**2 # torch.Size([M,B])\n    loss1 = torch.mean(loss1,dim=-1).mean(dim=0)\n\n    # run the code\n    #loss2 =\n\n    return loss1 + loss2\n\n\nmodel = toy_net(HIDDENS)\nopt = torch.optim.Adam(model.parameters(),lr=1e-5)\nbatch_size=2048\niterations=5000\nlosses = []\nfor _ in tqdm.tqdm(range(iterations)):\n    opt.zero_grad()\n    samples = teacher.sample(batch_size)\n    loss,*_ =  sliced_score_estimation_vr(model,samples)\n    loss.backward()\n    opt.step()\n    losses.append(loss.item())\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [03:35&lt;00:00, 23.23it/s]\n\n\n\nplt.plot(losses);\n\n\n\n\n\n\n\n\n\nplot_loss_quiever(model,teacher)\n\n\n\n\n\n\n\n\n\n\nProblem of score estimation\n\n\n\nAlt Text"
  },
  {
    "objectID": "projects/langevien.html#noise-conditional-score-network-ncsn-on-toy-examples",
    "href": "projects/langevien.html#noise-conditional-score-network-ncsn-on-toy-examples",
    "title": "Score Estimations, Langevin Generative models and NCSN",
    "section": "2. Noise Conditional Score Network (NCSN) on toy Examples",
    "text": "2. Noise Conditional Score Network (NCSN) on toy Examples\nOnce we have trained a score-based model \\(S_{\\theta}(x) = \\nabla_{x} \\log \\mathbb{P}(x)\\) we can use an iterative procedure called Langevin dynamics to draw samples from it.\nLangevin dynamics provides an MCMC procedure to sample from a distribution \\(\\mathbb{P}(x)\\) using only its score function the score function. Specifically, it initializes the chain from an arbitrary prior distribution \\(x_{0} \\sim \\pi(x)\\) and then iterates the following\n\\[x_{k+1} = x_{k} + \\epsilon \\nabla_{x} \\log \\mathbb{P}(x) + \\sqrt{2\\epsilon}z, \\quad z \\sim \\mathcal{N}(z|0,I)\\]\nWhen \\(\\epsilon \\to 0\\) as well as \\(K \\to \\infty\\) , \\(x_{k}\\) obtained from the procedure in Langevin Dynamics algorithm converges to a sample from \\(\\mathbb{P}(x)\\) under some regularity conditions. In practice, the error is negligible when \\(\\epsilon\\) is sufficiently small and \\(K\\) is sufficiently large.\n\n\n\nAlt Text\n\n\nNext, we estimate the score function of each noise-perturbed distribution \\(\\nabla \\log \\mathbb{P}{\\sigma_{i}}(x)\\), by training a Noise Conditional Score-Based Model \\(S(x_{i},\\sigma_{i})\\),with score matching, such that:\n\\[S(x_{i},\\sigma_{i}) = \\nabla_{x} \\log \\mathbb{P}_{\\sigma}(x_{i})\\]\n\n\n\nAlt Text\n\n\nThe training objective for \\(S_{\\theta}(x_{i},\\sigma_{i})\\) is a weighted sum of Fisher divergences for all noise scales. In particular, we use the objective below.\n\\[ \\sum_{i=1}^{L} \\lambda(i) \\mathbb{E}_{\\mathbb{P}_{\\sigma_{i}}}||s_{\\theta}(x,\\sigma_{i}) - \\nabla_{x} \\log \\mathbb{P}_{\\sigma_{i}}(x)||_{2}^{2}\\]\nAfter training our noise-conditional score-based model \\(S_{\\theta}(x,\\sigma_{i})\\), we can produce samples from it by running Langevin dynamics for \\(i = L,L-1,...,1\\) in sequence. This method is called annealed Langevin dynamics since the noise scale \\(\\sigma_{i}\\) decreases (anneals) gradually over time.\n\n\n\nAlt Text\n\n\n\ndef visualize(teacher, model, left_bound=-1., right_bound=1., savefig=None, step=None, device=None):\n\n        #---------------------------------------------------#\n        fig,ax = plt.subplots(2,3, figsize=(27,18),sharex=True, sharey=True,dpi=150 )\n\n        mesh = []\n        grid_size = 100\n        x = np.linspace(left_bound, right_bound, grid_size)\n        y = np.linspace(left_bound, right_bound, grid_size)\n        for i in x:\n            for j in y:\n                mesh.append(np.asarray([i, j]))\n\n        mesh = np.stack(mesh, axis=0)\n        mesh = torch.from_numpy(mesh).float()\n        if device is not None:\n            mesh = mesh.to(device)\n\n        logp_true = teacher.log_prob(mesh)\n        logp_true = logp_true.view(grid_size, grid_size).exp()\n\n        ax[0,0].grid(False)\n        ax[0,0].axis('off')\n        ax[0,0].set_title('Data density', fontsize=16)\n        ax[0,0].imshow(np.flipud(logp_true.cpu().numpy()), cmap='inferno')\n\n\n\n        #---------------------------------------------------------#\n\n        grid_size = 20\n        mesh = []\n        x = np.linspace(left_bound, right_bound, grid_size)\n        y = np.linspace(left_bound, right_bound, grid_size)\n        for i in x:\n            for j in y:\n                mesh.append(np.asarray([i, j]))\n\n        mesh = np.stack(mesh, axis=0)\n        mesh = torch.from_numpy(mesh).float()\n        if device is not None:\n            mesh = mesh.to(device)\n\n\n        scores = model(  mesh.detach().to(DEVICE) )\n        mesh = mesh.detach().cpu().numpy()\n        scores = scores.detach().cpu().numpy()\n\n        ax[0,1].grid(False)\n        ax[0,1].axis('off')\n        ax[0,1].axis('square')\n        ax[0,1].quiver(mesh[:, 0], mesh[:, 1], scores[:, 0], scores[:, 1], width=0.005)\n        ax[0,1].set_title('Estimated scores', fontsize=16)\n\n\n\n        #-------------------------------------------------------------#\n\n        samples = teacher.sample(1280)\n        samples = samples.detach().cpu().numpy()\n        ax[0,2].scatter(samples[:, 0], samples[:, 1], s=0.5)\n        ax[0,2].axis('square')\n        ax[0,2].set_title('data samples',fontsize=16)\n        ax[0,2].set_xlim([left_bound, right_bound])\n        ax[0,2].set_ylim([left_bound, right_bound])\n\n\n        #------------------------------------------------------------#\n\n\n        samples_ = torch.rand(1280, 2) * (right_bound - left_bound) + left_bound\n\n        samples = ncsn.langevin_dynamics(model, samples_).detach().numpy()\n        ax[1,0].scatter(samples[:, 0], samples[:, 1], s=0.5)\n        ax[1,0].axis('square')\n        ax[1,0].axis('square')\n        ax[1,0].set_title('Model Langevin dynamics',fontsize=16)\n        ax[1,0].set_xlim([left_bound, right_bound])\n        ax[1,0].set_ylim([left_bound, right_bound])\n\n\n        #-----------------------------------------------------------#\n\n\n        scores = data_score(torch.from_numpy(mesh) )\n        scores = scores.detach().numpy()\n\n        ax[1,1].axis('off')\n        ax[1,1].quiver(mesh[:, 0], mesh[:, 1], scores[:, 0], scores[:, 1], width=0.005)\n        ax[1,1].set_title('True Data scores', fontsize=16)\n        ax[1,1].axis('square')\n\n\n\n        samples = torch.rand(1280, 2) * (right_bound - left_bound) + left_bound\n\n        samples = ncsn.langevin_dynamics(data_score, samples).detach().numpy()\n        ax[1,2].scatter(samples[:, 0], samples[:, 1], s=0.1)\n        ax[1,2].axis('square')\n        ax[1,2].set_title('True Langevin dynamics data',fontsize=16)\n        ax[1,2].set_xlim([left_bound, right_bound])\n        ax[1,2].set_ylim([left_bound, right_bound])\n\n\n        \"\"\"\n        samples = torch.rand(1280, 2) * (right_bound - left_bound) + left_bound\n        sigmas = torch.exp(torch.linspace(np.log(20), 0., 10)).to(DEVICE)\n        labels = torch.linspace(1,10,10).to(DEVICE)\n        samples = ncsn.anneal_langevin_dynamics(ncsn.score, samples.to(DEVICE), sigmas\n                                             , labels).detach().cpu().numpy()\n\n        plt.scatter(samples[:, 0], samples[:, 1], s=0.2)\n        plt.axis('square')\n        plt.title('Right Annealed Langevin dynamics samples')\n        plt.xlim([left_bound, right_bound])\n        plt.ylim([left_bound, right_bound])\n        plt.show()\n        \"\"\"\n        fig.tight_layout()\n        plt.show()\n\n\nclass NCSN(torch.nn.Module):\n\n    def __init__(self, score, teacher, train_steps, lr, batch_size):\n        super().__init__()\n        self.train_steps = train_steps\n        self.score = score\n        self.teacher = teacher\n        self.lr = lr\n        self.batch_size = batch_size\n\n    def langevin_dynamics(self, score, init, lr=0.1, step=1000):\n\n        \"\"\"\n        score - torch.nn.Module\n        init  - torch.Size([B,N])\n        \"\"\"\n        for step in range(step):\n            init = init + score(init)*lr + torch.randn_like(init,device=init.device)*np.sqrt(2*lr)\n        return init\n\n    def anneal_langevin_dynamics(self, score, init, sigmas, lr=0.1, n_steps_each=100):\n\n        \"\"\"\n        score   - space-time torch.nn.Module\n        init    - torch.Size([B,N])\n        sigmas  - List\n        \"\"\"\n        #with torch.no_grad\n        for sigma in sigmas:\n            current_lr = lr*sigma**2/sigmas[-1]**2\n            for step in range(n_steps_each):\n                init = init + 0.5*current_lr*score(init, sigma).detach()\n                init = init + torch.randn_like(init, device=init.device)*np.sqrt(current_lr)\n\n        return init\n\n    def anneal_dsm_score_estimation(self,scorenet, samples, labels, sigmas, anneal_power=2.):\n\n        batch_size = samples.shape[0]\n        samples = samples.repeat(len(sigmas),1).reshape(len(sigmas),-1,samples.shape[-1])\n        perturbed_samples = samples + torch.randn_like(samples)*sigmas.reshape(-1,1,1)\n\n        scores = scorenet( perturbed_samples.reshape(-1,samples.shape[-1]),\n                           labels.view(-1,1).expand( len(sigmas), batch_size).flatten().view(-1) )\n\n        target = -1.*(perturbed_samples.reshape(-1,samples.shape[-1]) - samples.reshape(-1, samples.shape[-1]) )*\\\n                 (sigmas.view(-1,1).expand(len(sigmas),  batch_size).flatten().view(-1,1))**2\n\n        loss = 1/2.*((scores - target)**2).sum(dim = -1)\n\n        loss =  loss*\\\n              (sigmas.view(-1,1).expand(len(sigmas), batch_size).flatten().view(-1) )\n\n        return loss.mean(dim=0)\n\n    def train_(self, iterations = 10000, batch_size = 128):\n\n        \"\"\"\n        hidden_units = 128\n        score = torch.nn.Sequential(\n            torch.nn.Linear(3, hidden_units),\n            torch.nn.Softplus(),\n            torch.nn.Linear(hidden_units, hidden_units),\n            torch.nn.Softplus(),\n            torch.nn.Linear(hidden_units, 2),\n        )\n        \"\"\"\n        losses = []\n        optimizer = torch.optim.Adam(self.score.parameters(), lr=0.001)\n        teacher = GMMDist(dim=2)\n\n        for step in  tqdm(range(iterations)):\n            samples = teacher.sample((batch_size,)).to(DEVICE)\n\n            #loss, *_ = sliced_score_estimation_vr(score, samples, n_particles=1)\n\n            loss = self.anneal_dsm_score_estimation(self.score, samples, labels = torch.linspace(1,10,10).to(DEVICE) ,\n                                                    sigmas=torch.exp(torch.linspace(np.log(20), 0., 10)).to(DEVICE))\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            losses.append(loss.item())\n\n        return self.score, teacher, losses\n\n    def train(self):\n        opt_score = torch.optim.Adam(self.score.parameters(),lr=self.lr)\n        for step in tqdm.tqdm(range(self.train_steps)):\n            samples = self.teacher.sample(self.batch_size)\n            opt_score.zero_grad()\n            loss,*_ = sliced_score_estimation_vr(self.score, samples, n_particles = 1)\n            loss.backward()\n            opt_score.step()\n        visualize(self.teacher, self.score, -8, 8)\n\n\nhidden_units = 128\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(2, hidden_units),\n    torch.nn.Softplus(),\n    torch.nn.Linear(hidden_units, hidden_units),\n    torch.nn.Softplus(),\n    torch.nn.Linear(hidden_units, 2),\n)\nncsn = NCSN(model, teachers[0], train_steps = 1000,lr=1e-3,batch_size=128)\n\n\nDEVICE=\"cpu\"\n\n\nncsn.train()\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:02&lt;00:00, 482.13it/s]\n\n\n\n\n\n\n\n\n\n\n\n\nAlt Text"
  },
  {
    "objectID": "projects/langevien.html#questions",
    "href": "projects/langevien.html#questions",
    "title": "Score Estimations, Langevin Generative models and NCSN",
    "section": "Questions",
    "text": "Questions"
  },
  {
    "objectID": "projects/project-example.html",
    "href": "projects/project-example.html",
    "title": "Project Example",
    "section": "",
    "text": "This project focuses on [briefly describe the project, its goals, and significance].\n\n\n\nObjective 1: [Describe the first objective]\nObjective 2: [Describe the second objective]\n\n\n\n\n[Provide a brief overview of the methods used in the project, including any specific techniques or tools.]\n\n\n\n[Summarize the outcomes of the project, including any findings or results.]\n\n\n\n[Discuss any potential future work or next steps related to the project.]\n\n\n\n[Include any references or citations relevant to the project, if applicable.]"
  },
  {
    "objectID": "projects/project-example.html#objectives",
    "href": "projects/project-example.html#objectives",
    "title": "Project Example",
    "section": "",
    "text": "Objective 1: [Describe the first objective]\nObjective 2: [Describe the second objective]"
  },
  {
    "objectID": "projects/project-example.html#methodology",
    "href": "projects/project-example.html#methodology",
    "title": "Project Example",
    "section": "",
    "text": "[Provide a brief overview of the methods used in the project, including any specific techniques or tools.]"
  },
  {
    "objectID": "projects/project-example.html#outcomes",
    "href": "projects/project-example.html#outcomes",
    "title": "Project Example",
    "section": "",
    "text": "[Summarize the outcomes of the project, including any findings or results.]"
  },
  {
    "objectID": "projects/project-example.html#future-work",
    "href": "projects/project-example.html#future-work",
    "title": "Project Example",
    "section": "",
    "text": "[Discuss any potential future work or next steps related to the project.]"
  },
  {
    "objectID": "projects/project-example.html#references",
    "href": "projects/project-example.html#references",
    "title": "Project Example",
    "section": "",
    "text": "[Include any references or citations relevant to the project, if applicable.]"
  },
  {
    "objectID": "Jupyter_note_book.html",
    "href": "Jupyter_note_book.html",
    "title": "Breast Cancer classification Using Machine Learning Classifier",
    "section": "",
    "text": "# import libraries\nimport pandas as pd # for data manupulation or analysis\nimport numpy as np # for numeric calculation\nimport matplotlib.pyplot as plt # for data visualization\nimport seaborn as sns # for data visualization\nimport pickle #for dumping the model or we can use joblib library\n\n\n# Suppressing Warnings\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "Jupyter_note_book.html#create-dataframe",
    "href": "Jupyter_note_book.html#create-dataframe",
    "title": "Breast Cancer classification Using Machine Learning Classifier",
    "section": "Create DataFrame",
    "text": "Create DataFrame\n\n# create datafrmae\ncancer_df = pd.DataFrame(np.c_[cancer_dataset['data'],cancer_dataset['target']],\n             columns = np.append(cancer_dataset['feature_names'], ['target']))\n\n\n# DataFrame to CSV file\ncancer_df.to_csv('breast_cancer_dataframe.csv')\n\n\n# Head of cancer DataFrame\ncancer_df.head(6) \n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\ntarget\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n0.07871\n...\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n0.0\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n0.05667\n...\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n0.0\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n0.05999\n...\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n0.0\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n0.2597\n0.09744\n...\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n0.0\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n0.1809\n0.05883\n...\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n0.0\n\n\n5\n12.45\n15.70\n82.57\n477.1\n0.12780\n0.17000\n0.1578\n0.08089\n0.2087\n0.07613\n...\n23.75\n103.40\n741.6\n0.1791\n0.5249\n0.5355\n0.1741\n0.3985\n0.12440\n0.0\n\n\n\n\n6 rows √ó 31 columns\n\n\n\n\n# Tail of cancer DataFrame\ncancer_df.tail(6) \n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\ntarget\n\n\n\n\n563\n20.92\n25.09\n143.00\n1347.0\n0.10990\n0.22360\n0.31740\n0.14740\n0.2149\n0.06879\n...\n29.41\n179.10\n1819.0\n0.14070\n0.41860\n0.6599\n0.2542\n0.2929\n0.09873\n0.0\n\n\n564\n21.56\n22.39\n142.00\n1479.0\n0.11100\n0.11590\n0.24390\n0.13890\n0.1726\n0.05623\n...\n26.40\n166.10\n2027.0\n0.14100\n0.21130\n0.4107\n0.2216\n0.2060\n0.07115\n0.0\n\n\n565\n20.13\n28.25\n131.20\n1261.0\n0.09780\n0.10340\n0.14400\n0.09791\n0.1752\n0.05533\n...\n38.25\n155.00\n1731.0\n0.11660\n0.19220\n0.3215\n0.1628\n0.2572\n0.06637\n0.0\n\n\n566\n16.60\n28.08\n108.30\n858.1\n0.08455\n0.10230\n0.09251\n0.05302\n0.1590\n0.05648\n...\n34.12\n126.70\n1124.0\n0.11390\n0.30940\n0.3403\n0.1418\n0.2218\n0.07820\n0.0\n\n\n567\n20.60\n29.33\n140.10\n1265.0\n0.11780\n0.27700\n0.35140\n0.15200\n0.2397\n0.07016\n...\n39.42\n184.60\n1821.0\n0.16500\n0.86810\n0.9387\n0.2650\n0.4087\n0.12400\n0.0\n\n\n568\n7.76\n24.54\n47.92\n181.0\n0.05263\n0.04362\n0.00000\n0.00000\n0.1587\n0.05884\n...\n30.37\n59.16\n268.6\n0.08996\n0.06444\n0.0000\n0.0000\n0.2871\n0.07039\n1.0\n\n\n\n\n6 rows √ó 31 columns\n\n\n\n\n# Information of cancer Dataframe\ncancer_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 569 entries, 0 to 568\nData columns (total 31 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   mean radius              569 non-null    float64\n 1   mean texture             569 non-null    float64\n 2   mean perimeter           569 non-null    float64\n 3   mean area                569 non-null    float64\n 4   mean smoothness          569 non-null    float64\n 5   mean compactness         569 non-null    float64\n 6   mean concavity           569 non-null    float64\n 7   mean concave points      569 non-null    float64\n 8   mean symmetry            569 non-null    float64\n 9   mean fractal dimension   569 non-null    float64\n 10  radius error             569 non-null    float64\n 11  texture error            569 non-null    float64\n 12  perimeter error          569 non-null    float64\n 13  area error               569 non-null    float64\n 14  smoothness error         569 non-null    float64\n 15  compactness error        569 non-null    float64\n 16  concavity error          569 non-null    float64\n 17  concave points error     569 non-null    float64\n 18  symmetry error           569 non-null    float64\n 19  fractal dimension error  569 non-null    float64\n 20  worst radius             569 non-null    float64\n 21  worst texture            569 non-null    float64\n 22  worst perimeter          569 non-null    float64\n 23  worst area               569 non-null    float64\n 24  worst smoothness         569 non-null    float64\n 25  worst compactness        569 non-null    float64\n 26  worst concavity          569 non-null    float64\n 27  worst concave points     569 non-null    float64\n 28  worst symmetry           569 non-null    float64\n 29  worst fractal dimension  569 non-null    float64\n 30  target                   569 non-null    float64\ndtypes: float64(31)\nmemory usage: 137.9 KB\n\n\n\n# Numerical distribution of data\ncancer_df.describe() \n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\ntarget\n\n\n\n\ncount\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n...\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n\n\nmean\n14.127292\n19.289649\n91.969033\n654.889104\n0.096360\n0.104341\n0.088799\n0.048919\n0.181162\n0.062798\n...\n25.677223\n107.261213\n880.583128\n0.132369\n0.254265\n0.272188\n0.114606\n0.290076\n0.083946\n0.627417\n\n\nstd\n3.524049\n4.301036\n24.298981\n351.914129\n0.014064\n0.052813\n0.079720\n0.038803\n0.027414\n0.007060\n...\n6.146258\n33.602542\n569.356993\n0.022832\n0.157336\n0.208624\n0.065732\n0.061867\n0.018061\n0.483918\n\n\nmin\n6.981000\n9.710000\n43.790000\n143.500000\n0.052630\n0.019380\n0.000000\n0.000000\n0.106000\n0.049960\n...\n12.020000\n50.410000\n185.200000\n0.071170\n0.027290\n0.000000\n0.000000\n0.156500\n0.055040\n0.000000\n\n\n25%\n11.700000\n16.170000\n75.170000\n420.300000\n0.086370\n0.064920\n0.029560\n0.020310\n0.161900\n0.057700\n...\n21.080000\n84.110000\n515.300000\n0.116600\n0.147200\n0.114500\n0.064930\n0.250400\n0.071460\n0.000000\n\n\n50%\n13.370000\n18.840000\n86.240000\n551.100000\n0.095870\n0.092630\n0.061540\n0.033500\n0.179200\n0.061540\n...\n25.410000\n97.660000\n686.500000\n0.131300\n0.211900\n0.226700\n0.099930\n0.282200\n0.080040\n1.000000\n\n\n75%\n15.780000\n21.800000\n104.100000\n782.700000\n0.105300\n0.130400\n0.130700\n0.074000\n0.195700\n0.066120\n...\n29.720000\n125.400000\n1084.000000\n0.146000\n0.339100\n0.382900\n0.161400\n0.317900\n0.092080\n1.000000\n\n\nmax\n28.110000\n39.280000\n188.500000\n2501.000000\n0.163400\n0.345400\n0.426800\n0.201200\n0.304000\n0.097440\n...\n49.540000\n251.200000\n4254.000000\n0.222600\n1.058000\n1.252000\n0.291000\n0.663800\n0.207500\n1.000000\n\n\n\n\n8 rows √ó 31 columns\n\n\n\n\ncancer_df.isnull().sum()\n\nmean radius                0\nmean texture               0\nmean perimeter             0\nmean area                  0\nmean smoothness            0\nmean compactness           0\nmean concavity             0\nmean concave points        0\nmean symmetry              0\nmean fractal dimension     0\nradius error               0\ntexture error              0\nperimeter error            0\narea error                 0\nsmoothness error           0\ncompactness error          0\nconcavity error            0\nconcave points error       0\nsymmetry error             0\nfractal dimension error    0\nworst radius               0\nworst texture              0\nworst perimeter            0\nworst area                 0\nworst smoothness           0\nworst compactness          0\nworst concavity            0\nworst concave points       0\nworst symmetry             0\nworst fractal dimension    0\ntarget                     0\ndtype: int64"
  },
  {
    "objectID": "Jupyter_note_book.html#heatmap-of-a-correlation-matrix",
    "href": "Jupyter_note_book.html#heatmap-of-a-correlation-matrix",
    "title": "Breast Cancer classification Using Machine Learning Classifier",
    "section": "Heatmap of a correlation matrix",
    "text": "Heatmap of a correlation matrix\n\ncancer_df.corr()#gives the correlation between them\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\ntarget\n\n\n\n\nmean radius\n1.000000\n0.323782\n0.997855\n0.987357\n0.170581\n0.506124\n0.676764\n0.822529\n0.147741\n-0.311631\n...\n0.297008\n0.965137\n0.941082\n0.119616\n0.413463\n0.526911\n0.744214\n0.163953\n0.007066\n-0.730029\n\n\nmean texture\n0.323782\n1.000000\n0.329533\n0.321086\n-0.023389\n0.236702\n0.302418\n0.293464\n0.071401\n-0.076437\n...\n0.912045\n0.358040\n0.343546\n0.077503\n0.277830\n0.301025\n0.295316\n0.105008\n0.119205\n-0.415185\n\n\nmean perimeter\n0.997855\n0.329533\n1.000000\n0.986507\n0.207278\n0.556936\n0.716136\n0.850977\n0.183027\n-0.261477\n...\n0.303038\n0.970387\n0.941550\n0.150549\n0.455774\n0.563879\n0.771241\n0.189115\n0.051019\n-0.742636\n\n\nmean area\n0.987357\n0.321086\n0.986507\n1.000000\n0.177028\n0.498502\n0.685983\n0.823269\n0.151293\n-0.283110\n...\n0.287489\n0.959120\n0.959213\n0.123523\n0.390410\n0.512606\n0.722017\n0.143570\n0.003738\n-0.708984\n\n\nmean smoothness\n0.170581\n-0.023389\n0.207278\n0.177028\n1.000000\n0.659123\n0.521984\n0.553695\n0.557775\n0.584792\n...\n0.036072\n0.238853\n0.206718\n0.805324\n0.472468\n0.434926\n0.503053\n0.394309\n0.499316\n-0.358560\n\n\nmean compactness\n0.506124\n0.236702\n0.556936\n0.498502\n0.659123\n1.000000\n0.883121\n0.831135\n0.602641\n0.565369\n...\n0.248133\n0.590210\n0.509604\n0.565541\n0.865809\n0.816275\n0.815573\n0.510223\n0.687382\n-0.596534\n\n\nmean concavity\n0.676764\n0.302418\n0.716136\n0.685983\n0.521984\n0.883121\n1.000000\n0.921391\n0.500667\n0.336783\n...\n0.299879\n0.729565\n0.675987\n0.448822\n0.754968\n0.884103\n0.861323\n0.409464\n0.514930\n-0.696360\n\n\nmean concave points\n0.822529\n0.293464\n0.850977\n0.823269\n0.553695\n0.831135\n0.921391\n1.000000\n0.462497\n0.166917\n...\n0.292752\n0.855923\n0.809630\n0.452753\n0.667454\n0.752399\n0.910155\n0.375744\n0.368661\n-0.776614\n\n\nmean symmetry\n0.147741\n0.071401\n0.183027\n0.151293\n0.557775\n0.602641\n0.500667\n0.462497\n1.000000\n0.479921\n...\n0.090651\n0.219169\n0.177193\n0.426675\n0.473200\n0.433721\n0.430297\n0.699826\n0.438413\n-0.330499\n\n\nmean fractal dimension\n-0.311631\n-0.076437\n-0.261477\n-0.283110\n0.584792\n0.565369\n0.336783\n0.166917\n0.479921\n1.000000\n...\n-0.051269\n-0.205151\n-0.231854\n0.504942\n0.458798\n0.346234\n0.175325\n0.334019\n0.767297\n0.012838\n\n\nradius error\n0.679090\n0.275869\n0.691765\n0.732562\n0.301467\n0.497473\n0.631925\n0.698050\n0.303379\n0.000111\n...\n0.194799\n0.719684\n0.751548\n0.141919\n0.287103\n0.380585\n0.531062\n0.094543\n0.049559\n-0.567134\n\n\ntexture error\n-0.097317\n0.386358\n-0.086761\n-0.066280\n0.068406\n0.046205\n0.076218\n0.021480\n0.128053\n0.164174\n...\n0.409003\n-0.102242\n-0.083195\n-0.073658\n-0.092439\n-0.068956\n-0.119638\n-0.128215\n-0.045655\n0.008303\n\n\nperimeter error\n0.674172\n0.281673\n0.693135\n0.726628\n0.296092\n0.548905\n0.660391\n0.710650\n0.313893\n0.039830\n...\n0.200371\n0.721031\n0.730713\n0.130054\n0.341919\n0.418899\n0.554897\n0.109930\n0.085433\n-0.556141\n\n\narea error\n0.735864\n0.259845\n0.744983\n0.800086\n0.246552\n0.455653\n0.617427\n0.690299\n0.223970\n-0.090170\n...\n0.196497\n0.761213\n0.811408\n0.125389\n0.283257\n0.385100\n0.538166\n0.074126\n0.017539\n-0.548236\n\n\nsmoothness error\n-0.222600\n0.006614\n-0.202694\n-0.166777\n0.332375\n0.135299\n0.098564\n0.027653\n0.187321\n0.401964\n...\n-0.074743\n-0.217304\n-0.182195\n0.314457\n-0.055558\n-0.058298\n-0.102007\n-0.107342\n0.101480\n0.067016\n\n\ncompactness error\n0.206000\n0.191975\n0.250744\n0.212583\n0.318943\n0.738722\n0.670279\n0.490424\n0.421659\n0.559837\n...\n0.143003\n0.260516\n0.199371\n0.227394\n0.678780\n0.639147\n0.483208\n0.277878\n0.590973\n-0.292999\n\n\nconcavity error\n0.194204\n0.143293\n0.228082\n0.207660\n0.248396\n0.570517\n0.691270\n0.439167\n0.342627\n0.446630\n...\n0.100241\n0.226680\n0.188353\n0.168481\n0.484858\n0.662564\n0.440472\n0.197788\n0.439329\n-0.253730\n\n\nconcave points error\n0.376169\n0.163851\n0.407217\n0.372320\n0.380676\n0.642262\n0.683260\n0.615634\n0.393298\n0.341198\n...\n0.086741\n0.394999\n0.342271\n0.215351\n0.452888\n0.549592\n0.602450\n0.143116\n0.310655\n-0.408042\n\n\nsymmetry error\n-0.104321\n0.009127\n-0.081629\n-0.072497\n0.200774\n0.229977\n0.178009\n0.095351\n0.449137\n0.345007\n...\n-0.077473\n-0.103753\n-0.110343\n-0.012662\n0.060255\n0.037119\n-0.030413\n0.389402\n0.078079\n0.006522\n\n\nfractal dimension error\n-0.042641\n0.054458\n-0.005523\n-0.019887\n0.283607\n0.507318\n0.449301\n0.257584\n0.331786\n0.688132\n...\n-0.003195\n-0.001000\n-0.022736\n0.170568\n0.390159\n0.379975\n0.215204\n0.111094\n0.591328\n-0.077972\n\n\nworst radius\n0.969539\n0.352573\n0.969476\n0.962746\n0.213120\n0.535315\n0.688236\n0.830318\n0.185728\n-0.253691\n...\n0.359921\n0.993708\n0.984015\n0.216574\n0.475820\n0.573975\n0.787424\n0.243529\n0.093492\n-0.776454\n\n\nworst texture\n0.297008\n0.912045\n0.303038\n0.287489\n0.036072\n0.248133\n0.299879\n0.292752\n0.090651\n-0.051269\n...\n1.000000\n0.365098\n0.345842\n0.225429\n0.360832\n0.368366\n0.359755\n0.233027\n0.219122\n-0.456903\n\n\nworst perimeter\n0.965137\n0.358040\n0.970387\n0.959120\n0.238853\n0.590210\n0.729565\n0.855923\n0.219169\n-0.205151\n...\n0.365098\n1.000000\n0.977578\n0.236775\n0.529408\n0.618344\n0.816322\n0.269493\n0.138957\n-0.782914\n\n\nworst area\n0.941082\n0.343546\n0.941550\n0.959213\n0.206718\n0.509604\n0.675987\n0.809630\n0.177193\n-0.231854\n...\n0.345842\n0.977578\n1.000000\n0.209145\n0.438296\n0.543331\n0.747419\n0.209146\n0.079647\n-0.733825\n\n\nworst smoothness\n0.119616\n0.077503\n0.150549\n0.123523\n0.805324\n0.565541\n0.448822\n0.452753\n0.426675\n0.504942\n...\n0.225429\n0.236775\n0.209145\n1.000000\n0.568187\n0.518523\n0.547691\n0.493838\n0.617624\n-0.421465\n\n\nworst compactness\n0.413463\n0.277830\n0.455774\n0.390410\n0.472468\n0.865809\n0.754968\n0.667454\n0.473200\n0.458798\n...\n0.360832\n0.529408\n0.438296\n0.568187\n1.000000\n0.892261\n0.801080\n0.614441\n0.810455\n-0.590998\n\n\nworst concavity\n0.526911\n0.301025\n0.563879\n0.512606\n0.434926\n0.816275\n0.884103\n0.752399\n0.433721\n0.346234\n...\n0.368366\n0.618344\n0.543331\n0.518523\n0.892261\n1.000000\n0.855434\n0.532520\n0.686511\n-0.659610\n\n\nworst concave points\n0.744214\n0.295316\n0.771241\n0.722017\n0.503053\n0.815573\n0.861323\n0.910155\n0.430297\n0.175325\n...\n0.359755\n0.816322\n0.747419\n0.547691\n0.801080\n0.855434\n1.000000\n0.502528\n0.511114\n-0.793566\n\n\nworst symmetry\n0.163953\n0.105008\n0.189115\n0.143570\n0.394309\n0.510223\n0.409464\n0.375744\n0.699826\n0.334019\n...\n0.233027\n0.269493\n0.209146\n0.493838\n0.614441\n0.532520\n0.502528\n1.000000\n0.537848\n-0.416294\n\n\nworst fractal dimension\n0.007066\n0.119205\n0.051019\n0.003738\n0.499316\n0.687382\n0.514930\n0.368661\n0.438413\n0.767297\n...\n0.219122\n0.138957\n0.079647\n0.617624\n0.810455\n0.686511\n0.511114\n0.537848\n1.000000\n-0.323872\n\n\ntarget\n-0.730029\n-0.415185\n-0.742636\n-0.708984\n-0.358560\n-0.596534\n-0.696360\n-0.776614\n-0.330499\n0.012838\n...\n-0.456903\n-0.782914\n-0.733825\n-0.421465\n-0.590998\n-0.659610\n-0.793566\n-0.416294\n-0.323872\n1.000000\n\n\n\n\n31 rows √ó 31 columns"
  },
  {
    "objectID": "Jupyter_note_book.html#suppor-vector-classifier",
    "href": "Jupyter_note_book.html#suppor-vector-classifier",
    "title": "Breast Cancer classification Using Machine Learning Classifier",
    "section": "Suppor vector Classifier",
    "text": "Suppor vector Classifier\n\n# Support vector classifier\nfrom sklearn.svm import SVC\nsvc_classifier = SVC()\nsvc_classifier.fit(X_train, y_train)\ny_pred_scv = svc_classifier.predict(X_test)\naccuracy_score(y_test, y_pred_scv)\n\n0.9385964912280702\n\n\n\nTrain with Standard scaled Data\n\n# Train with Standard scaled Data\nsvc_classifier2 = SVC()\nsvc_classifier2.fit(X_train_sc, y_train)\ny_pred_svc_sc = svc_classifier2.predict(X_test_sc)\naccuracy_score(y_test, y_pred_svc_sc)\n\n0.9649122807017544"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Scholarship interests\nMy scholarship interests lie at the intersection of data science and biomedical engineering."
  }
]