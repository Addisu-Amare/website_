<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>efficient_model_training â€“ Addisu Amare</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-1db3f541d99483cdc7a0d9cf38862b2c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../css/styles.css">
</head>

<body class="nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Addisu Amare</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../research.html"> 
<span class="menu-text">Scholarship</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../documents.html"> 
<span class="menu-text">Article</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../project.html"> 
<span class="menu-text">projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../Hobbies.html"> 
<span class="menu-text">My Hobbies</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../Jupyter_note_book.html"> 
<span class="menu-text">Cancer classification</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/Addisu-Amare"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<p><em>Efficient Deep learning training</em></p>
<section id="section" class="level1">
<h1></h1>
<section id="section-1" class="level2">
<h2 class="anchored" data-anchor-id="section-1"></h2>
<p>These days models and datasets are becoming <strong>larger</strong> and <strong>larger</strong> and it is becoming increasingly important to increase the efficiency of the training procedure by optimizing memory utilization, speeding up the training, or both.</p>
<p>In this work shop we will cover several techniques that could <strong>speed up</strong> the training of the model and even sometimes make the training <strong>feasible</strong>: * Batch size optimization * Automated mixed precision * Gradient accumulation * Gradient checkpointing * Model compilation (for <code>torch&gt;=2.0</code>)</p>
</section>
<section id="preparatory-stuff" class="level2">
<h2 class="anchored" data-anchor-id="preparatory-stuff">Preparatory stuff</h2>
<p>Loading stuff and defining helper functions</p>
<div id="cell-5" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm <span class="im">import</span> trange</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.cuda.amp <span class="im">import</span> GradScaler</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models <span class="im">import</span> get_model</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-6" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> print_memory_stats():</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"CUDA max memory allocated: </span><span class="sc">{</span>torch<span class="sc">.</span>cuda<span class="sc">.</span>max_memory_allocated() <span class="op">/</span> <span class="dv">2</span> <span class="op">**</span> <span class="dv">30</span><span class="sc">:.2f}</span><span class="ss"> GB."</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"CUDA max memory reserved: </span><span class="sc">{</span>torch<span class="sc">.</span>cuda<span class="sc">.</span>max_memory_reserved() <span class="op">/</span> <span class="dv">2</span> <span class="op">**</span> <span class="dv">30</span><span class="sc">:.2f}</span><span class="ss"> GB."</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Make sure that you are using CUDA Runtime!</p>
<div id="cell-8" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">"cuda"</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Since we have not started using GPU the cell below should output 0.</p>
<div id="cell-10" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="ac2f4695-5372-422b-a534-f091aa92d2d4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>print_memory_stats()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>CUDA max memory allocated: 0.00 GB.
CUDA max memory reserved: 0.00 GB.</code></pre>
</div>
</div>
<p>In the following next sections weâ€™ll illustrate the techiques on a model from the <code>torchvision</code> library. We will adopt <code>SwinTransformer-Base</code> for the demonstrations below.</p>
<div id="cell-12" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> get_model(<span class="st">"swin_b"</span>).to(device) <span class="co"># it is randomly initialized, but we do not care</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-13" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="40efd29e-ca5a-4539-d0de-3495da1b5929">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of paramerers: </span><span class="sc">{</span><span class="bu">sum</span>(param.numel() <span class="cf">for</span> param <span class="kw">in</span> model.parameters()) <span class="op">/</span> <span class="dv">10</span> <span class="op">**</span> <span class="dv">6</span> <span class="sc">:.2f}</span><span class="ss"> M"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of paramerers: 87.77 M</code></pre>
</div>
</div>
<p>Memory footprint should be roughly <code>4 * num_model_parameters</code> since by default we are working in single precision <code>float32</code>.</p>
<div id="cell-15" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="bf529789-1d60-47ef-e7d6-6f23550bba33">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>print_memory_stats()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>CUDA max memory allocated: 0.33 GB.
CUDA max memory reserved: 0.36 GB.</code></pre>
</div>
</div>
<p>Define a function that yields dummy inputs and targets.</p>
<div id="cell-17" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_dummy_dataloader(batch_size: <span class="bu">int</span>, img_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">224</span>):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">yield</span> torch.randn(batch_size, <span class="dv">3</span>, img_size, img_size), torch.randint(low<span class="op">=</span><span class="dv">0</span>, high<span class="op">=</span><span class="dv">1000</span>, size<span class="op">=</span>(batch_size,))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Baseline training loop.</p>
<div id="cell-19" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(model, loader, optimizer, loss_fn, num_steps):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>  samples_processed <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>  start <span class="op">=</span> time.perf_counter()</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i <span class="kw">in</span> trange(num_steps, total<span class="op">=</span>num_steps, desc<span class="op">=</span><span class="st">"Train"</span>):</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    inputs, targets <span class="op">=</span> <span class="bu">next</span>(loader)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    inputs, targets <span class="op">=</span> inputs.to(device), targets.to(device)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model(inputs)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> loss_fn(outputs, targets)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    samples_processed <span class="op">+=</span> <span class="bu">len</span>(inputs)</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>  end <span class="op">=</span> time.perf_counter()</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>  samples_per_sec <span class="op">=</span> samples_processed <span class="op">/</span> (end <span class="op">-</span> start)</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>  print_memory_stats()</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="ss">f"samples/sec: </span><span class="sc">{</span>samples_per_sec<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>  results <span class="op">=</span> {</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>      <span class="st">"samples/sec"</span>: samples_per_sec,</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>      <span class="st">"max_memory_allocated"</span>: torch.cuda.max_memory_allocated() <span class="op">/</span> <span class="dv">2</span> <span class="op">**</span> <span class="dv">30</span></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> results</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="batch-size-optimization" class="level2">
<h2 class="anchored" data-anchor-id="batch-size-optimization">Batch size optimization</h2>
<p>Maximizing the throughput (samples/second) leads to lower training cost. This is generally achieved by utilizing the GPU as much as possible and thus filling GPU memory to its limit.</p>
<p>Below we will show the dependence of the model throughput on the batch size.</p>
<div id="cell-22" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>training_steps <span class="op">=</span> <span class="dv">100</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-23" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="fdee6379-fd06-4cde-a1e5-7cde70c2342b">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>results_for_batch_size <span class="op">=</span> {}</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> batch_size <span class="kw">in</span> (<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">16</span>, <span class="dv">32</span>, <span class="dv">64</span>, <span class="dv">128</span>):</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">10</span>)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="ss">f"Batch size: </span><span class="sc">{</span>batch_size<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># prepare dataloader</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>  dataloader <span class="op">=</span> make_dummy_dataloader(batch_size)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># prepare optimizer</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>  optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">1e-4</span>)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># train</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">try</span>:</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    results_for_batch_size[batch_size] <span class="op">=</span> train(model, dataloader, optimizer, F.cross_entropy, training_steps)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>  <span class="cf">except</span>:</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">CUDA out of memory!"</span>)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>    print_memory_stats()</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">del</span> optimizer</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    torch.cuda.empty_cache()</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>    torch.cuda.reset_peak_memory_stats()</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">break</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>  <span class="kw">del</span> optimizer</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>  torch.cuda.empty_cache()</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>  torch.cuda.reset_peak_memory_stats()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>----------
Batch size: 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:12&lt;00:00,  7.71it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

CUDA max memory allocated: 1.67 GB.
CUDA max memory reserved: 1.78 GB.
samples/sec: 7.71
----------
Batch size: 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:10&lt;00:00,  9.18it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

CUDA max memory allocated: 1.68 GB.
CUDA max memory reserved: 1.86 GB.
samples/sec: 18.34
----------
Batch size: 4</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:16&lt;00:00,  6.20it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

CUDA max memory allocated: 2.34 GB.
CUDA max memory reserved: 2.59 GB.
samples/sec: 24.78
----------
Batch size: 8</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:28&lt;00:00,  3.46it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

CUDA max memory allocated: 2.78 GB.
CUDA max memory reserved: 3.11 GB.
samples/sec: 27.67
----------
Batch size: 16</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:56&lt;00:00,  1.78it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

CUDA max memory allocated: 4.43 GB.
CUDA max memory reserved: 4.89 GB.
samples/sec: 28.54
----------
Batch size: 32</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [01:46&lt;00:00,  1.06s/it]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

CUDA max memory allocated: 7.88 GB.
CUDA max memory reserved: 8.65 GB.
samples/sec: 30.11
----------
Batch size: 64</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Train:   1%|          | 1/100 [00:03&lt;05:09,  3.13s/it]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
CUDA out of memory!
CUDA max memory allocated: 14.00 GB.
CUDA max memory reserved: 14.58 GB.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
</div>
<p>Let us plot <code>samples/sec</code> and <code>max memory</code> vs <code>batch size</code>.</p>
<div id="cell-25" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>batch_sizes <span class="op">=</span> []</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>samples_sec_per_batch_size <span class="op">=</span> []</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>max_memory_per_batch_size <span class="op">=</span> []</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k, v <span class="kw">in</span> results_for_batch_size.items():</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>  batch_sizes.append(k)</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>  samples_sec_per_batch_size.append(v[<span class="st">'samples/sec'</span>])</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>  max_memory_per_batch_size.append(v[<span class="st">'max_memory_allocated'</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-26" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:479}}" data-outputid="07362732-1265-423e-9f0d-14c30c144e06">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(ncols<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">5</span>))</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(batch_sizes, samples_sec_per_batch_size, <span class="st">'-v'</span>)<span class="op">;</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">"Batch size"</span>, fontsize<span class="op">=</span><span class="dv">20</span>)<span class="op">;</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="st">"Samples/sec"</span>, fontsize<span class="op">=</span><span class="dv">20</span>)<span class="op">;</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(batch_sizes, max_memory_per_batch_size, <span class="st">'-v'</span>)<span class="op">;</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">"Batch size"</span>, fontsize<span class="op">=</span><span class="dv">20</span>)<span class="op">;</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_ylabel(<span class="st">"Max memory (Gb)"</span>, fontsize<span class="op">=</span><span class="dv">20</span>)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Efficient_model_training_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p><strong>Note!</strong></p>
<p>Batch size (together with learning rate) determine the degree of stochasticity of gradient descent.</p>
<p>Often, a certain magnitude of noise in optimization trajectory is crucial for finding well generalizing solutions.</p>
<p>Therefore, the batch size has to be chosen wisely ðŸ¤”.</p>
</section>
<section id="automated-mixed-precision" class="level2">
<h2 class="anchored" data-anchor-id="automated-mixed-precision">Automated mixed precision</h2>
<p>Machines work with number in finite precision. Different formats are defined in <a href="https://en.wikipedia.org/wiki/IEEE_754">IEEE754</a>.</p>
<p>Unlike some other applications, such as physics and engineering neural networks are more robust to numerical errors and up to a certain point one reduce precision without impact on performance.</p>
<p>Lower precision offers faster operations with numbers and reduced memory usage.</p>
<p>Below weâ€™ll illustrate some commonly used formats.</p>
<p><strong>FP32</strong> (default in PyTorch)</p>
<p>This format was the workhorse of deep learning for a long time.</p>
<ul>
<li>1 bit sign</li>
<li>8 bits exponent</li>
<li>23 bits fraction (aka mantissa)</li>
</ul>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/d2/Float_example.svg/1180px-Float_example.svg.png" width="600" alt="Deep Recurrent Neural Networks"></p>
<p>Range: ~1.18e-38 â€¦ ~3.40e38 with 6-9 significant decimal digits precision.</p>
<p><strong>FP16</strong></p>
<p>Arguably, the most popular format for large model training and inference:</p>
<ul>
<li>1 bit sign</li>
<li>5 bits exponent</li>
<li>10 bits fraction</li>
</ul>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/21/IEEE_754r_Half_Floating_Point_Format.svg/350px-IEEE_754r_Half_Floating_Point_Format.svg.png" width="300" alt="Deep Recurrent Neural Networks"></p>
<p>Range: ~5.96eâˆ’8 (6.10eâˆ’5) â€¦ 65504 with 4 significant decimal digits precision.</p>
<p><strong>BF16</strong></p>
<p>Brain Float format developed by Google. Has higher dynamical range compared to FP16, but lower precision.</p>
<ul>
<li>1 bit sign</li>
<li>8 bits exponent</li>
<li>7 bits fraction</li>
</ul>
<p>Range: ~5.96eâˆ’8 (6.10eâˆ’5) â€¦ 65504 with 4 significant decimal digits precision.</p>
<p><img src="https://miro.medium.com/v2/resize:fit:520/format:webp/0*m4rcg4_23-1qQ_kp.png" width="300" alt="Deep Recurrent Neural Networks"></p>
<p>Range: ~1.18e-38 â€¦ ~3.40e38 with 3 significant decimal digits.</p>
<p><strong>Note</strong></p>
<p>This format is supported only for NVIDIA architectures from Ampere and newer (A100, RTX30<code>**</code>, RTX40<code>**</code>). Older GPUs, such as fellow T4, do not have hardware support for it.</p>
<p>For more details on numerical formats I recommend <a href="https://moocaholic.medium.com/fp64-fp32-fp16-bfloat16-tf32-and-other-members-of-the-zoo-a1ca7897d407">this blog</a>.</p>
<p><strong>Automated mixed precision</strong> runs some operations <code>(nn.Linear, nn.Conv*d)</code> in <code>fp16</code> , whereas other operations that are more vulnerable to numerical approximations are run in higher precision <code>fp32</code>. Gradients w/r to model parameters and optimizer stats are typically kept in <code>fp32</code>. Parameter updates act on <code>fp32</code> copy of the model.</p>
<div id="cell-31" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(model, loader, optimizer, scaler, loss_fn, num_steps):</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>  <span class="co">"""</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="co">  Args:</span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="co">    model - model trained</span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="co">    loader - data loader</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a><span class="co">    optimizer - optimizer</span></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a><span class="co">    scaler - gradient scaler that shifts the gradient range</span></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a><span class="co">    loss_fn - task loss function</span></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a><span class="co">    num_steps.- number of training steps</span></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a><span class="co">  Returns:</span></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a><span class="co">    dict with performance stats</span></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a><span class="co">  """</span></span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>  samples_processed <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>  start <span class="op">=</span> time.perf_counter()</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i <span class="kw">in</span> trange(num_steps, total<span class="op">=</span>num_steps, desc<span class="op">=</span><span class="st">"Train"</span>):</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>    inputs, targets <span class="op">=</span> <span class="bu">next</span>(loader)</span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>    inputs, targets <span class="op">=</span> inputs.to(device), targets.to(device)</span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.autocast(device_type<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>torch.float16):</span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a>      outputs <span class="op">=</span> model(inputs)</span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a>      loss <span class="op">=</span> loss_fn(outputs, targets)</span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="kw">not</span> loss.isnan(), <span class="st">"Loss is NaN. Terminating training."</span></span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a>    scaler.scale(loss).backward()</span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>    scaler.step(optimizer)</span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>    scaler.update()</span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a>    samples_processed <span class="op">+=</span> <span class="bu">len</span>(inputs)</span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a>  end <span class="op">=</span> time.perf_counter()</span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a>  samples_per_sec <span class="op">=</span> samples_processed <span class="op">/</span> (end <span class="op">-</span> start)</span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb33-30"><a href="#cb33-30" aria-hidden="true" tabindex="-1"></a>  print_memory_stats()</span>
<span id="cb33-31"><a href="#cb33-31" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="ss">f"samples/sec: </span><span class="sc">{</span>samples_per_sec<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb33-32"><a href="#cb33-32" aria-hidden="true" tabindex="-1"></a>  results <span class="op">=</span> {</span>
<span id="cb33-33"><a href="#cb33-33" aria-hidden="true" tabindex="-1"></a>      <span class="st">"samples/sec"</span>: samples_per_sec,</span>
<span id="cb33-34"><a href="#cb33-34" aria-hidden="true" tabindex="-1"></a>      <span class="st">"max_memory_allocated"</span>: torch.cuda.max_memory_allocated() <span class="op">/</span> <span class="dv">2</span> <span class="op">**</span> <span class="dv">30</span></span>
<span id="cb33-35"><a href="#cb33-35" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb33-36"><a href="#cb33-36" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> results</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-32" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="2c94a176-238b-4903-f7bc-13117f020e36">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>results_for_batch_size_amp <span class="op">=</span> {}</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> batch_size <span class="kw">in</span> (<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">16</span>, <span class="dv">32</span>, <span class="dv">64</span>, <span class="dv">128</span>):</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">10</span>)</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="ss">f"Batch size: </span><span class="sc">{</span>batch_size<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># prepare dataloader</span></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>  dataloader <span class="op">=</span> make_dummy_dataloader(batch_size)</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># prepare optimizer</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>  optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">1e-4</span>)</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># prepare scaler</span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>  scaler <span class="op">=</span> GradScaler()</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># train</span></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>  <span class="cf">try</span>:</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>    results_for_batch_size_amp[batch_size] <span class="op">=</span> train(model, dataloader, optimizer, scaler, F.cross_entropy, training_steps)</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>  <span class="cf">except</span>:</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">CUDA out of memory!"</span>)</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>    print_memory_stats()</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">del</span> optimizer</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>    torch.cuda.empty_cache()</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>    torch.cuda.reset_peak_memory_stats()</span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">break</span></span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>  <span class="kw">del</span> optimizer</span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>  torch.cuda.empty_cache()</span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>  torch.cuda.reset_peak_memory_stats()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>----------
Batch size: 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08&lt;00:00, 11.23it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

CUDA max memory allocated: 1.67 GB.
CUDA max memory reserved: 1.82 GB.
samples/sec: 11.22
----------
Batch size: 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:09&lt;00:00, 10.05it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

CUDA max memory allocated: 1.68 GB.
CUDA max memory reserved: 1.84 GB.
samples/sec: 20.10
----------
Batch size: 4</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:09&lt;00:00, 10.48it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

CUDA max memory allocated: 1.73 GB.
CUDA max memory reserved: 1.91 GB.
samples/sec: 41.89
----------
Batch size: 8</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:11&lt;00:00,  8.94it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

CUDA max memory allocated: 2.31 GB.
CUDA max memory reserved: 2.52 GB.
samples/sec: 71.42
----------
Batch size: 16</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:18&lt;00:00,  5.44it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

CUDA max memory allocated: 3.42 GB.
CUDA max memory reserved: 3.88 GB.
samples/sec: 86.95
----------
Batch size: 32</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:34&lt;00:00,  2.94it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

CUDA max memory allocated: 5.82 GB.
CUDA max memory reserved: 6.44 GB.
samples/sec: 94.02
----------
Batch size: 64</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [01:06&lt;00:00,  1.50it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

CUDA max memory allocated: 10.66 GB.
CUDA max memory reserved: 11.57 GB.
samples/sec: 95.90
----------
Batch size: 128</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Train:   0%|          | 0/100 [00:00&lt;?, ?it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
CUDA out of memory!
CUDA max memory allocated: 1.61 GB.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
CUDA max memory reserved: 2.44 GB.</code></pre>
</div>
</div>
<div id="cell-33" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>batch_sizes_amp <span class="op">=</span> []</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>samples_sec_per_batch_size_amp <span class="op">=</span> []</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>max_memory_per_batch_size_amp <span class="op">=</span> []</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k, v <span class="kw">in</span> results_for_batch_size_amp.items():</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>  batch_sizes_amp.append(k)</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>  samples_sec_per_batch_size_amp.append(v[<span class="st">'samples/sec'</span>])</span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>  max_memory_per_batch_size_amp.append(v[<span class="st">'max_memory_allocated'</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-34" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:479}}" data-outputid="4bc81f5b-3b34-4f52-ec3f-8140ac24b638">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(ncols<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">5</span>))</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(batch_sizes, samples_sec_per_batch_size, <span class="st">'-v'</span>, label<span class="op">=</span><span class="st">'fp32'</span>)<span class="op">;</span></span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(batch_sizes_amp, samples_sec_per_batch_size_amp, <span class="st">'-v'</span>, label<span class="op">=</span><span class="st">'amp'</span>)<span class="op">;</span></span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">"Batch size"</span>, fontsize<span class="op">=</span><span class="dv">20</span>)<span class="op">;</span></span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="st">"Samples/sec"</span>, fontsize<span class="op">=</span><span class="dv">20</span>)<span class="op">;</span></span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].legend()<span class="op">;</span></span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(batch_sizes, max_memory_per_batch_size, <span class="st">'-v'</span>, label<span class="op">=</span><span class="st">'fp32'</span>)<span class="op">;</span></span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(batch_sizes_amp, max_memory_per_batch_size_amp, <span class="st">'-v'</span>, label<span class="op">=</span><span class="st">'amp'</span>)<span class="op">;</span></span>
<span id="cb55-11"><a href="#cb55-11" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">"Batch size"</span>, fontsize<span class="op">=</span><span class="dv">20</span>)<span class="op">;</span></span>
<span id="cb55-12"><a href="#cb55-12" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_ylabel(<span class="st">"Max memory (Gb)"</span>, fontsize<span class="op">=</span><span class="dv">20</span>)<span class="op">;</span></span>
<span id="cb55-13"><a href="#cb55-13" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].legend()<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Efficient_model_training_files/figure-html/cell-18-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-35" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="1edb3317-8c55-45b1-88a9-713d42f6d8f6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Speedup: </span><span class="sc">{</span><span class="bu">max</span>(samples_sec_per_batch_size_amp) <span class="op">/</span> <span class="bu">max</span>(samples_sec_per_batch_size)<span class="sc">:.2f}</span><span class="ss">x"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Speedup: 2.32x</code></pre>
</div>
</div>
<p>With <strong>amp</strong> we have accelerated training by more than 2x!</p>
<p>Largest batch size that fits onto the memory of device has almost doubled as well.</p>
<section id="grad-scaler" class="level3">
<h3 class="anchored" data-anchor-id="grad-scaler">Grad scaler</h3>
<p>In the example above weâ€™ve observed some mysterious entity called GradScaler. Why do we need it?</p>
<p>Note, that <code>fp16</code> has much narrower range of representable values compared to <code>fp32</code>.</p>
<p><img src="https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/_images/loss_scaling.png" alt="Deep Recurrent Neural Networks"></p>
<p>If the values inside a tensor are too small or to large they may not fit inside <code>fp16</code>, that may lead to divergence, especially if training large models for a long time.</p>
<p><img src="https://docscontent.nvidia.com/dims4/default/d785744/2147483647/strip/true/crop/707x421+0+0/resize/1414x842!/format/webp/quality/90/?url=https%3A%2F%2Fk3-prod-nvidia-docs.s3.us-west-2.amazonaws.com%2Fbrightspot%2Fdita%2F00000189-949d-d46e-abe9-bcdf9f8c0000%2Fdeeplearning%2Fperformance%2Fmixed-precision-training%2Fgraphics%2Ftraining-iteration.png" width="550" alt="Deep Recurrent Neural Networks"></p>
<p><code>GradScaler</code> tracks the magnitude of the loss, builds historgram of gradients and shifts it towards the range that is well represented by the fp16 numerical format.</p>
<p>Let us illustrate the problem on a toy example below.</p>
<div id="cell-38" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> torch.randn(<span class="dv">8192</span>, <span class="dv">8192</span>, device<span class="op">=</span>device, dtype<span class="op">=</span>torch.float16, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> torch.randn(<span class="dv">8</span>, <span class="dv">8192</span>, device<span class="op">=</span>device, dtype<span class="op">=</span>torch.float16)</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> torch.randn(<span class="dv">8</span>, <span class="dv">8192</span>, device<span class="op">=</span>device, dtype<span class="op">=</span>torch.float16)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-39" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> F.mse_loss(X <span class="op">@</span> W.T, Y)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-40" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="ae7262e2-579f-4234-c6ee-ce94c8f108e5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>loss</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="22">
<pre><code>tensor(inf, device='cuda:0', dtype=torch.float16, grad_fn=&lt;MseLossBackward0&gt;)</code></pre>
</div>
</div>
<p>Indeed this example is arfiticial, as one would init weights <span class="math inline">\(w_{ij} \in \mathcal{N}(0, \frac{1}{d})\)</span> but shows the point.</p>
<div id="cell-42" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="kw">del</span> W, X, Y, loss</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>torch.cuda.empty_cache()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p><strong>Materials for further study.</strong></p>
<ul>
<li><p><a href="https://pytorch.org/docs/stable/notes/amp_examples.html">PyTorch amp examples</a></p></li>
<li><p><a href="https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html">Great blog about amp by NVIDIA</a></p></li>
<li><p><a href="https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/fp8_primer.html">A page about fp8 format in Transformer engine</a></p></li>
</ul>
</section>
</section>
<section id="gradient-accumulation" class="level2">
<h2 class="anchored" data-anchor-id="gradient-accumulation">Gradient accumulation</h2>
<p>Commonly, a researcher or a practioner follows standard training recipes from papers. However, it is often the case that hardware doesnâ€™t allow to train on the large batches that are affordable for Google and OpenAI guys.</p>
<p>In principle, one can fit â€˜almostâ€™ arbitrary large batch size via gradient checkpointing illustrated below.</p>
<p>The idea behind gradient accumulation is to instead of calculating the gradients for the whole batch at once to do it in smaller steps. The way we do that is to calculate the gradients iteratively in smaller batches by doing a forward and backward pass through the model and accumulating the gradients in the process. When enough gradients are accumulated we run the modelâ€™s optimization step. This way we can easily increase the overall batch size to numbers that would never fit into the GPUâ€™s memory. In turn, however, the added forward and backward passes can slow down the training a bit.</p>
<div id="cell-45" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(model, loader, optimizer, scaler, loss_fn, num_steps, grad_accum_steps<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>  <span class="co">"""</span></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a><span class="co">  Args:</span></span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a><span class="co">    model - model trained</span></span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a><span class="co">    loader - data loader</span></span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a><span class="co">    optimizer - optimizer</span></span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a><span class="co">    scaler - gradient scaler that shifts the gradient range</span></span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a><span class="co">    loss_fn - task loss function</span></span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a><span class="co">    num_steps - number of training steps</span></span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a><span class="co">    grad_accum_steps - number of gradient accumulation steps</span></span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a><span class="co">  Returns:</span></span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a><span class="co">    dict with performance stats</span></span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a><span class="co">  """</span></span>
<span id="cb63-14"><a href="#cb63-14" aria-hidden="true" tabindex="-1"></a>  samples_processed <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb63-15"><a href="#cb63-15" aria-hidden="true" tabindex="-1"></a>  start <span class="op">=</span> time.perf_counter()</span>
<span id="cb63-16"><a href="#cb63-16" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i <span class="kw">in</span> trange(num_steps, total<span class="op">=</span>num_steps, desc<span class="op">=</span><span class="st">"Train"</span>):</span>
<span id="cb63-17"><a href="#cb63-17" aria-hidden="true" tabindex="-1"></a>    inputs, targets <span class="op">=</span> <span class="bu">next</span>(loader)</span>
<span id="cb63-18"><a href="#cb63-18" aria-hidden="true" tabindex="-1"></a>    inputs, targets <span class="op">=</span> inputs.to(device), targets.to(device)</span>
<span id="cb63-19"><a href="#cb63-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.autocast(device_type<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>torch.float16):</span>
<span id="cb63-20"><a href="#cb63-20" aria-hidden="true" tabindex="-1"></a>      outputs <span class="op">=</span> model(inputs)</span>
<span id="cb63-21"><a href="#cb63-21" aria-hidden="true" tabindex="-1"></a>      loss <span class="op">=</span> loss_fn(outputs, targets)</span>
<span id="cb63-22"><a href="#cb63-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="kw">not</span> loss.isnan(), <span class="st">"Loss is NaN. Terminating training."</span></span>
<span id="cb63-23"><a href="#cb63-23" aria-hidden="true" tabindex="-1"></a>    scaler.scale(loss <span class="op">/</span> grad_accum_steps).backward()</span>
<span id="cb63-24"><a href="#cb63-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> grad_accum_steps <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb63-25"><a href="#cb63-25" aria-hidden="true" tabindex="-1"></a>        scaler.step(optimizer)</span>
<span id="cb63-26"><a href="#cb63-26" aria-hidden="true" tabindex="-1"></a>        scaler.update()</span>
<span id="cb63-27"><a href="#cb63-27" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb63-28"><a href="#cb63-28" aria-hidden="true" tabindex="-1"></a>    samples_processed <span class="op">+=</span> <span class="bu">len</span>(inputs)</span>
<span id="cb63-29"><a href="#cb63-29" aria-hidden="true" tabindex="-1"></a>  end <span class="op">=</span> time.perf_counter()</span>
<span id="cb63-30"><a href="#cb63-30" aria-hidden="true" tabindex="-1"></a>  samples_per_sec <span class="op">=</span> samples_processed <span class="op">/</span> (end <span class="op">-</span> start)</span>
<span id="cb63-31"><a href="#cb63-31" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb63-32"><a href="#cb63-32" aria-hidden="true" tabindex="-1"></a>  print_memory_stats()</span>
<span id="cb63-33"><a href="#cb63-33" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="ss">f"samples/sec: </span><span class="sc">{</span>samples_per_sec<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb63-34"><a href="#cb63-34" aria-hidden="true" tabindex="-1"></a>  results <span class="op">=</span> {</span>
<span id="cb63-35"><a href="#cb63-35" aria-hidden="true" tabindex="-1"></a>      <span class="st">"samples/sec"</span>: samples_per_sec,</span>
<span id="cb63-36"><a href="#cb63-36" aria-hidden="true" tabindex="-1"></a>      <span class="st">"max_memory_allocated"</span>: torch.cuda.max_memory_allocated() <span class="op">/</span> <span class="dv">2</span> <span class="op">**</span> <span class="dv">30</span></span>
<span id="cb63-37"><a href="#cb63-37" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb63-38"><a href="#cb63-38" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> results</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>In the example below largest batch size, that fit into our memory was 64. Let us use it as a <code>microbatch_size</code>.</p>
<p>Total batch size is <code>microbatch_size * grad_accum_steps</code>.</p>
<div id="cell-47" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="e979c400-e85b-43e6-ddd7-a22ed80ee052">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>results_for_batch_size_grad_accum <span class="op">=</span> {}</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>microbatch_size <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>training_steps <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> batch_size <span class="kw">in</span> (<span class="dv">512</span>, <span class="dv">1024</span>, <span class="dv">2048</span>):</span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">10</span>)</span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="ss">f"Batch size: </span><span class="sc">{</span>batch_size<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a>  grad_accum_steps <span class="op">=</span> batch_size <span class="op">//</span> microbatch_size</span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># prepare dataloader</span></span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a>  dataloader <span class="op">=</span> make_dummy_dataloader(microbatch_size)</span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># prepare optimizer</span></span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true" tabindex="-1"></a>  optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">1e-4</span>)</span>
<span id="cb64-14"><a href="#cb64-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># prepare scaler</span></span>
<span id="cb64-15"><a href="#cb64-15" aria-hidden="true" tabindex="-1"></a>  scaler <span class="op">=</span> GradScaler()</span>
<span id="cb64-16"><a href="#cb64-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># train</span></span>
<span id="cb64-17"><a href="#cb64-17" aria-hidden="true" tabindex="-1"></a>  results_for_batch_size_grad_accum[batch_size] <span class="op">=</span> train(</span>
<span id="cb64-18"><a href="#cb64-18" aria-hidden="true" tabindex="-1"></a>    model, dataloader, optimizer, scaler, F.cross_entropy, training_steps, grad_accum_steps</span>
<span id="cb64-19"><a href="#cb64-19" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb64-20"><a href="#cb64-20" aria-hidden="true" tabindex="-1"></a>  <span class="kw">del</span> optimizer</span>
<span id="cb64-21"><a href="#cb64-21" aria-hidden="true" tabindex="-1"></a>  torch.cuda.empty_cache()</span>
<span id="cb64-22"><a href="#cb64-22" aria-hidden="true" tabindex="-1"></a>  torch.cuda.reset_peak_memory_stats()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>----------
Batch size: 512</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [01:23&lt;00:00,  1.20it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

CUDA max memory allocated: 10.56 GB.
CUDA max memory reserved: 10.74 GB.
samples/sec: 76.79
----------
Batch size: 1024</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [01:22&lt;00:00,  1.21it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

CUDA max memory allocated: 10.56 GB.
CUDA max memory reserved: 10.84 GB.
samples/sec: 77.72
----------
Batch size: 2048</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [01:22&lt;00:00,  1.22it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

CUDA max memory allocated: 10.56 GB.
CUDA max memory reserved: 10.86 GB.
samples/sec: 78.00</code></pre>
</div>
</div>
<div id="cell-48" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>batch_sizes_grad_accum <span class="op">=</span> []</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>samples_sec_per_batch_size_grad_accum <span class="op">=</span> []</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>max_memory_per_batch_size_grad_accum <span class="op">=</span> []</span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k, v <span class="kw">in</span> results_for_batch_size_grad_accum.items():</span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a>  batch_sizes_grad_accum.append(k)</span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a>  samples_sec_per_batch_size_grad_accum.append(v[<span class="st">'samples/sec'</span>])</span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a>  max_memory_per_batch_size_grad_accum.append(v[<span class="st">'max_memory_allocated'</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-49" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:484}}" data-outputid="18e8d285-d783-450c-c0e4-77c9e0dea86b">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(ncols<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">5</span>))</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(batch_sizes_grad_accum, samples_sec_per_batch_size_grad_accum, <span class="st">'-v'</span>)<span class="op">;</span></span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">"Batch size"</span>, fontsize<span class="op">=</span><span class="dv">20</span>)<span class="op">;</span></span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="st">"Samples/sec"</span>, fontsize<span class="op">=</span><span class="dv">20</span>)<span class="op">;</span></span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(batch_sizes_grad_accum, max_memory_per_batch_size_grad_accum, <span class="st">'-v'</span>)<span class="op">;</span></span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">"Batch size"</span>, fontsize<span class="op">=</span><span class="dv">20</span>)<span class="op">;</span></span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_ylabel(<span class="st">"Max memory (Gb)"</span>, fontsize<span class="op">=</span><span class="dv">20</span>)<span class="op">;</span></span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_yticks([<span class="fl">10.5</span>, <span class="fl">10.6</span>])<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Efficient_model_training_files/figure-html/cell-27-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>One can fit large batch size with gradient checkpointing.</p>
<p><strong>Note</strong>, however, that it doesnâ€™t make the training faster in termps of <code>samples/sec</code>.</p>
<p>## Gradient checkpointing</p>
<p>Even when we set the batch size to 1 and use gradient accumulation we can still run out of memory when working with large models. In order to compute the gradients during the backward pass all activations from the forward pass are normally saved. This can create a big memory overhead. Alternatively, one could forget all activations during the forward pass and recompute them on demand during the backward pass.</p>
<p>This allows one reduce memory footprint at the cost of additional computations.</p>
<div id="cell-52" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> types <span class="im">import</span> MethodType</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.checkpoint <span class="im">import</span> checkpoint_sequential</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Below weâ€™ll adopt the training loop from AMP section</p>
<div id="cell-54" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(model, loader, optimizer, scaler, loss_fn, num_steps):</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>  <span class="co">"""</span></span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a><span class="co">  Args:</span></span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a><span class="co">    model - model trained</span></span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a><span class="co">    loader - data loader</span></span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a><span class="co">    optimizer - optimizer</span></span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a><span class="co">    scaler - gradient scaler that shifts the gradient range</span></span>
<span id="cb75-8"><a href="#cb75-8" aria-hidden="true" tabindex="-1"></a><span class="co">    loss_fn - task loss function</span></span>
<span id="cb75-9"><a href="#cb75-9" aria-hidden="true" tabindex="-1"></a><span class="co">    num_steps.- number of training steps</span></span>
<span id="cb75-10"><a href="#cb75-10" aria-hidden="true" tabindex="-1"></a><span class="co">  Returns:</span></span>
<span id="cb75-11"><a href="#cb75-11" aria-hidden="true" tabindex="-1"></a><span class="co">    dict with performance stats</span></span>
<span id="cb75-12"><a href="#cb75-12" aria-hidden="true" tabindex="-1"></a><span class="co">  """</span></span>
<span id="cb75-13"><a href="#cb75-13" aria-hidden="true" tabindex="-1"></a>  samples_processed <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb75-14"><a href="#cb75-14" aria-hidden="true" tabindex="-1"></a>  start <span class="op">=</span> time.perf_counter()</span>
<span id="cb75-15"><a href="#cb75-15" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i <span class="kw">in</span> trange(num_steps, total<span class="op">=</span>num_steps, desc<span class="op">=</span><span class="st">"Train"</span>):</span>
<span id="cb75-16"><a href="#cb75-16" aria-hidden="true" tabindex="-1"></a>    inputs, targets <span class="op">=</span> <span class="bu">next</span>(loader)</span>
<span id="cb75-17"><a href="#cb75-17" aria-hidden="true" tabindex="-1"></a>    inputs, targets <span class="op">=</span> inputs.to(device), targets.to(device)</span>
<span id="cb75-18"><a href="#cb75-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.autocast(device_type<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>torch.float16):</span>
<span id="cb75-19"><a href="#cb75-19" aria-hidden="true" tabindex="-1"></a>      outputs <span class="op">=</span> model(inputs)</span>
<span id="cb75-20"><a href="#cb75-20" aria-hidden="true" tabindex="-1"></a>      loss <span class="op">=</span> loss_fn(outputs, targets)</span>
<span id="cb75-21"><a href="#cb75-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="kw">not</span> loss.isnan(), <span class="st">"Loss is NaN. Terminating training."</span></span>
<span id="cb75-22"><a href="#cb75-22" aria-hidden="true" tabindex="-1"></a>    scaler.scale(loss).backward()</span>
<span id="cb75-23"><a href="#cb75-23" aria-hidden="true" tabindex="-1"></a>    scaler.step(optimizer)</span>
<span id="cb75-24"><a href="#cb75-24" aria-hidden="true" tabindex="-1"></a>    scaler.update()</span>
<span id="cb75-25"><a href="#cb75-25" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb75-26"><a href="#cb75-26" aria-hidden="true" tabindex="-1"></a>    samples_processed <span class="op">+=</span> <span class="bu">len</span>(inputs)</span>
<span id="cb75-27"><a href="#cb75-27" aria-hidden="true" tabindex="-1"></a>  end <span class="op">=</span> time.perf_counter()</span>
<span id="cb75-28"><a href="#cb75-28" aria-hidden="true" tabindex="-1"></a>  samples_per_sec <span class="op">=</span> samples_processed <span class="op">/</span> (end <span class="op">-</span> start)</span>
<span id="cb75-29"><a href="#cb75-29" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb75-30"><a href="#cb75-30" aria-hidden="true" tabindex="-1"></a>  print_memory_stats()</span>
<span id="cb75-31"><a href="#cb75-31" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="ss">f"samples/sec: </span><span class="sc">{</span>samples_per_sec<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb75-32"><a href="#cb75-32" aria-hidden="true" tabindex="-1"></a>  results <span class="op">=</span> {</span>
<span id="cb75-33"><a href="#cb75-33" aria-hidden="true" tabindex="-1"></a>      <span class="st">"samples/sec"</span>: samples_per_sec,</span>
<span id="cb75-34"><a href="#cb75-34" aria-hidden="true" tabindex="-1"></a>      <span class="st">"max_memory_allocated"</span>: torch.cuda.max_memory_allocated() <span class="op">/</span> <span class="dv">2</span> <span class="op">**</span> <span class="dv">30</span></span>
<span id="cb75-35"><a href="#cb75-35" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb75-36"><a href="#cb75-36" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> results</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Let us create the model again for the sake of safety</p>
<div id="cell-56" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> get_model(<span class="st">"swin_b"</span>).to(device)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-57" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>num_segments <span class="op">=</span> <span class="bu">len</span>(model.features) <span class="co"># we set checkpoint after every SwinTransformer block</span></span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a><span class="co"># a hack to change forward pass of a model</span></span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> custom_forward(<span class="va">self</span>, x):</span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>    x.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> checkpoint_sequential(model.features, num_segments, x)</span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.norm(x)</span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.permute(x)</span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.avgpool(x)</span>
<span id="cb77-10"><a href="#cb77-10" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.flatten(x)</span>
<span id="cb77-11"><a href="#cb77-11" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.head(x)</span>
<span id="cb77-12"><a href="#cb77-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x</span>
<span id="cb77-13"><a href="#cb77-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-14"><a href="#cb77-14" aria-hidden="true" tabindex="-1"></a>model.forward <span class="op">=</span> MethodType(custom_forward, model)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-58" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="1a9afe95-f4c3-4ade-e247-1741053efca7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>results_for_batch_size_grad_ckpt <span class="op">=</span> {}</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> batch_size <span class="kw">in</span> (<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">16</span>, <span class="dv">32</span>, <span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">256</span>, <span class="dv">512</span>, <span class="dv">1024</span>):</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">10</span>)</span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="ss">f"Batch size: </span><span class="sc">{</span>batch_size<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># prepare dataloader</span></span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>  dataloader <span class="op">=</span> make_dummy_dataloader(batch_size)</span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># prepare optimizer</span></span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a>  optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">1e-4</span>)</span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># prepare scaler</span></span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a>  scaler <span class="op">=</span> GradScaler()</span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># train</span></span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a>  <span class="cf">try</span>:</span>
<span id="cb78-13"><a href="#cb78-13" aria-hidden="true" tabindex="-1"></a>    training_steps <span class="op">=</span> <span class="bu">min</span>(<span class="dv">4096</span> <span class="op">//</span> batch_size, <span class="dv">100</span>)</span>
<span id="cb78-14"><a href="#cb78-14" aria-hidden="true" tabindex="-1"></a>    results_for_batch_size_grad_ckpt[batch_size] <span class="op">=</span> train(model, dataloader, optimizer, scaler, F.cross_entropy, training_steps)</span>
<span id="cb78-15"><a href="#cb78-15" aria-hidden="true" tabindex="-1"></a>  <span class="cf">except</span>:</span>
<span id="cb78-16"><a href="#cb78-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">CUDA out of memory!"</span>)</span>
<span id="cb78-17"><a href="#cb78-17" aria-hidden="true" tabindex="-1"></a>    print_memory_stats()</span>
<span id="cb78-18"><a href="#cb78-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">del</span> optimizer</span>
<span id="cb78-19"><a href="#cb78-19" aria-hidden="true" tabindex="-1"></a>    torch.cuda.empty_cache()</span>
<span id="cb78-20"><a href="#cb78-20" aria-hidden="true" tabindex="-1"></a>    torch.cuda.reset_peak_memory_stats()</span>
<span id="cb78-21"><a href="#cb78-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">break</span></span>
<span id="cb78-22"><a href="#cb78-22" aria-hidden="true" tabindex="-1"></a>  <span class="kw">del</span> optimizer</span>
<span id="cb78-23"><a href="#cb78-23" aria-hidden="true" tabindex="-1"></a>  torch.cuda.empty_cache()</span>
<span id="cb78-24"><a href="#cb78-24" aria-hidden="true" tabindex="-1"></a>  torch.cuda.reset_peak_memory_stats()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>----------
Batch size: 1</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Train:   0%|          | 0/100 [00:00&lt;?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:542: UserWarning: torch.utils.checkpoint.checkpoint_sequential: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:16&lt;00:00,  6.14it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

CUDA max memory allocated: 2.14 GB.
CUDA max memory reserved: 2.23 GB.
samples/sec: 6.14
----------
Batch size: 2</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:15&lt;00:00,  6.30it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

CUDA max memory allocated: 2.14 GB.
CUDA max memory reserved: 2.24 GB.
samples/sec: 12.59
----------
Batch size: 4</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:15&lt;00:00,  6.32it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

CUDA max memory allocated: 2.14 GB.
CUDA max memory reserved: 2.30 GB.
samples/sec: 25.27
----------
Batch size: 8</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:20&lt;00:00,  4.91it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

CUDA max memory allocated: 2.38 GB.
CUDA max memory reserved: 2.60 GB.
samples/sec: 39.26
----------
Batch size: 16</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:33&lt;00:00,  3.01it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

CUDA max memory allocated: 3.03 GB.
CUDA max memory reserved: 3.28 GB.
samples/sec: 48.19
----------
Batch size: 32</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [01:02&lt;00:00,  1.60it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

CUDA max memory allocated: 4.34 GB.
CUDA max memory reserved: 4.69 GB.
samples/sec: 51.15
----------
Batch size: 64</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [01:18&lt;00:00,  1.23s/it]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

CUDA max memory allocated: 6.98 GB.
CUDA max memory reserved: 8.12 GB.
samples/sec: 52.18
----------
Batch size: 128</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:18&lt;00:00,  2.44s/it]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

CUDA max memory allocated: 12.23 GB.
CUDA max memory reserved: 14.29 GB.
samples/sec: 52.44
----------
Batch size: 256</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Train:   0%|          | 0/16 [00:02&lt;?, ?it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
CUDA out of memory!
CUDA max memory allocated: 14.10 GB.
CUDA max memory reserved: 14.51 GB.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
</div>
<div id="cell-59" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb99"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a>batch_sizes_grad_ckpt <span class="op">=</span> []</span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a>samples_sec_per_batch_size_grad_ckpt <span class="op">=</span> []</span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a>max_memory_per_batch_size_grad_ckpt <span class="op">=</span> []</span>
<span id="cb99-4"><a href="#cb99-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k, v <span class="kw">in</span> results_for_batch_size_grad_ckpt.items():</span>
<span id="cb99-5"><a href="#cb99-5" aria-hidden="true" tabindex="-1"></a>  batch_sizes_grad_ckpt.append(k)</span>
<span id="cb99-6"><a href="#cb99-6" aria-hidden="true" tabindex="-1"></a>  samples_sec_per_batch_size_grad_ckpt.append(v[<span class="st">'samples/sec'</span>])</span>
<span id="cb99-7"><a href="#cb99-7" aria-hidden="true" tabindex="-1"></a>  max_memory_per_batch_size_grad_ckpt.append(v[<span class="st">'max_memory_allocated'</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-60" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:479}}" data-outputid="2c391d9d-e87b-4b50-abd6-72073edacf74">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(ncols<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">5</span>))</span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(batch_sizes_amp, samples_sec_per_batch_size_amp, <span class="st">'-v'</span>, label<span class="op">=</span><span class="st">'amp'</span>)<span class="op">;</span></span>
<span id="cb100-4"><a href="#cb100-4" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(batch_sizes_grad_ckpt, samples_sec_per_batch_size_grad_ckpt, <span class="st">'-v'</span>, label<span class="op">=</span><span class="st">'grad_ckpt'</span>)<span class="op">;</span></span>
<span id="cb100-5"><a href="#cb100-5" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">"Batch size"</span>, fontsize<span class="op">=</span><span class="dv">20</span>)<span class="op">;</span></span>
<span id="cb100-6"><a href="#cb100-6" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="st">"Samples/sec"</span>, fontsize<span class="op">=</span><span class="dv">20</span>)<span class="op">;</span></span>
<span id="cb100-7"><a href="#cb100-7" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].legend()<span class="op">;</span></span>
<span id="cb100-8"><a href="#cb100-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-9"><a href="#cb100-9" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(batch_sizes_amp, max_memory_per_batch_size_amp, <span class="st">'-v'</span>, label<span class="op">=</span><span class="st">'amp'</span>)<span class="op">;</span></span>
<span id="cb100-10"><a href="#cb100-10" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(batch_sizes_grad_ckpt, max_memory_per_batch_size_grad_ckpt, <span class="st">'-v'</span>, label<span class="op">=</span><span class="st">'grad_ckpt'</span>)<span class="op">;</span></span>
<span id="cb100-11"><a href="#cb100-11" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">"Batch size"</span>, fontsize<span class="op">=</span><span class="dv">20</span>)<span class="op">;</span></span>
<span id="cb100-12"><a href="#cb100-12" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_ylabel(<span class="st">"Max memory (Gb)"</span>, fontsize<span class="op">=</span><span class="dv">20</span>)<span class="op">;</span></span>
<span id="cb100-13"><a href="#cb100-13" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].legend()<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Efficient_model_training_files/figure-html/cell-34-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p><strong>Nice!</strong> ðŸ™€</p>
<p>Now we can fit even larger batch size <strong>128</strong> onto a single <code>T4</code> GPU!</p>
<p>Note, that this tecnique doesnâ€™t typically accelerate training.</p>
<p>However, in case the model is pretty large (such as modern transformer), and inputs are pretty large (large images or long sequences), <code>gradient_checkpointing</code> is very reasonable option to try.</p>
<p>Some frameworks and libraries support <code>gradient_checkpointing</code> out of the box: * <code>timm</code> - <code>set_grad_checkpointing</code> method in some models * <code>transformers</code> ðŸ¤— - <code>model.gradient_checkpointing_enable()</code></p>
<p>Additional resources: * <a href="https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9">Article on gradient checkpointing</a></p>
</section>
<section id="model-compilation" class="level2">
<h2 class="anchored" data-anchor-id="model-compilation">Model compilation</h2>
<p>Since <code>torch &gt; 2.0</code> users can compile their model prior to running. Compilation can make PyTorch code run faster by JIT-compiling PyTorch code into optimized kernels, all while requiring minimal code changes.</p>
<p>However, note that the compilation takes some time, as the users familiar with compiled languages may remember.</p>
<p>If the run is very short it may be not the best option.</p>
<p>For long enough runtime <code>torch.compile</code> offers nontrivial speed-up of training and inference.</p>
<p>Below we will illustrate the benefit of <code>torch.compile</code> on RMSNorm, a commonly used normalization layer in modern LLMs.</p>
<div id="cell-65" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb101"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RMSNorm(nn.Module):</span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, hidden_size, eps<span class="op">=</span><span class="fl">1e-6</span>):</span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb101-5"><a href="#cb101-5" aria-hidden="true" tabindex="-1"></a><span class="co">        RMSNorm is equivalent to T5LayerNorm</span></span>
<span id="cb101-6"><a href="#cb101-6" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb101-7"><a href="#cb101-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb101-8"><a href="#cb101-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weight <span class="op">=</span> nn.Parameter(torch.ones(hidden_size))</span>
<span id="cb101-9"><a href="#cb101-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.variance_epsilon <span class="op">=</span> eps</span>
<span id="cb101-10"><a href="#cb101-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-11"><a href="#cb101-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, hidden_states):</span>
<span id="cb101-12"><a href="#cb101-12" aria-hidden="true" tabindex="-1"></a>        input_dtype <span class="op">=</span> hidden_states.dtype</span>
<span id="cb101-13"><a href="#cb101-13" aria-hidden="true" tabindex="-1"></a>        hidden_states <span class="op">=</span> hidden_states.to(torch.float32)</span>
<span id="cb101-14"><a href="#cb101-14" aria-hidden="true" tabindex="-1"></a>        variance <span class="op">=</span> hidden_states.<span class="bu">pow</span>(<span class="dv">2</span>).mean(<span class="op">-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb101-15"><a href="#cb101-15" aria-hidden="true" tabindex="-1"></a>        hidden_states <span class="op">=</span> hidden_states <span class="op">*</span> torch.rsqrt(variance <span class="op">+</span> <span class="va">self</span>.variance_epsilon)</span>
<span id="cb101-16"><a href="#cb101-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.weight <span class="op">*</span> hidden_states.to(input_dtype)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-66" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Llama-2-7b sizes</span></span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb102-3"><a href="#cb102-3" aria-hidden="true" tabindex="-1"></a>sequence_length <span class="op">=</span> <span class="dv">4096</span></span>
<span id="cb102-4"><a href="#cb102-4" aria-hidden="true" tabindex="-1"></a>hidden_size <span class="op">=</span> <span class="dv">4096</span></span>
<span id="cb102-5"><a href="#cb102-5" aria-hidden="true" tabindex="-1"></a><span class="co"># benchmark settings</span></span>
<span id="cb102-6"><a href="#cb102-6" aria-hidden="true" tabindex="-1"></a>warmup_iters <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb102-7"><a href="#cb102-7" aria-hidden="true" tabindex="-1"></a>benchmark_iters <span class="op">=</span> <span class="dv">1000</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-67" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb103"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a><span class="co"># generate random sequence</span></span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> torch.randn(batch_size, sequence_length, hidden_size, device<span class="op">=</span>device, dtype<span class="op">=</span>torch.float16)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Define RMSNorm layer</p>
<div id="cell-69" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb104"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a>rms_norm <span class="op">=</span> RMSNorm(hidden_size).to(device<span class="op">=</span>device, dtype<span class="op">=</span>torch.float16)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-70" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb105"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(warmup_iters):</span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a>  rms_norm(inputs)</span>
<span id="cb105-3"><a href="#cb105-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-4"><a href="#cb105-4" aria-hidden="true" tabindex="-1"></a>times <span class="op">=</span> []</span>
<span id="cb105-5"><a href="#cb105-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(benchmark_iters):</span>
<span id="cb105-6"><a href="#cb105-6" aria-hidden="true" tabindex="-1"></a>  start <span class="op">=</span> torch.cuda.Event(enable_timing<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb105-7"><a href="#cb105-7" aria-hidden="true" tabindex="-1"></a>  end <span class="op">=</span> torch.cuda.Event(enable_timing<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb105-8"><a href="#cb105-8" aria-hidden="true" tabindex="-1"></a>  start.record()</span>
<span id="cb105-9"><a href="#cb105-9" aria-hidden="true" tabindex="-1"></a>  rms_norm(inputs)</span>
<span id="cb105-10"><a href="#cb105-10" aria-hidden="true" tabindex="-1"></a>  end.record()</span>
<span id="cb105-11"><a href="#cb105-11" aria-hidden="true" tabindex="-1"></a>  torch.cuda.synchronize()</span>
<span id="cb105-12"><a href="#cb105-12" aria-hidden="true" tabindex="-1"></a>  times.append(start.elapsed_time(end))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-71" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="9328ddef-de6d-4f52-8170-96dd39fa0ecc">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb106"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Average latency: </span><span class="sc">{</span>np<span class="sc">.</span>mean(times)<span class="sc">:.3f}</span><span class="ss"> ms"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Average latency: 2.627 ms</code></pre>
</div>
</div>
<p>Let us compile the layer.</p>
<div id="cell-73" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb108"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a>rms_norm <span class="op">=</span> torch.<span class="bu">compile</span>(rms_norm)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-74" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb109"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(warmup_iters):</span>
<span id="cb109-2"><a href="#cb109-2" aria-hidden="true" tabindex="-1"></a>  rms_norm(inputs)</span>
<span id="cb109-3"><a href="#cb109-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb109-4"><a href="#cb109-4" aria-hidden="true" tabindex="-1"></a>times <span class="op">=</span> []</span>
<span id="cb109-5"><a href="#cb109-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(benchmark_iters):</span>
<span id="cb109-6"><a href="#cb109-6" aria-hidden="true" tabindex="-1"></a>  start <span class="op">=</span> torch.cuda.Event(enable_timing<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb109-7"><a href="#cb109-7" aria-hidden="true" tabindex="-1"></a>  end <span class="op">=</span> torch.cuda.Event(enable_timing<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb109-8"><a href="#cb109-8" aria-hidden="true" tabindex="-1"></a>  start.record()</span>
<span id="cb109-9"><a href="#cb109-9" aria-hidden="true" tabindex="-1"></a>  rms_norm(inputs)</span>
<span id="cb109-10"><a href="#cb109-10" aria-hidden="true" tabindex="-1"></a>  end.record()</span>
<span id="cb109-11"><a href="#cb109-11" aria-hidden="true" tabindex="-1"></a>  torch.cuda.synchronize()</span>
<span id="cb109-12"><a href="#cb109-12" aria-hidden="true" tabindex="-1"></a>  times.append(start.elapsed_time(end))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-75" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="676236b3-5afc-4a7e-9df0-1087a7bdecd7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb110"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Average latency: </span><span class="sc">{</span>np<span class="sc">.</span>mean(times)<span class="sc">:.3f}</span><span class="ss"> ms"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Average latency: 0.449 ms</code></pre>
</div>
</div>
<p>4-5x speedup!</p>
<p><img src="https://resizing.flixster.com/1Y1Dup_mSJqfq4y-QXTwqbuG45Y=/fit-in/705x460/v2/https://resizing.flixster.com/-XZAfHZM39UwaGJIFWKAE8fS0ak=/v3/t/assets/p2845607_e_h9_ab.jpg" width="300" alt="Deep Recurrent Neural Networks"></p>
<div id="cell-77" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb112"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a><span class="kw">del</span> rms_norm, inputs</span>
<span id="cb112-2"><a href="#cb112-2" aria-hidden="true" tabindex="-1"></a>torch.cuda.empty_cache()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Now let us apply torch compile to training the model from previous examples.</p>
<div id="cell-79" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb113"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(model, loader, optimizer, scaler, loss_fn, num_steps, warmup_steps<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb113-2"><a href="#cb113-2" aria-hidden="true" tabindex="-1"></a>  <span class="co">"""</span></span>
<span id="cb113-3"><a href="#cb113-3" aria-hidden="true" tabindex="-1"></a><span class="co">  Args:</span></span>
<span id="cb113-4"><a href="#cb113-4" aria-hidden="true" tabindex="-1"></a><span class="co">    model - model trained</span></span>
<span id="cb113-5"><a href="#cb113-5" aria-hidden="true" tabindex="-1"></a><span class="co">    loader - data loader</span></span>
<span id="cb113-6"><a href="#cb113-6" aria-hidden="true" tabindex="-1"></a><span class="co">    optimizer - optimizer</span></span>
<span id="cb113-7"><a href="#cb113-7" aria-hidden="true" tabindex="-1"></a><span class="co">    scaler - gradient scaler that shifts the gradient range</span></span>
<span id="cb113-8"><a href="#cb113-8" aria-hidden="true" tabindex="-1"></a><span class="co">    loss_fn - task loss function</span></span>
<span id="cb113-9"><a href="#cb113-9" aria-hidden="true" tabindex="-1"></a><span class="co">    num_steps - number of training steps</span></span>
<span id="cb113-10"><a href="#cb113-10" aria-hidden="true" tabindex="-1"></a><span class="co">    num_steps - number of warmup steps</span></span>
<span id="cb113-11"><a href="#cb113-11" aria-hidden="true" tabindex="-1"></a><span class="co">  Returns:</span></span>
<span id="cb113-12"><a href="#cb113-12" aria-hidden="true" tabindex="-1"></a><span class="co">    dict with performance stats</span></span>
<span id="cb113-13"><a href="#cb113-13" aria-hidden="true" tabindex="-1"></a><span class="co">  """</span></span>
<span id="cb113-14"><a href="#cb113-14" aria-hidden="true" tabindex="-1"></a>  samples_processed <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb113-15"><a href="#cb113-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># warmup</span></span>
<span id="cb113-16"><a href="#cb113-16" aria-hidden="true" tabindex="-1"></a>  start <span class="op">=</span> time.perf_counter()</span>
<span id="cb113-17"><a href="#cb113-17" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(warmup_steps):</span>
<span id="cb113-18"><a href="#cb113-18" aria-hidden="true" tabindex="-1"></a>    inputs, targets <span class="op">=</span> <span class="bu">next</span>(loader)</span>
<span id="cb113-19"><a href="#cb113-19" aria-hidden="true" tabindex="-1"></a>    inputs, targets <span class="op">=</span> inputs.to(device), targets.to(device)</span>
<span id="cb113-20"><a href="#cb113-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.autocast(device_type<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>torch.float16):</span>
<span id="cb113-21"><a href="#cb113-21" aria-hidden="true" tabindex="-1"></a>      outputs <span class="op">=</span> model(inputs)</span>
<span id="cb113-22"><a href="#cb113-22" aria-hidden="true" tabindex="-1"></a>      loss <span class="op">=</span> loss_fn(outputs, targets)</span>
<span id="cb113-23"><a href="#cb113-23" aria-hidden="true" tabindex="-1"></a>    scaler.scale(loss).backward()</span>
<span id="cb113-24"><a href="#cb113-24" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb113-25"><a href="#cb113-25" aria-hidden="true" tabindex="-1"></a>  end <span class="op">=</span> time.perf_counter()</span>
<span id="cb113-26"><a href="#cb113-26" aria-hidden="true" tabindex="-1"></a>  compile_time <span class="op">=</span> end <span class="op">-</span> start</span>
<span id="cb113-27"><a href="#cb113-27" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="ss">f"Compilation took </span><span class="sc">{</span>(end <span class="op">-</span> start)<span class="sc">:.2f}</span><span class="ss">s."</span>)</span>
<span id="cb113-28"><a href="#cb113-28" aria-hidden="true" tabindex="-1"></a>  <span class="co"># start of training loop</span></span>
<span id="cb113-29"><a href="#cb113-29" aria-hidden="true" tabindex="-1"></a>  start <span class="op">=</span> time.perf_counter()</span>
<span id="cb113-30"><a href="#cb113-30" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i <span class="kw">in</span> trange(num_steps , total<span class="op">=</span>num_steps, desc<span class="op">=</span><span class="st">"Train"</span>):</span>
<span id="cb113-31"><a href="#cb113-31" aria-hidden="true" tabindex="-1"></a>    inputs, targets <span class="op">=</span> <span class="bu">next</span>(loader)</span>
<span id="cb113-32"><a href="#cb113-32" aria-hidden="true" tabindex="-1"></a>    inputs, targets <span class="op">=</span> inputs.to(device), targets.to(device)</span>
<span id="cb113-33"><a href="#cb113-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.autocast(device_type<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>torch.float16):</span>
<span id="cb113-34"><a href="#cb113-34" aria-hidden="true" tabindex="-1"></a>      outputs <span class="op">=</span> model(inputs)</span>
<span id="cb113-35"><a href="#cb113-35" aria-hidden="true" tabindex="-1"></a>      loss <span class="op">=</span> loss_fn(outputs, targets)</span>
<span id="cb113-36"><a href="#cb113-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="kw">not</span> loss.isnan(), <span class="st">"Loss is NaN. Terminating training."</span></span>
<span id="cb113-37"><a href="#cb113-37" aria-hidden="true" tabindex="-1"></a>    scaler.scale(loss).backward()</span>
<span id="cb113-38"><a href="#cb113-38" aria-hidden="true" tabindex="-1"></a>    scaler.step(optimizer)</span>
<span id="cb113-39"><a href="#cb113-39" aria-hidden="true" tabindex="-1"></a>    scaler.update()</span>
<span id="cb113-40"><a href="#cb113-40" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb113-41"><a href="#cb113-41" aria-hidden="true" tabindex="-1"></a>    samples_processed <span class="op">+=</span> <span class="bu">len</span>(inputs)</span>
<span id="cb113-42"><a href="#cb113-42" aria-hidden="true" tabindex="-1"></a>  end <span class="op">=</span> time.perf_counter()</span>
<span id="cb113-43"><a href="#cb113-43" aria-hidden="true" tabindex="-1"></a>  samples_per_sec <span class="op">=</span> samples_processed <span class="op">/</span> (end <span class="op">-</span> start)</span>
<span id="cb113-44"><a href="#cb113-44" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb113-45"><a href="#cb113-45" aria-hidden="true" tabindex="-1"></a>  print_memory_stats()</span>
<span id="cb113-46"><a href="#cb113-46" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="ss">f"samples/sec: </span><span class="sc">{</span>samples_per_sec<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb113-47"><a href="#cb113-47" aria-hidden="true" tabindex="-1"></a>  results <span class="op">=</span> {</span>
<span id="cb113-48"><a href="#cb113-48" aria-hidden="true" tabindex="-1"></a>      <span class="st">"samples/sec"</span>: samples_per_sec,</span>
<span id="cb113-49"><a href="#cb113-49" aria-hidden="true" tabindex="-1"></a>      <span class="st">"max_memory_allocated"</span>: torch.cuda.max_memory_allocated() <span class="op">/</span> <span class="dv">2</span> <span class="op">**</span> <span class="dv">30</span>,</span>
<span id="cb113-50"><a href="#cb113-50" aria-hidden="true" tabindex="-1"></a>      <span class="st">"compile_time"</span>: compile_time</span>
<span id="cb113-51"><a href="#cb113-51" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb113-52"><a href="#cb113-52" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> results</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-80" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb114"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> get_model(<span class="st">"swin_b"</span>).to(device)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-81" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="16a0ccbf-cc42-404c-96a4-96b4625cfb0b">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb115"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> torch.<span class="bu">compile</span>(model)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.
  self.pid = os.fork()</code></pre>
</div>
</div>
<div id="cell-82" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="e9ec3df3-17f2-4d2b-c55e-01c99aced112">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb117"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a>results_for_batch_size_compile <span class="op">=</span> {}</span>
<span id="cb117-2"><a href="#cb117-2" aria-hidden="true" tabindex="-1"></a>training_steps <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb117-3"><a href="#cb117-3" aria-hidden="true" tabindex="-1"></a>warmup_steps <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb117-4"><a href="#cb117-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb117-5"><a href="#cb117-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> batch_size <span class="kw">in</span> (<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">16</span>, <span class="dv">32</span>, <span class="dv">64</span>):</span>
<span id="cb117-6"><a href="#cb117-6" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">10</span>)</span>
<span id="cb117-7"><a href="#cb117-7" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="ss">f"Batch size: </span><span class="sc">{</span>batch_size<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb117-8"><a href="#cb117-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># prepare dataloader</span></span>
<span id="cb117-9"><a href="#cb117-9" aria-hidden="true" tabindex="-1"></a>  dataloader <span class="op">=</span> make_dummy_dataloader(batch_size)</span>
<span id="cb117-10"><a href="#cb117-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># prepare optimizer</span></span>
<span id="cb117-11"><a href="#cb117-11" aria-hidden="true" tabindex="-1"></a>  optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">1e-4</span>)</span>
<span id="cb117-12"><a href="#cb117-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># prepare scaler</span></span>
<span id="cb117-13"><a href="#cb117-13" aria-hidden="true" tabindex="-1"></a>  scaler <span class="op">=</span> GradScaler()</span>
<span id="cb117-14"><a href="#cb117-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># train</span></span>
<span id="cb117-15"><a href="#cb117-15" aria-hidden="true" tabindex="-1"></a>  <span class="cf">try</span>:</span>
<span id="cb117-16"><a href="#cb117-16" aria-hidden="true" tabindex="-1"></a>    results_for_batch_size_compile[batch_size] <span class="op">=</span> train(model, dataloader, optimizer, scaler, F.cross_entropy, training_steps, warmup_steps)</span>
<span id="cb117-17"><a href="#cb117-17" aria-hidden="true" tabindex="-1"></a>  <span class="cf">except</span>:</span>
<span id="cb117-18"><a href="#cb117-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">CUDA out of memory!"</span>)</span>
<span id="cb117-19"><a href="#cb117-19" aria-hidden="true" tabindex="-1"></a>    print_memory_stats()</span>
<span id="cb117-20"><a href="#cb117-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">del</span> optimizer</span>
<span id="cb117-21"><a href="#cb117-21" aria-hidden="true" tabindex="-1"></a>    torch.cuda.empty_cache()</span>
<span id="cb117-22"><a href="#cb117-22" aria-hidden="true" tabindex="-1"></a>    torch.cuda.reset_peak_memory_stats()</span>
<span id="cb117-23"><a href="#cb117-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">break</span></span>
<span id="cb117-24"><a href="#cb117-24" aria-hidden="true" tabindex="-1"></a>  <span class="kw">del</span> optimizer</span>
<span id="cb117-25"><a href="#cb117-25" aria-hidden="true" tabindex="-1"></a>  torch.cuda.empty_cache()</span>
<span id="cb117-26"><a href="#cb117-26" aria-hidden="true" tabindex="-1"></a>  torch.cuda.reset_peak_memory_stats()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>----------
Batch size: 1
Compilation took 110.28s.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08&lt;00:00, 11.23it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

CUDA max memory allocated: 1.67 GB.
CUDA max memory reserved: 1.80 GB.
samples/sec: 11.22
----------
Batch size: 2
Compilation took 228.16s.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:10&lt;00:00,  9.47it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

CUDA max memory allocated: 1.68 GB.
CUDA max memory reserved: 1.80 GB.
samples/sec: 18.93
----------
Batch size: 4
Compilation took 0.65s.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:09&lt;00:00, 10.61it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

CUDA max memory allocated: 1.73 GB.
CUDA max memory reserved: 1.91 GB.
samples/sec: 42.42
----------
Batch size: 8
Compilation took 0.79s.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:11&lt;00:00,  8.85it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

CUDA max memory allocated: 2.30 GB.
CUDA max memory reserved: 2.50 GB.
samples/sec: 70.78
----------
Batch size: 16
Compilation took 1.45s.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:18&lt;00:00,  5.40it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

CUDA max memory allocated: 3.42 GB.
CUDA max memory reserved: 3.86 GB.
samples/sec: 86.31
----------
Batch size: 32
Compilation took 2.82s.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:34&lt;00:00,  2.92it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

CUDA max memory allocated: 5.83 GB.
CUDA max memory reserved: 6.46 GB.
samples/sec: 93.34
----------
Batch size: 64
Compilation took 5.45s.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [01:08&lt;00:00,  1.47it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

CUDA max memory allocated: 10.66 GB.
CUDA max memory reserved: 11.63 GB.
samples/sec: 93.77</code></pre>
</div>
</div>
<p>How long does the compilation take? â²</p>
<div id="cell-84" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb133"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb133-1"><a href="#cb133-1" aria-hidden="true" tabindex="-1"></a>batch_sizes_compile <span class="op">=</span> []</span>
<span id="cb133-2"><a href="#cb133-2" aria-hidden="true" tabindex="-1"></a>samples_sec_per_batch_size_compile <span class="op">=</span> []</span>
<span id="cb133-3"><a href="#cb133-3" aria-hidden="true" tabindex="-1"></a>max_memory_per_batch_size_compile <span class="op">=</span> []</span>
<span id="cb133-4"><a href="#cb133-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k, v <span class="kw">in</span> results_for_batch_size_compile.items():</span>
<span id="cb133-5"><a href="#cb133-5" aria-hidden="true" tabindex="-1"></a>  batch_sizes_compile.append(k)</span>
<span id="cb133-6"><a href="#cb133-6" aria-hidden="true" tabindex="-1"></a>  samples_sec_per_batch_size_compile.append(v[<span class="st">'samples/sec'</span>])</span>
<span id="cb133-7"><a href="#cb133-7" aria-hidden="true" tabindex="-1"></a>  max_memory_per_batch_size_compile.append(v[<span class="st">'max_memory_allocated'</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-85" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:484}}" data-outputid="ebe82f17-808d-425f-abce-a0393b8476ed">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb134"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(ncols<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">5</span>))</span>
<span id="cb134-2"><a href="#cb134-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-3"><a href="#cb134-3" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(batch_sizes_amp, samples_sec_per_batch_size_amp, <span class="st">'-v'</span>, label<span class="op">=</span><span class="st">'amp'</span>)<span class="op">;</span></span>
<span id="cb134-4"><a href="#cb134-4" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].plot(batch_sizes_compile, samples_sec_per_batch_size_compile, <span class="st">'-v'</span>, label<span class="op">=</span><span class="st">'compile'</span>)<span class="op">;</span></span>
<span id="cb134-5"><a href="#cb134-5" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">"Batch size"</span>, fontsize<span class="op">=</span><span class="dv">20</span>)<span class="op">;</span></span>
<span id="cb134-6"><a href="#cb134-6" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="st">"Samples/sec"</span>, fontsize<span class="op">=</span><span class="dv">20</span>)<span class="op">;</span></span>
<span id="cb134-7"><a href="#cb134-7" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].legend()<span class="op">;</span></span>
<span id="cb134-8"><a href="#cb134-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-9"><a href="#cb134-9" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(batch_sizes_amp, max_memory_per_batch_size_amp, <span class="st">'-v'</span>, label<span class="op">=</span><span class="st">'amp'</span>)<span class="op">;</span></span>
<span id="cb134-10"><a href="#cb134-10" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(batch_sizes_compile, max_memory_per_batch_size_compile, <span class="st">'-v'</span>, label<span class="op">=</span><span class="st">'compile'</span>)<span class="op">;</span></span>
<span id="cb134-11"><a href="#cb134-11" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">"Batch size"</span>, fontsize<span class="op">=</span><span class="dv">20</span>)<span class="op">;</span></span>
<span id="cb134-12"><a href="#cb134-12" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_ylabel(<span class="st">"Max memory (Gb)"</span>, fontsize<span class="op">=</span><span class="dv">20</span>)<span class="op">;</span></span>
<span id="cb134-13"><a href="#cb134-13" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].legend()<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Efficient_model_training_files/figure-html/cell-50-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Do you observe benefits from compilation?</p>
<p>Additional resources * <a href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html">PyTorch tutorial</a></p>
</section>
<section id="summary-and-concluding-remarks" class="level2">
<h2 class="anchored" data-anchor-id="summary-and-concluding-remarks">Summary and concluding remarks</h2>
<p>In the table below we summarize the benefits proposed by each method covered in the seminar.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Method</th>
<th>Speed</th>
<th>Memory</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Batch size</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="even">
<td>Mixed precision training</td>
<td>Yes</td>
<td>Yes*</td>
</tr>
<tr class="odd">
<td>Gradient accumulation</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr class="even">
<td>Gradient checkpointing</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr class="odd">
<td>Compilation</td>
<td>Yes</td>
<td>No</td>
</tr>
</tbody>
</table>
<p>Some notes are worth being mentioned. * AMP is a great choice for small model, when the model takes small fraction of the system VRAM. However, it requires to store a version of model both in fp16 and fp32 precision and may even increase memory overhead when working with large models.</p>
<p>Additional materials * <a href="https://huggingface.co/docs/transformers/v4.20.1/en/perf_train_gpu_one#efficient-training-on-a-single-gpu">Great overview with examples on Huggingface on efficient training</a> ðŸ¤—</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "î§‹";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>