{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCq6aF0rVLtb"
      },
      "source": [
        "#Attention_mechanismand RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyyOEwvgGk_U"
      },
      "source": [
        "# Recurrent neural networks (RNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKctr4a1sLP-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gq_BGEctsYuE"
      },
      "outputs": [],
      "source": [
        "# Hyper-parameters\n",
        "\n",
        "num_classes = 10\n",
        "n_iters = 3000\n",
        "batch_size = 100\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCAKjeHmO1SP"
      },
      "outputs": [],
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data',\n",
        "                                           train=True,\n",
        "                                           transform=transforms.ToTensor(),\n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data',\n",
        "                                          train=False,\n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader: These loaders handle the shuffling and batching of the dataset during training and testing, respectively.\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2e2Y7Emusc-9"
      },
      "outputs": [],
      "source": [
        "print(train_dataset.train_data.size())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WANwnjnOsgk8"
      },
      "outputs": [],
      "source": [
        "print(train_dataset.train_labels.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvTAmNjusvrv"
      },
      "outputs": [],
      "source": [
        "print(test_dataset.test_data.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dr1bnT7IszH9"
      },
      "outputs": [],
      "source": [
        "print(test_dataset.test_labels.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AT-UP4hxeIv"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/ritchieng/deep-learning-wizard/dc6fb5ccfaf6ca4760f673c2384330d5b2069bf2/docs/deep_learning/practical_pytorch/images/rnn4n.png\" alt=\"Deep Recurrent Neural Networks\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ec4PMG0uUI9P"
      },
      "outputs": [],
      "source": [
        "num_epochs = n_iters / (len(train_dataset) / batch_size) # This calculation ensures that the model goes through the entire dataset\n",
        "num_epochs = int(num_epochs)\n",
        "learning_rate = 0.001\n",
        "\n",
        "input_size = 28\n",
        "sequence_length = 28\n",
        "hidden_size = 128\n",
        "num_layers = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXEL77iI5ZQz"
      },
      "outputs": [],
      "source": [
        "# Fully connected neural network with one hidden layer\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        # Hidden dimensions\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "\n",
        "        # Number of hidden layers\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # TODO\n",
        "        # Building your RNN\n",
        "        # batch_first=True causes input/output tensors to be of shape:\n",
        "        # (batch_dim, seq_dim, input_dim) -> x needs to be: (batch_size, seq, input_size)\n",
        "        self.rnn = ...\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # TODO\n",
        "        # Initialize hidden state with zeros\n",
        "        # (layer_dim, batch_size, hidden_dim)\n",
        "        h0 = ...\n",
        "\n",
        "        # x: (n, 28, 28), h0: (2, n, 128)\n",
        "\n",
        "        # Forward propagate RNN\n",
        "        out, _ = self.rnn(x, h0)\n",
        "\n",
        "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "        # out: (n, 28, 128)\n",
        "\n",
        "        # Decode the hidden state of the last time step\n",
        "        out = out[:, -1, :]\n",
        "        # out: (n, 128)\n",
        "\n",
        "        out = self.fc(out)\n",
        "        # out: (n, 10)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhKXdRM0PCu4"
      },
      "outputs": [],
      "source": [
        "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # origin shape: [N, 1, 28, 28]\n",
        "        # resized: [N, 28, 28]\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the 10000 test images: {acc} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG_ZieWB8nwk"
      },
      "source": [
        "### Deep RNN\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/ritchieng/deep-learning-wizard/dc6fb5ccfaf6ca4760f673c2384330d5b2069bf2/docs/deep_learning/practical_pytorch/images/rnn6.png\" alt=\"Deep Recurrent Neural Networks\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuNL0F9-Y1RP"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "# Increase number of Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYLbLmi4wDL9"
      },
      "outputs": [],
      "source": [
        "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # origin shape: [N, 1, 28, 28]\n",
        "        # resized: [N, 28, 28]\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the 10000 test images: {acc} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtxLH0Mb9YRV"
      },
      "source": [
        "### Bidirectional RNN\n",
        "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20230302163012/Bidirectional-Recurrent-Neural-Network-2.png\" alt=\"Bidirectional Recurrent Neural Networks\" width=\"600\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcbmsAz0UXli"
      },
      "outputs": [],
      "source": [
        "num_layers = 1\n",
        "class BiRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(BiRNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # TODO: Add \"bidirectional=True\" argument to the RNN model\n",
        "        self.rnn = ...\n",
        "        self.fc = nn.Linear(hidden_size * 2, num_classes)  # Multiply by 2 for bidirectional\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # TODO\n",
        "        # Initialize hidden state with zeros\n",
        "        # num_layers * 2\n",
        "        h0 = ...\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3RGNbf-zVPq"
      },
      "outputs": [],
      "source": [
        "model = BiRNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # origin shape: [N, 1, 28, 28]\n",
        "        # resized: [N, 28, 28]\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the 10000 test images: {acc} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwT9r8kI2aIH"
      },
      "source": [
        "For more information:\n",
        "[deeplearningwizard](https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_recurrent_neuralnetwork/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipnCuess_nkN"
      },
      "source": [
        "# LSTM\n",
        "\n",
        "<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\" alt=\"LSTM\" width=\"600\">\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://www.deeplearningwizard.com/deep_learning/practical_pytorch/images/lstm2.png\" alt=\"LSTM\" width=\"800\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TgS3Ct85ZQ_"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(LSTM, self).__init__()\n",
        "\n",
        "        # Hidden dimensions\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "\n",
        "        # Number of hidden layers\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "\n",
        "        # Building your LSTM\n",
        "        # batch_first=True causes input/output tensors to be of shape\n",
        "        # (batch_dim, seq_dim, feature_dim)\n",
        "        # -> x needs to be: (batch_size, seq, input_size)\n",
        "        self.lstm = ...\n",
        "\n",
        "        # Readout layer\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Initialize hidden state with zeros\n",
        "        h0 = ...\n",
        "\n",
        "        # TODO: Initialize cell state\n",
        "        c0 = ...\n",
        "\n",
        "        # x: (n, 28, 28), h0: (2, n, 128)\n",
        "\n",
        "        # Forward propagate LSTM\n",
        "\n",
        "        out, _ = self.lstm(x, (h0,c0))\n",
        "\n",
        "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "        # out: (n, 28, 128)\n",
        "\n",
        "        # Decode the hidden state of the last time step\n",
        "        out = out[:, -1, :]\n",
        "        # out: (n, 128)\n",
        "\n",
        "        out = self.fc(out)\n",
        "        # out: (n, 10)\n",
        "        return out\n",
        "\n",
        "model = LSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # origin shape: [N, 1, 28, 28]\n",
        "        # resized: [N, 28, 28]\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the 10000 test images: {acc} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noTZk1zF_8Zg"
      },
      "source": [
        "### Bidirectional LSTM\n",
        "\n",
        "<img src=\"https://www.baeldung.com/wp-content/uploads/sites/4/2022/01/bilstm-1-1024x384.png\" alt=\"BiLSTM\" width=\"600\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3po86gTVd8_"
      },
      "outputs": [],
      "source": [
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # -> x needs to be: (batch_size, seq, input_size)\n",
        "        # \"bidirectional=True\"\n",
        "\n",
        "        self.lstm = ...\n",
        "        self.fc = nn.Linear(hidden_size * 2, num_classes)  # Multiply by 2 for bidirectional\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Set initial hidden states and cell states\n",
        "        #  num_layers * 2\n",
        "        h0 = ...\n",
        "        c0 = ...\n",
        "        # x: (n, seq, input_size), h0: (num_layers*num_directions, n, hidden_size)\n",
        "\n",
        "        # Forward propagate LSTM\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # out: tensor of shape (batch_size, seq_length, hidden_size*num_directions)\n",
        "        # out: (n, seq, hidden_size*2)\n",
        "\n",
        "        # Decode the hidden state of the last time step\n",
        "        out = out[:, -1, :]\n",
        "        # out: (n, hidden_size*2)\n",
        "\n",
        "        out = self.fc(out)\n",
        "        # out: (n, num_classes)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8bakRZ_WhVC"
      },
      "outputs": [],
      "source": [
        "model = BiLSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # origin shape: [N, 1, 28, 28]\n",
        "        # resized: [N, 28, 28]\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network on the 10000 test images: {acc} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owezC0mp3KO-"
      },
      "source": [
        "For more information:\n",
        "[deeplearningwizard](https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_lstm_neuralnetwork/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8__oGYD5ZRB"
      },
      "source": [
        "# Building a GPT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgO7yYzc5ZRB",
        "outputId": "cc6daaa3-6125-44bf-e8e6-e1c844451cce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-04-11 10:38:30--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-04-11 10:38:30 (20.1 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Let's download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVdoGoLr5ZRB"
      },
      "outputs": [],
      "source": [
        "# read it in to inspect it\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ln5Pgwk15ZRB",
        "outputId": "51e9603e-33eb-4a84-9c0e-036f77c45198"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ],
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i99ib5qk5ZRC",
        "outputId": "bcde11c7-78c6-4987-918a-e7068fe80689"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3znzMcq5ZRC",
        "outputId": "59e16e5c-a8d0-49db-ca36-ec5bc3f6d055"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ],
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LKogC1f5ZRC",
        "outputId": "16bcf60f-ea60-4d2f-a72e-b93ca66cda4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ],
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJpZrE6s5ZRC",
        "outputId": "66da158c-3829-4466-dece-74da74fac85c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ],
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_cRXl4X5ZRC"
      },
      "outputs": [],
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roiSea685ZRD",
        "outputId": "0d96e1a6-1945-40a4-e4bb-9ec05375cf13"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpGuXDPh5ZRD",
        "outputId": "25560f60-a912-4e45-f644-26070b3ffdd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRxeSUoP5ZRD",
        "outputId": "0ac2849a-fee2-411b-bd70-a23fd8944608"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----\n",
            "when input is [24] the target: 43\n",
            "when input is [24, 43] the target: 58\n",
            "when input is [24, 43, 58] the target: 5\n",
            "when input is [24, 43, 58, 5] the target: 57\n",
            "when input is [24, 43, 58, 5, 57] the target: 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
            "when input is [44] the target: 53\n",
            "when input is [44, 53] the target: 56\n",
            "when input is [44, 53, 56] the target: 1\n",
            "when input is [44, 53, 56, 1] the target: 58\n",
            "when input is [44, 53, 56, 1, 58] the target: 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52] the target: 58\n",
            "when input is [52, 58] the target: 1\n",
            "when input is [52, 58, 1] the target: 58\n",
            "when input is [52, 58, 1, 58] the target: 46\n",
            "when input is [52, 58, 1, 58, 46] the target: 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
            "when input is [25] the target: 17\n",
            "when input is [25, 17] the target: 27\n",
            "when input is [25, 17, 27] the target: 10\n",
            "when input is [25, 17, 27, 10] the target: 0\n",
            "when input is [25, 17, 27, 10, 0] the target: 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOaDdt4q5ZRD",
        "outputId": "f3b524f3-f5f8-46ad-8cca-b4e91dacb6bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
          ]
        }
      ],
      "source": [
        "print(xb) # our input to the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdl5COb0H_1f"
      },
      "source": [
        "### N-gram Language Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XS-I6CA55ZRD",
        "outputId": "6b656d48-2f42-4ae4-ac96-837889a00961"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YJq2d6H5ZRD"
      },
      "outputs": [],
      "source": [
        " # create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff32VlD45ZRD",
        "outputId": "9c1aa713-e026-42c4-f66c-b5ea9413c095"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.6241626739501953\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "for steps in range(10000): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2A94VMy5ZRD",
        "outputId": "555196a8-b731-4140-fd81-869cadd3b149"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "And-werigouge t blrd moid y scad apapry ty an,\n",
            "VII:\n",
            "ABk onendas hthhave d irifThe myero mpthe aininis t, uthteawind ly fr bu$Howe t\n",
            "ICEn thy theDWI' alit cofait? 'Becorpllim heouron, tos, thedeeno; me touler'sha ithas, annook ald ow ld.\n",
            "Whembit d ifene\n",
            "Frutartut fe d pas imelear the: lak oer ome qullt; hel ofZARENoomeisokns noke y;\n",
            "He;\n",
            "\n",
            "PHishoulerotht ir nd shisinomiveet, ff h vinpis hod.\n",
            "Y:\n",
            "RELLUSAbull rthat s may; houn hen chaloueat cean ugh twe Y:\n",
            "\n",
            "mpeyo alound ilacizororexaceigiey s t wive R\n"
          ]
        }
      ],
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwPjstbc5ZRD"
      },
      "source": [
        "\n",
        "\n",
        "### The mathematical trick in self-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWIxuurR5ZRD",
        "outputId": "f02bcf95-7965-41b6-f0bc-515165787f66"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# consider the following toy example:\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-fiIAHS5ZRD",
        "outputId": "aaf777ab-69b6-4485-bf8c-01852d2627bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ],
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNTJbDei5ZRE"
      },
      "outputs": [],
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NG1e_Z715ZRE",
        "outputId": "8c455497-1d8d-446b-9d28-c22597578e4e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf')) # future can't communicate with past\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow2, xbow3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgz7L2Vq5ZRE",
        "outputId": "c3db9c5d-ef04-4c18-eaff-20683af18647"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yviglybc5ZRH",
        "outputId": "4dface20-fd65-49fd-9757-5405f30a6079"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaoRRBhuLfCl",
        "outputId": "3b57de81-f249-41cf-f108-389361a5fd24"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-MqvzltLg9w",
        "outputId": "d6fc8ffd-6ad6-4c7c-8cca-9ecae7142802"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Sz804FrL_hU",
        "outputId": "4c082a78-8b20-452f-f6c5-05ede447621d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0Nya1MVO-cy",
        "outputId": "2dc3edc8-618a-4346-a6f5-186c0a568049"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.4264, 0.5736, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3151, 0.3022, 0.3827, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3007, 0.2272, 0.2467, 0.2253, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1635, 0.2048, 0.1776, 0.1616, 0.2926, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1403, 0.2272, 0.1454, 0.1244, 0.2678, 0.0949, 0.0000, 0.0000],\n",
              "        [0.1554, 0.1815, 0.1224, 0.1213, 0.1428, 0.1603, 0.1164, 0.0000],\n",
              "        [0.0952, 0.1217, 0.1130, 0.1453, 0.1137, 0.1180, 0.1467, 0.1464]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdu8fUJ75ZRI"
      },
      "source": [
        "Notes:\n",
        "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "\n",
        "* sentence 1: The bank1 of the river.\n",
        "* sentence 2: Money in the bank2.\n",
        "\n",
        "![alt text](https://files.readme.io/298afce-image.png)\n",
        "\n",
        "![alt text](https://files.readme.io/5f8c5fb-image.png)\n",
        "\n",
        "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_Fa49kU5ZRI"
      },
      "source": [
        "## Attention\n",
        "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.  The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
        "\n",
        "We call our particular attention \"Scaled Dot-Product Attention\".   The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$.  We compute the dot products of the query with all keys, divide each by $\\sqrt{d_k}$, and apply a softmax function to obtain the weights on the values.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQbn0XxJ5ZRI"
      },
      "source": [
        "![alt text](https://raw.githubusercontent.com/harvardnlp/annotated-transformer/master/images/ModalNet-19.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAb2ombp5ZRI"
      },
      "source": [
        "\n",
        "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix $Q$.   The keys and values are also packed together into matrices $K$ and $V$.  We compute the matrix of outputs as:\n",
        "\n",
        "$$\n",
        "   \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ML88645ZRI"
      },
      "source": [
        "![alt text](https://lena-voita.github.io/resources/lectures/seq2seq/transformer/qkv_explained-min.png)\n",
        "Source: [Lena Voita's Lecture about Seq2Seq](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxc2r7HX_QbS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.scale = self.dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Args:\n",
        "            x: Tensor of shape (batch_size, seq_len, input_dim)\n",
        "\n",
        "        Returns:\n",
        "            Tensor of shape (batch_size, seq_len, input_dim)\n",
        "        '''\n",
        "        # Your code is here\n",
        "        # Extract batch size, sequence length, and input dimension.\n",
        "\n",
        "        # Perform linear transformation and reshape for queries, keys, and values.\n",
        "\n",
        "        # Unbind into separate queries, keys, and values.\n",
        "\n",
        "        # Scale the queries.\n",
        "\n",
        "        # Compute attention scores.\n",
        "\n",
        "        # Apply softmax to obtain attention weights.\n",
        "\n",
        "        # Compute weighted sum of values using attention weights.\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2ybwNCU_w2q"
      },
      "outputs": [],
      "source": [
        "x = torch.ones(11, 12, 8)\n",
        "assert Attention(8)(x).shape == x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI3w-ThWBTSl"
      },
      "source": [
        "# Multi-head attention\n",
        "\n",
        "\n",
        "![](Attention.png)\n",
        "\n",
        "- Divide each vector in a sequence into `num_heads` vectors ($d$ mod `num_heads` = 0)\n",
        "- Apply attention layers independently, concatenate the result\n",
        "$$\\text{head}_i = \\text{Attention}(Q_i, K_i, V_i)$$\n",
        "$$ \\textrm{concat} \\left( \\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h \\right) $$\n",
        "- Apply an extra linear layer to mix independent attention branches\n",
        "- **How to implement without loops?**\n",
        "\n",
        "![alt text](https://uvadlc-notebooks.readthedocs.io/en/latest/_images/multihead_attention.svg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQzpOiXBBw0j"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8):\n",
        "        super().__init__()\n",
        "        if dim % num_heads:\n",
        "            raise ValueError('dim % num_heads != 0')\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Args:\n",
        "            x: Tensor of shape (batch_size, seq_len, input_dim)\n",
        "\n",
        "        Returns:\n",
        "            Tensor of shape (batch_size, seq_len, input_dim)\n",
        "        '''\n",
        "        # Hint: you might want to use torch.permute function\n",
        "\n",
        "        # Extract batch size, sequence length, and input dimension.\n",
        "\n",
        "        # Perform linear transformation and reshape for queries, keys, and values.\n",
        "        # qkv: 3 × B × num_heads × N × head_dim\n",
        "\n",
        "        # Unbind into separate queries, keys, and values.\n",
        "\n",
        "        # Scale the queries.\n",
        "\n",
        "        # Compute attention scores.\n",
        "\n",
        "        # Apply softmax to obtain attention weights.\n",
        "\n",
        "        # Compute weighted sum of values using attention weights.\n",
        "\n",
        "        # x: B × num_heads × N × head_dim\n",
        "\n",
        "        # Reshape and transpose back to original shape.\n",
        "\n",
        "        # x: B × N × (num_heads × head_dim)\n",
        "\n",
        "        # Project the output.\n",
        "\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHf9UkyfBx_1"
      },
      "outputs": [],
      "source": [
        "MultiHeadAttention(128, 8)(torch.ones(11, 12, 128)).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5z3D0-2S817"
      },
      "source": [
        "# ViT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "madeIO5PEkvJ"
      },
      "source": [
        "### Einops.rearrange\n",
        "\n",
        "https://github.com/arogozhnikov/einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCpqdrLHDc-y"
      },
      "outputs": [],
      "source": [
        "!python3 -m pip install einops -q\n",
        "from einops import rearrange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n06zLD4uEq62"
      },
      "outputs": [],
      "source": [
        "# Transposition:\n",
        "rearrange(torch.arange(1024).reshape(2, 4, 8, 16), 'aa b c d -> d c b aa').shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikd0GDn-EsBz"
      },
      "outputs": [],
      "source": [
        "res = rearrange(torch.arange(30).reshape(5, 6), 'a (b c) -> a b c', b=2, c=3)\n",
        "res.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44-jNoTWE7Ij"
      },
      "source": [
        "## Patches crafting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xT7qgkP0E67w",
        "outputId": "b98a565b-2558-4e32-e35e-6e867c2f9ba9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m41.0/44.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m898.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! python3 -m pip install einops -q\n",
        "from einops import rearrange\n",
        "\n",
        "def img2patches(img, patch_size=8):\n",
        "    '''\n",
        "    Args:\n",
        "        img: (batch_size, c, h, w) Tensor\n",
        "\n",
        "    Returns:\n",
        "        (batch_size, num_patches, vectorized_patch) Tensor\n",
        "    '''\n",
        "    # Your code is here\n",
        "    # Rearrange the image tensor to extract patches\n",
        "\n",
        "    return rearrange(img, 'batch_size c (h ph) (w pw) -> batch_size (h w) (c ph pw)',\n",
        "                     ph=patch_size, pw=patch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YswGSOazAorx"
      },
      "source": [
        "##  Build ViT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqEx_2kVAO4I"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/oseledets/dl2023/b5018a354b1a10e7f498d3a8649f604f4d63d920/seminars/seminar-9/vit.webp\" alt=\"ViT\" width=\"600\">\n",
        "\n",
        "\n",
        "* Split an image into patches\n",
        "\n",
        "* Flatten the patches\n",
        "\n",
        "* Produce lower-dimensional linear embeddings from the flattened patches\n",
        "\n",
        "* Add positional embeddings\n",
        "\n",
        "* Feed the sequence as an input to a standard transformer encoder\n",
        "\n",
        "* Pretrain the model with image labels (fully supervised on a huge dataset)\n",
        "\n",
        "* Finetune on the downstream dataset for image classification\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3jJ9v4yQExT"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            dim,\n",
        "            num_heads,\n",
        "            mlp_ratio=4,  # ratio between hidden_dim and input_dim in MLP\n",
        "            act_layer=nn.GELU,\n",
        "            norm_layer=nn.LayerNorm\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # Your code is here\n",
        "        # Initialize instance variables.\n",
        "\n",
        "        # Layer normalization before the attention mechanism.\n",
        "\n",
        "        # Multi-head self-attention mechanism.\n",
        "\n",
        "        # Layer normalization after the attention mechanism.\n",
        "\n",
        "        # Define the MLP (Multi-Layer Perceptron):\n",
        "          # Linear transformation with dim * mlp_ratio output features.\n",
        "          # Activation function.\n",
        "          # Linear transformation to map back to the original dimension.\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Your code is here\n",
        "\n",
        "        # Add the output of attention mechanism to the input (with normalization).\n",
        "\n",
        "        # Add the output of MLP to the input (with normalization).\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qz0psf5xXizv"
      },
      "outputs": [],
      "source": [
        "depth = 12\n",
        "many_layers = nn.Sequential(*[Block(128, 8) for _ in range(depth)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ojnWg-sFAMp"
      },
      "source": [
        "![](vit.webp)\n",
        "\n",
        "- CLS token: an extra learnable token\n",
        "- Position embeddings: `x = x + pos_embedding`, where `pos_embedding` is trained for every element is a sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rw4vAMt2kosm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ux1zhlQ5GBHi"
      },
      "outputs": [],
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(\n",
        "                    self,\n",
        "                    img_size=(224, 224),\n",
        "                    patch_size=16,\n",
        "                    in_chans=3,\n",
        "                    num_classes=10,\n",
        "                    embed_dim=768,\n",
        "                    depth=12,\n",
        "                    num_heads=12,\n",
        "                    mlp_ratio=4,\n",
        "                    class_token=True,\n",
        "                    norm_layer=nn.LayerNorm,\n",
        "                    act_layer=nn.GELU\n",
        "            ):\n",
        "        # Your code is here\n",
        "\n",
        "        # Initialize instance variables.\n",
        "\n",
        "        # Size of patches used for tokenization.\n",
        "\n",
        "        # Sequential container for the Transformer blocks.\n",
        "\n",
        "        # Projection layer for patches.\n",
        "\n",
        "        # Length of positional embeddings.\n",
        "\n",
        "        # Learnable token for classification.\n",
        "\n",
        "        # Linear layer for classification.\n",
        "\n",
        "\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Args:\n",
        "            x: (batch_size, in_channels, img_size[0], img_size[1])\n",
        "\n",
        "        Return:\n",
        "            (batch_size, num_classes) probabilities\n",
        "        '''\n",
        "        # Your code here\n",
        "\n",
        "        # Convert input image into patches.\n",
        "\n",
        "        # Project patches into the embedding space.\n",
        "\n",
        "        # Add positional embeddings.\n",
        "\n",
        "        # Add classification token.\n",
        "\n",
        "        # Pass through Transformer blocks.\n",
        "\n",
        "        # Extract only the CLS token.\n",
        "\n",
        "        # Pass the CLS token through the classification layer.\n",
        "\n",
        "\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nySebNJ1A_8U"
      },
      "outputs": [],
      "source": [
        "ViT()(torch.ones(5, 3, 224, 224)).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwgmrRGCxHlz"
      },
      "source": [
        "https://github.com/lucidrains/vit-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lW9f_xf_xFne",
        "outputId": "da8f22c8-7969-4c93-dd17-2e382087482f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting vit-pytorch\n",
            "  Downloading vit_pytorch-1.6.5-py3-none-any.whl (100 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.3/100.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops>=0.7.0 (from vit-pytorch)\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from vit-pytorch) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from vit-pytorch) (0.17.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit-pytorch) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit-pytorch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit-pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit-pytorch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit-pytorch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit-pytorch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10->vit-pytorch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10->vit-pytorch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10->vit-pytorch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10->vit-pytorch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10->vit-pytorch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10->vit-pytorch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10->vit-pytorch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10->vit-pytorch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10->vit-pytorch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10->vit-pytorch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10->vit-pytorch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit-pytorch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10->vit-pytorch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->vit-pytorch) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->vit-pytorch) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->vit-pytorch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->vit-pytorch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, einops, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, vit-pytorch\n",
            "Successfully installed einops-0.7.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 vit-pytorch-1.6.5\n"
          ]
        }
      ],
      "source": [
        "!pip install vit-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHf2w7JAxLT9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from vit_pytorch import ViT\n",
        "\n",
        "v = ViT(\n",
        "    image_size = 256,\n",
        "    patch_size = 32,\n",
        "    num_classes = 1000,\n",
        "    dim = 1024,\n",
        "    depth = 6,\n",
        "    heads = 16,\n",
        "    mlp_dim = 2048,\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1\n",
        ")\n",
        "\n",
        "img = torch.randn(1, 3, 256, 256)\n",
        "\n",
        "preds = v(img) # (1, 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvP_QijO89xm"
      },
      "source": [
        "# HuggingFace\n",
        "\n",
        "Hugging Face is an open-source library that provides easy access to state-of-the-art transformer-based models for NLP tasks. It offers a comprehensive set of tools for working with these models, including loading pre-trained models, fine-tuning on custom datasets, and deploying models for inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcsmaM7wiY5z"
      },
      "source": [
        "### Getting started on a task with a pipeline\n",
        "\n",
        "The easiest way to use a pre-trained model on a given task is to use pipeline(). 🤗 Transformers provides the following tasks out of the box:\n",
        "Sentiment analysis: is a text positive or negative?\n",
        "\n",
        "1. Text generation: provide a prompt and the model will generate what follows.\n",
        "2. Name entity recognition (NER): in an input sentence, label each word with the entity it represents (person, place, etc.)\n",
        "3. Question answering: provide the model with some context and a question, extract the answer from the context.\n",
        "4. Filling masked text: given a text with masked words (e.g., replaced by [MASK]), fill the blanks.\n",
        "5. Summarization: generate a summary of a long text.\n",
        "6. Language Translation: translate a text into another language.\n",
        "7. Feature extraction: return a tensor representation of the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVJUSak5F1tz"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nkiMQgwirgo"
      },
      "source": [
        "### GPT2\n",
        "\n",
        "#### Model description\n",
        "\n",
        "**GPT-2** is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences.\n",
        "\n",
        "More precisely, inputs are sequences of continuous text of a certain length and the targets are the same sequence, shifted one token (word or piece of word) to the right. The model uses internally a mask-mechanism to make sure the predictions for the token i only uses the inputs from 1 to i but not the future tokens.\n",
        "\n",
        "This way, the model learns an inner representation of the English language that can then be used to extract features useful for downstream tasks. The model is best at what it was pretrained for however, which is generating texts from a prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhEJJK36jApe"
      },
      "source": [
        "### Text generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5CeZ7c_jE76"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, set_seed\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oh029QYxjK6l"
      },
      "outputs": [],
      "source": [
        "generator = pipeline('text-generation', model='gpt2')\n",
        "set_seed(42)\n",
        "generator(\"Hello, in this seminar we will learn how to,\", max_length=60, num_return_sequences=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPKRx9Z1jQnx"
      },
      "outputs": [],
      "source": [
        "generator(\"Machine learning is evolving technology\", max_length=10, num_return_sequences=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTkd8VcUjYMv"
      },
      "source": [
        "### Sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YabWA_FSjZZs"
      },
      "outputs": [],
      "source": [
        "# Allocate a pipeline for sentiment-analysis\n",
        "classifier = pipeline('sentiment-analysis')\n",
        "classifier('The weather is awesome!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0tXsP-LkVym"
      },
      "source": [
        "### Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrIE3oGfkXF2"
      },
      "outputs": [],
      "source": [
        "# Allocate a pipeline for question-answering\n",
        "question_answerer = pipeline('question-answering')\n",
        "question_answerer({\n",
        "    'question': 'What is the Newtons third law of motion?',\n",
        "    'context': 'Newton’s third law of motion states that, \"For every action there is equal and opposite reaction\"'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xO8tP9oEkg7C"
      },
      "outputs": [],
      "source": [
        "nlp = pipeline(\"question-answering\")\n",
        "\n",
        "context = r\"\"\"\n",
        "Micorsoft was founded by Bill gates and Paul allen in the year 1975.\n",
        "The property of being prime (or not) is called primality.\n",
        "A simple but slow method of verifying the primality of a given number n is known as trial division.\n",
        "It consists of testing whether n is a multiple of any integer between 2 and itself.\n",
        "Algorithms much more efficient than trial division have been devised to test the primality of large numbers.\n",
        "These include the Miller–Rabin primality test, which is fast but has a small probability of error, and the AKS primality test, which always produces the correct answer in polynomial time but is too slow to be practical.\n",
        "Particularly fast methods are available for numbers of special forms, such as Mersenne numbers.\n",
        "As of January 2016, the largest known prime number has 22,338,618 decimal digits.\n",
        "\"\"\"\n",
        "\n",
        "#Question 1\n",
        "result = nlp(question=\"What is a simple method to verify primality?\", context=context)\n",
        "\n",
        "print(f\"Answer 1: '{result['answer']}'\")\n",
        "\n",
        "#Question 2\n",
        "result = nlp(question=\"When did Bill gates founded Microsoft?\", context=context)\n",
        "\n",
        "print(f\"Answer 2: '{result['answer']}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Jm08cOsV_Yx"
      },
      "source": [
        "### BERT\n",
        "\n",
        "The BERT model was proposed in BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It’s a bidirectional transformer pretrained using a combination of masked language modeling objective and next sentence prediction on a large corpus comprising the Toronto Book Corpus and Wikipedia.\n",
        "\n",
        "The abstract from the paper is the following:\n",
        "\n",
        "> We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\n",
        "\n",
        "BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQpuivjmlXMa"
      },
      "source": [
        "### Text prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjNxlDyPlTUV"
      },
      "outputs": [],
      "source": [
        "unmasker = pipeline('fill-mask', model='bert-base-cased')\n",
        "unmasker(\"Hello, My name is [MASK].\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKEpbCM1lkhc"
      },
      "source": [
        "### Text Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewk4_wLEl064"
      },
      "outputs": [],
      "source": [
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "ARTICLE = \"\"\"The Apollo program, also known as Project Apollo, was the third United States human spaceflight program carried out by the National Aeronautics and Space Administration (NASA), which accomplished landing the first humans on the Moon from 1969 to 1972.\n",
        "First conceived during Dwight D. Eisenhower's administration as a three-man spacecraft to follow the one-man Project Mercury which put the first Americans in space,\n",
        "Apollo was later dedicated to President John F. Kennedy's national goal of \"landing a man on the Moon and returning him safely to the Earth\" by the end of the 1960s, which he proposed in a May 25, 1961, address to Congress.\n",
        "Project Mercury was followed by the two-man ProjectGemini (1962–66).\n",
        "The first manned flight of Apollo was in 1968.\n",
        "Apollo ran from 1961 to 1972, and was supported by the two-man Gemini program which ran concurrently with it from 1962 to 1966.\n",
        "Gemini missions developed some of the space travel techniques that were necessary for the success of the Apollo missions.\n",
        "Apollo used Saturn family rockets as launch vehicles.\n",
        "Apollo/Saturn vehicles were also used for an Apollo Applications Program, which consisted of Skylab, a space station that supported three manned missions in 1973–74, and the Apollo–Soyuz Test Project, a joint Earth orbit mission with the Soviet Union in 1975.\n",
        " \"\"\"\n",
        "\n",
        "summary=summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False)[0]\n",
        "\n",
        "print(summary['summary_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Sj_V48FpWTy"
      },
      "source": [
        "### English to German translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JP2pfiUvpaGp"
      },
      "outputs": [],
      "source": [
        "# English to German\n",
        "translator_ger = pipeline(\"translation_en_to_de\")\n",
        "print(\"German: \",translator_ger(\"Joe Biden became the 46th president of U.S.A.\", max_length=40)[0]['translation_text'])\n",
        "\n",
        "# English to French\n",
        "translator_fr = pipeline('translation_en_to_fr')\n",
        "print(\"French: \",translator_fr(\"Joe Biden became the 46th president of U.S.A\",  max_length=40)[0]['translation_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4mpWPgPqPwY"
      },
      "source": [
        "### Fill MASK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKbMqtrxciSb"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
        "unmasker(\"Artificial Intelligence [MASK] take over the world.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujIGeQrvGIhI"
      },
      "source": [
        "In conclusion, Hugging Face provides a user-friendly interface for working with transformer-based models in NLP. We've covered how to load pre-trained models, fine-tune them for specific tasks, and even implement custom transformers. With its extensive documentation and active community, Hugging Face is an invaluable tool for NLP practitioners."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTVjMJiOBMv2"
      },
      "source": [
        "# Mamba\n",
        "\n",
        "<img src=\"https://github.com/state-spaces/mamba/blob/main/assets/selection.png?raw=true\" alt=\"Mamba\" >"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbtckhZLBCpc"
      },
      "outputs": [],
      "source": [
        "!pip install causal-conv1d>=1.2.0\n",
        "!pip install mamba-ssm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJkr9JaCCYXj"
      },
      "outputs": [],
      "source": [
        "from transformers import MambaConfig, MambaForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-2.8b-hf\")\n",
        "model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-2.8b-hf\")\n",
        "input_ids = tokenizer(\"LOOK!\", return_tensors=\"pt\")[\"input_ids\"]\n",
        "\n",
        "out = model.generate(input_ids, max_new_tokens=10)\n",
        "print(tokenizer.batch_decode(out))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXofEUBdDx5g"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "PhD",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7a4b3960547ae30807a4950c470fa8e07d0754b1ba6745fcabd143662a5dd1c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
