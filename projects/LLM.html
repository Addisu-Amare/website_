<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Training large models* – Addisu Amare</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-1db3f541d99483cdc7a0d9cf38862b2c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../css/styles.css">
</head>

<body class="nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Addisu Amare</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../research.html"> 
<span class="menu-text">Scholarship</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../documents.html"> 
<span class="menu-text">Article</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../project.html"> 
<span class="menu-text">projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../Hobbies.html"> 
<span class="menu-text">My Hobbies</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../Jupyter_note_book.html"> 
<span class="menu-text">Cancer classification</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/Addisu-Amare"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Training large models*</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="point-to-note" class="level2">
<h2 class="anchored" data-anchor-id="point-to-note">Point to note</h2>
<p>In this notebook, you will learn how to finetune large language models with limited GPU memory.</p>
<p>For the last few years models have greatly increased in size and many of them do not fit onto the standard consumer GPU, not said about finetuning these models in conventional way.</p>
<p>However, there exists several techniques that allow one to inference and even finetune large models given modest resources.</p>
<p><strong>Reduction of the model size</strong></p>
<p>There existst numerous approaches to model compression (that require a separate lecture for an overview) and the one of the most succesfull in the context of LLM is PTQ (post-training quantization) that stores the weight in low precision.</p>
<p>4-bit quantization typically leads to minor degration in performance relative to the floating point baseline offerring huge memory savings: * a model in <code>half</code> precision requires <code>16 bits</code> per parameter * a model quantized to 4-bits requires <code>4+eps bits</code> per parameter (there is small overhead on the storage of quantization statistics)</p>
<p>Therefore, we have almost <code>4x</code> reduction in memory!</p>
<p><strong>Reduction of the memory on optimizer states</strong></p>
<p>Another challenge are the optimizer states. For <code>Adam</code> optimizer commonly adopted for training Transformers one needs <code>4 bytes</code> for gradients, and first and second optimizer moment (one may try to store some of these in half precision, but it tends to incur instability).</p>
<p>Therefore, the total memory required to train something like <code>Llama-7b</code>, <code>Mistral-7b</code>, <code>gemma-7b</code> exceeds <code>80Gb</code> of high-end <code>A100, H100</code>.</p>
<p>Finetuning only the <code>lm.head</code> may not suffice for more complicated tasks.</p>
<p>Thus we search for something in between - that allows to adapt in some sense every transformer layer, but with small number of trainable parameters.</p>
<p>Different approach to train small subset of parameters are known in the literature as <strong>parameter-efficient finetuning</strong> (PEFT) methods.</p>
<p>We will cover two known tecnhiques for parameter-efficient finetuning: * Prompt tuning * LoRA adapters</p>
</section>
<section id="preparation" class="level2">
<h2 class="anchored" data-anchor-id="preparation">Preparation</h2>
<div id="cell-5" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="8d1be260-9e41-499c-f688-ce701eb30eba">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install accelerate <span class="co"># needed to reduce RAM consumption and integration with bits and bytes</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install bitsandbytes <span class="co"># to work with quantized models</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install peft <span class="co"># to work with PEFT techniques</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Looking in indexes: https://pypi.org/simple/
Requirement already satisfied: accelerate in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (0.28.0)
Requirement already satisfied: numpy&gt;=1.17 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from accelerate) (1.26.4)
Requirement already satisfied: packaging&gt;=20.0 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from accelerate) (24.0)
Requirement already satisfied: psutil in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from accelerate) (5.9.0)
Requirement already satisfied: pyyaml in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from accelerate) (6.0.1)
Requirement already satisfied: torch&gt;=1.10.0 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from accelerate) (2.2.2)
Requirement already satisfied: huggingface-hub in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from accelerate) (0.22.2)
Requirement already satisfied: safetensors&gt;=0.3.1 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from accelerate) (0.4.2)
Requirement already satisfied: filelock in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from torch&gt;=1.10.0-&gt;accelerate) (3.13.1)
Requirement already satisfied: typing-extensions&gt;=4.8.0 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from torch&gt;=1.10.0-&gt;accelerate) (4.9.0)
Requirement already satisfied: sympy in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from torch&gt;=1.10.0-&gt;accelerate) (1.12)
Requirement already satisfied: networkx in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from torch&gt;=1.10.0-&gt;accelerate) (3.1)
Requirement already satisfied: jinja2 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from torch&gt;=1.10.0-&gt;accelerate) (3.1.3)
Requirement already satisfied: fsspec in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from torch&gt;=1.10.0-&gt;accelerate) (2024.2.0)
Requirement already satisfied: requests in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from huggingface-hub-&gt;accelerate) (2.31.0)
Requirement already satisfied: tqdm&gt;=4.42.1 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from huggingface-hub-&gt;accelerate) (4.66.2)
Requirement already satisfied: MarkupSafe&gt;=2.0 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from jinja2-&gt;torch&gt;=1.10.0-&gt;accelerate) (2.1.3)
Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from requests-&gt;huggingface-hub-&gt;accelerate) (2.0.4)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from requests-&gt;huggingface-hub-&gt;accelerate) (3.4)
Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from requests-&gt;huggingface-hub-&gt;accelerate) (2.1.0)
Requirement already satisfied: certifi&gt;=2017.4.17 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from requests-&gt;huggingface-hub-&gt;accelerate) (2024.2.2)
Requirement already satisfied: mpmath&gt;=0.19 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from sympy-&gt;torch&gt;=1.10.0-&gt;accelerate) (1.3.0)
Looking in indexes: https://pypi.org/simple/
Requirement already satisfied: bitsandbytes in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (0.43.1)
Requirement already satisfied: torch in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from bitsandbytes) (2.2.2)
Requirement already satisfied: numpy in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)
Requirement already satisfied: filelock in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from torch-&gt;bitsandbytes) (3.13.1)
Requirement already satisfied: typing-extensions&gt;=4.8.0 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from torch-&gt;bitsandbytes) (4.9.0)
Requirement already satisfied: sympy in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from torch-&gt;bitsandbytes) (1.12)
Requirement already satisfied: networkx in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from torch-&gt;bitsandbytes) (3.1)
Requirement already satisfied: jinja2 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from torch-&gt;bitsandbytes) (3.1.3)
Requirement already satisfied: fsspec in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from torch-&gt;bitsandbytes) (2024.2.0)
Requirement already satisfied: MarkupSafe&gt;=2.0 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from jinja2-&gt;torch-&gt;bitsandbytes) (2.1.3)
Requirement already satisfied: mpmath&gt;=0.19 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from sympy-&gt;torch-&gt;bitsandbytes) (1.3.0)
Looking in indexes: https://pypi.org/simple/
Requirement already satisfied: peft in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (0.10.0)
Requirement already satisfied: numpy&gt;=1.17 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from peft) (1.26.4)
Requirement already satisfied: packaging&gt;=20.0 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from peft) (24.0)
Requirement already satisfied: psutil in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from peft) (5.9.0)
Requirement already satisfied: pyyaml in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from peft) (6.0.1)
Requirement already satisfied: torch&gt;=1.13.0 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from peft) (2.2.2)
Requirement already satisfied: transformers in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from peft) (4.40.0.dev0)
Requirement already satisfied: tqdm in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from peft) (4.66.2)
Requirement already satisfied: accelerate&gt;=0.21.0 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from peft) (0.28.0)
Requirement already satisfied: safetensors in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from peft) (0.4.2)
Requirement already satisfied: huggingface-hub&gt;=0.17.0 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from peft) (0.22.2)
Requirement already satisfied: filelock in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from huggingface-hub&gt;=0.17.0-&gt;peft) (3.13.1)
Requirement already satisfied: fsspec&gt;=2023.5.0 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from huggingface-hub&gt;=0.17.0-&gt;peft) (2024.2.0)
Requirement already satisfied: requests in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from huggingface-hub&gt;=0.17.0-&gt;peft) (2.31.0)
Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from huggingface-hub&gt;=0.17.0-&gt;peft) (4.9.0)
Requirement already satisfied: sympy in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from torch&gt;=1.13.0-&gt;peft) (1.12)
Requirement already satisfied: networkx in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from torch&gt;=1.13.0-&gt;peft) (3.1)
Requirement already satisfied: jinja2 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from torch&gt;=1.13.0-&gt;peft) (3.1.3)
Requirement already satisfied: regex!=2019.12.17 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from transformers-&gt;peft) (2023.12.25)
Requirement already satisfied: tokenizers&lt;0.19,&gt;=0.14 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from transformers-&gt;peft) (0.15.2)
Requirement already satisfied: MarkupSafe&gt;=2.0 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from jinja2-&gt;torch&gt;=1.13.0-&gt;peft) (2.1.3)
Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from requests-&gt;huggingface-hub&gt;=0.17.0-&gt;peft) (2.0.4)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from requests-&gt;huggingface-hub&gt;=0.17.0-&gt;peft) (3.4)
Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from requests-&gt;huggingface-hub&gt;=0.17.0-&gt;peft) (2.1.0)
Requirement already satisfied: certifi&gt;=2017.4.17 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from requests-&gt;huggingface-hub&gt;=0.17.0-&gt;peft) (2024.2.2)
Requirement already satisfied: mpmath&gt;=0.19 in /home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages (from sympy-&gt;torch&gt;=1.13.0-&gt;peft) (1.3.0)</code></pre>
</div>
</div>
<div id="cell-6" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="1c3761f2-571f-40ee-a945-a327d87c026d">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>huggingface<span class="op">-</span>cli login</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>
    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|
    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|
    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|
    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|
    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|

    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .
Token: 
Add token as git credential? (Y/n) n
Token is valid (permission: write).
Your token has been saved to /root/.cache/huggingface/token
Login successful</code></pre>
</div>
</div>
<div id="cell-7" class="cell" data-outputid="6e4b47f8-2c00-4b96-9bef-f94b0f48d5a2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.auto <span class="im">import</span> trange</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> transformers</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModelForCausalLM</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
/home/dkuznedelev/miniconda3/envs/pysparse/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(</code></pre>
</div>
</div>
<div id="cell-8" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> torch.cuda.is_available(), <span class="st">"No CUDA, no party"</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-9" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:198,&quot;referenced_widgets&quot;:[&quot;d7a9a510eec3443a822762095076aa73&quot;,&quot;5acb1f1c08c14afa92c36c1d0ad653fa&quot;,&quot;f5d6cd575bf14d509e6e5d4c7c6a553b&quot;,&quot;4d920ee1366a4a3b8c2c6194408494d3&quot;,&quot;d32d0ac99c19494da88056ed35a1bb6c&quot;,&quot;4aa59688a6c048658df6966d0fe7a511&quot;,&quot;4280422ff04244c88dfc2cd70b4f0ab9&quot;,&quot;f607abe91bfe4f5fb5c6fe63c5894228&quot;,&quot;78a0f6029e5b42e29f99315b09c224fd&quot;,&quot;04212dbad9b944da83535d6e827ebc2b&quot;,&quot;73b3172495b6490db1ff1186d2b8108f&quot;]}}" data-outputid="db4f81fe-02a4-4fc1-c469-92d46f1c5676">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">'mistralai/Mistral-7B-v0.1'</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co"># loading tokenizer</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_name)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>tokenizer.pad_token_id <span class="op">=</span> tokenizer.eos_token_id</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co"># loading model</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    model_name,</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    device_map<span class="op">=</span><span class="st">'auto'</span>,</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    low_cpu_mem_usage<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    offload_state_dict<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    load_in_4bit<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    torch_dtype<span class="op">=</span>torch.float32,  <span class="co"># weights are 4-bit; layernorms and activations are fp32</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> param <span class="kw">in</span> model.parameters():</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    param.requires_grad<span class="op">=</span><span class="va">False</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>model.gradient_checkpointing_enable()  <span class="co"># only store a small subset of activations, re-compute the rest.</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>model.enable_input_require_grads()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Downloading shards: 100%|██████████| 2/2 [00:00&lt;00:00, 3943.87it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:19&lt;00:00,  9.96s/it]</code></pre>
</div>
</div>
</section>
<section id="prompt-tuning" class="level2">
<h2 class="anchored" data-anchor-id="prompt-tuning">Prompt tuning</h2>
<p>Prompt tuning injects learnable tokens in the prompts that are optimized via backpropagation.</p>
<p><img src="https://thumtblog.github.io/images/robust-prefix-tuning/observation-good.png" width="600px"></p>
<p>Number of learnable parameters is: <span class="math inline">\(N_{tokens} \times d_{embed}\)</span>.</p>
<p>This approach is pretty cheap and is known to work pretty good in simple cases.</p>
<div id="cell-11" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="773d9e2e-776a-4a66-d30b-f64dba524fef">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">'A quick brown fox'</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>batch <span class="op">=</span> tokenizer(prompt, return_tensors<span class="op">=</span><span class="st">'pt'</span>, return_token_type_ids<span class="op">=</span><span class="va">False</span>).to(device)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">7</span>):</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    next_token <span class="op">=</span> model(<span class="op">**</span>batch).logits[<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>].argmax(<span class="op">-</span><span class="dv">1</span>).reshape(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    batch[<span class="st">'input_ids'</span>] <span class="op">=</span> torch.cat([batch[<span class="st">'input_ids'</span>], next_token], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    batch[<span class="st">'attention_mask'</span>] <span class="op">=</span> torch.cat([batch[<span class="st">'attention_mask'</span>], torch.ones_like(next_token)], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Output: </span><span class="sc">{</span>tokenizer<span class="sc">.</span>decode(batch[<span class="st">'input_ids'</span>][<span class="dv">0</span>].cpu().numpy().tolist(), skip_special_tokens<span class="op">=</span><span class="va">True</span>)<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Output: A quick brown fox jumps over a lazy dog.</code></pre>
</div>
</div>
<p><img src="https://static.wikia.nocookie.net/theodd1souts/images/e/ee/Odd_alphabet.jpg/revision/latest/scale-to-width-down/1000?cb=20180616072819" width="400px"></p>
<p>What a blatant lie!</p>
<p>This particular fox assures you that it didn’t in fact jump over the lazy dog.</p>
<p>No, sir! The fox was just minding its own business.</p>
<p>Your task is to train the model to say truth: no dog was jumped over today.</p>
<div id="cell-13" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="ca068d48-ba54-4030-d974-3c18188aaff3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>the_truth <span class="op">=</span> <span class="st">"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!"</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>batch <span class="op">=</span> tokenizer(the_truth, return_tensors<span class="op">=</span><span class="st">'pt'</span>, return_token_type_ids<span class="op">=</span><span class="va">False</span>).to(device)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> model(<span class="op">**</span>batch)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>next_word_logits <span class="op">=</span> outputs.logits[:, :<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>true_next_tokens <span class="op">=</span> batch[<span class="st">'input_ids'</span>][:, <span class="dv">1</span>:]</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> F.cross_entropy(next_word_logits.flatten(<span class="dv">0</span>, <span class="dv">1</span>), true_next_tokens.flatten(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss: 3.06</code></pre>
</div>
</div>
<p><strong>Your task</strong></p>
<p>Implement prompt tuning using the template below.</p>
<div id="cell-15" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> WordEmbeddingsWithLearnedPrompts(nn.Module):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co">    To perform prompt tuning, you will need to replace model's original word embeddings with a layer - THIS layer</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co">     - that inserts trainable prompts instead of the first N token embeddings. """</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, word_embeddings: nn.Embedding, num_prompts: <span class="bu">int</span>):</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.original_word_embeddings <span class="op">=</span> word_embeddings</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_prompts <span class="op">=</span> num_prompts</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.learnable_prompts <span class="op">=</span> nn.Parameter(</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>            torch.randn(<span class="dv">1</span>, num_prompts, word_embeddings.embedding_dim),</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>            requires_grad<span class="op">=</span><span class="va">True</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>          )</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, input_ids: torch.LongTensor):</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># input_ids shape: [batch_size, seq length]</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> input_ids.dtype <span class="op">==</span> torch.int64</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> input_ids.shape[<span class="dv">1</span>] <span class="op">&gt;</span> <span class="va">self</span>.num_prompts</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> torch.<span class="bu">all</span>(input_ids[:, :<span class="va">self</span>.num_prompts] <span class="op">==</span> tokenizer.pad_token_id).item(), <span class="st">"don't forget to prepend several BOS tokens to input_ids"</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Your task: embed input_ids, but replace the first :num_prompts: tokens with self.learnable_prompts</span></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This is because we will prepend :num_prompts: padding tokens at the beginning</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># After you are done, you must produce a word embedding vector for each token in input_ids,</span></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># except that the first :num_prompts: vectors should equal learnable_prompts;</span></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># any additional vectors after first :num_prompts: ones should be embedded as usual</span></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Note: since you're dealing with trainable params, please torch.cat instead of item assignment</span></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>        learnable_embeddings <span class="op">=</span> <span class="va">self</span>.learnable_prompts.repeat(input_ids.shape[<span class="dv">0</span>], <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>        inputs_embeddings <span class="op">=</span> <span class="va">self</span>.original_word_embeddings(input_ids[:, <span class="va">self</span>.num_prompts:])</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>        embeddings <span class="op">=</span> torch.cat((learnable_embeddings, inputs_embeddings), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> embeddings</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-16" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="e90c9bf8-c002-44fb-b2d3-0b378e8340e8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>num_prompts <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>test_emb_layer <span class="op">=</span> WordEmbeddingsWithLearnedPrompts(model.model.embed_tokens, num_prompts<span class="op">=</span>num_prompts).to(device)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>test_input_ids <span class="op">=</span> tokenizer(<span class="st">"a cat say on a may"</span>, return_tensors<span class="op">=</span><span class="st">'pt'</span>)[<span class="st">'input_ids'</span>].to(device)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>space_for_prompts <span class="op">=</span> torch.full(</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    size<span class="op">=</span>(<span class="bu">len</span>(test_input_ids), num_prompts),</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    fill_value<span class="op">=</span>tokenizer.pad_token_id,</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    dtype<span class="op">=</span>torch.int64,</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span>device</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>test_inputs_with_prompts <span class="op">=</span> torch.cat([space_for_prompts, test_input_ids], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.cuda.amp.autocast():</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>  test_prompt_embeddings <span class="op">=</span> test_emb_layer(test_inputs_with_prompts)</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> test_prompt_embeddings.shape[:<span class="dv">2</span>] <span class="op">==</span> test_inputs_with_prompts.shape</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> test_prompt_embeddings.shape[<span class="op">-</span><span class="dv">1</span>] <span class="op">==</span> model.config.hidden_size</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> torch.allclose(test_prompt_embeddings[:, :num_prompts], test_emb_layer.learnable_prompts.<span class="bu">float</span>())</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> torch.allclose(test_prompt_embeddings[:, num_prompts:], model.model.embed_tokens(test_input_ids).<span class="bu">float</span>())</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Looks legit!"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Looks legit!</code></pre>
</div>
</div>
<div id="cell-17" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> <span class="bu">isinstance</span>(model.model.embed_tokens, nn.Embedding), <span class="st">"you have already replaced the embedding layer. If the replacement is broken, please reload the model"</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>model.model.embed_tokens <span class="op">=</span> WordEmbeddingsWithLearnedPrompts(model.model.embed_tokens, num_prompts<span class="op">=</span>num_prompts).to(device)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-18" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> torch.optim.Adam([model.model.embed_tokens.learnable_prompts], lr<span class="op">=</span><span class="fl">0.01</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Prepare batch</p>
<div id="cell-20" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>the_truth <span class="op">=</span> <span class="st">"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!"</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>batch <span class="op">=</span> tokenizer(the_truth, return_tensors<span class="op">=</span><span class="st">'pt'</span>, return_token_type_ids<span class="op">=</span><span class="va">False</span>).to(device)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>space_for_prompts <span class="op">=</span> torch.full([<span class="bu">len</span>(test_input_ids), num_prompts], fill_value<span class="op">=</span>tokenizer.pad_token_id,</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>                               dtype<span class="op">=</span>torch.int64, device<span class="op">=</span>device)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>batch[<span class="st">'input_ids'</span>] <span class="op">=</span> torch.cat([space_for_prompts, batch[<span class="st">'input_ids'</span>]], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>batch[<span class="st">'attention_mask'</span>] <span class="op">=</span> torch.cat([torch.ones_like(space_for_prompts), batch[<span class="st">'attention_mask'</span>]], dim<span class="op">=</span><span class="dv">1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-21" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:234,&quot;referenced_widgets&quot;:[&quot;62b0c9498db1466f8458133b2865f2fd&quot;,&quot;a2127aaacaf5468fbe08487f19526bdf&quot;,&quot;b4f8024025674d4fa2e0a3926095568d&quot;,&quot;2931fb7a17c7471e9535e3f0e8422bb4&quot;,&quot;94c5c6f4417a4f1a95708aca6497acbf&quot;,&quot;562e3c774d1b43579e8830e4cb719fb3&quot;,&quot;7f75c7fdbce54e3a91a5095bb39edf0f&quot;,&quot;bcf987caa730499fa0bda0fbffed7dfb&quot;,&quot;fd4e5fd6ba7b4654bdeebaeb0e0c4816&quot;,&quot;1e1c9348c5634c548d7ae46bbcf7d8e1&quot;,&quot;d8d1e2f302604fec8f448f236bc9a90a&quot;]}}" data-outputid="b32a1c31-2d9a-4c4d-c4e4-8fc9c4d14d74">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>n_iter <span class="op">=</span> <span class="dv">250</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>atol <span class="op">=</span> <span class="fl">0.17</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>pbar <span class="op">=</span> trange(n_iter)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> pbar:</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>  outputs <span class="op">=</span> model(<span class="op">**</span>batch)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>  next_word_logits <span class="op">=</span> outputs.logits[:, num_prompts:<span class="op">-</span><span class="dv">1</span>, :]</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>  true_next_tokens <span class="op">=</span> batch[<span class="st">'input_ids'</span>][:, num_prompts <span class="op">+</span> <span class="dv">1</span>:]</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>  loss <span class="op">=</span> F.cross_entropy(next_word_logits.flatten(<span class="dv">0</span>, <span class="dv">1</span>), true_next_tokens.flatten(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>  loss.backward()</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>  opt.step()</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>  opt.zero_grad()</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>  pbar.set_description(<span class="ss">f"Loss </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> loss <span class="op">&lt;</span> atol:</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">break</span></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> loss.item() <span class="op">&lt;=</span> atol</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Good job!"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loss 0.17:  16%|█▋        | 41/250 [00:07&lt;00:40,  5.20it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Good job!</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
</div>
<div id="cell-22" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="5c9789b2-4dc8-44f5-e3b5-01fa476115cc">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">'A quick brown fox'</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>batch <span class="op">=</span> tokenizer(prompt, return_tensors<span class="op">=</span><span class="st">'pt'</span>, return_token_type_ids<span class="op">=</span><span class="va">False</span>).to(device)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>batch[<span class="st">'input_ids'</span>] <span class="op">=</span> torch.cat([space_for_prompts, batch[<span class="st">'input_ids'</span>]], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>batch[<span class="st">'attention_mask'</span>] <span class="op">=</span> torch.cat([torch.ones_like(space_for_prompts), batch[<span class="st">'attention_mask'</span>]], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">15</span>):</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>    next_token <span class="op">=</span> model(<span class="op">**</span>batch).logits[<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>].argmax(<span class="op">-</span><span class="dv">1</span>).reshape(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>    batch[<span class="st">'input_ids'</span>] <span class="op">=</span> torch.cat([batch[<span class="st">'input_ids'</span>], next_token], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>    batch[<span class="st">'attention_mask'</span>] <span class="op">=</span> torch.cat([batch[<span class="st">'attention_mask'</span>], torch.ones_like(next_token)], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Output: </span><span class="sc">{</span>tokenizer<span class="sc">.</span>decode(batch[<span class="st">'input_ids'</span>][<span class="dv">0</span>, num_prompts:].cpu().numpy().tolist(), skip_special_tokens<span class="op">=</span><span class="va">True</span>)<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Output: A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway</code></pre>
</div>
</div>
<p>If you did everything right, the model will deny that the fox jumped over the lazy dog</p>
<section id="using-huggingface-peft-2-points" class="level3">
<h3 class="anchored" data-anchor-id="using-huggingface-peft-2-points">Using HuggingFace PEFT (2 points)</h3>
<p><a href="https://github.com/huggingface/peft">PEFT</a> is a transformer’s 🤗 sister library that allows you to apply various __p__arameter __e__fficient __f__ine-__t__uning methods to pre-trained transformers. This library provides an implementation of the common PEFT techniques: * LoRA * Prefix-Tuning * Prompt-Tuning * IA3 * and more</p>
<div id="cell-25" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> peft</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-26" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">del</span> model</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>torch.cuda.empty_cache()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-27" class="cell" data-outputid="fd1507f4-4fe7-4270-b730-02f4322fcbb5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># re-loading model</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>    model_name,</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    device_map<span class="op">=</span><span class="st">'auto'</span>,</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>    low_cpu_mem_usage<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>    offload_state_dict<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>    load_in_4bit<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>    torch_dtype<span class="op">=</span>torch.float32,  <span class="co"># weights are 4-bit; layernorms and activations are fp32</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> param <span class="kw">in</span> model.parameters():</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>    param.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>model.gradient_checkpointing_enable()  <span class="co"># only store a small subset of activations, re-compute the rest.</span></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>model.enable_input_require_grads()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Downloading shards: 100%|██████████| 2/2 [00:00&lt;00:00, 2776.77it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:24&lt;00:00, 12.17s/it]</code></pre>
</div>
</div>
<p>Sanity check that we have reloaded the model</p>
<div id="cell-29" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> <span class="bu">isinstance</span>(model.model.embed_tokens, nn.Embedding), <span class="st">"please reload the model"</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-30" class="cell" data-outputid="49411e8e-f370-42d9-f2ea-78cf13f5ca0d">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>peft_config <span class="op">=</span> peft.PromptTuningConfig(task_type<span class="op">=</span>peft.TaskType.CAUSAL_LM, num_virtual_tokens<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> peft.get_peft_model(model, peft_config)  <span class="co"># note: for most peft methods, this line also modifies model in-place</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>model.print_trainable_parameters()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>trainable params: 65,536 || all params: 7,241,797,632 || trainable%: 0.000904968673943746</code></pre>
</div>
</div>
<p><strong>Your task</strong></p>
<p>Optimize the PEFT-wrapped model to achieve next token prediction <code>loss &lt; 0.17</code>, but this time using PEFT</p>
<p><strong>Note</strong></p>
<p>You no longer need to prepend PAD tokens, but you still need to skip <code>:num_virtual_tokens</code>: first logits.</p>
<p>Finally, generate the sentence to make sure that the model learned the truth.</p>
<div id="cell-32" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>the_truth <span class="op">=</span> <span class="st">"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!"</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>batch <span class="op">=</span> tokenizer(the_truth, return_tensors<span class="op">=</span><span class="st">'pt'</span>, return_token_type_ids<span class="op">=</span><span class="va">False</span>).to(device)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-33" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-34" class="cell" data-outputid="5ce6586f-a565-4ff5-f1ca-bc1892623a54">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>n_iter <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>atol <span class="op">=</span> <span class="fl">0.17</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>pbar <span class="op">=</span> trange(n_iter)</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> pbar:</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>  outputs <span class="op">=</span> model(<span class="op">**</span>batch)</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>  next_word_logits <span class="op">=</span> outputs.logits[:, num_prompts:<span class="op">-</span><span class="dv">1</span>, :]</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>  true_next_tokens <span class="op">=</span> batch[<span class="st">'input_ids'</span>][:, <span class="dv">1</span>:]</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>  loss <span class="op">=</span> F.cross_entropy(next_word_logits.flatten(<span class="dv">0</span>, <span class="dv">1</span>), true_next_tokens.flatten(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>  loss.backward()</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>  opt.step()</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>  opt.zero_grad()</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>  pbar.set_description(<span class="ss">f"Loss </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> loss <span class="op">&lt;</span> atol:</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">break</span></span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> loss.item() <span class="op">&lt;=</span> atol</span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Good job!"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loss 0.17:  15%|█▌        | 30/200 [00:06&lt;00:38,  4.38it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Good job!</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
</div>
<div id="cell-35" class="cell" data-outputid="1e5f0bc9-5e9e-438f-e5b6-41189ba63bde">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">'A quick brown fox'</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>batch <span class="op">=</span> tokenizer(prompt, return_tensors<span class="op">=</span><span class="st">'pt'</span>, return_token_type_ids<span class="op">=</span><span class="va">False</span>).to(device)</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">15</span>):</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>    next_token <span class="op">=</span> model(<span class="op">**</span>batch).logits[<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>].argmax(<span class="op">-</span><span class="dv">1</span>).reshape(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>    batch[<span class="st">'input_ids'</span>] <span class="op">=</span> torch.cat([batch[<span class="st">'input_ids'</span>], next_token], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>    batch[<span class="st">'attention_mask'</span>] <span class="op">=</span> torch.cat([batch[<span class="st">'attention_mask'</span>], torch.ones_like(next_token)], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Output: </span><span class="sc">{</span>tokenizer<span class="sc">.</span>decode(batch[<span class="st">'input_ids'</span>][<span class="dv">0</span>].cpu().numpy().tolist(), skip_special_tokens<span class="op">=</span><span class="va">True</span>)<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Output: A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway</code></pre>
</div>
</div>
</section>
</section>
<section id="parameter-efficient-finetuning-with-lora" class="level2">
<h2 class="anchored" data-anchor-id="parameter-efficient-finetuning-with-lora">Parameter-efficient finetuning with LoRA</h2>
<p>When training on more serious tasks, you can use low-rank adapters based on the LoRA paper.</p>
<p>The core idea is to add low-rank adapters in parallel with existing linear layers, like this:</p>
<p><img src="https://i.imgur.com/6bQLNiG.png" width="300px"></p>
<p>Specifically, the application of adapter looks as follows: <span class="math display">\[
y = W_{0} x + \frac{\alpha}{r} B A x
\]</span> Above: * <span class="math inline">\(W_{0}\)</span> - is the original weight * A, B - are learnable matrices * r - is their rank * <span class="math inline">\(\alpha\)</span> - is the relative weight of the weight update</p>
<p>In the original LoRA paper, the adapters were only added to attention projection matrices.</p>
<p>However, subsequent works show that it is useful to adapt FFNs as well. But before we do any training, we need to implement the basic LoRA layer.</p>
<div id="cell-37" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:221}}" data-outputid="337796cc-a39e-460c-d676-a6f3fbdb72eb">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># re-load the model to remove any previous PEFT tuners</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>    model_name,</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>    device_map<span class="op">=</span><span class="st">'auto'</span>,</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>    low_cpu_mem_usage<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>    offload_state_dict<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>    load_in_4bit<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>    torch_dtype<span class="op">=</span>torch.float32,  <span class="co"># weights are 4-bit; layernorms and activations are fp32</span></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> param <span class="kw">in</span> model.parameters():</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>    param.requires_grad<span class="op">=</span><span class="va">False</span></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>model.gradient_checkpointing_enable()  <span class="co"># only store a small subset of activations, re-compute the rest.</span></span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>model.enable_input_require_grads()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Downloading shards: 100%|██████████| 2/2 [00:00&lt;00:00, 6413.31it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:25&lt;00:00, 12.99s/it]</code></pre>
</div>
</div>
<p><strong>Your task</strong></p>
<p>Implement LoRA adapter.</p>
<div id="cell-39" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LoRALayer(nn.Module):</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Wraps a linear layer with LoRA-like adapter. Wraps an existing OPT linear layer"""</span></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, module: nn.Linear, rank: <span class="bu">int</span>):</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.module <span class="op">=</span> module  <span class="co"># pre-trained (frozen) linear layer</span></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.adapter_A <span class="op">=</span> nn.Parameter(torch.empty(module.in_features, rank, device<span class="op">=</span>module.weight.device))</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>        nn.init.kaiming_uniform_(<span class="va">self</span>.adapter_A, a<span class="op">=</span><span class="dv">5</span> <span class="op">**</span> <span class="fl">0.5</span>)</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.adapter_B <span class="op">=</span> nn.Parameter(torch.zeros(rank, module.out_features, device<span class="op">=</span>module.weight.device))</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs):</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply self.module and LoRA adapter, return the sum (self.module outputs + adapter outputs)</span></span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>        <span class="co">#  &lt;YOUR CODE HERE&gt;</span></span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>        module_outputs <span class="op">=</span> <span class="va">self</span>.module(inputs)</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>        adapter_outputs <span class="op">=</span> inputs <span class="op">@</span> <span class="va">self</span>.adapter_A.unsqueeze(<span class="dv">0</span>) <span class="op">@</span> <span class="va">self</span>.adapter_B.unsqueeze(<span class="dv">0</span>)</span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> module_outputs <span class="op">+</span> adapter_outputs</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Test implementation</p>
<div id="cell-41" class="cell" data-quarto-private-1="{&quot;key&quot;:&quot;colab&quot;,&quot;value&quot;:{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}}" data-outputid="a512d69c-e18f-40fd-fa3d-73868ef2fd60">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>test_linear <span class="op">=</span> nn.Linear(<span class="dv">128</span>, <span class="dv">128</span>)</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>test_linear.weight.data[...] <span class="op">=</span> torch.eye(<span class="dv">128</span>)</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>test_adapter <span class="op">=</span> LoRALayer(test_linear, rank<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> torch.allclose(test_adapter(torch.ones(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">128</span>)), test_linear.bias <span class="op">+</span> <span class="dv">1</span>), <span class="st">"please check your forward pass"</span></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>test_adapter.adapter_A.data[...] <span class="op">=</span> torch.linspace(<span class="fl">0.1</span>, <span class="op">-</span><span class="fl">0.5</span>, <span class="dv">128</span> <span class="op">*</span> <span class="dv">8</span>).view(<span class="dv">128</span>, <span class="dv">8</span>)</span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>test_adapter.adapter_B.data[...] <span class="op">=</span> torch.linspace(<span class="fl">0.5</span>, <span class="op">-</span><span class="fl">0.1</span>, <span class="dv">128</span> <span class="op">*</span> <span class="dv">8</span>).view(<span class="dv">8</span>, <span class="dv">128</span>)</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>test_linear.bias.data[...] <span class="op">=</span> torch.linspace(<span class="fl">1.</span>, <span class="op">-</span><span class="fl">1.</span>, <span class="dv">128</span>)</span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>dummy_loss <span class="op">=</span> F.mse_loss(test_adapter(torch.ones(<span class="dv">1</span>, <span class="dv">128</span>) <span class="op">/</span> <span class="dv">128</span>).squeeze(), torch.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">128</span>))</span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> torch.allclose(dummy_loss, torch.tensor(<span class="fl">1.3711389</span>), rtol<span class="op">=</span><span class="dv">0</span>, atol<span class="op">=</span><span class="fl">1e-4</span>)</span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>dummy_loss.backward()</span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> <span class="bu">all</span>(w.grad <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">for</span> w <span class="kw">in</span> [test_adapter.adapter_A, test_adapter.adapter_B]), <span class="st">"some adapter weights have no grad"</span></span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> torch.allclose(test_adapter.adapter_A.grad.<span class="bu">sum</span>(), torch.tensor(<span class="op">-</span><span class="fl">0.60158</span>), rtol<span class="op">=</span><span class="dv">0</span>, atol<span class="op">=</span><span class="fl">1e-4</span>), <span class="st">"bad grad w.r.t. A"</span></span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> torch.allclose(test_adapter.adapter_B.grad.<span class="bu">sum</span>(), torch.tensor(<span class="fl">0.9931</span>), rtol<span class="op">=</span><span class="dv">0</span>, atol<span class="op">=</span><span class="fl">1e-4</span>), <span class="st">"bad grad w.r.t. B"</span></span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a><span class="co"># note: bad grad means that your code is different from LoRA paper OR that your code is not autograd-friendly (e.g. no_grad)</span></span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a><span class="kw">del</span> dummy_loss, test_linear, test_adapter</span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"All tests passed!"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>All tests passed!</code></pre>
</div>
</div>
<section id="apply-lora-to-the-model" class="level3">
<h3 class="anchored" data-anchor-id="apply-lora-to-the-model">Apply LoRA to the model</h3>
<p>The code below applies LoRA adapters on top of <code>Q/K/V</code> linear layers of Transformer attention.</p>
<p>You may also choose to modify other layers:</p>
<pre><code>self_attn.o_proj - attention output projection
mlp.up_proj, mlp.gate_proj, mlp.down_proj - transformer feedforward layers
lm_head - output LM head</code></pre>
<div id="cell-43" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>lora_rank <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>attention_layer_name <span class="op">=</span> <span class="st">'Attention'</span></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, module <span class="kw">in</span> model.model.layers.named_modules():</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> attention_layer_name <span class="kw">in</span> <span class="bu">repr</span>(<span class="bu">type</span>(module)):</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>        module.q_proj <span class="op">=</span> LoRALayer(module.q_proj, rank<span class="op">=</span>lora_rank).to(device)</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>        module.k_proj <span class="op">=</span> LoRALayer(module.k_proj, rank<span class="op">=</span>lora_rank).to(device)</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>        module.v_proj <span class="op">=</span> LoRALayer(module.v_proj, rank<span class="op">=</span>lora_rank).to(device)</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> <span class="bu">sum</span>(<span class="bu">isinstance</span>(module, LoRALayer) <span class="cf">for</span> module <span class="kw">in</span> model.modules()) <span class="op">==</span> <span class="dv">96</span> <span class="co"># for Mistral-7b</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-44" class="cell" data-outputid="e309d2c4-d0fc-4380-d450-ef1c3bb4db79">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>batch <span class="op">=</span> tokenizer(<span class="st">"This model wants to share its greatest secret:"</span>, return_tensors<span class="op">=</span><span class="st">'pt'</span>, return_token_type_ids<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="co"># test a single training step, make sure we get meaningful gradients</span></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.cuda.amp.autocast(dtype<span class="op">=</span>torch.float32):</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> model.forward(<span class="op">**</span>batch)</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>    (out.logits.norm() <span class="op">/</span> <span class="dv">100</span>).backward()</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, module <span class="kw">in</span> <span class="bu">enumerate</span>(model.modules()):</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(module, LoRALayer):</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> module.adapter_B.grad <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span></span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> module.adapter_B.grad.norm().item() <span class="op">&gt;</span> <span class="dv">0</span></span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a>model.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Grad check successful, well done!"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Grad check successful, well done!</code></pre>
</div>
</div>
<p>Let us finetune the model on some custom dataset</p>
<div id="cell-46" class="cell" data-outputid="261c97cd-2908-4660-81a7-c652b3548f4e">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> load_dataset(<span class="st">"Abirate/english_quotes"</span>)</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> data.<span class="bu">map</span>(<span class="kw">lambda</span> samples: tokenizer(samples[<span class="st">'quote'</span>]), batched<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>model._hf_peft_config_loaded <span class="op">=</span> <span class="va">True</span>  <span class="co"># silence a warning from HF trainer</span></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> transformers.Trainer(</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model,</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">=</span>data[<span class="st">'train'</span>],</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>transformers.TrainingArguments(</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>        per_device_train_batch_size<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>        gradient_accumulation_steps<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a>        warmup_steps<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a>        max_steps<span class="op">=</span><span class="dv">200</span>,</span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a>        learning_rate<span class="op">=</span><span class="fl">2e-4</span>,</span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a>        fp16<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a>        logging_steps<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a>        output_dir<span class="op">=</span><span class="st">'outputs'</span></span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true" tabindex="-1"></a>    data_collator<span class="op">=</span>transformers.DataCollatorForLanguageModeling(tokenizer, mlm<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb50-20"><a href="#cb50-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Silence the warnings. Please re-enable for inference!</span></span>
<span id="cb50-21"><a href="#cb50-21" aria-hidden="true" tabindex="-1"></a>model.config.use_cache <span class="op">=</span> <span class="va">False</span></span>
<span id="cb50-22"><a href="#cb50-22" aria-hidden="true" tabindex="-1"></a>trainer.train()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<div class="ansi-escaped-output">
<pre>Detected kernel version 5.4.161, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.

Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.

<span class="ansi-blue-fg ansi-bold">wandb</span>: Currently logged in as: <span class="ansi-yellow-fg">spiridon_sun_rotator</span> (<span class="ansi-yellow-fg">ist</span>). Use <span class="ansi-bold">`wandb login --relogin`</span> to force relogin
</pre>
</div>
</div>
<div class="cell-output cell-output-display">
wandb version 0.16.6 is available!  To upgrade, please run:
 $ pip install wandb --upgrade
</div>
<div class="cell-output cell-output-display">
Tracking run with wandb version 0.16.5
</div>
<div class="cell-output cell-output-display">
Run data is saved locally in <code>/home/dkuznedelev/TestStuff/wandb/run-20240420_145457-ofe14mto</code>
</div>
<div class="cell-output cell-output-display">
Syncing run <strong><a href="https://wandb.ai/ist/huggingface/runs/ofe14mto/workspace" target="_blank">fiery-monkey-363</a></strong> to <a href="https://wandb.ai/ist/huggingface" target="_blank">Weights &amp; Biases</a> (<a href="https://wandb.me/run" target="_blank">docs</a>)<br>
</div>
<div class="cell-output cell-output-display">
 View project at <a href="https://wandb.ai/ist/huggingface" target="_blank">https://wandb.ai/ist/huggingface</a>
</div>
<div class="cell-output cell-output-display">
 View run at <a href="https://wandb.ai/ist/huggingface/runs/ofe14mto/workspace" target="_blank">https://wandb.ai/ist/huggingface/runs/ofe14mto/workspace</a>
</div>
<div class="cell-output cell-output-display">
<div>
      
      <progress value="200" max="200" style="width:300px; height:20px; vertical-align: middle;"></progress>
      [200/200 06:40, Epoch 1/2]
    </div>
    
<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">Step</th>
<th data-quarto-table-cell-role="th">Training Loss</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1.645500</td>
</tr>
<tr class="even">
<td>2</td>
<td>1.144600</td>
</tr>
<tr class="odd">
<td>3</td>
<td>1.568900</td>
</tr>
<tr class="even">
<td>4</td>
<td>1.325900</td>
</tr>
<tr class="odd">
<td>5</td>
<td>1.454300</td>
</tr>
<tr class="even">
<td>6</td>
<td>1.517300</td>
</tr>
<tr class="odd">
<td>7</td>
<td>1.326500</td>
</tr>
<tr class="even">
<td>8</td>
<td>1.528400</td>
</tr>
<tr class="odd">
<td>9</td>
<td>1.748900</td>
</tr>
<tr class="even">
<td>10</td>
<td>1.943000</td>
</tr>
<tr class="odd">
<td>11</td>
<td>1.181900</td>
</tr>
<tr class="even">
<td>12</td>
<td>1.154000</td>
</tr>
<tr class="odd">
<td>13</td>
<td>1.011400</td>
</tr>
<tr class="even">
<td>14</td>
<td>2.002500</td>
</tr>
<tr class="odd">
<td>15</td>
<td>1.485600</td>
</tr>
<tr class="even">
<td>16</td>
<td>1.445700</td>
</tr>
<tr class="odd">
<td>17</td>
<td>1.219000</td>
</tr>
<tr class="even">
<td>18</td>
<td>0.857100</td>
</tr>
<tr class="odd">
<td>19</td>
<td>1.460900</td>
</tr>
<tr class="even">
<td>20</td>
<td>1.308300</td>
</tr>
<tr class="odd">
<td>21</td>
<td>1.599100</td>
</tr>
<tr class="even">
<td>22</td>
<td>1.280100</td>
</tr>
<tr class="odd">
<td>23</td>
<td>2.156600</td>
</tr>
<tr class="even">
<td>24</td>
<td>1.734400</td>
</tr>
<tr class="odd">
<td>25</td>
<td>1.045600</td>
</tr>
<tr class="even">
<td>26</td>
<td>1.711600</td>
</tr>
<tr class="odd">
<td>27</td>
<td>1.587400</td>
</tr>
<tr class="even">
<td>28</td>
<td>1.008600</td>
</tr>
<tr class="odd">
<td>29</td>
<td>1.314400</td>
</tr>
<tr class="even">
<td>30</td>
<td>1.325600</td>
</tr>
<tr class="odd">
<td>31</td>
<td>1.420500</td>
</tr>
<tr class="even">
<td>32</td>
<td>1.416400</td>
</tr>
<tr class="odd">
<td>33</td>
<td>1.168900</td>
</tr>
<tr class="even">
<td>34</td>
<td>1.681500</td>
</tr>
<tr class="odd">
<td>35</td>
<td>0.751400</td>
</tr>
<tr class="even">
<td>36</td>
<td>1.230400</td>
</tr>
<tr class="odd">
<td>37</td>
<td>1.142900</td>
</tr>
<tr class="even">
<td>38</td>
<td>1.534200</td>
</tr>
<tr class="odd">
<td>39</td>
<td>1.144900</td>
</tr>
<tr class="even">
<td>40</td>
<td>1.400100</td>
</tr>
<tr class="odd">
<td>41</td>
<td>1.642500</td>
</tr>
<tr class="even">
<td>42</td>
<td>1.232600</td>
</tr>
<tr class="odd">
<td>43</td>
<td>1.183000</td>
</tr>
<tr class="even">
<td>44</td>
<td>1.195500</td>
</tr>
<tr class="odd">
<td>45</td>
<td>0.838600</td>
</tr>
<tr class="even">
<td>46</td>
<td>1.061100</td>
</tr>
<tr class="odd">
<td>47</td>
<td>1.040700</td>
</tr>
<tr class="even">
<td>48</td>
<td>1.292800</td>
</tr>
<tr class="odd">
<td>49</td>
<td>1.018700</td>
</tr>
<tr class="even">
<td>50</td>
<td>1.109300</td>
</tr>
<tr class="odd">
<td>51</td>
<td>1.517000</td>
</tr>
<tr class="even">
<td>52</td>
<td>1.007300</td>
</tr>
<tr class="odd">
<td>53</td>
<td>1.019400</td>
</tr>
<tr class="even">
<td>54</td>
<td>1.003200</td>
</tr>
<tr class="odd">
<td>55</td>
<td>1.212900</td>
</tr>
<tr class="even">
<td>56</td>
<td>0.932100</td>
</tr>
<tr class="odd">
<td>57</td>
<td>1.888600</td>
</tr>
<tr class="even">
<td>58</td>
<td>1.015500</td>
</tr>
<tr class="odd">
<td>59</td>
<td>1.218300</td>
</tr>
<tr class="even">
<td>60</td>
<td>1.432200</td>
</tr>
<tr class="odd">
<td>61</td>
<td>1.546300</td>
</tr>
<tr class="even">
<td>62</td>
<td>1.210100</td>
</tr>
<tr class="odd">
<td>63</td>
<td>1.345500</td>
</tr>
<tr class="even">
<td>64</td>
<td>1.323900</td>
</tr>
<tr class="odd">
<td>65</td>
<td>1.228500</td>
</tr>
<tr class="even">
<td>66</td>
<td>1.338300</td>
</tr>
<tr class="odd">
<td>67</td>
<td>1.189500</td>
</tr>
<tr class="even">
<td>68</td>
<td>1.201700</td>
</tr>
<tr class="odd">
<td>69</td>
<td>1.581700</td>
</tr>
<tr class="even">
<td>70</td>
<td>1.210800</td>
</tr>
<tr class="odd">
<td>71</td>
<td>0.878800</td>
</tr>
<tr class="even">
<td>72</td>
<td>1.939500</td>
</tr>
<tr class="odd">
<td>73</td>
<td>1.443000</td>
</tr>
<tr class="even">
<td>74</td>
<td>1.632700</td>
</tr>
<tr class="odd">
<td>75</td>
<td>1.409200</td>
</tr>
<tr class="even">
<td>76</td>
<td>1.494800</td>
</tr>
<tr class="odd">
<td>77</td>
<td>0.958200</td>
</tr>
<tr class="even">
<td>78</td>
<td>1.408700</td>
</tr>
<tr class="odd">
<td>79</td>
<td>1.303300</td>
</tr>
<tr class="even">
<td>80</td>
<td>1.333900</td>
</tr>
<tr class="odd">
<td>81</td>
<td>1.254000</td>
</tr>
<tr class="even">
<td>82</td>
<td>1.233000</td>
</tr>
<tr class="odd">
<td>83</td>
<td>1.209700</td>
</tr>
<tr class="even">
<td>84</td>
<td>1.356200</td>
</tr>
<tr class="odd">
<td>85</td>
<td>1.588900</td>
</tr>
<tr class="even">
<td>86</td>
<td>1.362600</td>
</tr>
<tr class="odd">
<td>87</td>
<td>1.530000</td>
</tr>
<tr class="even">
<td>88</td>
<td>1.184700</td>
</tr>
<tr class="odd">
<td>89</td>
<td>1.421600</td>
</tr>
<tr class="even">
<td>90</td>
<td>0.811200</td>
</tr>
<tr class="odd">
<td>91</td>
<td>1.541900</td>
</tr>
<tr class="even">
<td>92</td>
<td>0.990900</td>
</tr>
<tr class="odd">
<td>93</td>
<td>1.591600</td>
</tr>
<tr class="even">
<td>94</td>
<td>1.610900</td>
</tr>
<tr class="odd">
<td>95</td>
<td>1.387000</td>
</tr>
<tr class="even">
<td>96</td>
<td>1.570800</td>
</tr>
<tr class="odd">
<td>97</td>
<td>1.172800</td>
</tr>
<tr class="even">
<td>98</td>
<td>1.243300</td>
</tr>
<tr class="odd">
<td>99</td>
<td>1.256600</td>
</tr>
<tr class="even">
<td>100</td>
<td>1.069300</td>
</tr>
<tr class="odd">
<td>101</td>
<td>1.100700</td>
</tr>
<tr class="even">
<td>102</td>
<td>1.149700</td>
</tr>
<tr class="odd">
<td>103</td>
<td>1.367400</td>
</tr>
<tr class="even">
<td>104</td>
<td>1.206000</td>
</tr>
<tr class="odd">
<td>105</td>
<td>0.897100</td>
</tr>
<tr class="even">
<td>106</td>
<td>1.335100</td>
</tr>
<tr class="odd">
<td>107</td>
<td>1.414700</td>
</tr>
<tr class="even">
<td>108</td>
<td>1.371800</td>
</tr>
<tr class="odd">
<td>109</td>
<td>1.348400</td>
</tr>
<tr class="even">
<td>110</td>
<td>1.309900</td>
</tr>
<tr class="odd">
<td>111</td>
<td>1.457400</td>
</tr>
<tr class="even">
<td>112</td>
<td>1.102300</td>
</tr>
<tr class="odd">
<td>113</td>
<td>1.187800</td>
</tr>
<tr class="even">
<td>114</td>
<td>1.721400</td>
</tr>
<tr class="odd">
<td>115</td>
<td>1.201500</td>
</tr>
<tr class="even">
<td>116</td>
<td>1.538200</td>
</tr>
<tr class="odd">
<td>117</td>
<td>1.146500</td>
</tr>
<tr class="even">
<td>118</td>
<td>1.159400</td>
</tr>
<tr class="odd">
<td>119</td>
<td>1.149700</td>
</tr>
<tr class="even">
<td>120</td>
<td>1.370800</td>
</tr>
<tr class="odd">
<td>121</td>
<td>1.472100</td>
</tr>
<tr class="even">
<td>122</td>
<td>1.641800</td>
</tr>
<tr class="odd">
<td>123</td>
<td>1.538600</td>
</tr>
<tr class="even">
<td>124</td>
<td>1.115500</td>
</tr>
<tr class="odd">
<td>125</td>
<td>1.262900</td>
</tr>
<tr class="even">
<td>126</td>
<td>1.571300</td>
</tr>
<tr class="odd">
<td>127</td>
<td>1.078500</td>
</tr>
<tr class="even">
<td>128</td>
<td>1.308100</td>
</tr>
<tr class="odd">
<td>129</td>
<td>1.794200</td>
</tr>
<tr class="even">
<td>130</td>
<td>1.242800</td>
</tr>
<tr class="odd">
<td>131</td>
<td>1.391200</td>
</tr>
<tr class="even">
<td>132</td>
<td>1.118600</td>
</tr>
<tr class="odd">
<td>133</td>
<td>1.622200</td>
</tr>
<tr class="even">
<td>134</td>
<td>1.262600</td>
</tr>
<tr class="odd">
<td>135</td>
<td>1.144300</td>
</tr>
<tr class="even">
<td>136</td>
<td>1.469000</td>
</tr>
<tr class="odd">
<td>137</td>
<td>1.485600</td>
</tr>
<tr class="even">
<td>138</td>
<td>1.506100</td>
</tr>
<tr class="odd">
<td>139</td>
<td>1.469300</td>
</tr>
<tr class="even">
<td>140</td>
<td>1.339100</td>
</tr>
<tr class="odd">
<td>141</td>
<td>1.356000</td>
</tr>
<tr class="even">
<td>142</td>
<td>1.611500</td>
</tr>
<tr class="odd">
<td>143</td>
<td>1.608200</td>
</tr>
<tr class="even">
<td>144</td>
<td>1.247000</td>
</tr>
<tr class="odd">
<td>145</td>
<td>1.321400</td>
</tr>
<tr class="even">
<td>146</td>
<td>1.235200</td>
</tr>
<tr class="odd">
<td>147</td>
<td>1.497200</td>
</tr>
<tr class="even">
<td>148</td>
<td>1.588800</td>
</tr>
<tr class="odd">
<td>149</td>
<td>0.995400</td>
</tr>
<tr class="even">
<td>150</td>
<td>1.087800</td>
</tr>
<tr class="odd">
<td>151</td>
<td>1.262000</td>
</tr>
<tr class="even">
<td>152</td>
<td>0.789100</td>
</tr>
<tr class="odd">
<td>153</td>
<td>1.301900</td>
</tr>
<tr class="even">
<td>154</td>
<td>1.645700</td>
</tr>
<tr class="odd">
<td>155</td>
<td>0.801000</td>
</tr>
<tr class="even">
<td>156</td>
<td>1.637800</td>
</tr>
<tr class="odd">
<td>157</td>
<td>1.023600</td>
</tr>
<tr class="even">
<td>158</td>
<td>1.232900</td>
</tr>
<tr class="odd">
<td>159</td>
<td>1.115200</td>
</tr>
<tr class="even">
<td>160</td>
<td>1.139100</td>
</tr>
<tr class="odd">
<td>161</td>
<td>0.983800</td>
</tr>
<tr class="even">
<td>162</td>
<td>0.947200</td>
</tr>
<tr class="odd">
<td>163</td>
<td>1.043300</td>
</tr>
<tr class="even">
<td>164</td>
<td>0.727700</td>
</tr>
<tr class="odd">
<td>165</td>
<td>1.044900</td>
</tr>
<tr class="even">
<td>166</td>
<td>0.865200</td>
</tr>
<tr class="odd">
<td>167</td>
<td>1.069000</td>
</tr>
<tr class="even">
<td>168</td>
<td>1.226500</td>
</tr>
<tr class="odd">
<td>169</td>
<td>1.231700</td>
</tr>
<tr class="even">
<td>170</td>
<td>0.771800</td>
</tr>
<tr class="odd">
<td>171</td>
<td>1.027600</td>
</tr>
<tr class="even">
<td>172</td>
<td>1.291300</td>
</tr>
<tr class="odd">
<td>173</td>
<td>0.824400</td>
</tr>
<tr class="even">
<td>174</td>
<td>0.949600</td>
</tr>
<tr class="odd">
<td>175</td>
<td>0.758100</td>
</tr>
<tr class="even">
<td>176</td>
<td>1.237400</td>
</tr>
<tr class="odd">
<td>177</td>
<td>0.999600</td>
</tr>
<tr class="even">
<td>178</td>
<td>1.272400</td>
</tr>
<tr class="odd">
<td>179</td>
<td>0.806900</td>
</tr>
<tr class="even">
<td>180</td>
<td>0.747600</td>
</tr>
<tr class="odd">
<td>181</td>
<td>0.786800</td>
</tr>
<tr class="even">
<td>182</td>
<td>0.774600</td>
</tr>
<tr class="odd">
<td>183</td>
<td>0.893300</td>
</tr>
<tr class="even">
<td>184</td>
<td>0.823300</td>
</tr>
<tr class="odd">
<td>185</td>
<td>0.833500</td>
</tr>
<tr class="even">
<td>186</td>
<td>1.235300</td>
</tr>
<tr class="odd">
<td>187</td>
<td>1.432900</td>
</tr>
<tr class="even">
<td>188</td>
<td>1.026600</td>
</tr>
<tr class="odd">
<td>189</td>
<td>0.864100</td>
</tr>
<tr class="even">
<td>190</td>
<td>0.964800</td>
</tr>
<tr class="odd">
<td>191</td>
<td>0.779100</td>
</tr>
<tr class="even">
<td>192</td>
<td>1.434800</td>
</tr>
<tr class="odd">
<td>193</td>
<td>1.208800</td>
</tr>
<tr class="even">
<td>194</td>
<td>0.724900</td>
</tr>
<tr class="odd">
<td>195</td>
<td>1.303000</td>
</tr>
<tr class="even">
<td>196</td>
<td>0.866500</td>
</tr>
<tr class="odd">
<td>197</td>
<td>0.940800</td>
</tr>
<tr class="even">
<td>198</td>
<td>1.549100</td>
</tr>
<tr class="odd">
<td>199</td>
<td>0.537700</td>
</tr>
<tr class="even">
<td>200</td>
<td>1.167400</td>
</tr>
</tbody>
</table>
<p>
</p></div>
<div class="cell-output cell-output-display" data-execution_count="66">
<pre><code>TrainOutput(global_step=200, training_loss=1.2602548521757126, metrics={'train_runtime': 413.8954, 'train_samples_per_second': 7.731, 'train_steps_per_second': 0.483, 'total_flos': 1.3073398340124672e+16, 'train_loss': 1.2602548521757126, 'epoch': 1.28})</code></pre>
</div>
</div>
</section>
<section id="inference-finetuned-model" class="level3">
<h3 class="anchored" data-anchor-id="inference-finetuned-model">Inference finetuned model</h3>
<div id="cell-48" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>model.config.use_cache <span class="op">=</span> <span class="va">True</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-49" class="cell" data-outputid="b4709e20-a6e1-44cf-b908-5104bccf48f4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>batch <span class="op">=</span> tokenizer(<span class="st">"Two things are infinite: "</span>, return_tensors<span class="op">=</span><span class="st">'pt'</span>)</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.cuda.amp.autocast():</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>  output_tokens <span class="op">=</span> model.generate(<span class="op">**</span>batch, max_new_tokens<span class="op">=</span><span class="dv">24</span>)</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n\n</span><span class="sc">{</span>tokenizer<span class="sc">.</span>decode(output_tokens[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>

Two things are infinite:  the universe and human stupidity; and I'm not sure about the universe.

- Albert Einstein

</code></pre>
</div>
</div>
<p>PEFT library provides implemenation of LoRA as well.</p>
<div id="cell-51" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> peft <span class="im">import</span> LoraConfig, get_peft_model</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-52" class="cell" data-outputid="3b43d9b0-50b1-461f-a157-e70c8812bbda">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co"># re-load the model to remove any previous PEFT tuners</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>    model_name,</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>    device_map<span class="op">=</span><span class="st">'auto'</span>,</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>    low_cpu_mem_usage<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>    offload_state_dict<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>    load_in_4bit<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>    torch_dtype<span class="op">=</span>torch.float32,  <span class="co"># weights are 4-bit; layernorms and activations are fp32</span></span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> param <span class="kw">in</span> model.parameters():</span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a>    param.requires_grad<span class="op">=</span><span class="va">False</span></span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a>model.gradient_checkpointing_enable()  <span class="co"># only store a small subset of activations, re-compute the rest.</span></span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a>model.enable_input_require_grads()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Downloading shards: 100%|██████████| 2/2 [00:00&lt;00:00, 2402.24it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:25&lt;00:00, 12.70s/it]</code></pre>
</div>
</div>
<div id="cell-53" class="cell" data-outputid="12814095-f074-4d2d-e03c-9ad0efd4e977">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> LoraConfig(</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>    r<span class="op">=</span><span class="dv">16</span>, <span class="co"># rank of the LoRA adapter</span></span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>    lora_alpha<span class="op">=</span><span class="dv">16</span>, <span class="co"># weight of LoRA adapter</span></span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>    target_modules<span class="op">=</span>[<span class="st">"q_proj"</span>, <span class="st">"k_proj"</span>], <span class="co"># layers to apply LoRA</span></span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>    lora_dropout<span class="op">=</span><span class="fl">0.1</span>, <span class="co"># dropout in LoRA layers</span></span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>    bias<span class="op">=</span><span class="st">"none"</span>, <span class="co"># whether to add bias</span></span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>    task_type<span class="op">=</span><span class="st">"CAUSAL_LM"</span></span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> get_peft_model(model, config)</span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a>model.print_trainable_parameters()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>trainable params: 6,815,744 || all params: 7,248,547,840 || trainable%: 0.0940290959023318</code></pre>
</div>
</div>
<p>You can train model exactly in the same way as before.</p>
<p><strong>Note</strong> PEFT allows you to save the adapter weights alone and push to hub.</p>
<div id="cell-55" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>model.push_to_hub(<span class="st">"username/adapter_name"</span>, use_auth_token<span class="op">=</span><span class="va">True</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Note, for floating models LoRA adapter can be <strong>seamlessly</strong> merged into model, thus incuring <strong>zero</strong> overhead on inference.</p>
<p>However, for quantized models things are more subtle and one has to process them in parallel with the main weight or apply some additional hack to merge them into the model.</p>
</section>
</section>
<section id="materials-for-further-study" class="level2">
<h2 class="anchored" data-anchor-id="materials-for-further-study">Materials for further study</h2>
<ul>
<li><a href="https://huggingface.co/docs/peft/v0.10.0/en/index">PEFT documentation</a></li>
<li><a href="https://arxiv.org/abs/2106.09685">LoRA paper</a></li>
<li><a href="https://arxiv.org/abs/2104.08691">Prompt tuning paper</a></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>